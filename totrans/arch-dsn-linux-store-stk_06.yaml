- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Understanding I/O Handling and Scheduling in the Block Layer
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解块层中的I/O处理和调度
- en: “The key is not to prioritize what’s on your schedule, but to schedule your
    priorities.” – Stephen Covey
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “关键不是优先安排你的日程，而是安排你的优先事项。” – 史蒂芬·柯维
- en: '[*Chapter 4*](B19430_04.xhtml#_idTextAnchor072) and [*Chapter 5*](B19430_05.xhtml#_idTextAnchor090)
    of this book focused on the role of the block layer in the kernel. We were able
    to see what constitutes a block device, the major data structures in the block
    layer, the multi-queue block I/O framework, and the device mapper. This chapter
    will focus on another important function of the block layer – scheduling.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第4章*](B19430_04.xhtml#_idTextAnchor072) 和 [*第5章*](B19430_05.xhtml#_idTextAnchor090)
    聚焦于内核中块层的作用。我们能够看到构成块设备的内容、块层中的主要数据结构、多队列块I/O框架和设备映射器。本章将重点介绍块层的另一个重要功能——调度。'
- en: Scheduling is an extremely critical component of any system, as the decisions
    taken by a scheduler can have a major say in dictating the overall system performance.
    The I/O scheduling in the block layer is no exception to this rule. The I/O scheduler
    holds significant importance in deciding the manner and timing of delivery for
    an I/O request to the lower layers. Given this, it becomes crucial to carefully
    analyze the I/O patterns of an application, as certain requests require prioritization
    over others.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 调度是任何系统中至关重要的组件，因为调度器做出的决策对整体系统性能的影响至关重大。块层中的I/O调度也不例外。I/O调度器在决定I/O请求传递给下层的方式和时机方面具有重要作用。因此，仔细分析应用程序的I/O模式变得至关重要，因为某些请求需要优先处理。
- en: This chapter will introduce us to the different I/O schedulers available in
    the block layer and their modus operandi. Each scheduler uses a different set
    of techniques to dispatch I/O requests to the lower layers. As we have mentioned
    repeatedly, when working with block devices, performance is a key concern. The
    block layer has gone through several enhancements so that maximum performance
    can be extracted from disk drives. This includes the development of schedulers
    to handle modern and high-performing storage devices.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍块层中可用的不同I/O调度器及其工作方式。每个调度器使用不同的技术集将I/O请求分发到下层。正如我们反复提到的，在处理块设备时，性能是一个关键问题。块层经历了几次改进，以便从磁盘驱动器中提取最大性能。这包括开发调度器来处理现代和高性能存储设备。
- en: We’ll start by introducing the common techniques used by the different schedulers
    to handle I/O requests more efficiently. Although these techniques were developed
    for traditional spinning drives, they are still considered useful for modern flash
    drives. The primary goal of these techniques was to reduce disk-seeking operations
    for mechanical drives, as these have an adverse effect on their performance. Most
    schedulers make use of these methods by default, regardless of the underlying
    storage hardware.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍不同调度器用来更高效处理I/O请求的常见技术。尽管这些技术是为传统的旋转硬盘开发的，但它们对于现代闪存驱动器仍然被认为是有用的。这些技术的主要目标是减少机械硬盘的磁盘寻道操作，因为这些操作会对性能产生不利影响。大多数调度器默认使用这些方法，无论底层存储硬件是什么。
- en: The major topic of discussion in this chapter will be the different I/O scheduling
    flavors available in the kernel. The older disk schedulers were developed for
    devices accessed using the single-queue mechanism and have become outdated, as
    they cannot scale up to meet the performance of modern drives. In the last few
    years, four multi-queue I/O schedulers have been integrated into the kernel. These
    schedulers are able to map I/O requests to multiple queues.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的主要话题将是内核中可用的不同I/O调度类型。早期的磁盘调度器是为使用单队列机制访问的设备开发的，它们已经过时，因为无法扩展以满足现代硬盘的性能需求。在过去几年中，四个多队列I/O调度器已被集成到内核中。这些调度器能够将I/O请求映射到多个队列。
- en: 'In this chapter, we’re going to discuss the following main topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主要内容：
- en: Understanding the I/O handling techniques in the block layer
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解块层中的I/O处理技术
- en: 'Explaining the I/O schedulers in Linux:'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释Linux中的I/O调度器：
- en: The MQ-deadline scheduler – guaranteeing a start service time
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: MQ-deadline调度器 – 保证开始服务时间
- en: Budget fair queuing – providing proportional disk share
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预算公平排队 – 提供按比例分配的磁盘共享
- en: Kyber – prioritizing throughput
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kyber – 优先考虑吞吐量
- en: None – minimal scheduling overhead
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: None – 最小化调度开销
- en: Discussing the scheduling conundrum
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论调度难题
- en: Technical requirements
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: It would be helpful to have some knowledge about disk I/O basics to understand
    the concepts presented in this chapter. Having an idea about the different types
    of storage media, and concepts such as disk seek time and rotational latency,
    will help to comprehend the material presented in this chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解本章中提出的概念，了解一些磁盘 I/O 基础知识会非常有帮助。了解不同类型的存储介质，以及磁盘寻址时间和旋转延迟等概念，有助于理解本章中介绍的内容。
- en: The commands and examples presented in this chapter are distribution-agnostic
    and can be run on any Linux operating system, such as Debian, Ubuntu, Red Hat,
    and Fedora. There are quite a few references to the kernel source code. If you
    want to download the kernel source, you can download it from [https://www.kernel.org](https://www.kernel.org).
    The code segments referred to in this chapter and book are from the `5.19.9` kernel.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中提供的命令和示例是与发行版无关的，可以在任何 Linux 操作系统上运行，例如 Debian、Ubuntu、Red Hat 和 Fedora。本书中有不少涉及内核源代码的内容。如果你想下载内核源代码，可以从[https://www.kernel.org](https://www.kernel.org)下载。本章和本书中提到的代码段来自`5.19.9`内核。
- en: Understanding the I/O handling techniques in block layer
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解块层中的 I/O 处理技术
- en: While exploring the block layer in [*Chapter 4*](B19430_04.xhtml#_idTextAnchor072)
    and [*Chapter 5*](B19430_05.xhtml#_idTextAnchor090), we often mentioned the performance
    sensitivity of block devices and how the block layer has to make informed and
    intelligent decisions to extract their maximum potential. So far, we haven’t really
    discussed any of the techniques that help to enhance the performance of block
    devices.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索[**第 4 章**](B19430_04.xhtml#_idTextAnchor072)和[**第 5 章**](B19430_05.xhtml#_idTextAnchor090)的块层时，我们经常提到块设备的性能敏感性，以及块层如何做出有根据且智能的决策，以发挥其最大潜力。到目前为止，我们还没有真正讨论任何能够帮助提升块设备性能的技术。
- en: Going back to the era of spinning drives, the performance of storage drives
    was a major bottleneck in the I/O stack. Mechanical drives offered decent performance
    when doing sequential I/O operations. However, for random workloads, their performance
    deteriorates quite drastically. This is understandable, as mechanical drives have
    to *seek* requested locations on disk by spinning and positioning the read-write
    head on specific locations. The greater the number of random seeks, the greater
    the performance penalty. Filesystems created on top of block devices try to implement
    some practices that attempt to optimize disk performance, but it is impossible
    to avoid random operations altogether.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 回到旋转硬盘的时代，存储驱动器的性能是 I/O 栈中的一个主要瓶颈。机械硬盘在执行顺序 I/O 操作时表现得相当不错。然而，对于随机工作负载，它们的性能会急剧下降。这是可以理解的，因为机械硬盘必须通过旋转并将读写头定位到特定位置来*寻址*请求的磁盘位置。随机寻址的次数越多，性能惩罚就越大。在块设备上创建的文件系统尝试实施一些优化磁盘性能的做法，但完全避免随机操作几乎是不可能的。
- en: Given the enormous seek times of mechanical drives, it is imperative that some
    sort of optimization is applied to reduce seeking, before I/O requests are handed
    over to the underlying storage. Simply handing them down to the underlying physical
    storage seems a primitive approach. This is where I/O schedulers come to the fore.
    The I/O schedulers in the block layer employ some common methods to ensure that
    the overhead caused by random access operations is minimized. These techniques
    address some of the performance issues of spinning drives, although they might
    not have a considerable effect when using flash drives, as they are not impacted
    by random operations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于机械硬盘的巨大寻址时间，在将 I/O 请求交给底层存储之前，必须应用某种优化来减少寻址。简单地将请求交给底层物理存储显得有些原始。这时，I/O 调度程序就显得尤为重要。块层中的
    I/O 调度程序采用一些常见方法，以确保尽量减少随机访问操作所带来的开销。这些技术解决了旋转硬盘的一些性能问题，尽管在使用闪存硬盘时，它们的效果可能不明显，因为闪存硬盘不受随机操作的影响。
- en: 'Most schedulers employ a combination of the following techniques to optimize
    disk performance:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数调度程序结合以下几种技术来优化磁盘性能：
- en: Sorting
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排序
- en: Merging
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合并
- en: Coalescing
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合并
- en: Plugging
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插件
- en: Let’s discuss these in a bit more detail.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论这些。
- en: Sorting
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排序
- en: 'Let’s say that four I/O requests, A, B, C, and D, are received for sectors
    2, 3, 1, and 4, respectively, in that particular order, as illustrated in *Figure
    6**.1*. If the requests are delivered to the underlying spinning drive in this
    manner, they will be completed in that order:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有四个I/O请求A、B、C和D，分别对应扇区2、3、1和4，按照该顺序接收，如*图6.1*所示。如果请求按此顺序传递给底层旋转硬盘，它们将按照以下顺序完成：
- en: '![Figure 6.1 – Disk seeking](img/B19430_06_01.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 磁盘寻址](img/B19430_06_01.jpg)'
- en: Figure 6.1 – Disk seeking
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 磁盘寻址
- en: This means that after completing requests A and B in a sequential manner, for
    request C, the read-write head of the drive will have to go back to sector 1\.
    After completing request C, it will have to perform another *seek* and move forward
    to sector D. It’s not difficult to see the inefficiency that results from such
    an approach. If the requests are simply handed over in the received order, the
    disk performance will suffer.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，在按顺序完成请求A和B之后，对于请求C，磁头将必须回到扇区1。完成请求C后，它还需执行另一个*寻址*操作并前往扇区D。从这种方法所带来的低效是显而易见的。如果请求按接收的顺序直接交给磁盘，磁盘性能将大打折扣。
- en: For spinning drives, random access operations kill performance, as the disk
    has to perform multiple *seek* operations. If incoming requests are simply inserted
    at the end of a first-in-first-out queue, each request in the queue will involve
    separate processing, and the overhead caused by random seeking will increase.
    Therefore, most schedulers keep the request queues ordered and try to insert new
    incoming requests in a sorted manner. The request queue is sorted sector by sector.
    This ensures that requests operating on neighboring sectors can be performed sequentially.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于旋转硬盘，随机访问操作会大大降低性能，因为磁盘必须执行多个*寻址*操作。如果传入请求仅仅按照先进先出的队列顺序插入，队列中的每个请求都需要单独处理，随机寻址所带来的开销将增加。因此，大多数调度器保持请求队列有序，并尽量以排序的方式插入新传入的请求。请求队列按扇区逐个排序。这确保了对相邻扇区的请求能够顺序执行。
- en: Merging
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并
- en: 'Merging acts as a compliment to the sorting mechanism and further tries to
    reduce random access. It can be performed in two ways, frontward and backward.
    Two requests can be merged if they are intended for contiguous sectors. If an
    I/O request enters the scheduler and it adjoins to an already enqueued request,
    then it qualifies as a front or back merge candidate. If the incoming request
    is merged with an existing request, it is called a back merge. The concept of
    back merging is shown in *Figure 6**.2*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 合并作为排序机制的补充，进一步减少了随机访问。合并可以通过两种方式进行：前向合并和回合并。如果两个请求针对的是连续的扇区，它们就可以合并。如果一个I/O请求进入调度器并与已入队的请求相邻，它就符合前向或回合并的条件。如果传入请求与现有请求合并，则称为回合并。回合并的概念如*图6.2*所示：
- en: '![Figure 6.2 – A back merge](img/B19430_06_02.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 回合并](img/B19430_06_02.jpg)'
- en: Figure 6.2 – A back merge
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 回合并
- en: 'In the same vein, when a newly generated request is combined with an existing
    request, it is referred to as a front merge, as shown in *Figure 6**.3*:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 同理，当新生成的请求与现有请求合并时，称之为前向合并，如*图6.3*所示：
- en: '![Figure 6.3 – A front merge](img/B19430_06_03.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 前向合并](img/B19430_06_03.jpg)'
- en: Figure 6.3 – A front merge
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 前向合并
- en: The idea is simple – avoid continuous trips to random locations. This is most
    effective for spinning mechanical drives. By default, most block layer schedulers
    attempt to merge an incoming request with an existing one.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个思路很简单——避免不断地访问随机位置。这对旋转机械硬盘尤其有效。默认情况下，大多数块层调度器会尝试将传入请求与现有请求合并。
- en: Coalescing
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并
- en: 'The coalescing operation includes both front and back merges. Coalescing happens
    when a new I/O request closes the gap between two existing requests, as shown
    in the following figure:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 合并操作包括前向合并和回合并。合并发生在一个新的I/O请求填补了两个现有请求之间的空隙，如下图所示：
- en: '![Figure 6.4 – Coalescing](img/B19430_06_04.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 合并](img/B19430_06_04.jpg)'
- en: Figure 6.4 – Coalescing
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 合并
- en: Coalescing is employed to reduce the overhead associated with small and frequent
    I/O operations, particularly for spinning hard disk drives. By coalescing multiple
    requests, the disk can perform sequential reads and writes, resulting in faster
    I/O operations and reduced disk head movement.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 合并被用来减少小而频繁的I/O操作的开销，尤其是在旋转硬盘驱动器中。通过合并多个请求，磁盘可以执行顺序读写，从而加快I/O操作速度并减少磁头的移动。
- en: Plugging
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 插入
- en: The kernel uses the concept of *plugging* to stop requests from being processed
    in the queue. We’re talking about improving performance here, so how does putting
    requests on hold help out? As we’ve learned, merging has a very positive effect
    on the drive performance. However, for smaller I/O requests to merge into a larger
    unified request, there must be existing requests for adjacent sectors in the queue.
    Therefore, in order to perform merging, the kernel first has to build up the request
    queue with a few requests so that there is a greater probability of merging. Plugging
    the queue helps to batch requests in anticipation of opportunities for merge and
    sort operations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 内核使用*plugging*概念来停止队列中请求的处理。我们在这里讨论的是提升性能，那么为何将请求挂起有助于提升性能呢？正如我们所学，合并对硬盘性能有非常积极的作用。然而，为了让较小的I/O请求合并成一个较大的统一请求，队列中必须存在相邻扇区的现有请求。因此，为了执行合并，内核首先需要通过一些请求来填充请求队列，从而增加合并的概率。将队列插入（plugging）有助于批量处理请求，为合并和排序操作的机会做好准备。
- en: 'Plugging is a technique used to ensure that there are enough requests in the
    queue for potential merging operations. It involves waiting for additional requests
    to fill up the request queue and helps regulate the dispatch rate of requests
    to the device queue. The purpose of plugging is to control the dispatch rate of
    requests to the device queue. When there are no pending requests or a very small
    number of them in the block device queue, incoming requests are not dispatched
    to the device driver immediately. This results in the device being in a plugged
    state. The following figure demonstrates this concept:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 插入（plugging）是一种确保队列中有足够请求以进行潜在合并操作的技术。它涉及等待更多的请求填满请求队列，并帮助调节请求向设备队列的调度速率。插入的目的是控制请求向设备队列的调度速率。当块设备队列中没有待处理的请求或仅有非常少的请求时，传入的请求不会立即调度到设备驱动程序，这样设备就处于插入（plugged）状态。以下图示说明了这一概念：
- en: '![Figure 6.5 – Plugging](img/B19430_06_05.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 插入（Plugging）](img/B19430_06_05.jpg)'
- en: Figure 6.5 – Plugging
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 插入（Plugging）
- en: Plugging is executed at the process level rather than at the device level. The
    kernel initiates a plug sequence when a process carries out I/O operations. After
    the process has completed submitting its I/O requests to the queue, the requests
    are forwarded to the block layer and then dispatched to the device driver. A device
    is considered unplugged once the process has finished submitting I/O requests.
    If an application is blocked during a plug sequence, the scheduler proceeds to
    process the requests that are already in the queue.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 插入（plugging）是在进程级别而非设备级别执行的。当进程执行I/O操作时，内核会启动一个插入序列。在进程提交其I/O请求到队列之后，请求会被转发到块层（block
    layer），然后调度到设备驱动程序。一旦进程完成提交I/O请求，设备就被认为是“拔出”（unplugged）状态。如果在插入序列中应用程序被阻塞，调度器将继续处理已经在队列中的请求。
- en: Having discussed the most commonly used I/O scheduling techniques found in I/O
    schedulers, let us now delve into the reasoning and principles behind the decision-making
    process of the most widely used I/O schedulers in Linux.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了I/O调度器中最常用的I/O调度技术之后，接下来让我们深入探讨Linux中最广泛使用的I/O调度器的决策过程背后的推理和原理。
- en: Explaining the Linux I/O schedulers
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释Linux I/O调度器
- en: 'Disk schedulers are an interesting topic. They serve as a bridge between the
    block layer and low-level device drivers. The requests issued to a block device
    are altered by an I/O scheduler and handed over to the device drivers. It is the
    job of the scheduler to perform operations such as merging, sorting, and plugging
    on the I/O requests and divide the storage resources among the queued I/O requests.
    One of the notable advantages of the disk schedulers in Linux is their Plug and
    Play capability, allowing them to be switched in real time. Additionally, depending
    on the characteristics of the storage hardware being used, a distinct scheduler
    can be assigned to each block device in the system. The selection of a disk scheduler
    is not something that frequently comes under the radar, unless you’re trying to
    extract the maximum from your system. The I/O scheduler is in charge of deciding
    the order in which I/O requests will be delivered to the device driver. The order
    is decided with a priority on the following tasks:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘调度器是一个有趣的话题。它们作为块层和低级设备驱动程序之间的桥梁。发出的对块设备的请求会被I/O调度器修改后交给设备驱动程序。调度器的工作是对I/O请求进行合并、排序和插入等操作，并将存储资源分配给排队的I/O请求。Linux中磁盘调度器的一个显著优势是它们的即插即用功能，允许它们实时切换。此外，根据所使用存储硬件的特性，可以为系统中的每个块设备分配不同的调度器。磁盘调度器的选择并不是经常被关注的事情，除非你试图从系统中挤出最大性能。I/O调度器负责决定I/O请求交给设备驱动程序的顺序。这个顺序是基于以下任务的优先级来决定的：
- en: Reducing disk seeking
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低磁盘寻道
- en: Ensuring fairness among I/O requests
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保I/O请求的公平性
- en: Maximizing disk throughput
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化磁盘吞吐量
- en: Reducing latency for time-sensitive tasks
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低对时间敏感任务的延迟
- en: It’s a strenuous task to strike a balance among these goals. The different schedulers
    make use of multiple queues to achieve these goals. Operations such as merging
    and sorting are performed in request queues. The schedulers also perform additional
    processing, as per their internal algorithms, in these queues. Once the requests
    are ready, they are handed over to the dispatch queue managed by the device drivers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些目标之间取得平衡是一项艰巨的任务。不同的调度器利用多个队列来实现这些目标。合并和排序等操作会在请求队列中执行。调度器还会根据其内部算法在这些队列中执行额外的处理。一旦请求准备就绪，它们会被交给设备驱动程序管理的调度队列。
- en: The major performance optimizations in earlier block layer designs were directed
    toward hard disk drives. This is especially true for the disk scheduling algorithms.
    Most of the I/O handling techniques that we’ve discussed so far are most useful
    when the underlying storage media consists of rotating mechanical drives. As we’ll
    see in [*Chapter 7*](B19430_07.xhtml#_idTextAnchor124), SSDs and NVMe drives are
    different beasts of nature and are not impacted by the limitations that impede
    a mechanical drive.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 早期块层设计中的主要性能优化针对的是硬盘驱动器。这一点对于磁盘调度算法尤其适用。我们到目前为止讨论的大部分I/O处理技术，在底层存储介质由旋转机械驱动器组成时最为有效。正如我们将在[*第七章*](B19430_07.xhtml#_idTextAnchor124)中看到的，SSD和NVMe驱动器是不同性质的硬件，并不受阻碍机械驱动器的限制影响。
- en: 'The scheduler controls the behavior of the underlying disks and thus plays
    a vital role in dictating the performance of an application. Just like the varying
    natures of physical storage, every application is also built differently. It is
    imperative to know the type of workload for the environment being tuned. There
    is no single scheduler that can be deemed fit enough to match the varying I/O
    characteristics of all applications. When choosing a scheduler, it is crucial
    to ask the following questions:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器控制底层磁盘的行为，因此在决定应用程序性能时起着至关重要的作用。就像物理存储的不同特性一样，每个应用程序的构建方式也不同。了解所调优环境的工作负载类型至关重要。没有任何一个调度器能够被认为是足够适合匹配所有应用程序不同I/O特性的。选择调度器时，必须问以下问题：
- en: What is the host system type – that is, is it a desktop, laptop, virtual machine,
    or a server?
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主机系统类型是什么——是桌面、笔记本、虚拟机还是服务器？
- en: What sort of workload will be running? What type of application? Databases,
    multi-user desktop interface, games, or videos?
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将运行什么样的工作负载？是什么类型的应用程序？数据库、多用户桌面界面、游戏还是视频？
- en: Is the hosted application a processor or I/O-bound?
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 托管的应用程序是处理器密集型还是I/O密集型？
- en: What is the backend storage media type? HDDs, SSDs, or NVMe?
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后端存储介质类型是什么？HDD、SSD还是NVMe？
- en: Is the storage local to the host? Or is it provisioned from a large enterprise
    storage area network?
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储是本地的还是来自大型企业存储区域网络的？
- en: The I/O requests generated by a real-time application should be completed within
    a certain deadline. For instance, when streaming a video through a multimedia
    player, it has to be guaranteed that the frames will be read in time so that the
    video can be played without any glitches. On the other hand, interactive applications
    have to wait for the completion of a task before proceeding to the next one. For
    example, when writing in a document editor, the end user expects the editor to
    respond immediately when a key is pressed. Plus, the text has to appear in the
    same order in which it was typed.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 实时应用生成的I/O请求应在特定的截止时间内完成。例如，在通过多媒体播放器播放视频时，必须保证视频帧能够及时读取，以便视频能够流畅播放。另一方面，交互式应用必须等待任务完成后才能进行下一个任务。例如，在文档编辑器中输入文字时，最终用户期望编辑器在按下键时立即响应。并且，文本必须按输入的顺序显示出来。
- en: For individual systems, the choice of a scheduler may not matter much and default
    settings might suffice. For servers running enterprise workloads, margins are
    much finer, and how a scheduler handles I/O requests may well decide the overall
    performance of the application. As we have repeatedly mentioned in this book,
    disk I/O is much slower than the processor and memory subsystems. Therefore, any
    decision regarding the choice of a disk scheduler should be accompanied by a lot
    of consideration and performance benchmarking.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个系统，调度器的选择可能不太重要，默认设置可能足够。对于运行企业工作负载的服务器，性能要求就更为严格，调度器如何处理I/O请求可能决定了应用程序的整体性能。正如我们在本书中反复提到的，磁盘I/O比处理器和内存子系统慢得多。因此，任何关于选择磁盘调度器的决策都应该经过深思熟虑，并伴随着性能基准测试。
- en: Disk scheduling should not be confused with CPU scheduling. To process any request,
    both I/O and CPU time are required. In simpler terms, a process requests time
    from the CPU, after which it is able to run (if the time is allocated). The process
    can issue read or write requests to the disk. It is then the job of the disk scheduler
    to order and shepherd those requests to the underlying disks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘调度不应与CPU调度混淆。处理任何请求时，都需要I/O和CPU时间。简单来说，一个进程向CPU请求时间，获得时间后它就可以运行（如果时间被分配）。进程可以向磁盘发出读写请求。然后，磁盘调度程序的工作就是对这些请求进行排序，并将它们引导到底层磁盘。
- en: I/O schedulers in Linux are also referred to as elevators. The elevator algorithm,
    also called `SCAN`, compares the operation of legacy mechanical drives with elevators
    or lifts. When elevators go either up or down, they keep going in that same direction
    and stop to drop off people along the way. In disk scheduling, the read-write
    head of the drive starts from one end of the disk and moves toward the other end,
    while servicing requests along the way. To continue the analogy, mechanical drives
    need to read (`pick up`) and write (`drop off`) requests (`people`) at different
    disk locations (`floors`).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Linux中的I/O调度器也称为电梯。电梯算法，也叫`SCAN`，将传统机械硬盘的操作与电梯或升降机进行比较。当电梯上下移动时，它会保持一个方向并在途中停下让人上下。在磁盘调度中，驱动的读写头从磁盘的一端开始，向另一端移动，同时服务沿途的请求。继续这个类比，机械硬盘需要在不同的磁盘位置（“楼层”）读取（“接载”）和写入（“卸载”）请求（“人”）。
- en: The different types of I/O schedulers available in the kernel are suited to
    particular use cases, some more than others. As we learned in [*Chapter 5*](B19430_05.xhtml#_idTextAnchor090),
    the single-queue framework does not scale up to meet the performance levels of
    modern storage devices. The advancements in drive technologies and multi-core
    systems led to the development of the multi-queue block I/O framework. Even with
    the implementation of this framework, the kernel still lacked an important ingredient
    when dealing with modern drives – an I/O scheduler designed to work with multi-queue
    devices. Schedulers that were designed for the single-queue framework and intended
    to be used with single-queue devices do not function optimally with modern drives.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 内核中可用的不同类型的I/O调度器适用于特定的使用场景，有些比其他的更合适。正如我们在[*第5章*](B19430_05.xhtml#_idTextAnchor090)中学到的，单队列框架不能满足现代存储设备的性能要求。驱动技术和多核系统的进步促使了多队列块I/O框架的发展。即使实现了这个框架，内核在处理现代驱动时仍然缺少一个重要的成分——一个能够与多队列设备配合使用的I/O调度器。为单队列框架设计、用于单队列设备的调度器，在现代驱动上并不能发挥最佳性能。
- en: '*Figure 6**.6* highlights the various types of I/O schedulers available for
    both single and multi-queue frameworks:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.6* 突出了适用于单队列和多队列框架的各种I/O调度器类型：'
- en: '![Figure 6.6 – Different I/O Scheduling options](img/B19430_06_06.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – 不同的I/O调度选项](img/B19430_06_06.jpg)'
- en: Figure 6.6 – Different I/O Scheduling options
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 不同的I/O调度选项
- en: 'Single-queue I/O schedulers are deprecated and have not been a part of the
    kernel since version 5.0\. Although you can disable these and revert back to the
    single-queue schedulers, the latest kernel releases default to the multi-queue
    schedulers, and as such we will keep our focus on the multi-queue schedulers that
    are a part of the kernel. There are four major players in this category. These
    schedulers map I/O requests to multiple queues, which are handled by kernel threads
    distributed across the multiple CPU cores:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 单队列I/O调度器已被弃用，并且自版本5.0起不再包含在内核中。尽管你可以禁用这些并恢复为单队列调度器，但最新的内核版本默认使用多队列调度器，因此我们将重点关注作为内核一部分的多队列调度器。这个类别中有四个主要的调度器。这些调度器将I/O请求映射到多个队列，这些队列由分布在多个CPU核心上的内核线程处理：
- en: MQ-deadline
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MQ-截止日期
- en: '**Budget Fair** **Queuing** (**BFQ**)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预算公平** **排队** (**BFQ**)'
- en: Kyber
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kyber
- en: None
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无
- en: Let’s take a look at the operational logic of these schedulers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这些调度器的操作逻辑。
- en: The MQ-deadline scheduler – guaranteeing a start service time
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MQ-截止日期调度器 – 保证启动服务时间
- en: The deadline scheduler, as the name suggests, imposes a deadline to service
    I/O requests. Due to its latency-oriented design, it is often used for latency-sensitive
    workloads. Because of its high performance, it has also been adopted for multi-queue
    devices. Its implementation for multi-queue devices is known as *mq-deadline*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 截止日期调度器顾名思义，为I/O请求服务施加一个截止日期。由于其以延迟为导向的设计，它通常用于延迟敏感的工作负载。由于其高性能，它也被采用于多队列设备。它在多队列设备上的实现被称为*mq-deadline*。
- en: 'The primary objective of the deadline scheduler is to ensure that a request
    has a designated start service time. This is accomplished by enforcing a deadline
    on all I/O operations, which helps prevent requests from being neglected. The
    deadline scheduler makes use of the following queues:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 截止日期调度器的主要目标是确保每个请求都有指定的启动服务时间。这是通过对所有I/O操作强制实施一个截止日期来实现的，帮助防止请求被忽视。截止日期调度器利用以下队列：
- en: '**Sorted**: The read and write operations in this queue are sorted by the sector
    numbers they are to access.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排序**：该队列中的读写操作按其访问的扇区号进行排序。'
- en: '**Deadline**: The deadline queue is a standard **First-In-First-Out** (**FIFO**)
    queue that contains requests sorted by their deadlines. To prevent starvation
    of requests, the deadline scheduler utilizes separate instances of the deadline
    queue for read and write requests, assigning an expiration time to each I/O request.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**截止日期**：截止日期队列是一个标准的**先进先出**（**FIFO**）队列，包含按截止日期排序的请求。为了防止请求饥饿，截止日期调度器为读写请求使用单独的截止日期队列实例，并为每个I/O请求分配一个过期时间。'
- en: The deadline scheduler places each I/O request in both the sorted and deadline
    queues. Before deciding which request to serve, the deadline scheduler selects
    a queue from which to choose a read or write request. If there are requests in
    both the read and write queues, read queues are preferred. This is because write
    requests can starve read operations. This makes the deadline scheduler extremely
    effective for read-heavy workloads.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 截止日期调度器将每个I/O请求放入排序队列和截止日期队列中。在决定服务哪个请求之前，截止日期调度器会从中选择一个队列来选择读请求或写请求。如果读写队列中都有请求，优先选择读队列。这是因为写请求可能会导致读操作饥饿。这使得截止日期调度器在读密集型工作负载中非常有效。
- en: 'The operational logic of the deadline scheduler is depicted in the following
    figure:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 截止日期调度器的操作逻辑如以下图所示：
- en: '![Figure 6.7 – The MQ-deadline I/O scheduler](img/B19430_06_07.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – MQ-截止日期 I/O 调度器](img/B19430_06_07.jpg)'
- en: Figure 6.7 – The MQ-deadline I/O scheduler
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – MQ-截止日期 I/O 调度器
- en: 'The I/O requests to be served are decided as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要服务的I/O请求决定如下：
- en: Let’s say that the scheduler has decided to serve read requests. It will check
    the first request in the deadline queue. If the timer associated with that request
    has expired, it will be handed over to the dispatch queue and inserted at its
    tail end. The scheduler then turns its focus to the sorted queue and selects a
    batch of requests (16 requests by default) following the chosen request. This
    is done to increase the sequential operations. Think of how an elevator drops
    off people on different floors along the way to its final destination. The number
    of requests in a batch is a tunable parameter and can be changed.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设调度器已经决定处理读请求。它将检查截止时间队列中的第一个请求。如果该请求的计时器已到期，它将被交给调度队列，并插入到调度队列的尾部。调度器接着会将注意力转向排序队列，并选择一个请求批次（默认
    16 个请求），这些请求会紧随选中的请求后面。这样做是为了增加顺序操作。可以想象为电梯在前往最终目的地的途中在不同楼层停靠并放人。每批请求的数量是一个可调参数，可以进行更改。
- en: It can also happen that there are no requests with expired deadlines in the
    deadline queue. In that case, the scheduler will examine the last request that
    was serviced from the sorted queue and choose the subsequent request in the sequence.
    The scheduler will then select a batch of 16 requests that follow the chosen request.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 也可能发生在截止时间队列中没有任何请求的截止时间已经过期。在这种情况下，调度器会检查排序队列中最后一个被处理的请求，并选择序列中的下一个请求。调度器随后会选择一个包含
    16 个请求的批次，这些请求紧跟在被选择的请求后面。
- en: After processing each batch of requests, the deadline scheduler checks to see
    whether requests in the write deadline queue have been starved for too long, and
    then decides whether to start a new batch of read or write operations.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在处理完每个请求批次后，截止时间调度器会检查写操作截止时间队列中的请求是否已被饿死太久，然后决定是否开始新的读或写操作批次。
- en: 'The following figure explains this process. If a read request for sector number
    19 on the disk is received, it is assigned a deadline and inserted at the tail
    end of the deadline queue for read operations. Based on the sector number, this
    request is also placed in the sorted sector queue, just behind the request for
    sector 11\. The operational flow of the deadline scheduler, regarding how requests
    are processed, is demonstrated in *Figure 6**.8*:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 下图解释了这个过程。如果收到一个针对磁盘第 19 扇区的读请求，它会被分配一个截止时间，并插入到读操作的截止时间队列的尾端。根据扇区号，该请求也会被放入排序的扇区队列，位于第
    11 扇区请求之后。截止时间调度器的操作流程，关于请求如何处理，在*图 6.8*中展示：
- en: '![Figure 6.8 – Request handling in the MQ-deadline I/O scheduler](img/B19430_06_08.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – MQ-deadline I/O 调度器中的请求处理](img/B19430_06_08.jpg)'
- en: Figure 6.8 – Request handling in the MQ-deadline I/O scheduler
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – MQ-deadline I/O 调度器中的请求处理
- en: 'The following snippet code of mq-deadline in `block/mq-deadline` dictates some
    of the behavior illustrated in the preceding figure. The expiry deadline for read
    requests (HZ/2) is 500 milliseconds, whereas for writes, it is 5 seconds (5*HZ).
    This ensures that read requests have higher precedence. The term `HZ` represents
    the clock ticks generated per second. The definition of `writes_starved` indicates
    that reads can starve writes. The writes are only serviced once against two rounds
    of reads. `fifo_batch` sets the number of requests that can be batched together:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 `block/mq-deadline` 中 mq-deadline 的代码片段，它控制了前述图示中的某些行为。读请求的截止时间（HZ/2）为 500
    毫秒，而写请求的截止时间则为 5 秒（5*HZ）。这确保了读请求具有更高的优先级。术语 `HZ` 代表每秒钟生成的时钟滴答数。`writes_starved`
    的定义表示读请求可以饿死写请求。写请求仅在经过两轮读请求后才能得到服务。`fifo_batch` 设置了可以批量处理的请求数量：
- en: '[PRE0]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To summarize, the deadline scheduler strives to reduce I/O latency by implementing
    start service times for every incoming request. Each new request is assigned a
    deadline timer. When the expiry time for a request is reached, the scheduler will
    forcefully service that request to prevent request starvation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，截止时间调度器通过为每个传入请求实现开始服务时间，努力减少 I/O 延迟。每个新请求都会分配一个截止时间计时器。当请求的截止时间到达时，调度器会强制处理该请求，以防止请求饿死。
- en: Budget fair queuing – providing proportional disk share
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预算公平排队 – 提供按比例的磁盘共享
- en: The **Budget Fair Queuing** (**BFQ**) scheduler is a relative newcomer in the
    world of disk schedulers, but it has gained considerable popularity. It is modeled
    after the **Completely Fair Queuing** (**CFQ**) scheduler. It provides fairly
    good response times and is considered particularly suitable for slower devices.
    With its rich and comprehensive scheduling techniques, the BFQ is often thought
    to be one of the most complete disk schedulers, although its sophisticated design
    also makes it the most complex scheduler among the lot.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**预算公平排队**（**BFQ**）调度器在磁盘调度器的世界中是一个相对较新的成员，但它已经获得了相当大的普及。它的设计灵感来源于**完全公平排队**（**CFQ**）调度器。它提供了相当不错的响应时间，并且被认为特别适用于较慢的设备。凭借其丰富而全面的调度技术，BFQ通常被认为是最完整的磁盘调度器之一，尽管其复杂的设计也使其成为所有调度器中最复杂的一个。'
- en: BFQ is a proportional share disk scheduler. The primary goal of BFQ is to be
    fair to all I/O requests. To achieve this fairness, it makes use of some intricate
    techniques. Internally, BFQ uses the `Worst-case Fair Weighted Fair Queuing+ (B-WF2Q+)`
    algorithm to aid in scheduling decisions.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: BFQ是一个按比例分配的磁盘调度器。BFQ的主要目标是公平地对待所有I/O请求。为了实现这种公平性，它采用了一些复杂的技术。在内部，BFQ使用`最坏情况公平加权公平排队+（B-WF2Q+）`算法来辅助调度决策。
- en: 'The BFQ scheduler guarantees a proportional share of the disk resources to
    every process in the system. It collects the I/O requests in the following two
    queues:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: BFQ调度器为系统中的每个进程保证一个与磁盘资源成比例的份额。它将I/O请求收集到以下两个队列中：
- en: '**Per-process queues**: The BFQ scheduler allocates a queue for every process.
    Each per-process queue contains synchronous I/O requests.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个进程队列**：BFQ调度器为每个进程分配一个队列。每个进程队列包含同步I/O请求。'
- en: '**Per-device queue**: All the asynchronous I/O requests are collected in a
    per-device queue. This queue is shared among processes.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每设备队列**：所有异步I/O请求都被收集到每个设备的队列中。这个队列是进程共享的。'
- en: 'Whenever a new queue is created, it is assigned a variable budget. Unlike most
    schedulers, which allocate time slices, this budget is implemented as the number
    of sectors that each process is allowed to transfer when it is next scheduled
    to access the disk resources. The value of this budget is what ultimately determines
    the share of disk throughput for each process. As such, its calculation is complex
    and based on a multitude of factors. The major factors in this calculation are
    the I/O weight and the recent I/O activity of the process. Based on these observations,
    the scheduler assigns a budget that is proportional to a process’s I/O activity.
    The I/O weight of a process has a default value, but it can be changed. The assignment
    of the budget is such that a single process is not able to hog all the bandwidth
    of available storage resources. *Figure 6**.9* shows the different queues used
    by the BFQ scheduler:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 每当创建一个新队列时，它会被分配一个变量预算。与大多数调度器通过分配时间片不同，BFQ的预算是通过每个进程下次调度访问磁盘资源时允许转移的扇区数来实现的。这个预算的数值最终决定了每个进程的磁盘吞吐量份额。因此，预算的计算是复杂的，并且基于多个因素。主要的因素包括I/O权重和进程的近期I/O活动。根据这些观察，调度器为进程分配一个与其I/O活动成比例的预算。进程的I/O权重有一个默认值，但可以更改。预算的分配方式确保单个进程无法独占所有存储资源的带宽。*图6.9*展示了BFQ调度器使用的不同队列：
- en: '![Figure 6.9 – The BFQ I/O scheduler](img/B19430_06_09.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9 – BFQ I/O调度器](img/B19430_06_09.jpg)'
- en: Figure 6.9 – The BFQ I/O scheduler
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – BFQ I/O调度器
- en: 'When it comes to servicing I/O requests, some of the factors affecting scheduling
    decisions are described as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务I/O请求时，一些影响调度决策的因素如下所述：
- en: The BFQ scheduler selects the queue to be served through the C-LOOK algorithm.
    It picks up the first request from the selected queue and hands it to the driver.
    The budget of the queue gets decremented by the size of the request This is explained
    in a bit more detail at the end of this discussion. BFQ exclusively serves one
    queue at a time.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BFQ调度器通过C-LOOK算法选择要服务的队列。它从所选队列中取出第一个请求并将其交给驱动程序。队列的预算会根据请求的大小减少。这个过程在本讨论的最后部分会做更详细的说明。BFQ每次只服务一个队列。
- en: The BFQ scheduler prioritizes scheduling processes that have smaller I/O budgets.
    Normally, these are the processes that have a small but random set of I/O requests.
    In contrast, I/O intensive processes with a large number of sequential I/O requests
    are assigned a larger budget. When selecting a process queue for servicing, the
    BFQ scheduler chooses the queue with the lowest I/O budget, granting exclusive
    access to the disk resources. This approach achieves two objectives. First, processes
    with smaller budgets receive prompt service and do not have to wait excessively.
    Second, I/O-bound processes with larger budgets receive a proportionately greater
    share of disk resources, promoting sequential I/O operations and thereby enhancing
    disk performance. The BFQ scheduler makes use of a slightly unorthodox approach
    to increase disk throughput – performing disk idling by checking for synchronous
    I/O requests. When an application generates synchronous I/O requests, it enters
    a blocking state and waits for the operation to complete. Mostly, these are read
    requests, as write operations are asynchronous and can be directly performed in
    cache. If the last request in the process queue is synchronous, the process goes
    into a waiting state. This request is not dispatched immediately to the disk,
    as the BFQ scheduler allows the process to generate another request. During this
    time frame, the drive remains idle. More often than not, the process generates
    another request, as it waits for the current synchronous request to complete before
    issuing new requests. The new request is normally adjacent to the last request,
    which improves the chance of sequential operations. At times, this approach can
    backfire and might not always have a positive impact on performance.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BFQ调度器优先安排具有较小I/O预算的进程。通常，这些是具有一小部分随机I/O请求的进程。相比之下，具有大量顺序I/O请求的I/O密集型进程分配了较大预算。在选择要为其提供服务的进程队列时，BFQ调度器选择具有最低I/O预算的队列，为磁盘资源提供独占访问。这种方法实现了两个目标。首先，具有较小预算的进程能够及时得到服务，而无需过度等待。其次，具有较大预算的I/O密集型进程获得了比例更大的磁盘资源份额，促进顺序I/O操作，从而提升磁盘性能。BFQ调度器利用稍微非正统的方法来增加磁盘吞吐量，通过检查同步I/O请求进行磁盘空闲。当应用程序生成同步I/O请求时，它进入阻塞状态并等待操作完成。这些通常是读取请求，因为写入操作是异步的，可以直接在缓存中执行。如果进程队列中的最后一个请求是同步的，则进程进入等待状态。此请求不会立即发送到磁盘，因为BFQ调度器允许进程生成另一个请求。在此时间段内，驱动器保持空闲状态。通常情况下，进程会生成另一个请求，因为它等待当前同步请求完成后才会发出新请求。新请求通常与上一个请求相邻，这提高了顺序操作的机会。有时，这种方法可能适得其反，并且可能并不总是对性能产生积极影响。
- en: If two processes work on neighboring areas on a disk, it makes sense to combine
    their requests so that sequential operations can be increased. In this case, BFQ
    merges the queues of both processes to enable the consolidation of requests. The
    incoming requests are compared with the next request of the in-service process,
    and if the two requests are close, the request queues for both processes are merged.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个进程在磁盘上相邻区域上操作，合并它们的请求是有意义的，以便增加顺序操作。在这种情况下，BFQ合并两个进程的队列，以实现请求的整合。传入的请求与正在服务的进程的下一个请求进行比较，如果两个请求接近，则合并两个进程的请求队列。
- en: In the event that an application executing read requests depletes its queue
    while still having a surplus budget, the disk will be idled briefly to give that
    process a chance to issue another I/O request.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果执行读取请求的应用程序在仍然有剩余预算的情况下耗尽其队列，磁盘将短暂空闲，以使该进程有机会发出另一个I/O请求。
- en: 'The scheduler continues to serve the queue until one of the following events
    occurs:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器在以下事件之一发生前一直为队列提供服务：
- en: The queue budget is exhausted
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 队列预算用尽。
- en: All queue requests have been completed
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有队列请求都已完成。
- en: The idling timer expires while waiting for a new request from the process
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在等待来自进程的新请求时，空闲计时器超时。
- en: Too much time has been spent while servicing the queue
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在为队列提供服务时花费了过多时间。
- en: 'Upon examining the BFQ code found in `block/bfq-iosched.c`, you will discover
    a notable concept known as the **charge factor** for asynchronous requests:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看`block/bfq-iosched.c`中找到的BFQ代码时，您将发现一个被称为**费用因子**的显著概念，用于异步请求：
- en: '[PRE1]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It was mentioned earlier that when a request to be serviced is selected from
    a queue, the budget of the queue is decremented by the size of the request – that
    is, the number of sectors in the request. This is true for synchronous requests,
    but for asynchronous requests, this cost is much higher. This is also one of the
    ways reads are prioritized over writes. For asynchronous requests, the queue is
    charged with the number of sectors in the request, multiplied by the value of
    `bfq_async_charge_factor`, which is three. According to the kernel documentation,
    the current value for the charge factor parameter was determined by following
    a tuning process that involved various hardware and software configurations.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当从队列中选择一个请求进行处理时，队列的预算会减少该请求的大小——也就是请求中的扇区数。这对于同步请求是成立的，但对于异步请求来说，这个成本要高得多。这也是读取操作优先于写入操作的一种方式。对于异步请求，队列会按请求中的扇区数进行收费，并乘以`bfq_async_charge_factor`的值，即3。根据内核文档，当前的收费因子值是通过一项调优过程确定的，该过程涉及了各种硬件和软件配置。
- en: In summary, the BFQ scheduler employs equitable queuing approaches by apportioning
    a proportion of the I/O throughput to each process. It makes use of per-process
    queues for synchronous requests and a per-device queue for asynchronous requests.
    It assigns a budget to each process. This budget is calculated based on the I/O
    priority and the number of sectors transferred by the process when it was scheduled
    the last time. Although the BFQ scheduler is complex and incurs a slightly larger
    overhead compared to other schedulers, it is widely used, as it improves system
    response times and minimizes latency for time-sensitive applications.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，BFQ调度器通过分配一定比例的I/O吞吐量给每个进程，采用公平排队的方法。它使用每个进程的队列来处理同步请求，并使用每个设备的队列来处理异步请求。它为每个进程分配一个预算。这个预算是根据I/O优先级以及进程上次调度时传输的扇区数来计算的。尽管BFQ调度器比较复杂，并且相较于其他调度器会带来稍微更大的开销，但它被广泛使用，因为它提高了系统的响应时间，并减少了对时间敏感应用的延迟。
- en: Kyber – prioritizing throughput
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kyber – 优先考虑吞吐量
- en: The Kyber scheduler is also a relatively newer entry in the disk scheduling
    world. Although the BFQ scheduler is older than the Kyber scheduler, both officially
    became a part of the kernel version 4.12\. The Kyber scheduler is specifically
    designed for modern high-performing storage devices.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Kyber调度器也是磁盘调度领域中相对较新的一个成员。尽管BFQ调度器比Kyber调度器更早，但两者都在内核版本4.12中正式成为一部分。Kyber调度器特别为现代高性能存储设备设计。
- en: Historically, the ultimate goal of disk schedulers has been to reduce seek times
    for mechanical drives so that the overhead caused by random access operations
    can be decreased. Consequently, the different disk schedulers have used complex
    and sophisticated techniques to achieve this common goal. Each scheduler prioritizes
    certain aspects of performance in varying ways, which introduces an additional
    overhead while processing I/O requests. As modern drives, such as SSDs and NVMe,
    are not hampered by random access operations, some of the complicated techniques
    used by certain schedulers might not apply to these devices. For instance, the
    BFQ scheduler has a slightly high overhead for each request, so it is not considered
    ideal for systems to have high throughput drives. This is where the Kyber scheduler
    comes in handy.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，磁盘调度器的最终目标是减少机械硬盘的寻道时间，从而降低随机访问操作所带来的开销。因此，不同的磁盘调度器使用了复杂且精密的技术来实现这一共同目标。每个调度器以不同的方式优先考虑某些性能方面，这在处理I/O请求时会引入额外的开销。随着现代硬盘，如SSD和NVMe，已经不再受随机访问操作的限制，一些调度器所使用的复杂技术可能不再适用于这些设备。例如，BFQ调度器每个请求的开销稍微较高，因此它不被认为适合具有高吞吐量硬盘的系统。这就是Kyber调度器发挥作用的地方。
- en: 'The Kyber scheduler doesn’t have complex internal scheduling algorithms. It
    is intended to be used in environments that comprise high-performing storage devices.
    It uses a very straightforward approach and implements some basic policies to
    marshal I/O requests. The Kyber scheduler splits the underlying device into multiple
    domains. The idea is to maintain a queue for the different types of I/O requests.
    Upon inspecting the code found in `block/kyber-iosched.c`, we can observe the
    presence of the following request types:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Kyber 调度器没有复杂的内部调度算法。它旨在用于包含高性能存储设备的环境。它使用非常简单的方法并实施一些基本策略来管理 I/O 请求。Kyber 调度器将底层设备划分为多个域。其理念是为不同类型的
    I/O 请求维护队列。通过检查 `block/kyber-iosched.c` 中的代码，我们可以观察到以下请求类型：
- en: '[PRE2]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The Kyber scheduler categorizes the requests as follows – reads, writes, discard,
    and other requests. The Kyber scheduler maintains queues for these types of requests.
    The discard request is used for devices such as SSDs. The filesystem on top of
    the device can issue this request to discard blocks not in use by the filesystem.
    For the type of request mentioned previously, the scheduler implements a limit
    on the corresponding number of operations in the device queue:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Kyber 调度器将请求分为以下几类——读取、写入、丢弃和其他请求。Kyber 调度器为这些类型的请求维护队列。丢弃请求用于如 SSD 这样的设备。设备上的文件系统可以发出此请求，以丢弃文件系统未使用的块。对于前面提到的请求类型，调度器会对设备队列中相应操作的数量进行限制：
- en: '[PRE3]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The crux of Kyber’s scheduling approach is to limit the size of dispatch queues.
    This directly correlates with the time spent waiting for I/O requests in the request
    queue. The scheduler only sends a limited number of operations to the dispatch
    queue, which ensures that the dispatch queue is not too crowded. This results
    in the swift processing of the requests in the dispatch queue. Consequently, the
    I/O operations in the request queues don’t have to wait too long to be serviced.
    This approach results in reduced latency. The following figure illustrates the
    logic of the Kyber scheduler:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Kyber 调度方法的关键在于限制调度队列的大小。这直接与在请求队列中等待 I/O 请求所花费的时间相关。调度器只将有限数量的操作发送到调度队列，确保调度队列不会过于拥挤，从而实现调度队列中请求的快速处理。因此，请求队列中的
    I/O 操作不必等待太长时间即可得到服务。这种方法减少了延迟。以下图示说明了 Kyber 调度器的逻辑：
- en: '![Figure 6.10 – The Kyber I/O scheduler](img/B19430_06_10.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – Kyber I/O 调度器](img/B19430_06_10.jpg)'
- en: Figure 6.10 – The Kyber I/O scheduler
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – Kyber I/O 调度器
- en: To determine the number of requests to be allowed in the dispatch queue, the
    Kyber scheduler uses a simple but effective approach. It calculates the completion
    time of each request, and based on this feedback, it adjusts the number of requests
    in the dispatch queue. Further, the target latencies for reads and synchronous
    writes are tunable parameters and can be changed. Based on their values, the scheduler
    will throttle requests in order to meet these target latencies.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定允许进入调度队列的请求数量，Kyber 调度器采用了一种简单但有效的方法。它计算每个请求的完成时间，并根据这一反馈调整调度队列中的请求数量。此外，读取和同步写入的目标延迟是可调参数，可以进行修改。根据这些值，调度器将限制请求以满足这些目标延迟。
- en: The Kyber scheduler prioritizes requests in the read queue over those in the
    write queue, unless a write request has been outstanding for too long, meaning
    the target latency has been breached.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Kyber 调度器优先处理读取队列中的请求，而不是写入队列中的请求，除非写入请求已经等待过长时间，意味着目标延迟已被突破。
- en: The Kyber scheduler is a performance powerhouse when it comes to modern storage
    devices. It is tailored for high-speed storage devices, such as SSDs and NVMe,
    and prioritizes low-latency I/O operations. This scheduler dynamically adjusts
    itself by scrutinizing I/O requests and enables the establishment of target latencies
    for both synchronous writes and reads. Consequently, it regulates I/O requests
    to meet the specified objectives.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Kyber 调度器在现代存储设备方面是一款性能强大的工具。它专为高速存储设备（如 SSD 和 NVMe）量身定制，并优先考虑低延迟 I/O 操作。该调度器通过仔细检查
    I/O 请求动态调整自身，并使得可以为同步写入和读取操作设定目标延迟。因此，它调节 I/O 请求，以满足指定的目标。
- en: None – minimal scheduling overhead
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无 – 最小调度开销
- en: 'The scheduling of I/O requests is a multifaceted problem. The scheduler has
    to take care of several aspects, such as reordering requests in the queue, allocating
    a portion of disk shares to each process, controlling the execution duration of
    every request, and making sure that individual requests do not monopolize the
    available storage resources. Each scheduler assumes that the host itself cannot
    optimize requests. Therefore, it jumps in and applies complex techniques to try
    and make the most of the available storage resources. The more sophisticated the
    scheduling technique, the greater the processing overhead. While optimizing requests,
    the schedulers generally make some assumptions about the underlying device. This
    works well unless the lower layers in the stack have better visibility of the
    available storage resources and can handle making scheduling decisions themselves,
    such as the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: I/O 请求的调度是一个多方面的问题。调度程序必须处理多个方面，例如重新排序队列中的请求、为每个进程分配一定比例的磁盘资源、控制每个请求的执行时间，并确保单个请求不会垄断可用的存储资源。每个调度程序都假设主机本身无法优化请求。因此，它会介入并应用复杂的技术，试图最大限度地利用可用的存储资源。调度技术越复杂，处理开销就越大。在优化请求时，调度程序通常会对底层设备做出一些假设。除非堆栈的低层能够更好地了解可用存储资源，并能自行做出调度决策，否则这种方法通常有效，例如：
- en: In high-end storage settings, such as storage area networks, storage arrays
    frequently include their own scheduling logic, since they possess deeper insight
    into the nuances of the underlying devices. As a result, the scheduling of I/O
    requests typically transpires at the lower layer. When using raid controllers,
    the host system doesn’t have complete knowledge about the underlying disks. Even
    if the scheduler applies some optimizations to I/O requests, it might not make
    much of a difference, as the host system lacks the visibility to accurately re-order
    the requests to lower seek time. In such cases, it makes sense to simply dispatch
    the requests to the raid controller.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高端存储环境中，如存储区域网络，存储阵列通常包括它们自己的调度逻辑，因为它们对底层设备的细微差别有更深的了解。因此，I/O 请求的调度通常发生在较低层次。当使用
    RAID 控制器时，主机系统无法完全了解底层磁盘。即使调度程序对 I/O 请求进行了一些优化，可能也没有太大区别，因为主机系统缺乏足够的可见性，无法准确地重新排序请求以降低寻道时间。在这种情况下，直接将请求发送到
    RAID 控制器是有意义的。
- en: Most scheduler optimizations are directed toward slow mechanical drives. If
    the environment consists of SSDs and NVMe drives, the processing overhead associated
    with these scheduling optimizations may seem excessive.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数调度程序优化针对的是较慢的机械硬盘。如果环境中使用的是 SSD 和 NVMe 驱动器，那么这些调度优化带来的处理开销可能显得过于冗余。
- en: In such cases, a unique but effective solution is to use the *none* scheduler.
    The none scheduler is the multi-queue *no-op I/O scheduler*. For single-queue
    devices, the same functionality was achieved through the *no-op* scheduler.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一种独特但有效的解决方案是使用 *none* 调度程序。none 调度程序是一个多队列的 *no-op I/O 调度程序*。对于单队列设备，相同的功能是通过
    *no-op* 调度程序实现的。
- en: The none scheduler is the most straightforward of all schedulers, as it performs
    no scheduling optimizations. Every incoming I/O request is appended to a FIFO
    queue and delegated to the block device for handling. This strategy proves beneficial
    when it has been established that the host must not endeavor to rearrange requests
    according to their included sector numbers. The none scheduler has a single request
    queue that includes both read and write I/O requests. Due to its rudimentary approach,
    Although the none I/O scheduler imposes minimal overhead, it does not ensure any
    particular quality of service. The none scheduler also does not perform any reordering
    of requests. It only does request merging to reduce seek time and improve throughput.
    Unlike all the other schedulers, the none scheduler has no tunables or settings
    for optimization. The request merging operation is the entire extent of its complexity.
    Because of this, the none scheduler uses a minimal amount of CPU instructions
    per I/O request. The operation of the none scheduler is based on the assumption
    that devices at the lower layer, such as raid controllers or storage controllers,
    will optimize I/O performance.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: none调度程序是所有调度程序中最简单的，它不执行任何调度优化。每个传入的I/O请求都会被追加到一个FIFO队列中，并委托给块设备处理。当已经确定主机不应根据包含的扇区号重新排列请求时，这一策略非常有用。none调度程序有一个单一的请求队列，包含读写I/O请求。由于其原始的方法，尽管none
    I/O调度程序对系统的开销最小，但它不确保任何特定的服务质量。none调度程序也不对请求进行重新排序，它只进行请求合并以减少寻道时间并提高吞吐量。与所有其他调度程序不同，none调度程序没有优化的可调参数或设置。请求合并操作是其复杂性的全部。由于这个原因，none调度程序每个I/O请求所需的CPU指令最少。none调度程序的操作基于这样的假设：底层设备，如RAID控制器或存储控制器，将优化I/O性能。
- en: 'The simple operational logic of the none scheduler is shown in *Figure 6**.11*:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: none调度程序的简单操作逻辑如*图6.11*所示：
- en: '![Figure 6.11 – The none I/O scheduler](img/B19430_06_11.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11 – none I/O调度程序](img/B19430_06_11.jpg)'
- en: Figure 6.11 – The none I/O scheduler
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – none I/O调度程序
- en: Although every environment has a lot of variables, based on the mode of operation,
    the none scheduler seems to be the preferred scheduler for enterprise storage
    area networks, as it does not make any assumptions about the underlying physical
    devices, and it does not implement any scheduling decisions that can compete or
    clash with the logic of the lower level I/O controllers.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个环境都有许多变量，但根据操作模式，none调度程序似乎是企业存储区域网络的首选调度程序，因为它不会对底层物理设备做出任何假设，并且不会实施任何可能与低层I/O控制器逻辑冲突的调度决策。
- en: Given the profusion of options to choose from, it can be challenging to determine
    which scheduler is most suitable for your needs. In the subsequent section, we
    will outline common usage scenarios for the schedulers we have covered in this
    chapter.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于可供选择的选项繁多，确定哪个调度程序最适合你的需求可能是一个挑战。在接下来的部分中，我们将概述本章中介绍的调度程序的常见使用场景。
- en: Discussing the scheduling conundrum
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论调度难题
- en: We’ve discussed and explained how the different I/O scheduling flavors go about
    their business, but the selection of a scheduler should always be accompanied
    by benchmark results gathered through real application workloads. As mentioned
    earlier, most of the time, default settings might be good enough. It’s only when
    you try to achieve peak efficiency, you try and tinker with the default settings.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论并解释了不同I/O调度策略的工作方式，但选择调度程序时应始终伴随通过实际应用工作负载收集的基准测试结果。如前所述，大多数时候，默认设置可能已经足够好。只有当你尝试达到最佳效率时，才会尝试调整默认设置。
- en: 'The pluggable nature of these schedulers means that we can change the I/O scheduler
    for a block device on the fly. There are two ways to do this. The currently active
    scheduler for a particular disk device can be checked through `sysfs`. In the
    following example, the active scheduler is set to `mq-deadline`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这些调度程序的可插拔特性意味着我们可以动态地更改块设备的I/O调度程序。有两种方法可以做到这一点。可以通过`sysfs`检查特定磁盘设备的当前活动调度程序。在以下示例中，活动调度程序设置为`mq-deadline`：
- en: '[PRE4]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To change the active scheduler, write the name of the desired scheduler to
    the scheduler file. For instance, to set the BFQ scheduler for `sda`, use the
    following command:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改活动调度程序，将所需调度程序的名称写入调度程序文件。例如，要为`sda`设置BFQ调度程序，可以使用以下命令：
- en: '[PRE5]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The preceding method will only set the scheduler temporarily and revert to default
    settings after a reboot. To make this change permanent, edit the `/etc/default/grub`
    file and add the `elevator=bfq` parameter to the `GRUB_CMDLINE_LINUX_DEFAULT`
    line. Then, regenerate the `GRUB` configuration and reboot the system.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法只会暂时设置调度器，并在重启后恢复默认设置。要使此更改永久生效，请编辑 `/etc/default/grub` 文件，并将 `elevator=bfq`
    参数添加到 `GRUB_CMDLINE_LINUX_DEFAULT` 行中。然后，重新生成 `GRUB` 配置并重启系统。
- en: Merely changing the scheduler will not result in two-fold performance gains.
    Usually, the improvement figure will be somewhere between 10–20%.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 单纯改变调度器并不会带来两倍的性能提升。通常，性能提升在 10% 到 20% 之间。
- en: 'Although each environment is different and scheduler performance may vary depending
    upon several variables, as a baseline, the following are some of the use cases
    of the schedulers that we’ve discussed in this chapter:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个环境不同，调度器的性能可能会根据多个变量而有所变化，但作为基准，以下是我们在本章中讨论的调度器的一些使用场景：
- en: '| **Use case** | **Recommended** **I/O scheduler** |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| **使用场景** | **推荐的** **I/O 调度器** |'
- en: '| A desktop GUI, interactive applications, and soft real-time applications,
    such as audio and video players | BFQ, as it guarantees good system responsiveness
    and low latency for time-sensitive applications |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 桌面 GUI、交互式应用程序和软实时应用程序，如音频和视频播放器 | BFQ，因为它能保证良好的系统响应性和低延迟，适用于时间敏感型应用 |'
- en: '| Traditional mechanical drives | BFQ or MQ-deadline – both are considered
    suitable for slower drives. Kyber/none are biased in favor of faster disks. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 传统机械驱动 | BFQ 或 MQ-deadline – 都被认为适用于较慢的驱动。Kyber/none 偏向于支持更快的磁盘。 |'
- en: '| High-performing SSDs and NVMe drives as local storage | Preferably none,
    but Kyber might also be a good alternative in some cases |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 高性能 SSD 和 NVMe 驱动作为本地存储 | 最好使用 none，但在某些情况下 Kyber 也可能是一个不错的替代方案 |'
- en: '| Enterprise storage arrays | None, as most storage arrays have built-in logic
    to schedule I/Os more efficiently |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 企业存储阵列 | None，因为大多数存储阵列内置了逻辑来更高效地调度 I/O |'
- en: '| Virtualized environments | MQ-deadline is a good option. If the hypervisor
    layer does its own I/O scheduling, then using the none scheduler might be beneficial.
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 虚拟化环境 | MQ-deadline 是一个不错的选择。如果虚拟机管理程序层已经自行调度 I/O，那么使用 none 调度器可能会有帮助。 |'
- en: Table 6.1 – Typical use cases for I/O schedulers
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.1 – I/O 调度器的典型使用场景
- en: Please note that these are not strict use cases, as often, several conditions
    might be overlapping. The type of application, workload, host system, and storage
    media are just some of the factors that must be kept in mind before deciding on
    a scheduler. Typically, the deadline scheduler is regarded as a versatile choice,
    due to its modest CPU overhead. BFQ performs well in desktop environments, whereas
    none and Kyber are better suited for high-end storage devices.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些并非严格的使用场景，因为通常多个条件可能会重叠。应用类型、工作负载、主机系统和存储介质等因素在选择调度器之前都需要考虑。通常，deadline
    调度器被认为是一个多用途的选择，因为它的 CPU 开销较小。BFQ 在桌面环境中表现良好，而 none 和 Kyber 更适合高端存储设备。
- en: Summary
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter provided an overview of I/O scheduling, which is a critical function
    of the block layer. When a read or write request passes through all the layers
    of the virtual filesystem, it eventually arrives at the block layer. The chapter
    explored the various types of I/O schedulers and their characteristics, including
    their advantages and disadvantages. The block layer includes multiple I/O schedulers
    that are suitable for particular use cases. The choice of an I/O scheduler plays
    a vital role in determining how I/O requests will be handled at the lower layer.
    To make more performance-oriented decisions, most schedulers employ some common
    techniques that aid in improving overall disk performance. The techniques that
    we discussed in this chapter are merging, coalescing, sorting, and plugging.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了 I/O 调度，这是块层的一个关键功能。当一个读写请求穿过虚拟文件系统的所有层时，最终会到达块层。本章探讨了各种 I/O 调度器及其特性，包括优点和缺点。块层包含多个适用于特定使用场景的
    I/O 调度器。I/O 调度器的选择在决定如何处理下层的 I/O 请求中起着至关重要的作用。为了做出更具性能导向的决策，大多数调度器使用一些常见技术，以帮助提高整体磁盘性能。本章讨论的技术包括合并、聚合、排序和插入。
- en: We also explained the different scheduling options available in the kernel.
    The kernel has a separate set of I/O schedulers for single- and multi-queue devices.
    The single-queue schedulers have been deprecated since kernel version 5.0\. The
    multi-queue scheduling options include the multi-queue Deadline scheduler, BFQ,
    Kyber, and the none scheduler. Each of these schedulers is suited to specific
    use cases, and there is no single recommendation, which can be applied to all
    situations. The MQ-deadline scheduler has good all-around performance. The BFQ
    scheduler is more oriented toward interactive applications, while Kyber and None
    are geared toward high-end storage devices. To choose a scheduler, it is imperative
    to know details about the environment, which includes details such as the type
    of workload, application, host system, and backend physical media.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还解释了内核中不同的调度选项。内核为单队列和多队列设备提供了不同的I/O调度程序。自内核版本5.0起，单队列调度器已经被弃用。多队列调度选项包括多队列Deadline调度器、BFQ、Kyber和None调度器。每个调度器适用于特定的使用场景，没有一个适用于所有情况的单一推荐方案。MQ-deadline调度器具有良好的通用性能。BFQ调度器更倾向于交互式应用，而Kyber和None则面向高端存储设备。选择调度器时，了解环境的详细信息至关重要，其中包括工作负载类型、应用程序、主机系统和后端物理介质等细节。
- en: This chapter concludes part two of the book, in which we delved into the block
    layer. In the next chapter, we’ll see the different types of storage media available
    today and explain the differences between them.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了本书第二部分的内容，我们深入探讨了块层。下一章，我们将看到当前可用的不同类型存储介质，并解释它们之间的差异。
- en: 'Part 3: Descending into the Physical Layer'
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：深入物理层
- en: This part will introduce you to the architecture and major components of the
    SCSI subsystem in the Linux kernel. You will also be introduced to the different
    types of physical storage media available today and the differences in their implementation.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分将介绍Linux内核中SCSI子系统的架构和主要组件。你还将了解当前可用的不同类型物理存储介质及其实现差异。
- en: 'This part contains the following chapters:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 7*](B19430_07.xhtml#_idTextAnchor124), *The SCSI Subsystem*'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B19430_07.xhtml#_idTextAnchor124)，*SCSI子系统*'
- en: '[*Chapter 8*](B19430_08.xhtml#_idTextAnchor134), *Illustrating the Layout of
    Physical Media*'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B19430_08.xhtml#_idTextAnchor134)，*物理介质布局示意*'
