["```\ndnf update -y\n```", "```\ndnf install httpd -y\n```", "```\nfirewall-cmd --permanent --add-service=http; firewall-cmd --reload\n```", "```\nsystemctl enable --now httpd\n```", "```\n<!DOCTYPE html>\n<html>\n<body>\n<h1> Running on web1</h1>\n</body>\n</html>\n```", "```\ndnf install -y haproxy\n```", "```\nfirewall-cmd --permanent --add-service=http; firewall-cmd --reload\n```", "```\nfirewall-cmd --permanent --add-port=8080/tcp; firewall-cmd --reload\n```", "```\n    systemctl start haproxy\n    ```", "```\n    systemctl status haproxy\n    ```", "```\n    systemctl reload haproxy\n    ```", "```\ndnf -y install keepalived\n```", "```\nsystemctl enable --now keepalived\n```", "```\njournalctl -u keepalived\n```", "```\ndnf -y install httpd\n```", "```\n<Location /server-status>\n   SetHandler server-status\n   Order Deny,Allow\n   Deny from all\n   Allow from 127.0.0.1\n</Location>\n```", "```\nfirewall-cmd --permanent --add-service=http; firewall-cmd --reload\n```", "```\nsystemctl start httpd\n```", "```\nwget localhost4/server-status\n```", "```\ndnf config-manager --enable  ol8_addons\n```", "```\ndnf install -y pacemaker pcs\n```", "```\npasswd hacluster\n```", "```\nsystemctl enable --now pcsd.service\n```", "```\nfirewall-cmd --permanent --add-service=high-availability ; firewall-cmd --reload\n```", "```\npcs host auth web1 web2 -u hacluster\n```", "```\npcs cluster setup webapp web1 web2\n```", "```\npcs cluster start --all\n```", "```\npcs cluster status\n```", "```\npcs resource defaults update\n```", "```\npcs property set stonith-enabled=false\n```", "```\npcs resource create AppIP ocf:heartbeat:IPaddr2 ip=192.168.56.204 \\\n cidr_netmask=24 op monitor interval=15s\n```", "```\npcs resource create AppWebServer ocf:heartbeat:apache \\\nconfigfile=/etc/httpd/conf/httpd.conf \\\nstatusurl=http://127.0.0.1/server-status \\\n op monitor interval=15s\n```", "```\npcs resource group add WebApp AppIP\npcs resource group add WebApp AppWebServer\n```", "```\npcs status\n```", "```\npcs cluster start --all\n```", "```\nsystemctl enable --now corosync\nsystemctl enable --now pacemaker\n```", "```\npcs resource refresh AppWebServer\n```", "```\npcs resource move WebApp web2\n```", "```\npcs constraint\n```", "```\n    mkfs.xfs -f -i size=512 -L glusterfs /dev/sdb\n    mkdir -p /data/glusterfs/volumes/bricks\n    echo 'LABEL=glusterfs /data/glusterfs/volumes/bricks xfs defaults  0 0' |sudo tee -a /etc/fstab\n    df command to verify that the filesystem is mounted, as seen in the following example:\n    ```", "```\n    [root@gluster3 ~]# more /etc/hosts\n    127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n    ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n    192.168.200.110 gluster1.m57.local    gluster1\n    192.168.200.125 gluster2.m57.local    gluster2\n    glusterfs and Gluster server software can be installed and started. This is done with the following commands:\n\n    ```", "```\n\n    ```", "```\n    firewall-cmd --permanent --add-service=glusterfs\n    firewall-cmd --reload\n    ```", "```\n    openssl genrsa -out /etc/ssl/glusterfs.key 2048\n    .pem file, since this is a self-signed certificate, you will need to enter your contact info into the certificate; however, it will work with the defaults!\n    ```", "```\n    [root@gluster1 etc]# gluster peer probe gluster2\n    gluster pool list command to see what nodes are peered with the node where you ran the command from; in the following example, the command was run on gluster2:\n\n    ```", "```\n    [root@gluster1 ~]# gluster pool list\n    UUID                                 Hostname        State\n    bc003cd0-f733-4a25-85fb-40a7c387d667                         gluster2        Connected\n    b13801f3-dcbd-487b-b3f3-2e95afa8b632                         localhost       Connected\n    ```", "```\n\n    ```", "```\n    /mnt mount point:\n\n    ```", "```\n\n    ```", "```\n    .pem files previously created and concatenate them into the /etc/ssl/glusterfs.ca file. This file should be placed on all nodes of the cluster.\n    ```", "```\n    data1 volume that was just created:\n\n    ```", "```\n    systemctl restart glusterd\n    ```", "```\n\n    ```", "```\n    gluster volume create data1  gluster{1,2}:/data/glusterfs/volumes/bricks/data1\n    ```", "```\n    gluster volume create data1 replica 2 gluster{1,2}:/data/glusterfs/volumes/bricks/data1\n    ```", "```\n    gluster volume create data1 replica 2 cluster{1,2,3,4}:/data/glusterfs/volumes/bricks/data1\n    ```", "```\n    gluster volume create data1 disperse 3 redundancy 1 \\ cluster{1,2,3}:/data/glusterfs/volumes/bricks/data1\n    ```", "```\n    gluster volume create data1 disperse 3 redundancy 1 \\ cluster{1,2,3,4,5,6}:/data/glusterfs/volumes/bricks/data1\n    ```", "```\ngluster volume create data1 disperse 3 redundancy 1 \\ cluster{1,2,3}:/data/glusterfs/volumes/bricks/data1 \\ cluster{1,2,3}:/data/glusterfs/volumes/bricks/data2\n```", "```\ngluster stop volume data1\n```", "```\ngluster volume add-brick data1 gluster3:/data/glusterfs/volumes/bricks/data1\n```", "```\ngluster volume status\n```", "```\n    nmcli connection add type bond con-name \"Bond 0\" ifname bond0 bond.options \"mode=balance-alb\"\n    ```", "```\n    enp0s3 and enp0s8:\n    ```", "```\nnmcli connection add type ethernet slave-type bond con-name bond0-if1 ifname enp0s3 master bond0\nnmcli connection add type ethernet slave-type bond con-name bond0-if2 ifname enp0s8 master bond0\n```", "```\n    nmcli connection up \"Bond 0\"\n    nmcli device command should look like the following:\n    ```", "```\ndnf -y install iptraf-ng\n```", "```\nping -f 192.168.200.179\n```", "```\ndnf –y install iperf3\n```", "```\nfirewall-cmd --permanent --add-port=8000-8010/tcp\nfirewall-cmd --reload\n```", "```\n[root@networking ~]# iperf3 -s -p 8001\n-----------------------------------------------------------\nServer listening on 8001\n-----------------------------------------------------------\n```", "```\n[root@networking ~]# iperf3 -s -p 8002\n-----------------------------------------------------------\nServer listening on 8002\n-----------------------------------------------------------\n```", "```\niperf -c 192.168.200.179 -p 8001\n```"]