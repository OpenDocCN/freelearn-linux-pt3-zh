<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer014">
			<h1 id="_idParaDest-127"><em class="italic"><a id="_idTextAnchor128"/>Chapter 5</em>: Patch Management Strategies</h1>
			<p>In day-to-day operations of your Linux systems probably the most common task for an average system administrator is going to be patch management (aka patching.) Unlike the Windows and macOS worlds, it is standard for the system administrator to handle a broad variety of operating system and application patching tasks covering both primary and third-party ecosystems. It is also standard for there to be built in, and sometimes third party, application management ecosystems to assist with this potentially daunting task.</p>
			<p>Patching, and of course system updates, are large parts of what we do and while it may feel mundane it is very important that it be something that we get right. And production Linux systems today have become much more complex and diverse than they were just ten years ago. And of course, patching has become more important than ever, something that we expect to only see increase over time as well.</p>
			<p>We will start by understanding how patches and updates are provided and what we mean by different installation methodologies.</p>
			<p>In this chapter we are going to learn about the following:</p>
			<ul>
				<li>Binary, Source, and Script Software Deployments</li>
				<li>Patching Theory and Strategies</li>
				<li>Compilation for the Administrator</li>
				<li>Linux Installation and Redeployment Strategies</li>
				<li>Rebooting Servers</li>
			</ul>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor129"/>Binary, source, and script software deployments</h1>
			<p>Software<a id="_idIndexMarker449"/> can<a id="_idIndexMarker450"/> come in all shapes and<a id="_idIndexMarker451"/> sizes. So, software deployments are not a one size fits all affair. The standard means of deploying software are directly as a binary package, through source code that needs to be compiled into a binary package, or as a script. We will dig into each of these as it is necessary to understand what each is and when they might be appropriate.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor130"/>Compiled and interpreted software</h2>
			<p>Many system<a id="_idIndexMarker452"/> administrators have never<a id="_idIndexMarker453"/> worked as developers and often are not aware of how software exists. There are two fundamental types of programming languages: compiled and interpreted.</p>
			<p>Compiled languages are written in one language (source code) and run through a compiler to produce binary executable code that can run directly on an operating system. This can be an oversimplification, but we are not developers and need only be concerned with the <a id="_idIndexMarker454"/>original code being compiled into a binary format. For Linux, this format is called <strong class="bold">ELF</strong>, which stands for <strong class="bold">Executable and Linkable Format</strong>. Compiled binaries run on the operating system.</p>
			<p>Interpreted languages are different. Instead of being compiled into a binary, they remain as written, and are processed in real time by a program called an interpreter which itself is a binary executable and which treats the code as an input file to process. So interpreted software requires that an appropriate interpreter for the language in which it is written is installed on the operating system in order for the software to work. For example, if you have a Python program that you want to be able to run, the system on which you want to run it will need to have a Python interpreter installed to process that file.</p>
			<p>Both approaches to software are completely normal and valid. As system administrators, we will work with both all of the time. Modern computers (and interpreters) are so fast that performance is of little concern between the two types and other concerns (mostly of the developers) are generally more important in deciding how a given piece of software will be written.</p>
			<p>Software is not quite this simplistic, there are bizarre concepts like code that looks like it is interpreted but is actually compiled at the last moment and run like a binary. Some languages like those based on .NET and Java are compiled, but not to a binary, and so are essentially an amalgam taking some benefits of both approaches.</p>
			<p>However, by and large we think of all software as either binary executable (runs directly on the operating system without assistance) or interpreted (requires a programming language environment or <em class="italic">platform</em> on which to run, on top of the operating system.) For the purposes of understanding code deployment, languages like .NET and Java, as well as <strong class="bold">JIT</strong> (<strong class="bold">just in time</strong>) compiled <a id="_idIndexMarker455"/>ones like Perl are lumped with interpreted languages due to behavior.</p>
			<p>Common languages that are generally precompiled include C, C++, Rust, and Go. Common languages that are interpreted, or act as though they are, include Python, PHP, Perl, and Ruby. To make matters more confusing, any interpreted language can be compiled. A standardly compiled language could even be interpreted! It is less <em class="italic">what a language does</em> as much as <em class="italic">what is it doing in this specific situation</em>? Essentially, any given language <em class="italic">can</em> be compiled or interpreted depending on how we treat it in practice. However, in the real world, no one is interpreting C++, and no one is compiling Python. But if you wanted to, it is possible.</p>
			<p>As system<a id="_idIndexMarker456"/> administrators, we really have no say<a id="_idIndexMarker457"/> in how software is built, we simply have to deploy software as it is given to us. What we have to understand most is how much control and influence we may have on the total platform. If we run a binary application, it is possible that our only real options are around the version of the kernel that we run. But if we are installing a PHP script, we may have to decide how to install PHP as well, what version to run, which PHP provider, and so forth. It can become rather complex in some situations.</p>
			<p>The majority of software that we will deploy is going to be binary in nearly all business scenarios. Often we might not even know (or care) about specific software as so much of the process will typically be handled for us. It is quite common to have to install software blindly.</p>
			<p>It is increasingly common for system administrators to install software that is built of scripts which are readily readable code. These files are simply processed by an interpreter. So, the software never exists on disk in binary form. Since modern computer systems are so powerful, the seemingly problematic lack of efficiency in this process is often no problem at all. Many popular software packages are now written and delivered in this way, so most system administrators will commonly work with script installations.</p>
			<p>In many cases, scripts are installed using the same automation methods as binary software packages making the entire process often transparent. As a system administrator, you might not always even know what kind of package you are deploying unless you dig into its underlying components. This becomes especially true of non-critical packages, and packages deployed using a dependency resolving system that handles any platform inclusion (for example, PHP or Python) for you, or if those dependencies were already installed for other components ahead of time.</p>
			<p>Today we expect that the installation of scripts will be a common task that may not represent the majority of all packages that are deployed to a system but that can easily form the majority of primary workload code on a system. By that I mean that the operating system, supporting utilities, and large system libraries will often be binary packages. But the final workload, for which we are running the system in the first place, will have a very good chance of being a script rather than a compiled binary.</p>
			<p>And, finally, the last type is the source code that cannot be run as is and must first be compiled into binary packages before being run. We are going to cover this topic, in depth, in just two sections, so I am only going to touch on it briefly here. You could argue that this approach is still a binary installation because the resulting deployed package is binary and <a id="_idIndexMarker458"/>that is totally correct. However, the workflow that must be followed to get<a id="_idIndexMarker459"/> and deploy that binary is quite different and makes this a valid tertiary deployment option. Some systems implement compilation steps automatically, and so it is plausible to have a deployment package that is compiling and installing a binary, and the system administrator is not even aware that it is happening.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor131"/>Misleading use of source installation</h2>
			<p>For reasons we <a id="_idIndexMarker460"/>will dig into a little later in this chapter, installing from source code generally developed a bad reputation. In some ways this was deserved, and in some ways it was not. </p>
			<p>Because source based installation is, for all intents and purposes, unique to the free and open source software world it was heavily targeted by vendors and IT practitioners in the 1990s and 2000s in an attempt to discredit it because it was cutting heavily into more traditional closed source products (and the jobs of people who only supported that software.) This was, of course, completely fabricated but as source licensing is complicated to understand it is easy to instill fear and doubt into those that fail to grasp the nuances of the topic.</p>
			<p>More importantly, however, source installation got a bad reputation because it was seen as being a generally unprofessional and unnecessary practice being promoted by system administrators who acted more like hobbyists and installed in this manner without real consideration for business needs. It was fun or looked impressive or their friends did it, so they did it, too. This, I am afraid to say, was broadly accurate. There was an era when lots of software was installed in unnecessarily complex and convoluted ways without regard for the business efficacy of the process. Not to say that source code compilation never has a place, it most certainly does. But even twenty years ago or more that place was in a niche, not the majority of deployment situations. So, in many ways, the bad reputation was earned honestly, but not completely.</p>
			<p>Today, however, it is not a big deal as source code compilation is nearly forgotten and almost no one today knows the standard processes with which to do it and the installation of the necessary tools is often banned or at least the tools are not readily available making<a id="_idIndexMarker461"/> compilation quite difficult, if even possible. Only software that has a strong need to be compiled is distributed in this fashion. So, the market has all but eliminated this in practice.</p>
			<p>But the fear and shaming of those that used to do compilation often still exists. Saying that someone is performing a source code installation remains a derogatory statement. Sadly, in an attempt to discredit even more software today, it has become common to use the term not to reference software that has to be compiled from source into binary, but to refer to script-based software which does not have a compilation step in this way. The term source code implies that the code has to be turned from source into binary. Scripts are not considered source code, at least not in this context. Technically, however, they are the original code, the source, but the implied bad step does not exist. But few people would follow up and can be easily misled by this little linguist trick. So, it is an easy way to take a manager who is just looking for an excuse to make an emotional decision, rather than more difficult rational, one, and mislead them. It sounds reasonable, and few will bother to actually think it through.</p>
			<p>The trick really comes from semantic shorthand, something that is always dangerous, especially in IT. The concern with self-compiling software has nothing to do with the availability of the source code, but from the need to compile it before using it. If that step did not exist, the existence of the source code is purely a positive for us. People then incorrectly refer to the compilation as a source code installation. This semantic mistake opens the doors for someone to take something that technically truly is a source code install, without compilation, and because the term has been used incorrectly for so long it becomes a negative connotation applied to the wrong thing, and no one understands why any of it is wrong or backwards.</p>
			<p>Linux offers us many standard enterprise methods for software deployments. The plethora of options, while powerful, is making it far more difficult to standardize and plan for long term support. </p>
			<p>One thing that is ubiquitous in all production Linux systems, regardless of the vendor, is a package management system that exists by default. More than anything else over the years, these package management systems have come to define one Linux based operating system from another. Several software packaging formats exist, but two, that is, DEB and RPM, have become dominant with all others remaining very niche.</p>
			<p>It is increasingly common for Linux distributions to either have multiple software packaging systems or to use multiple formats under a single software package management system. This variety is good as it gives us more options for how we may want to maintain<a id="_idIndexMarker462"/> specific packages on our systems, but it also means more complexity as well.</p>
			<p>As with all software deployments, we have a few standard concerns that are universal to all operating systems. First is whether software is self-contained or requires access to other packages or libraries. Traditionally, most software, especially on Linux and other UNIX-like operating systems, has been designed to reduce their size both to install and for the operating system itself, by utilizing extended system and third-party libraries (collectively called dependencies.) This means that we can have software that is as tiny as possible and other software that utilizes the same resources can share them on disk minimizing, sometimes significantly, how much we need to store and maintain. Updating a package or library for one piece of software will update it for all. The alternative is to have each individual software package come packaged with all of its own dependencies included with the package and available only within the singular package. This makes for much larger software installations and the potential for the same data to exist on the system multiple times. Possibly a great many times. This leads to bloat, but also makes individual software packages far easier to maintain as there is reduced interaction between different software components.</p>
			<p>For example, dozens or scores of software packages will potentially want to use the OpenSSL libraries. If each of twenty packages include OpenSSL, we have the same code stored on disk twenty times. Moreover, if an update for OpenSSL is released or, more importantly, if a bug is discovered and we need to patch OpenSSL we will need to patch it twenty times, not just once. Whereas if we used a single, shared OpenSSL library then whether we have one application that relies on it or one hundred, we would only need to patch the library to make sure that any bug or update had been addressed.</p>
			<p>Both approaches are completely valid. It used to be that shared libraries were a necessary evil because system storage was small and shared resources allowed for not just reduced disk usage, but better memory optimization because potentially a shared library might be able to be loaded into memory and shared by multiple pieces of software there as well. Today, we generally have more storage and memory than we can practically use, and this small efficiency is no longer necessary, even if potentially nice. Non-shared approaches trade this efficiency for the stability and flexibility of each package having their own dependencies included with them so that conflicting needs or an unavailability of shared resources does not pose a problem.</p>
			<p>The biggest<a id="_idIndexMarker463"/> advantage to shared resource approaches is probably that patching a known vulnerability can be far simpler. As an example, if we assume that OpenSSL (a broadly shared library) discovers a critical vulnerability and releases an update. If our systems have shared resources, we need only to find systems with OpenSSL installed and update that one package. All systems that depend on that package are automatically patched together. If OpenSSL were instead to be individually packaged with dozens of separate applications that all depend on it individually, we would need a way to identify that all of those packages use OpenSSL <em class="italic">and</em> patch all of them individually. A potentially daunting task. We rely on the package maintainers of every piece of software to do their due diligence, patch their dependencies quickly, and provide updated packages to us right away. Not something that happens too often. </p>
			<p>Often systems with multiple packaging approaches will use one type of package management and software repository system, such as DEB, when there are shared system components and many dependencies to handle. They will use another package management system, such as SNAP, when they are going to keep all dependencies included in the final package. But it is far more complex than that makes it sound, for example, make a DEB package that include all dependencies or one that expects them to be provided externally and shared. It is only a convention that DEB tends to be shared libraries for software packages. In Linux we also have a completely different set of concerns that you would be used to if coming from a Windows or macOS background: a software ecosystem tied to the operating system itself. In Linux, we expect our operating system (for example, RHEL, Ubuntu, Fedora, SUSE Tumbleweed, and others.) to not only include the basic operating system functionality and a few extremely basic utilities, but also a plethora of software of nearly every possible description including core libraries, programming languages, databases, web servers, end user applications, and on and on. In many cases, you might never use any software that did not come packaged with your Linux distribution of choice, and when you do add in third party software it is often a key application that represents a strategic line of business application or, somewhat obviously, is bespoke internally developed software.</p>
			<p>Because of this, when working with software on Linux we have to consider if we are going to use software that is built into the operating system, software that we acquire and install independently (this would include bespoke software), or a mix of the two where many components of the software come from the operating system, but some are provided otherwise.</p>
			<p>Digging into specifics of different packaging systems would not make sense, especially as they tend to overlap heavily in their general usage but be very unique as to real world usage. Now we <a id="_idIndexMarker464"/>know the options that exist for them. Software package management systems are more important in Linux than on other operating systems, such as Windows or macOS, because there is typically much more complexity in the big server systems that Linux tends to run, and the software being installed typically uses much broader sets of dependencies pulling components or support libraries from often many different projects. Linux packaging systems that maintain online repositories of the software, libraries, components, and so forth make this all reasonably possible. </p>
			<p>Probably the most important aspect of the large Linux software package management systems and their associated software repositories is that they allow the distribution vendors to assemble and test vast amounts and combinations of software against their exact kernel and component selection and configuration providing a large, reliable platform on which to deploy solutions.</p>
			<p>Best practice here is difficult. Really, we are left only with rules of thumb, but very strong ones. The rule of thumb is to use the vendor repos whenever possible for as much software deployment as you can. This might seem simple and obvious, but surprisingly there are a great many people who will still go and acquire software through a manual means and install it without the benefit of vendor testing and dependency management.</p>
			<p>The real best practice is, as you might expect, to get to know the package management ecosystem of your distribution(s) of choice so that you are well prepared to leverage their features. Features tend to include logging, version control, roll back, both patching and system update automation, check summing, automatic configuration, and more.</p>
			<p>The more common and foundational a software component is, the more likely you should have it supplied by the vendor as part of your distribution. The more niche and close to the line of business, the more likely that it will be acceptable to install it manually or through a non-standard process as end user products are far less likely to be included in a vendor software repository and are much more likely to have a need to carefully manage versions and update schedules rather than primarily caring about stability and testing with respect to the rest of the system.</p>
			<p>It is not uncommon for software vendors making products that are not included in distribution repositories to make and maintain their own repositories allowing you to configure their <a id="_idIndexMarker465"/>repository and still manage all installation and patching tasks via the standard tools.</p>
			<p>Software deployments are made up of so many special cases. It is tempting to want to delivery standard, clear <em class="italic">always do this</em> style of guidance, but software just does not work that way. Learn the tools of your system, use them when you can, be prepared to do or learn something unique for every workload that you have to deploy. Some, like WordPress, may turn out to be so standard that you never need to do anything but use the distributions own packages. Others may be so unique that you simple deploy a basic operating install, download the vendor's installer and it installs every needed piece of software, and potentially even compiles it! It just all depends, and more than anything else it will depend on how the software vendor chooses to build and package the software and if your distribution decides to include the package in the operating system or will require it separately.</p>
			<p>Now that we have a scope of the software installation processes and considerations, we can dive into the real heart of our concerns with patching and updates.</p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor132"/>Patching theory and strategies</h1>
			<p>One might think<a id="_idIndexMarker466"/> that patching is pretty straightforward, and that there would be little to discuss. This is not the case. In fact, if you talk to several system administrators you are bound to get some pretty widely varying opinions. Some people patch daily, some weekly, some wait as long as they can, some do so only haphazardly, and some believe that you should never patch at all (hey, it if isn't broke, don't fix it!)</p>
			<p>We should first establish why we patch our software. Patching, as opposed to updating or upgrading, implies that we are applying minor fixes to software to fix a known problem or bug but not to implement new features or functionality. Adding new features is generally considered to be an update. </p>
			<p>Most software vendors and operating system vendors honor this system and maintain patching systems that only address security or stability issues in their software between releases. In the Linux ecosystem this is primarily tied to an operating system release. So, for example, if you use Ubuntu 22.04 and you use its own patching mechanisms to patch the software that comes with the distribution, then you will safely get nothing but security and stability fixes for the existing software versions and not new versions, features, or functionality. The logic here is that upgrading to a new version may break the software, change usability, or cause other products that depend on that software to fail.</p>
			<p>Upgrading to new versions are assumed to only happen when a new version of the operating system itself comes out and then the operating system and all the packages included in it can be upgraded together at the same time. This allows the operating system vendor to, theoretically, test the software together as a singular package to give the customer (you) confidence that all of your software components will work together even after you move everything to new versions.</p>
			<p>So, we assume that if a patch has been made available to us, then this indicates that a vendor, quite likely in conjunction with our distribution vendor, has identified a problem that needs to be fixed, a fix has been created, it has been tested, and it is now being distributed. However, even with testing, many eyes watching for errors, and the intent to do nothing but fix known bugs, things can still go wrong. Both the patch itself can be bad and the process of patching can run into unexpected problems. This means that we have to remain cautious about patching no matter how good the intentions are of those providing the patches.</p>
			<p>When patching, we are left with two opposing concerns. One is that if the system is currently working for us, why introduce the risk (and effort) of the patching process when we do not have to. On the other hand, why keep running a system where known bugs are left exposed once a patch has been made available to us? We have to look at the concerns and pick a reasonable course of action.</p>
			<p>Risk aversion is really not a key concern here, we are not looking at expense versus risk but rather two nearly equal courses of action (from an effort and cost perspective) with two very different outcomes. We need to pick the approach that reduces risk the most for our business and that is all. It is not how risk averse we are but what our risk profile is like that matters most. If our business is heavily susceptible to small downtime events, then patching may be deprioritized. If our company has highly sensitive data that is a likely target or we are very sensitive to public relations blunders in the event of a breach, then we <a id="_idIndexMarker467"/>might patch very aggressively. To make a sensible determination we must understand how each approach creates and mitigates different risks and how those risks affect our specific organization.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor133"/>The risk of delayed patching</h2>
			<p>Simply pushing off<a id="_idIndexMarker468"/> patching does not eliminate certain types of risks. It may prove to have benefits but may also introduce even more risks depending on the situation. Under normal circumstances, new patches are made available with great frequency, potentially as often as multiple times per day, but at least multiple times per month. </p>
			<p>If we patch often, such as once a week, then theoretically we will normally have to deal with extremely few patches at any given time and any break or incompatibility will be relatively easy to identify and to roll back as there are so few patches to work with.</p>
			<p>If we save up patches over a period of time and only patch, for example, once a year then we have a few problems. First, the patching process may take quite some time as many patches may be needed. Second, if there is a break it may be very difficult to identify the offending patch as it could be lost in a sea of patches that all have to be deployed. And third, the greater volume of changes made at once, as well as the increased <em class="italic">drift</em> from any tested scenario (few, if any, vendors will test a system that is specifically as out of date as yours against a large volume of sudden changes) means that the chances of there being a break caused by the patching process is greater.</p>
			<p>Delaying patches, therefore, becomes a self-fulfilling prophecy in many cases where neigh sayers who avoid patching because <em class="italic">patches break things</em> will often see this come to pass because they create a situation where it is more likely to occur.</p>
			<p>There is no one size fits all approach. Every organization has to tailor the patching process to meet their needs. Some organizations will avoid patching altogether by making stateless systems and simply deploying new systems that were built already patched and destroying older, unpatched instances neatly sidestepping the problem altogether. But not every workload can be handled that way and not every organization is able to make that kind of infrastructure leap to enable that process.</p>
			<p>Some organizations run continuous patch testing processes to see how patches will be expected to<a id="_idIndexMarker469"/> behave in their environment. Some just avoid patching completely and hope for the best. Some patch blindly. We will discuss all of these options.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor134"/>Avoiding patches because of Windows</h2>
			<p>A culture<a id="_idIndexMarker470"/> of <em class="italic">patching avoidance</em> has sprung up in recent years within the ranks of system administrators. Given how critical patching is in general and how central it is to our careers this seems counter-intuitive. No one could be as strong of a cheerleader for rapid, regular patching as the system administrators.</p>
			<p>In the Windows world, patching is very unlike what it is like in the Linux, or any other world. It is often delayed, secretive, slow, unreliable, and worst of all buggy and error prone. Patching in Windows was always problematic but during the 2010s became so bad that it is no longer deterministic, can take much longer than simply deploying new systems, and fails with great regularity. </p>
			<p>And failures with Windows patching can mean almost anything from the patch simply failing to install and needing to devote resources to getting it to work, to causing software to fail and no longer function. Some patches can take many hours to run only to fail and then take hours to roll back!</p>
			<p>Because of this it has become common and almost expected that system administrators in the Windows realm will range from gun-shy about patches, to practicing total avoidance. This has sprawled to not just include patches but full system version updates as well. So now finding Windows systems that are years or even a decade out of date is becoming commonplace creating more and more security vulnerabilities throughout the ecosystem.</p>
			<p>Microsoft then has responded, not by fixing the patching problems, but by attempting to force patching without permission, and obscuring the patching process creating even more reliability problems and in many cases breaking patch management systems so dramatically that even administrators who desire to stay fully updated are often unsure how to do so or are simply unable to do so.</p>
			<p>These problems are unique to Microsoft today and are mostly unique to Microsoft in the modern era. We must not allow an emotional reaction to a uniquely bad situation influence our practices in the Linux or other realms that are not impacted nor influenced by Microsoft. Outside of Microsoft's isolated piece of the industry no other ecosystem has experienced these types of issues. Not in Ubuntu, Red Hat, Fedora, SUSE, IBM AIX, Oracle Solaris, FreeBSD, NetBSD, DragonFly BSD, macOS, Apple iOS, Android, Chromebooks, and on and on. Patching always carries risk and we should be aware of this, but Microsoft's problems are unique and have nothing to do with our practices in the rest of the industry. </p>
			<p>Patching can be <a id="_idIndexMarker471"/>automated or manual. Both approaches are perfectly fine. Automation requires less effort and can protect against patching being forgotten or deprioritized. In a large organization formal patching procedures that require manual intervention may be simple to ensure consistency, but in small organizations with just a few servers it can often be easy to overlook patching for months. When looking at manual versus automated patching just consider the potential reliability of the process and the cost (generally in time) of the human labor involved.</p>
			<p>The benefit to manual patching is that you have an opportunity for a human to <em class="italic">inspect</em> each package as it is patched and to test the system in real time as it occurs. If something was to go wrong, every detail of the patching process is fresh in their memory as they just performed it and they know exactly what to test and what to roll back or address if something fails.</p>
			<p>Automation benefits from happening automatically, potentially at very predictable times, and being able to happen even if no humans are present to do the work. Scheduling automated patching for evenings, overnight, weekends, or holidays can minimize impact to humans while speeding the patching process. Automated patching is unlikely to be missed and it is easy to send alerts when patching happens or when there are problems caused by patching.</p>
			<p>In most cases automation is going to be preferred to manual patching simply because it is less costly <a id="_idIndexMarker472"/>and nearly all manual benefits can be automated in some form as well, such as by having a human on standby to receive alerts in case of problems. </p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor135"/>Testing patches is rarely feasible</h2>
			<p>Everyone talks<a id="_idIndexMarker473"/> about how important it is to test patches. System administrators often demand it and attempt to refuse to apply patches until testing can be done (a convenient way to avoid having to patch systems at all.) And if patches ever go wrong, management will almost always demand to know why patch testing was not done beforehand. </p>
			<p>Here is the harsh reality: there is no practical or realistic means of testing patches on any scale. There, I said it. Say it out loud, go tell your bosses. The cost, both in time and money, to test patches is much larger than anyone believes. In order to thoroughly test patches, we need a replicated environment that gives us a mirror of our production environment so that we can test the patches that we want to deploy against software, hardware, and configurations. Attempting to shortcut this process does not work as it is the interplay of all these parts that make testing important. If you change anything, you may totally nullify the benefits (or worse, create a false sense of security) to a very expensive process.</p>
			<p>In a real-world environment, every system is effectively unique. There are exceptions, but generally, this is true. There are so many variables that are possible, including hardware manufacturing dates, varying parts, firmware versions, and on up the infrastructure stack (that is, code and components that sit closer to the final application at the top of the stack). Computer systems are so complex today that there are millions of variables that could result in a combination that causes a bug in a patch to be triggered.</p>
			<p>This is one of the reasons why virtualization is so important, it creates a middle layer of standardization that allows some of the more complex parts of the system to be standardized. So at least one portion, a very complex portion involving many drivers, can be reduced in complexity.</p>
			<p>In very rare organizations real patch testing is done. Doing so is costly. Generally, this involves carefully replicating the entire production environment right down to matching hardware versions, firmware revisions, patch history, and so forth. Every possible aspect should be identical. Patches must then be run through a battery of tests quite quickly in order to test any given patch in the company's range of scenarios.</p>
			<p>In practical terms this means duplicating all hardware and software and having a team dedicated to <a id="_idIndexMarker474"/>patch testing. Very few companies can afford that and even fewer still could justify the small amount of potential protection that it might provide.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor136"/>Timeliness of patching</h2>
			<p>Patching is an <a id="_idIndexMarker475"/>activity that generally has to happen extremely quickly, and as the industry matures the importance of rapid patching continues to increase. Those of us trained in the 1990s and earlier will tend to have memories of a time when patching a system had almost no time sensitivity because most computers were offline, and patches were almost exclusively for stability issues that if you had not experienced already that you were unlikely to experience. So, waiting months or years to patch a system, if you ever did, tended to not be a very big deal.</p>
			<p>Oh how times have changed. Software is so much bigger and more dynamic today, there are so many more interconnected layers, and all but the rarest of computer systems are now on the Internet and potentially exposed to active security threats and a dynamically changing environment all of the time. Everything, as pertains to patching, has been flipped on its ear over the past two decades, although most of the significant changes had happened by around 2001.</p>
			<p>Today patching tends to focus heavily on shoring up security gaps that have been discovered recently and every step of the process is one of rushing the patch to market before the bad guys are able to discover the vulnerability and exploit it. Bad actors know that patches will come for any vulnerability that they find quite quickly and so exploitation is all about speed. In some cases, it is the release of a patch itself that alerts the greater community to the existence of a vulnerability and so the action of releasing a patch triggers a sudden need for everyone to apply said patch in a way that was not necessary just hours before.</p>
			<p>Because of this, patching quickly is incredibly important. Attacks based on a patch are most likely to either already be occurring and will increase in a last-ditch effort to leverage a soon-to-be-dwindling vulnerability or will soon start as a previously unknown vulnerability becomes public knowledge. For many, this means that we want to consider patching in terms of hours rather than days or longer. We still have to consider potential stability risks or impacts that might occur during production hours, so patching immediately is rarely an option, but it is certainly possible when it makes sense.</p>
			<p>In Linux, because patching is generally quick and easy and almost always reliable, it is reasonable to consider patching throughout the production day in some cases, and daily patching in most other cases. Potentially smart approaches might include using a built-in randomizer to patch, somewhat randomly (for reasons of system load reduction) every four to eight hours, or having a scheduled patch time every day at ten in the evening or other appropriate time.</p>
			<p>In extreme environments, patching on a weekly schedule is possible and this was popular in the <a id="_idIndexMarker476"/>enterprise just one to two decades ago. Today, waiting up to six days to patch a known vulnerability borders on the reckless and should be done only with caution. Six days is a very long time in the world of exposed system vulnerabilities.</p>
			<p>The choice of time frames is generally based on workload patterns. Some workloads, like email or an inventory control system, might have little susceptibility to momentary disruption and can be rolled back or failed over quickly. So, patching these in the middle of the day might make sense. Most companies have their workloads have a cyclical use pattern throughout the day and can predict that an application becomes very lightly used from one to two in the morning, or perhaps that by seven in the evening every single user has signed out and even a ten-hour outage would be noticed by no one.</p>
			<p>Whether or not intra-day patterns exist or not we almost always see inter-day patterns on a weekly basis. A workload might be light on the weekends or heavy during the week or vice versa. Once in a while, especially with financial applications, the pattern is more monthly based with the beginning of the month probably seeing a heavy load and the middle of the month being light.</p>
			<p>Any given workload will typically need to be assessed to understand when patching is reasonable and practical. Sometimes we have to be creative with our timing to be able to get patching in when the workloads allow. If workloads cannot be interrupted for patching, then they cannot experience other, less planned, downtime events either and we should have a continuity plan that allows us to patch, repair, or failover in case anything happens. In most cases we can, when necessary, do zero impact patching through these methods. </p>
			<p>The old idea that patching can be saved as a special monthly activity or done only when a specific need is identified for the organization is no longer realistic. Waiting that long leaves systems dangerously exposed and any workload that claims to have only one time a month when it can be patched should be questioned as to how any workload can be both important, and unable to be patched. Conceptually the two things cannot go together. The more important a workload is, the more important that it be patched in a timely fashion.</p>
			<p>A common excuse for slow patching processes is that testing is required before rolling out a patch. On the surface this makes sense and sounds reasonable. You could probably sell this idea to non-technical management. There is nothing wrong with wanting to test a patch. But we have to consider that we are already, presumably, paying (or getting for free) a distribution vendor to test patches before we receive them. Those enterprise operating system vendors (Canonical, IBM, SUSE, and others.) have far more skills, experience, and resources to test patches than any but the largest organizations. If our testing does not add something significant to the extensive testing that we are already <a id="_idIndexMarker477"/>relying on them for, then our own internal testing process should be avoided, in order to avoid wasting resources and putting the organization at unnecessary risk by not getting potentially critical patches deployed promptly.</p>
			<p>Rapid, light testing can be reasonable if it is kept light enough and never used as an excuse to avoid timely patching. A common approach to useful patch testing is to have just a few systems that represent typical workload environments in your organization that run demonstration workloads where you can test each patch before it is rolled out to allow you to observe a successful install and test the patches at the highest level. This could be as little as a single virtual machine in a smaller organization. Consider if any testing at all is valuable, and if it is, keep the testing as light as you can to ensure that production patching happens as quickly as it can within the confines of the needs of your workloads.</p>
			<p>Standard patching strategies will also generally suggest that you start either with highly vulnerable workloads or with low priority workloads to focus on shoring up exposures or using low priority workloads as tests of Guinea pigs for more critical workloads. If your environment has one hundred virtual machines to patch, you can probably arrange a schedule that allows you to patch systems that are not critical first and slowly build up confidence in the patching process as you approach the patching of more critical or fragile workloads.</p>
			<p>Consider patching to be unquestionably one of the most important, and truly simple, tasks that you will be doing as a system administrator. Do not allow emotions or irrational advice from businesses or system administrators that do not understand what patching is (or influence from Windows admins) to lead you astray. Patching is hyper-critical and any organizational management that is not supportive of patching processes does not understand the risk and reward valuation of the process and we need to be prepared to explain it to them.</p>
			<p>Find a patching process that meets the needs of your organization. You do not need to patch on a rigid schedule, you do not need to patch the way that other organizations do. Find the testing that is adequate for you and the manual or automated patching process that keeps your systems updated without overly impacting the organization.</p>
			<p>We should have a pretty solid handle on patching concepts at this point and you probably even feel a certain sense of urgency to go examine your current servers to see how recently they <a id="_idIndexMarker478"/>have been patched. This is understandable and if you want to look at them now before continuing on, I am happy to wait. Better safe than sorry. When you return, we will talk about software compilation and relate that process to patching.</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor137"/>Compilations for the administrator</h1>
			<p>It was not all that<a id="_idIndexMarker479"/> long ago when major system administrator guidelines included a requirement that any mid-level or senior administrator had to be well acquainted with the details of standard software compilation processes. Of course, all knowledge is good, and we would never say that it should not be learned at all. However, even at the time, this seemed like an odd amount of <em class="italic">under-the-hood</em> development knowledge and knowledge about the packaging of individual software solutions expected to be known by a person in a non-development role. </p>
			<p>It would not be unlike if when ordering a new car from your favorite car company that it was expected to be delivered as parts and that every potential new car owner would be expected to assemble the car before driving it. It is important to note that this process was only ever possible in the open-source world and that the majority of software outside of the Linux space and even a significant portion within it cannot be compiled by the system administrator at all. So, the entire concept of this requirement was for an almost niche scenario and was not broadly applicable to system administration in the general sense, which alone should have been a serious red flag to organizations promoting this as an education and certification standard.</p>
			<p>As a system administrator, the idea that we would need to take source code from developers and custom compile it, using a compiler that we provide, in our own environment feels almost absurd. In the Windows world this would be all but impossible, who would have access to an appropriate compiler or any of the necessary tools? In Linux and BSD systems it is often plausible because a compiler or multiple compilers may be included with the base operating system.</p>
			<p>There was a time when compilation on target systems had some value. Often, generically compiled software was inefficient by necessity in order to support nearly any potential hardware, and with on-target compiling, we could leverage every last bit of performance from our very specific hardware combination. However, this was also in an era of long compile times due to limited CPU power and would result in systems taking potentially days to install rather than minutes. Software deployment was a lengthy, complex, and error-prone <a id="_idIndexMarker480"/>task. In the modern world, it would be almost unthinkable to waste the time and resources necessary to compile most software. When we can deploy entire fleets of servers in just a few minutes while using extremely little system resources, tying up many CPU cycles and spending hours to do the same task does not make sense from a resource perspective alone.</p>
			<p>However, compilation carries far more risks than just wasting time and system resources. It also means that we, potentially, get slightly different resulting software on different systems that we run, between times that we deploy, from systems that we have tested, or, more importantly, from systems that the vendor has tested. As discussed earlier, with the challenges that we face with testing patching scenarios, compilation makes testing vastly harder. Likewise, patching becomes much harder.</p>
			<p>Some software requires compilation today for deployment, generally software that leverages specific hardware drivers. This kind of software tends to be desktop software and not something that we would see on a server. That does not mean that we will never encounter it, but it should be quite uncommon. It is not technically wrong to self-compile software, but you should only be doing so if it is absolutely required and serves a very necessary purpose. It should not be done casually and there is no reason for a system administrator today to have any knowledge of the compilation process.</p>
			<p>Several additional new factors have arisen that make compilation as a standard process less possible. Twenty years ago, we had one major compiler in the Linux ecosystem, and it was assumed that all software being deployed as code would be written in C and targeted (and tested) by the development team against that one compiler. There was a very standard toolchain that convention said would be what was used, and it was almost always correct. Today, not only are there multiple standard compilers now being used with different projects using different ones for testing, but also several common compiled languages that are now in common use with their own compilers and tool chains. </p>
			<p>Compilation was never a single, standard process as was implied. However, the convention meant that it was very nearly so. Today. it is a scattered process for all of the reasons mentioned earlier. In addition to this, many projects that do require compilations by end users now build in a compilation process so that it acts much like a standalone deployer rather than requiring the system administrator to have any knowledge of the compilation or even be aware that the software being deployed is compiling! The world has changed in nearly every way.</p>
			<p>In the real world I have worked on tens of thousands of servers across thousands of clients and have not seen the assumed <em class="italic">standard</em> compilation process used in any organization for more than seventeen years and in the years before that it was still so rare as to be able to<a id="_idIndexMarker481"/> be generally ignored and always used under questionable circumstances when a system administrator was acting as if the business was a personal hobby rather than a serious business.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor138"/>The compilation era</h2>
			<p>In the early days of<a id="_idIndexMarker482"/> Linux, being able to compile your own software commonly was a major feature compared to closed source systems, such as Windows and Netware, where compilers were not free, and the operating system code was not made available. It meant being able to move between different architectures without too much effort and at the time system resources were at a premium so the small performance advantage possible from unique compilation sometimes meant a real difference in software performance. System administrators used to be passionate about compiler flags and versions. </p>
			<p>This trend was so dramatic that even entire operating systems were released around the concept. Most notably was Gentoo Linux where the entire operating system was custom compiled every time that it was deployed. This often led to people discussing how many days it would take to install a full operating system. The investment in initial installation was significant.</p>
			<p>In the 1990s, it was not uncommon for operating system installations to take many days. We rarely virtualized and so installations were often from physical media onto unique hardware which was time consuming even when things went well and there were often installation hiccups that would cause you to have to attempt an install multiple times. In that environment, also taking the time to compile software, or even the entire operating system, was not as crazy as it is today. But rest assured, it was not completely sane, either.</p>
			<p>The same era that saw compilation make sense because of resource constraints coincided with the pre-internet office world and the nascent Internet world where environmental threats looking to exploit unpatched computers were rare. Of course, computer viruses existed and were well known, but the ability to avoid them by not sharing physical media between systems allowed for high confidence in avoiding infection when well managed. So, the difficulties of patching custom compiled software did not present a problem most of the time. This was the era of <em class="italic">set and forget it</em> software. Compiled software was more likely to be forgotten than to be maintained. The effort of installation made the potential effort of patching monumental and very risky. When you custom compile code yourself there is a lot that can go wrong that could result in you <a id="_idIndexMarker483"/>being left without a working system.</p>
			<p>CPU and RAM resources used to be so tight on the majority of systems that, in most cases, having to wait days longer to be able to put a system into production was considered worth it, even if only to gain one percent in additional performance. What was often ignored was the fact that if it took all of that time and all of those resources to compile the initial installation, that those resources or more might be needed over and over again in the future to compile future patches or updates. This carried a truly enormous risk and would commonly result in a complete lack of patching as, once deployed, there was little means to bring the system down for hours or days to attempt a compilation step in the hopes of being able to update the system. </p>
			<p>Like so much in IT, there is a strong desire to build a house of cards and hope that, when it falls down, it is long enough in the future to qualify as someone else's problem. And sadly, that became a viable strategy as companies rarely associate failures with those that caused them and often blame, at random, whoever is at hand. Because of this, creating risky situations is often beneficial because any benefits will be attributed to the person who implemented it, and any disasters it causes will be attributed randomly at a later date.</p>
			<p>A system administrator today would still do well to learn traditional software compilation, if only to understand historical perspective and be prepared for any unusual situation that might arise. But today (and for many years previously) compiling our own software as a standard process should be avoided as a rule of thumb, it is simply not a good use of resources and introduces too much risk for no real benefit.</p>
			<p>When compilation is done today you cannot expect to be able to follow a generic process blindly as was often assumed (incorrectly) in the past. That convention, that protected so many administrators who just got lucky, is not a convention any longer. To compile software today we need instructions and guidance from the developers to have any hope of realistically knowing what tools, configurations, libraries, languages, and external packages may be required and how to make compilation work. There are so many potential moving parts and all of the knowledge here is developer knowledge, not IT knowledge at all, let alone system admin knowledge. System administrators have so much information that they need to know and deeply understand, it is one of the largest and most challenging technical arenas. The idea that system administrators should additionally learn deeply the knowledge and skills of a totally different field never made any sense. Why <a id="_idIndexMarker484"/>would developers even exist if all system administrators could perform these jobs in addition to their own duties? Software engineering is a huge field that requires immense knowledge to do well; it is both utterly absurd and insulting to developers to imply that a different discipline can perform their role casually without needing the same years of training and full-time dedication.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor139"/>Compilation by engineering department</h2>
			<p>There is a middle<a id="_idIndexMarker485"/> ground that we should mention that can make sense. That is having an engineering group that takes software in code form from developers (internal bespoke teams or open-source projects generally) and performs internal compilation either as an additional security step or for an extreme level of performance tuning or simply because the software in question requires special compilation such as when tied to kernel versions or drivers. </p>
			<p>By having a central group that is doing internal compilation and packaging source code into resultant binaries for the administration team they allow system administration processes to remain focused on standard, repeatable, and fast to deploy binary mechanisms while the organization gets the advantages of custom compilation.</p>
			<p>This approach is typically reserved for only the largest of organizations capable of maintaining rapid software packaging workflows so that patching, testing, and customization happen almost as fast as it would, should it be done by the software vendors. At large scale it can be beneficial.</p>
			<p>This process works because it does not put software compilation and packaging into the hands of administrators where it is awkward and highly problematic. </p>
			<p>Best practice is to compile software only when it is a requirement and not to do so otherwise, unless you have a department that can handle packaging compiled software rapidly and reliably fast enough to account for proper patch management needs and the overhead of doing so is justified by a savings at scale.</p>
			<p>Compilation is useful knowledge, but just because you <em class="italic">can </em>compile software does not mean that you <em class="italic">should</em>. Keep this skill in your back pocket for when you really need it or are directed to do so <a id="_idIndexMarker486"/>by your software vendor. Next, we will talk about how to deploy our Linux distribution itself and, even more importantly, how to <em class="italic">redeploy</em> a distribution after a disaster has struck.</p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor140"/>Linux deployment and redeployment</h1>
			<p>Today we have<a id="_idIndexMarker487"/> many <a id="_idIndexMarker488"/>potential methods for deploying, and just as importantly, redeploying, our servers (or our workstations, for that matter.) In my opinion, this topic was relatively unimportant in the past because most companies depended on <em class="italic">slow deployment</em> methods and their disaster recovery models depended on restoring, rather than redeploying, their systems. But there are so many more modern disaster recovery methods today that depend on the ability to rapidly deploy servers that we have to look at this topic with a new eye.</p>
			<p>With modern deployment technologies and techniques, it is not uncommon to be able to deploy a new base operating system in a matter of seconds when in the past, even heavily automated systems would often take many minutes if not hours (not even considering the possibilities that would come with custom compiled systems!). Of course, computers are just faster today, and this plays a role in speeding deployments. Vendors have improved installation procedures as well. This is not unique to Linux, but nearly any operating system.</p>
			<p>Even doing the most traditional or <em class="italic">ISO-based install</em> where we take installation media in the form of a DVD image and install from USB media or virtual media, we can generally do a full operating system install from scratch, manually, in a matter of minutes. Perhaps ten to fifteen minutes on normal hardware. This is pretty fast compared to how installs were done nearly twenty years ago.</p>
			<p>It was traditional in larger environments to use something akin to a response file to make these installation processes faster and more repeatable. This process would generally mean storing your installation ISO files somewhere on your network and storing a set of installation instruction files somewhere and defining systems in a list somewhere else often listing MAC addresses to assign appropriate configuration files to use. Essentially just automating the human responses used when performing a traditional install. Effective, but clunky.</p>
			<p>Today any form<a id="_idIndexMarker489"/> of ISO, or<a id="_idIndexMarker490"/> similar media-based installation is typically reserved for truly manual installations that are generally done by very small companies (that only ever build a few servers, and each is likely completely unique anyway) or special situations. There is nothing wrong with the older response automation methodology, but so many newer options exist that it has simply fallen by the wayside and continues to lose traction as a popular installation method. It was truly most effective in the pre-virtualization days when few installation automation options existed, and installations were necessarily one operating system per physical device making MAC address-based management highly effective. Today this would work effectively for platform (hypervisor) installation, but not so much for operating system installation.</p>
			<p>The advent of virtualization meant two big things changed. First, hypervisors installed to the bare metal of a physical server are rarely customized outside of the most basic options making the need for automation less (and the entire installation process is much smaller). And second, that operating system installation is now done in a non-physical space opening the field to more esoteric installation techniques than were previously available.</p>
			<p>In the virtual space, we can continue to install systems manually, and many people do. We can continue to use response files, and while I know of no one that continues to do this, I am confident that it is still widely practiced, especially as so many large-scale semi-automated deployment systems for this were in place already. However, now, we have readily available options to use pre-built system images that can be called from a library to install even faster. With a method such as this, an already built system is simply copied or cloned to make a new system. The initial image can be preconfigured with the latest patches and custom packages and often installed in under a minute; sometimes, in just a few seconds. If it is using certain kinds of storage, it can perform so quickly as to appear instantaneous. With containers, we can, sometimes, observe new systems initialized so quickly that we can barely detect that there is a build process at all (because, for all intents and purposes, there is not).</p>
			<p>What method you choose to use for deploying your servers is not the most important factor. All these deployment methods, and more, have a place. What we do want to consider is how quickly and reliably new systems can be built using the processes that you choose. When a new workload is needed, it is good to know that we can build a new server in a<a id="_idIndexMarker491"/> certain <a id="_idIndexMarker492"/>amount of time and feel confident in the final configuration of it. If a well-documented manual process achieves an acceptable result, then that is fine.</p>
			<p>For many businesses, the ability to deploy rapidly is not very important. What becomes extremely important is the ability to redeploy. Of course, for certain types of applications, like those well suited to cloud computing, rapid initial deployments are absolutely critical, but this remains a niche and not the norm and will for the foreseeable future. But redeploying implies typically that some level of disaster has struck and that a system has to be returned to functionality and in that case, it is exceedingly rare that we are not under pressure to put the system back into production as quickly as possible.</p>
			<p>So, it tends to be that redeployment speed, rather than deployment speed, is what matters more in our environments. However, because disaster recovery is rarely thought about in this way, the importance of this is often ignored during the only time that it can be realistically affected – meaning, we can only really address this during our initial design and implementation phases but typically ignore it until it is too late to effectively change. Additionally, redeployments need to be done with confidence so that what we have built quickly is built as we expect and behaves as we expect. Under these rushed and pressured conditions, it is easy to miss steps, ignore processes, take shortcuts, and make mistakes.</p>
			<p>The faster and more automated our systems are, the better chance we have to being able to turn out the same identical system time after time even under highly pressured circumstances. We should be planning for this situation when we make our initial deployment plans. Being able to recreate systems, without needing to resort to backup and restore mechanisms, can be a game changer for many companies who often feel forced to rely on a single approach to bring systems back online, when in many cases, superior alternatives may exist. We will dig much deeper into this when we talk about backups in a future chapter.</p>
			<p>Processes that allow us to recovery quickly also give us greater flexibility throughout our professional workflows. The ability to test patches, deployments, configurations, build temporary systems, and so forth. Flexibility is all about protecting against the unknown.</p>
			<p>Best practices in deployment processes are all about evaluating the time to engineer reliability deployment methods that work best for your environment and to streamline this as is sensible to allow for being able to restore a baseline operating system in a time period that makes the most sense for your environment.</p>
			<p>Some environments are able to build new servers, and configure them for their necessary workloads, so quickly that they actually choose to do this over performing activities such as patching, updates, or potentially even reboots! Instead, they will rapidly build <a id="_idIndexMarker493"/>and <a id="_idIndexMarker494"/>deploy a completely new virtual machine or container and destroy the old one. A really effective practice if you can make the process work for you.</p>
			<p>In our next section, we will discuss the importance of rebooting and testing your environment under regular, frequent, and planned conditions.</p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor141"/>Rebooting servers</h1>
			<p>Ask your average<a id="_idIndexMarker495"/> system administrator, or even a non-technical but interested third party, and they will tell you the importance of long uptimes on servers and how they want to see those ultra-high <em class="italic">time since reboots</em> on them. It feels natural, and nearly everyone brags about it. <em class="italic">My servers have not needed a reboot in three years!</em></p>
			<p>There are two key problems with this, however.</p>
			<p>The first problem is that <em class="italic">time since reboot</em> carries no business value, and business value determines IT value. So why should we care, let alone brag, about something that has no value? It might be interesting to know how long a system has managed to stay online, but an investor is not going to reap a reward from the fact that a computer system has gone an extended period of time without a reboot. We work for the good of the business, if we start to care about something other than resultant business value, we have lost our way. This happens when we focus on means instead of ends, server uptime easily carries an emotional value that <em class="italic">feels like it</em> might lead to good things and so we, as humans, often like to put proxies in place in our minds to simplify evaluating results and uptime is easily seen as a proxy for stability which is seen as a proxy for business value. All of this is false, none of those proxies are correct and, even worse, might be inverted.</p>
			<p>The second problem is the big one - high uptime itself represents a risk. The risk of the unknown. Our system changes over time from wear and tear on the hardware to patches, updates, and general changes to software. There can be data corruption or unrecoverable read errors. There might be configuration changes that do not work as expected. There are so many things that can happen regardless of if anyone has made intentional system changes or not. And the longer that we go from our last reboot, the less confidence that we can have that we know how the system will react.</p>
			<p>We always have to be confident, within reason, that a reboot will work. We cannot always control when<a id="_idIndexMarker496"/> a reboot will happen. We can attempt to have redundant power and redundant components, but every system has a chance of restarting and when they do, we want to have a high degree of confidence that it will reboot smoothly.</p>
			<p>The process of rebooting triggers many risks for a server. It reloads a lot of data from disk or other storage locations that likely has not been read in its entirety since the last reboot so the potential for corruption or other storage problems is heightened. It puts stress on the physical system, especially if a physical restart is coupled with the reboot. This is the time that it will be discovered that a file is missing, a file has corrupted, or a memory stick has finally started to fail. </p>
			<p>At first glance we might think that intentionally rebooting a system seems crazy, why would we want to encourage a failure to happen? But this is exactly what we want. We intentionally induce the potential for failure in order to hopefully avoid it at other times.</p>
			<p>The logic here is that we reboot at a time that is convenient or safe, <em class="italic">a green zone</em>. If at the time that we reboot we have a hardware failure or discover an unknown software problem, it is at a time where we know what triggered the problem (planned reboot), how long it has been since the last reboot (hopefully not very long), what changed in between (in the case of a software or configuration problem), or that a reboot was the exposure event for hardware failure. This should happen at a time that we have designated for<a id="_idIndexMarker497"/> fixing a potential problem. </p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor142"/>Finding your green zone</h2>
			<p>Your maintenance <a id="_idIndexMarker498"/>window or green zone is a designated time at which a workload is accepted to be unavailable in order to allow for regular administration tasks. Typical tasks might include a reboot, software installation, patching, database re-indexing, disk defragmentation, or other intensive tasks. Every industry, company, and even individual workload will be expected to have different time(s) when it is appropriate to assign a green zone. Do not expect that the correct green zone from one company will apply to a different company or even from one business unit to another within a single company.</p>
			<p>A common green zone is weekends. For many companies some, if not all, of their workloads can safely be unavailable from Friday evening until Monday morning without any business impact. Often, no one outside of IT would even be aware. A good strategy if this is the case is to perform any patching or similar tasks almost immediately upon the commencement of the green zone on Friday evening and then follow that work immediately with a reboot. If the reboot causes anything to fail, you have more than two and a half days to get it back up and running before anyone comes in and complains that systems are down. Two days of impact-free time allows for far better repairs than attempting to get a system back up and running when pressure is high, money is being lost, and the business is pushing for answers from the same team they want focused on fixing the problem.</p>
			<p>In my own experience, I once managed an application in which we measured database records that were known to have a consistent period of zero use for at least one minute every week, across all customers of the system. This gave us an effective zone of just sixty seconds, but if we planned carefully, we were able to do system reboots, software updates, patching, and more in that window. It was hardly convenient, but it was very cost effective compared to asking customers to give us a universal maintenance window or to run extra systems to cover for that one minute.</p>
			<p>Green zones can be creative and they might not be when you expect. They might be easy like long weekends, or maybe it happens during a recurring Tuesday lunch meeting. Work with your workload customers to learn when a workload is unused or not at risk.</p>
			<p>This is really all about planning. Only trigger problems when we are actually available to recover the system. This is so that we are far less likely to encounter have the same thing happen at a time when perhaps we are not available, or the workload is heavily in use. Never let a business say that a workload is too important to have planned downtime, that is oxymoronic. We have planned downtime specifically because workloads are critical. If a workload does not matter, then saving maintenance until things fail is not a problem. The more critical a workload is, the more planned downtime is crucial. In fact, any workload given no downtime (at least at a system level) could be designated as non-critical or non-production level simply from being given no planned maintenance. </p>
			<p>Compared to a car, the more important a vehicle is to you the more likely that you will take it out of service to have regular maintenance like oil changes and brake checks. You do this because <a id="_idIndexMarker499"/>planned maintenance is trivial, and an unplanned seized engine is not. We know naturally that it is better to maintain our cars than to let them fail. It is no different with our servers.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor143"/>Avoiding planned downtime is planning for unplanned downtime</h2>
			<p>If a resultant<a id="_idIndexMarker500"/> workload has no reasonable allowance for downtime, then additional strategies are necessary. If you are Amazon running a global online store, for example, even a minute of downtime might cost you a great many sales. If you are an investment bank, a minute of downtime could mean that orders are not completed properly, and millions of dollars could be lost. If you are a hospital, a minute might mean critical life support fails and deaths occur. And if you are a military, a minute could cost you a war, so that there are types of workload outages that we truly want to avoid is not in dispute. Clearly there are times when we need to go to extreme measures to make sure that downtime does not happen.</p>
			<p>In these cases, we need high availability at a level that allows us to take any arbitrary component of the infrastructure offline. This could be storage, networking, or any number of platform and compute resources. That means some amount of high availability at the application level so that we are able to properly patch everything from hard drive firmware to application libraries at the highest level and everything in between.</p>
			<p>Critical workloads need smooth running well secured infrastructure to keep them running. A good maintenance plan is at the absolute heart of making workloads reliable.</p>
			<p>There is no hard and fast rule about the frequency of reboots. But a good starting point is weekly and gauge from there what is appropriate for your environment. There is a tendency to opt for weekly or monthly because we often know that reboots are necessary, but we still think that they should be avoided. But, with rare exception, this is not true. We truly want to reboot as often as it is deemed safe and prudent to do so.</p>
			<p>Rebooting monthly, under current patching regimes, is about the longest that you would want to consider waiting for a standard schedule. Remember that any schedule needs to have some accommodations for a system being missed and having to wait for an additional cycle. So, if you plan for monthly, you need to be accepting of some systems going two months, from time to time, without maintenance due to technical or logistical problems.</p>
			<p>Weekly <a id="_idIndexMarker501"/>tends to be the most practical of schedules. Most workloads have a weekly usage pattern that makes it easy, or at least plausible, to allot a maintenance window. Weekly schedules are also good for users as they are easy to remember. For example, if a system reboots every Saturday morning at nine, users will get used to that and not try to use the system then even if they felt like working. It just becomes a habit. Weekly is frequent enough that the increased risk of the reboot process is very likely to evoke a pending hardware or software failure that would have otherwise occurred in the subsequent six days. This tends to be the best balance between convenience and reliability.</p>
			<p>We should always evaluate the opportunity to reboot more often. If our workload schedules allow for it, a daily reboot can be perfect, for example. This is how we generally treat end user workloads, encouraging systems to restart at the end of the day so that they are fresh and ready for the next day when staff arrive to work (whether virtual or physical does not matter.) Doing exactly the same with servers might make sense. </p>
			<p>Your reboot schedule should take into account your application update schedule. If the software that you run updates only rarely then a monthly reboot might make more sense. If you have workloads receiving nearly daily updates, then combining system reboots with application updates might make sense.</p>
			<p>System reboots are especially important after software updates and primary workloads (generally assumed not to be managed by the operating system vendor) as there are so many possibilities for services to not start up as expected, to need additional configuration, or simply run into bugs when rebooting. If you do not reboot in conjunction with software updates you lack the full confidence of knowing that when it was installed that a reboot worked successfully. If an issue arises later, knowing that reboots were working when the last system changes were made can go a long way to hastening the recovery process.</p>
			<p>If forgetting to reboot systems regularly is a common problem, then forgetting to have reboot monitoring is nearly ubiquitous. Even those IT departments that take reboot schedules seriously often never think to add <em class="italic">uptime monitoring</em> to their list of sensors to monitor in their environment. Reboot monitoring is generally pretty simple and can be generally done quite loosely. For example, in many of my environments where we desire the servers to reboot every week, we add a sensor for <em class="italic">uptime exceeding nine days</em>. If our monitoring system determines that a server has been up longer than nine days, it will email an alert. Missing one reboot event is not a major problem, and this gives plenty of time to avoid false positives and plenty of time to plan for manual intervention to find what caused the planned reboot to fail and to get it fixed before the next planned reboot should happen.</p>
			<p>Best practice here is to seek to reboot as often as is practical and to not avoid reboots for anything <a id="_idIndexMarker502"/>but a solid business need which cannot include the false need of <em class="italic">the system cannot go down</em>. Shoot for weekly or even daily and accept monthly if it is the best that can be mustered and be sure to add monitoring as a system not being rebooted is difficult to catch casually.</p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor144"/>Summary</h1>
			<p>System patching, updates, and reboots may feel very pedantic. And in some ways, I suppose that they are. But sometimes really important things can also be kind of boring. And really, patching and basic system maintenance should be boring. It should be predictable, reliable, and scheduled. And if at all possible, it should be automated.</p>
			<p>Patching should not become a challenge or a scary proposition. With proper planning, backups, testing and so forth, it is generally easy to have a reliable patching and even update processes that very rarely experience major issues of any kind. If we fail to make our patching and updates regular and reliable, we will begin to fear the process which will almost certainly lead us to avoid it more which will just exacerbate the problem.</p>
			<p>In the modern world of computing, there is always someone looking to exploit our systems and while nothing can protect against every possible attack, we can heavily mitigate our exposure through rapid, regular, and reliable patching.</p>
			<p>You should now be confident to evaluate your workloads, get each up to date, and begin implementing a formal patching, update, and reboot schedule across your fleet.</p>
			<p>In our next chapter we are going to look at databases and how they should be managed from the perspective of system administration.</p>
		</div>
	</div></body></html>