<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer018">
			<h1 id="_idParaDest-175"><em class="italic"><a id="_idTextAnchor181"/>Chapter 8</em>: Improving Administration Maturation with Automation through Scripting and DevOps</h1>
			<p>I think that it is safe to say that for most of us in system administration that scripting and automation and where we naturally gravitate towards for thinking of what creates the best opportunities for overall system improvement. This might be treated, and automation is very important, without question, but it is not the end all of system administration either. It is safe to say that the more that we learn to script and automate, the more that we have free time to focus our energies on tasks that only humans can do while also developing a deeper appreciation for what developers do which is always be helpful for those of us in IT.</p>
			<p>System automation is <a id="_idIndexMarker684"/>an area where it becomes much easier to obtain bragging rights as to what our daily task list looks like. When sitting around having beers at the proverbial system administrators cocktail lounge, we get little satisfaction over telling our compatriots how we wrote some really clean and easy to read documentation. But when we explain how we wrote a long, complicated script that takes hours of our weekly workload and turns it into a task that is run magically by the computer's scheduler we get kudos, attention, streamers, a ticker tape parade, fellow administrators buying us rounds of our favorite beverage and, if we are truly lucky, a pi√±ata.</p>
			<p>Automation is typically the area of administration that most of us find to be both the most exciting, and the scariest. There are more building blocks, more concepts to understand, than in other areas of system administration. For the most part, system administration is a lot like taking a history class where yes, there are real benefits to knowing more pieces of history so that you have a larger context when learning something new, but generally you can learn about any specific event without a large understanding of all of the events related to it and that led up to it and still come away having learned something valuable and essentially understanding it. You will not be lost when learning Roman history just because you did not grok Greek history first. But scripting and automation is a lot more like math class where if you fail to learn addition, then learning how to find the square root is going to be completely impossible. Scripting is a skill that builds on top of itself and to be really useful you will want to learn a bit of it.</p>
			<p>We are going to start by looking at unscripted command line administration in comparison to working with a graphical user interface and use that as a foundation to move into automation itself.</p>
			<p>In this chapter we are going to learn about the following:</p>
			<ul>
				<li>The GUI and the CLI: Administration best practices</li>
				<li>Automation maturity</li>
				<li><strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>)</li>
				<li><strong class="bold">Documentation First Administration</strong> (<strong class="bold">DFA</strong>)</li>
				<li>Modern tools of automation</li>
			</ul>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor182"/>The GUI and the CLI: Administration best practices</h2>
			<p>If you are<a id="_idIndexMarker685"/> coming to Linux <a id="_idIndexMarker686"/>from the Windows world, you may be excused from realizing that nearly everything should be done from the command line, not from the graphical user interface. But really, even on Windows, Microsoft has been very clear, for a very long time, that the desktop experience is really for end users and not for system<a id="_idIndexMarker687"/> administrators and that they recommend either using PowerShell as the administration tool of choice when working on a local system directly or any number of remove management tools that connect via an API. Microsoft pushes quite hard to encourage those installing their systems for the past few generations to install their operating systems and hypervisors without graphical user interfaces at all.</p>
			<p>Graphical User Interfaces, or GUIs as we will call them now to keep things short, present a lot of problems for system administrators. </p>
			<p>The first issue with GUIs is bloat. During the installation of an enterprise operating system, when a GUI is available it is often more than half of all of the code that will be deployed in a system. Every additional line of code means more data that we have to store, the more we store the more we have to back up; more code to worry about having bugs or flaws or intentional backdoors; more code to patch and maintain and so on. </p>
			<p>Next is performance. GUIs require much more compute power and memory consumption while running than do non-GUI systems. It is not uncommon for a GUI to require 2GB or more of additional system memory above and beyond what is needed for the system workloads. This might sound trivial, but especially if we are dealing with many systems consolidated onto a single piece of hardware it can add up very quickly. If we have twenty server virtual machines running on a single physical server it might not be uncommon in the Linux world for the average workload to only be between two and four gigabytes of <a id="_idIndexMarker688"/>memory. Adding <a id="_idIndexMarker689"/>two more gigabytes to each system would<a id="_idIndexMarker690"/> mean not only a nearly fifty percent increase, but forty gigabytes across the machines.</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor183"/>Consolidation and the age of squeezing systems</h2>
			<p>In the 1990s and 2000s, before<a id="_idIndexMarker691"/> the <a id="_idIndexMarker692"/>prevalence of virtualization, we were in an era where servers were gaining performance rapidly, but each individual system would only run a single operating system no matter how small the workloads on that system were. As systems became more powerful, much faster than software used more resources, there was a strong trend towards allowing system bloat because, at least when it came to hardware, it did not matter very much.</p>
			<p>CPU and memory resources would normally come in discrete chunks and to have enough we would normally have to overbuy. It was rare to run a system close to its limits because it was so difficult to expand systems in those days. So a system would typically have a lot of spare resources by design to allow for a large margin of error and, of course, growth. Because of these factors, running a GUI on a server was more or less trivial. </p>
			<p>Many factors have changed since those days. We could probably write an entire book just discussing why the industry temporarily moved from command line after decades of using nothing else and for a small blip from the mid-1990s to the early 2000s had GUIs seemingly taking over as the dominant approach in server management only to go right back to the command line by around 2005. Ignoring social trends driving changes we are concerned here with capacity concerns.</p>
			<p>Once virtualization became mainstream, and even more so as cloud computing began to become a major trend, the availability of spare resources for operating systems ceased to be a common thing, almost overnight. This might seem counterintuitive, given that virtualization inherently gives us more flexibility and power. But it also gives us the ability to scale down very effectively, and this is something that we did not have before. With virtualization we are rarely in a position of having dramatically excess system resources, especially predictably excess resources, and so there is a huge advantage to keeping individual virtual machines as lean as possible, and that means not running an enormous and largely useless GUI process. Very small businesses that still cannot combine their workloads to reach the effective lower bounds of a single server remain an exception to this rule and still have plenty of overhead to implement GUIs unless they would be good candidates for cloud computing.</p>
			<p>In a traditional business, where there are multiple servers, a major advantage of virtualization is consolidation and avoiding the installation of GUIs may mean that fifty or sixty<a id="_idIndexMarker693"/> workloads<a id="_idIndexMarker694"/> can be installed on a single physical server instead of thirty or forty on the exact same hardware. This equates to the need to buy fewer servers and that means cost savings not just from lowering purchase costs, but also lowering power consumption, cooling costs, datacenter real estate costs, software licenses, and even IT staff.</p>
			<p>If we look at an example on public cloud computing, we can see the advantages of not having a GUI even more easily. Small workloads, which could include email servers, web servers, proxies, telephone PBXs, and on and on might only cost between five and ten dollars per month to run on their own. Adding a GUI will easily cause the cost of a cloud hosted virtual machine to double from five to ten or ten to twenty and so forth as the GUI creates a need for more CPU power, more storage, and most importantly, much more memory. It does not much effort to see how moving from ten dollars per month for a workload to twenty dollars will add up exceptionally quickly. As most cloud-based workloads are quite small adding a GUI to each one could have staggering capacity consequences as much as doubling the compute cost of a company's infrastructure!</p>
			<p>The lack of appropriateness for a GUI is so dramatic that many vendors have traditionally not even provided a mechanism for attaching to a GUI in the cloud space. Amazon famously did not make GUIs possible on their standard cloud instances effectively forcing organizations to learn command line and even more advanced techniques involving management without a login. But nearly all cloud users opt for remote logins via a technology such as SSH. The cloud did more than anything else to demonstrate the risks and costs of the GUI. </p>
			<p>Prior to ubiquitous virtualization and cloud computing system administrators, especially those in the Windows world, would argue that GUIs just did not add that much overhead and that if they did anything to make someone's job easier that they were worth it. That myth has been exposed and no one can honestly make this claim today. GUIs are clearly nonsensical in any broad sense.</p>
			<p>Often the biggest selling point for command line management at a managerial level is about security. A GUI presents a much larger attack surface for a malicious actor to use to attempt to breach a system. All of that extra code alone makes things much easier for a would-be attacker. And, of course, the functionality of a GUI has to make for very enticing attack surfaces just by the nature of needing to have more means of being accessible. More lines of code, more access methods, more management paths, lower performance and more all come together for overall increased security risk. Taken all together it may not be incredibly major, but the increase in risk is real and is measurable or, at least, estimable.</p>
			<p>The final major point as to <a id="_idIndexMarker695"/>why<a id="_idIndexMarker696"/> command line management has become the <em class="italic">de facto</em> standard is efficiency. Yes, the very reason that so many point to as to why they chose to keep a GUI. The reality is that system administration is not a casual task, nor one where you can effectively just poke around and guess about what settings should be wear. To do the job well, or even safely, you must have a pretty solid understanding of a large number of items from operating specifics to general computing and networking understandings.</p>
			<p>The GUI in management was traditionally promoted as a tool for those that were not used to an environment to be able to be effective quickly with less training and knowledge. A great concept if you are talking about a janitor. In system administration the last thing that we want is someone without deep knowledge and experience being able to act like they know what they are doing. This is dangerous on many levels. GUIs, sadly, actually make it much harder for many organizations to evaluate which candidates are even minimally qualified for a technical position.</p>
			<p>Not only does a GUI pose a risk that someone without proper knowledge will start poking around, but for someone who knows what they are attempting to do the command line is vastly faster. It is faster for performing simple tasks, for performing most complex tasks, and it is far easier to script or automate. Command line management flows so easily directly into scripting that people often fail to be able to tell the two apart. If you ever truly compare tasks, it is not unheard of for command line work to take less than ten percent the amount of time that it takes to do the same task with a GUI!</p>
			<p>Command line is not just more efficient for a system, it also makes multi-system management much<a id="_idIndexMarker697"/> easier as commands can be duplicated across systems in ways that GUI actions cannot be. Command line management can also easily be recorded, catalogued, searched, and so forth. It is plausible to do the same with a GUI but it requires long video recording which results in large amounts of storage needs and no simple way to parse or turn into documentation, and so on.</p>
			<p>In the more modern era, we have also begun to face the problem of needing to perform most or all system administration remotely. This inadvertently played right into the command line's hand. The amount of data that needs to be sent, and the sensitivity to network lag for the command line are far smaller than for a GUI. A remote GUI session to a server generally uses a noticeable amount of network traffic. Remote GUI sessions are video streams, generally in pretty high resolution. In some cases, even a single user can cause network issues, especially if the server exists in a location with bad Internet access. The standard method for remote command line management is SSH.</p>
			<p>SSH remote sessions<a id="_idIndexMarker698"/> will work just fine even over an archaic dial up Internet connection. And even the slowest modern Internet service is enough to handle scores, if not hundreds, of simultaneous<a id="_idIndexMarker699"/> SSH <a id="_idIndexMarker700"/>users. This is something that you generally cannot do with remote GUI sessions. Command line is nearly as effective over a tiny Internet connection from the other side of the globe while GUI remote management suffers noticeably from any network blips, limitations, or distance.</p>
			<p>Command line is here to stay, but it is important to really understand why. It can be easy to forget that it is far more than just one or two small factors. There are really good reasons why you should be using command line whenever possible. Moving back and forth is not conducive to learning to be more efficient in either approach, as well. From a personal level it is expected that you would want to avoid the use of GUIs as much as possible so that you can focus on learning command line skills. Using the command line consistently is needed to really become efficient.</p>
			<p>Now we must acknowledge that there are a number of different command line options for Linux. We can use <strong class="source-inline">BASH</strong>, <strong class="source-inline">Fish</strong>, <strong class="source-inline">zsh</strong>, <strong class="source-inline">tcsh</strong>, PowerShell and more. Linux is, as we know, all about options and flexibility. This is a situation where less is probably more. Some of these shells are very nice and useful but, we must remember, that we are system administrators, and we need to make sure that we are totally familiar with the tools that we are likely to have access to in an emergency. Moving between shells is not particularly hard, especially in the Linux use case, but we should still be wary of spending time learning the nice keyboard shortcuts and auto-completion and other perks of a shell-like Fish or <strong class="source-inline">zsh</strong> because we may not be able to use those skills in the next job and that always has to be a consideration. And, in the case of an emergency if you were to get called to work on a system that you have not had a chance to set up previously you may be stuck with no option except for BASH. For me, this means that BASH is the only tool that I want to be learning.</p>
			<p>And there you have it. All of the logic and reasoning so that you can go back to management and explain why you need to be working from the command line, why you need staff that works from the<a id="_idIndexMarker701"/> command<a id="_idIndexMarker702"/> line, and why your systems should rarely get installed with any GUI environment at all. In our next section we are going to talk about maturity levels in automation for systems.</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor184"/>Automation maturity</h1>
			<p>While there is no formal <a id="_idIndexMarker703"/>system for measuring automation maturity levels, there are some basic concepts of automation maturity that we can discuss. The idea here is that organizations sit, more or less, along a continuum from having no automation to being fully automated with most organizations sitting somewhere in the middle, but more likely to be towards no automation than towards being fully automated.</p>
			<p>Not every organization needs to be, or even should be, completely automated. But in general, more automation is better when the cost to implement the automation is low enough to do so. Automation is not free, and it is quite possible to find organizations investing more in automating a process than it would cost to perform the duty manually over the lifespan of a system. We do not want to automate blindly only for the sake of automating.</p>
			<p>Typically, however, what we find is organizations skipping automation in nearly all cases and uses manual labour with all of its costs and risks instead. There is a natural tendency towards this because in the moment, any task will be easier if done manually. If we do not look ahead and invest, we would simply never automate, and this is often how companies view IT needs. If a task takes one hour to do manually and three hours to automate, that's the time of three tasks and hard to justify ignoring the fact that the same task will happen monthly and in four months would not only have been less effort to have automated, but the automation would make the task more reliable and consistent.</p>
			<p>Nearly any organization will benefit from automating more than they do today. There is no need to look at automation and feel that it is an all or nothing proposition. Automate what you can, skip what you cannot do. Be practical. Hit the low hanging fruit first. The more than you automate the more time you have to automate other things in the future. You will improve your automation skills as you practice, as well, making each new automation something easier than the last. Automation is a great example of that kind of thing that is really hard to do the first time but gets progressively easier and easier until it is just the obvious way to approach things and becomes second nature.</p>
			<p>Automation maturity is not exactly a direct continuum with each step <em class="italic">more mature</em> than the last. For example, if we look at scheduling tasks and scripting tasks, each of these can be done independent of the other. Both are useful on their own. We can script complex<a id="_idIndexMarker704"/> operations and run them manually. Or we can schedule simple, discrete commands to trigger things without human intervention. We can then put the two together to automatically kick off complex scripts that do many things at once. Which one do we consider first and which second is just arbitrary.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor185"/>Local and remote automation</h2>
			<p>In case it is not <a id="_idIndexMarker705"/>overly obvious, we have the choice of<a id="_idIndexMarker706"/> implementing automation either locally with tasks scheduled or triggered to run on the server in question, or we have the ability to push our automation from an outside source which can give us a sort of centralization of automation. There is a sort of hybrid approach where a local scheduler or agent reaches out to a central server to request automation information which is technically still local, just with a centralized store to mimic control.</p>
			<p>Often overlooked is the advantage of local automation being able to run even if remote systems become unavailable even to the point of the local system completely losing networking. My favorite task to keep local regardless of other automation decisions is system reboots. While it would be convenient to centralize reboots and I have seen organizations opt to do so I very much appreciate having a local, forced reboot that happens at least weekly and sometimes even daily. This gives me great peace of mind that even if something completely crashes on a server if it is still functional in any way that eventually a reboot process is going to make an attempt at restarting the machine and hopefully bringing it back online. A very niche need and one that may never be important to you, but I have witnessed systems become inaccessible for remote management while still providing their workloads and an automated, locally scheduled reboot brought them back online and making them accessible again.</p>
			<p>An increasingly popular happy medium approach is to have a central control repository that contains all of the desired automation which is then pulled by an agent on the end points being automated. This repository contains both the automation itself, such as scripts, as well as the scheduling information or triggers. Then the information is actualized by a local script that is able to keep functioning independently even if the remote repository fails or becomes unavailable. In this way you only really risk losing access to update the<a id="_idIndexMarker707"/> list of scheduled tasks to make changes<a id="_idIndexMarker708"/> to them or to the schedule. As long as you do not need to send out new updates to the automation you do not have to worry about your repository being offline. </p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor186"/>Command line</h2>
			<p>Not so much a <a id="_idIndexMarker709"/>legitimate maturity level but more of a basic starting point, so we could think of this as a level zero, is moving to the command line and the use of a proper shell environment for interactive (that is: non-automated.) As we just discussed, being on the command line and learning command line syntax and tools is the fundamental building block on which all subsequent automation is going to be based. Understanding how to do tasks at the command line, how to manipulate text files, how to filter logs, and other common command line tasks will build quickly into obvious automation capabilities.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor187"/>Scheduled tasks</h2>
			<p>The best and easiest <a id="_idIndexMarker710"/>place to begin with automation is the scheduling of tasks. This may sound like the simplest and most obvious step, but surprisingly many organizations never even get this far. Linux has long been a stronghold of reliable, easy to manage local scheduling with <strong class="source-inline">cron</strong> having been built into not only Linux, but essentially all UNIX systems for almost half a century as it was released in 1975. Cron is fast and efficient, ubiquitous and well known. Any experienced UNIX admin should be able to at least schedule a basic task when needed. Cron even handles tasks happening at boot time.</p>
			<p>Simple tasks of all sorts can be run <a id="_idIndexMarker711"/>through <strong class="source-inline">cron</strong>. Common tasks used in most environments could include system updates, data collection, reboots, file cleanups, system replications, and backups. You can schedule anything, of course, but these are some good ideas for first time automaters looking for obviously recurring system needs.</p>
			<p>Another common area for simple scheduled tasks are code updates via a repository like when we pull new <a id="_idIndexMarker712"/>code via <strong class="source-inline">git</strong>. Tasks such as code updates and subsequent database migrations can all be easily scheduled.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor188"/>Scripting</h2>
			<p>When we say automation everyone always immediately thinks about scripting. At the end of the day, <a id="_idIndexMarker713"/>nearly everything in automation is scripting either directly, or under the hood somewhere. Scripting delivers the power when we want to move beyond the simplest of tasks or just calling scripts that someone else has made.</p>
			<p>We cannot possibly teach scripting itself here, that is an entire topic in and of itself. Scripting is the closest that IT comes to crossing paths with the software development world. Where does combining IT command line tasks together turn into programming? Technically it is all programming, but an incredibly simplistic form of programming focused purely on system management tasks.</p>
			<p>Typically, on Linux we write scripts in the <a id="_idIndexMarker714"/>BASH shell. BASH is a very simple language designed to be primarily used interactively as a live shell, BASH is how we assume all command line interactions with Linux will be performed. BASH is relatively powerful and capable and nearly any script can be written in it. At least when starting out, most Linux admins will turn to the BASH shell that they are already using in their command line environment and add scripting elements organically to move from a single command, a few basic commands strung together, and into full scripting just a little at a time.</p>
			<p>Any shell, such as <strong class="source-inline">tcsh</strong>, <strong class="source-inline">ksh</strong>, <strong class="source-inline">fish</strong>, and <strong class="source-inline">zsh</strong>, will allow you to script and in many cases with more power and flexibility than you can with BASH. Traditional shells, like <strong class="source-inline">tcsh</strong>, <strong class="source-inline">ksh</strong>, and BASH, can be very limiting and cumbersome to attempt to use for advanced scripting. Apple for its macOS UNIX operating system has recently moved to <strong class="source-inline">zsh</strong> to modernize it compared to other UNIX systems. Typically, a Linux system is not going to have a more modern, advanced shell installed by default, even though they are easily available on essentially any Linux based operating system.</p>
			<p>You may work in an environment where an alternative shell is consistently provided or offered, or you may have the option of adding it yourself. If so, and especially if you will be doing cross platform scripting with macOS you might consider using <strong class="source-inline">zsh</strong> instead of BASH, or if you are<a id="_idIndexMarker715"/> doing a lot of Windows scripting its native shell PowerShell is also available on Linux.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor189"/>PowerShell on Linux</h2>
			<p>One of the <a id="_idIndexMarker716"/>weirdest things that you may ever encounter <a id="_idIndexMarker717"/>is the idea of running Microsoft's PowerShell on Linux. Many people are confused and believe that it does not even work. PowerShell does actually run just fine on Linux. The problem with PowerShell on Linux is that PowerShell users on Windows actually spend essentially no time at all learning PowerShell and nearly all of their time learning a range of CommandLets or small programs that can be called by PowerShell and easily combined with other little programs to give power to the system.</p>
			<p>On Linux, of course, the same thing happens. If you are scripting on Linux you will surely be using tools like sed, awk, cut, head, tail, grep and so forth. These tools are a lot like CommandLets, but are actually just every day system executables. If you were to port <strong class="source-inline">BASH</strong> or <strong class="source-inline">zsh</strong> over to Windows you would find that the tools that you are accustomed to using on Linux were still not available. That is because they are tiny programs that you are calling from BASH, not part of BASH itself. BASH is just the programming language. </p>
			<p>The reverse is also true. If you run PowerShell on Linux you still use sed, awk, cut, grep, head, tail and on and on. It is the language that has changed, not the operating system's suite of tooling and components.</p>
			<p>So, while there can be value in learning one scripting language and attempting to use it over and over again between different operating systems, there is not the value that one might assume. You will likely spend far more time tripping over integration quirks, misunderstandings, and poor documentation than you could ever recuperate from language learning efficiency. Books, online guides, example code and so on will never work for you if you try to use PowerShell on Linux. There will assume that you are trying to do Windows tasks with access to Windows tools, always. PowerShell is, at its core, designed to be a truly modern shell that uses operating system objects to do its heavy lifting. BASH instead is focused on text processing and manipulation as Linux is traditionally built on text files and needs a scripting engine that will easily accommodate that.</p>
			<p>Using something so foreign as PowerShell on Linux is a great tool for exposing where different components that we often see simply as <em class="italic">the command line</em> or <em class="italic">the shell</em> for what they are. If we use <strong class="source-inline">zsh</strong> on Linux, nearly everything that BASH has built in is replicated in <strong class="source-inline">zsh</strong>, and they both conventionally use the same operating system tools. PowerShell has few, if any, replicated built in commands and no shared conventions making it painfully obvious what was coming from the shell and what is part of the operating system outside of the shell.</p>
			<p>In general, however, it is most advised to do all scripting in languages that are well supported in your environment. Just like we have said in other areas of system administration, it is important to use tools that are ubiquitous, well understood, and appropriate for the<a id="_idIndexMarker718"/> environment. For most that means BASH<a id="_idIndexMarker719"/> exclusively. BASH is the only scripting environment that is going to be absolutely available on every Linux system that you ever encounter. Other shells or scripting languages might be common, but none other are so universal.</p>
			<p>When BASH proves to be too limiting for more advanced scripting it is uncommon to turn to another shell, such as <strong class="source-inline">zsh</strong>, as other shells remain very uncommon and generally lack in the extensive power that you are likely looking for once you are abandoning BASH. Traditionally it has been non-shell scripting languages that are used as BASH alternatives<a id="_idIndexMarker720"/> for <a id="_idIndexMarker721"/>advanced <a id="_idIndexMarker722"/>scripting<a id="_idIndexMarker723"/> such as <strong class="bold">Python</strong>, <strong class="bold">Perl</strong>, <strong class="bold">Tcl</strong>, <strong class="bold">PHP</strong>, and <strong class="bold">Ruby</strong>. Ruby has never gained much favor. PHP, while very common for certain tasks is pretty rare as a general system automation language. Perl and Tcl have fallen out of favor dramatically, but at one time Perl was the clear leader in system automation languages. That leaves Python as a very clear front runner for advanced scripting needs.</p>
			<p>Python has many advantages overall. It is decently fast. It is available on nearly any platform or operating system (including all Linux, alternative UNIX, macOS, and Windows.) It is quite easy to learn (often used as a first language for new programmers to learn.) It is very often already installed because a great many applications and tools on Linux depend on Python so you will regularly find it already installed even when it is not intentionally installed as a standard. Because it is used so commonly for these tasks, it has become increasingly well suited to the role as documentation in how to use Python in this way has grown and other tools written around it have sprung up.</p>
			<p>At this time, nearly all system scripting for Linux is done in BASH when possible and in Python when more power or flexible is really needed. All other languages are really niche use cases. This means that BASH and Python also have additional reasons that we should be strongly considering them when choosing languages for our own scripting: standardization.</p>
			<p>System automation is different than general programming. With broader programming developers spend years learning multiple languages, language families, constructs, and spend all of their time in their programming environments. Moving between languages, learning a new one, adapting to language changes and so on are all part of the daily life of a developer and the overhead to move between languages is very low. For a system administrator this is a bit different. In theory we spend very little time learning programming and rarely are exposed to any real variety of languages. So for administrators, having just one or two languages that you use is important for being able to find resources, examples, peer review, and others to provide support for our automation in the future. These are certainly not the only acceptable languages, but they do hold rather sizable advantages over most other options.</p>
			<p>Of course, scripting is a very general topic and scripts can run from being just a few lines of simple commands run sequentially to giant programs full of complex code. Growing your skill in scripting <a id="_idIndexMarker724"/>is a topic all to itself and well worth<a id="_idIndexMarker725"/> investing significant time into. Good scripts will generally include their own logging mechanisms, error detection, functions, reusable components, and more. You can essentially invest indefinitely in greater and greater programming skills to apply to system automation scripts.</p>
			<h3>Developer tooling for script writing</h3>
			<p>Whether you<a id="_idIndexMarker726"/> are just writing a very simple script or working on a masterpiece of automation to be passed down from generation to generation of system administrators in your organization, it may be worth taking an additional step to learn something about tooling used in software development to potentially aid in the script writing process.</p>
			<p>On the simplest side are tools like integrated development environments or IDEs that can make writing code faster and easier and help you to avoid errors. Developers nearly always use these tools, but system administrators will often overlook them as they feel that they write scripts so little of the time that learning another tool may not be worth it. And perhaps it is not, but the more tooling you learn the more likely you are to use it and to write more scripts. A good IDE can be free and quite easy to use, so is a good starting point as you can integrate one into your process without really spending any money and only a few minutes to download and install one.</p>
			<p>The other truly enormous toolset that developers almost universally use and system administrators rarely do are code repositories and version control tools like Git and Mercurial. With tools like these, and a central hosting of your code which is often associated with these tools, we can really leap forward in our script writing and management. These tools are also really useful for the management of other forms of textual data in our environments. Linux especially uses text-based configuration files which can be<a id="_idIndexMarker727"/> treated just like scripts and kept under version control and stored in version control systems. An excellent use of cross-domain skill sharing.</p>
			<p>Version control is certainly the most must have technique from the software development world for use in our own scripting. Version control allows us to track our changes over time, to test code and have a simple ability to roll back, it allows for integrating multiple team members into the management of the same scripts, it tracks changes by user, empowers code review and auditing, simplifies data protection and deployment and so much more. If you use only one major development technique, this is the one to use. At first it will feel cumbersome, but quickly it will become second nature and make so many things that you do so much easier.</p>
			<p>The development world has many other tools that we might potentially use like continuous integration, automated deployments, and code testing that all might provide to be useful depending on the scripting that we do, but nearly all of those are niche and completely optional even in a very heavily automated environment. Learning about these tools can expose you to options that may or may not make sense for your workflow, and will also give you great insight into how your potential development teams may be working.</p>
			<p>Look to software engineering as a source of ideas about how to better approach your own script writing, but do not feel that you need to or even should adopt every tool and technique. Scripting for automation and product development do have overlap, but are ultimately different activities.</p>
			<p>There is no secret to scripting other than just doing it. There are many good books available and resources online. Start with the simplest possible projects, look for opportunities to do scripting where you might have done work manually before. System tasks such as deployment or system setup checklists can be a great place to start. Or scripts to deploy a set of standard tools. Or perhaps a script to collect a specific set of data from many machines. I often find myself scripting data processing tasks. Once you start looking you will likely find many places where some new scripting skills can be put to work.</p>
			<p>One of the best places to start using unscheduled scripting is for basic build and installation tasks. Using scripts to perform initial system setup and configuration including installing packages, adding users, downloading files, setting up monitoring, and so on. These tasks generally offer large benefits at relatively little effort and can serve as being<a id="_idIndexMarker728"/> essentially a form of documentation listing any packages and configuration changes needed for a system. Documentation done in this way is highly definitive because it truly documents the actual process used rather than one that is intended or expected.</p>
			<h3>Documentation first </h3>
			<p>In software <a id="_idIndexMarker729"/>engineering circles there is a concept of writing tests to verify code. While not perfect, running tests makes it far less likely for software to have bugs because there are tests that look for expected behavior and ensure that it is happening. We can still have bugs, this is anything but a guarantee, but it is a great step. After decades of writing tests for code, the idea that it would be feasible to write tests before writing code was floated and in research it is sometimes found that in doing so not only are bugs reduced but code writing efficiency can actually improve simply because test writing encourages thinking about problem solving in good ways. Test-first coding was considered a breakthrough in approaching software development.</p>
			<p>This concept can carry over to the system administration world, in a manner of speaking, with the use of what I call documentation-first engineering. In this concept we start by writing documentation and then using that documentation to build the system. If it is not documented, we do not build it. Like test-driven coding, this approach forces us to think about how we want a system to work ahead of time which gives us another opportunity to make sure that what we are doing is well planned and sensible. And it allows us a chance to verify that our documentation is complete and sensible. When we write documentation after the fact it is far easier to make documentation that cannot really be followed for completing a task.</p>
			<p>In some cases, such as those with low automation levels, this could mean simply documenting what we can in a wiki or word processor document and working from that as we deploy a system. If we have higher levels of automation then we may actually write code as documentation that builds our systems for us. Since the code itself functions as the documentation it is not just documentation-first, but the documentation actually does the work which absolutely guarantees that the documentation is complete and correct! </p>
			<p>It is an intrinsic nature of automation to encourage better documentation and to move from documenting after the fact to before the fact and on to using the documentation itself as the build mechanism. This also means that we can potentially see double gains in efficiency as <a id="_idIndexMarker730"/>version control and backups and other mechanisms that we want to use for both documentation and scripting can be automatically applied to both.</p>
			<p>Using advanced tools for our scripting may also be considered a higher step of automation maturity in a way.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor190"/>Scripting combined with task scheduling</h2>
			<p>The hopefully <a id="_idIndexMarker731"/>obvious next step is to take task scheduling and our new found scripting knowledge and combine the two for even more power. Making complex tasks and making them able to run automatically without any human intervention.</p>
			<p>Common tasks to automate in this manner will often include software updates. Having a script that looks for the latest updates, downloads them, prepares the environment, and deploys them all automatically on a schedule is very handy. Nearly any complex set of tasks that should be performed together can be scheduled in this way whether it is every minute or just on the third Tuesday of the month. Scripts are also very good for dealing with conditional situations where actions should only be performed under certain conditions such as only if storage is beyond a certain level or if certain people are logged in.</p>
			<p>Almost a special case, and therefore well worth mentioning I believe, is using scheduled scripts to<a id="_idIndexMarker732"/> manage backups or replication.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor191"/>State management</h2>
			<p>One of the most <a id="_idIndexMarker733"/>amazing changes that we have<a id="_idIndexMarker734"/> experienced in system automation is the introduction of state machines and state management for systems. State can be a difficult concept to explain as this falls far outside of the normal thought processes in IT and system administration. State is often seen as the future of systems, however.</p>
			<p>In traditional systems administration and engineering we talk and think about tasks: how do we make a system get from point A to point B. In state theory, we do not talk about the <em class="italic">how</em> of managing systems. Instead, we focus only on the intended results or <em class="italic">resultant state.</em></p>
			<p><em class="italic">To think of it in another way: we start focusing on ends, instead of focusing on means. We move from process oriented to being goal oriented.</em></p>
			<p>This approach forces us to really change nearly everything that we think about and know about systems. It is a game changer in every sense and frees us, as humans, to focus far better on what we are good at while allowing the computer to do far better what it is good at.</p>
			<p>All of this magic is done by what is<a id="_idIndexMarker735"/> called a <em class="italic">state machine.</em> In the context of system administration, a state machine is an agent or portion of code that is given a document or series of documents that dictate the desired state of the system. State can refer to nearly anything about a system such as what packages are installed, what the current patch level is, the contents of configuration files, the ports open in the firewall, and the list of which services should be running. </p>
			<p>A state machine will take this documentation as to the intended state of the machine and guarantee (or at least attempt) to ensure that the system is in the state desired. If a package is missing, it will install it. If a service is not running, it will start it. If a configuration file is incorrect, it will correct it. The opposite is also true, if a program appears that is not supposed to be installed it will be removed. If a service starts that is not supposed to run, it will be shut down.</p>
			<p>The state machine typically runs every so many minutes or seconds and scans its state file and determines how the system should be, then scans the system to verify that everything it knows about how it is, and how it should be, match. It then takes whatever corrective action is required. Of course, under the hood, this is all done by complex scripts and system tools, that when assembled provide the power to enforce state. The degree to which corrective action can be taken by the state machine is determined by the power of the scripts that it has access to use. It is not unlimited, but generally on Linux a state machine will have enough power to do whatever is effectively needed in a real world, non-attack scenario and will remain highly effective at slowly or thwarting many less intensive attacks as well.</p>
			<p>In this way, in theory, a state machine keeps a system in a nearly constant state that we desire. With state machines we spend our time writing the documentation of how we want a system to be and we let the state machine itself worry about how to make the system behave in the desired way. This includes the initial setup of a machine to take a basic, vanilla operating system install and turn it into a functional component of a specific workload. State <a id="_idIndexMarker736"/>machines work at the hypervisor and <a id="_idIndexMarker737"/>cloud levels as well, allowing us to maintain a standard conceptual approach not just inside of an individual system, but at the platform level providing the systems in the first place.</p>
			<h3>The end of the login</h3>
			<p>The nature of state <a id="_idIndexMarker738"/>management is to encourage, if not enforce, the end of the concept of logging into a server for administration purposes altogether. Before we think of eliminating logins, however, state management systems serve to improve traditional remote management through state.</p>
			<p>Traditionally the biggest risks in remote administration are the needs to open ports and to have those ports open flexibly to allow for management from whatever location is necessary at the time. The idea of opening very few ports and locking them to a single IP address sounds like great security in theory, but is all but useless in real world practice. Having the flexibility for an administrator to log in quickly, from wherever they are at the time of an emergency, either requires too much exposure or far too many steps to limit access.</p>
			<p>Enter state management. With state management a system can be instructed via the state definition file stored centrally in a repository to enable the SSH service, open a random port that gets used for that SSH service, and to lock it to the current IP address of an administrator or group of administrators. And the version control system will easily track that the change was requested, when it was requested, and by whom. In theory there could trivially be an approval step included as well as part of the mechanism. Once recorded the system would authorize access for the specified administrator(s). And once they were done, or on a set schedule, the state management system will revert the changes, after it has all been documented, and completely close off all avenues of access. The potential for enhanced security is incredible.</p>
			<p>But that is only an interim step. With full state management we should, in theory, never need to log into a system at all. We should be able to do perform any management steps necessary via the state system itself or, even more appropriately, those steps should be being performed automatically by the state engine to ensure that proper state is maintained.</p>
			<p>To fully enable a login-less <a id="_idIndexMarker739"/>mechanism of this nature we have to combine something like state management with concepts that we talked about in the last chapter such as having remote log collection and alerting so that even for tasks like capacity planning that there is no need to be <a id="_idIndexMarker740"/>physically logged into an individual system. To traditional system administrators this often sounds like blasphemy and all but impossible, but this is how many companies operate today and it is entirely possible with the right work being done up front.</p>
			<p>For systems running on physical hardware inside of the office this might sound like overkill, and perhaps it is. For systems running on cloud servers this is highly practical in many cases. Human intervention should not be needed for a properly tested, documented, and running system. Manual management is difficult to document, very difficult to repeat, and highly error prone. Of course, human intervention can always be saved as a last resort, but the ability to remove it completely and depend on redeployment as a last resort is very obtainable today.</p>
			<p>Automation maturity models give us a sort of roadmap of how to get from where we are to where we hope that we could be. Certainly not every organization has to get to the point where state management is handling all of their needs. Not every environment even needs to be scripted! Most organizations will continue to benefit no matter how far our maturity level is taken. </p>
			<p>The final level of the maturity model we are saving for its own section. Taking what we have learned and applying the techniques we arrive at...</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor192"/>Infrastructure as code</h1>
			<p>Taking concepts <a id="_idIndexMarker741"/>that we have discussed here and looking at them another way, we discover the concept of <em class="italic">infrastructure as code.</em> Meaning that we can write code or configuration files that represent the entirety of our infrastructure. This is powerful and liberating.</p>
			<p>It is easy to confuse infrastructure as code concepts with state machine concepts because they will, in many cases, overlap quite extensively. There are critical differences, however. </p>
			<p>Infrastructure as code can go hand in hand with state machines, but state machines do not allow for imperative system definitions. Infrastructure as code can be used to define state, also known as a declarative approach to infrastructure as code, or an imperative approach by which operations are defined rather than final state, making it feel much more like traditional systems administration where we focus on the means rather than the ends or the <em class="italic">how</em> rather than the <em class="italic">goal</em>.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor193"/>Platforms and systems</h2>
			<p>Infrastructure<a id="_idIndexMarker742"/> refers <a id="_idIndexMarker743"/>to both the systems that we run, that is the operating system containers, as well as the platforms, that is hypervisors and physical machines, on which they run. For most of what we are looking at in this section and certainly concepts like infrastructure as code, the applicability is equal to both aspects of infrastructure. </p>
			<p>The tools and techniques that we apply at the system level will work with our platform level and vice versa. That means that we do not just get to rely on these awesome tools and techniques for configuring our operating systems and applications, but they can be used to actually deploy and build the virtual machine containers (both full virtualization and containerization) in both cloud and traditional non-cloud environments.</p>
			<p>Applying the same or similar tooling across these two domains means greater overall power through conceptual integration and a more complete picture of our infrastructure as a whole. A key benefit in much of modern computing is moving from purely seeing operating systems as the building blocks for workloads to also seeing the hypervisor as playing a direct role in workloads as well. Instead of the hypervisor simply providing a space for an operating system to contain a workload, the hypervisor or hypervisor cluster can serve as a platform that is workload aware and act as a tool in dividing up resources to provide as a workload component level. </p>
			<p>For example, the hypervisor or even hypervisor cluster level management will be aware that it is providing workload containers for application servers, proxies, processing nodes, storage, backups, databases, and so forth. Because the platform level is workload aware, it can then make intelligent provisioning decisions as to not only what kinds of resources will be needed, but also onto which nodes a workload should be deployed. If we have a three node hypervisor cluster and we provision three application virtual machines our provisioning should know to spread these out with only one virtual machine per node to increase redundancy; and it should know to do the same with the accompanying database that feeds those application servers. But it should also know to keep one application server on the same node as one database and to configure those to talk to the local database instance rather than a randomly non-local one. Bringing workload level awareness all the way down from the application, through the operating system, and<a id="_idIndexMarker744"/> down <a id="_idIndexMarker745"/>to the platform means better performance, with less effort, and more data protection.</p>
			<p>Typically, as we move into infrastructure as code, we naturally begin to merge systems and platform administration groups because it just makes sense to see these as two parts of a larger, holistic infrastructure vision. </p>
			<p>Imperative infrastructure as code design gives us many of the upfront benefits that we also saw with state machines, but require less work to set up initially and much more work to maintain over time. Because of this, imperative systems were considered normal when infrastructure as code was first introduced but as the market matured and declarative (stateful) tool sets and pre-built imperative structures were made available, the shift to declarative infrastructure as code was inevitable.</p>
			<p>Under the hood, of course, all systems like this are going to be imperative. Any declarative system ultimately uses pre-defined imperative steps to arrive at the desired state. So in order to have a declarative system for us to use, either we or someone else has to wrote imperative scripts and tools that get us from many different starting points and result in the same ending point. It takes a lot of time and testing to build these components even once a base state engine has been designed.</p>
			<p>These scripts have to exist for every task. If you want to define that a file must exist then we need tools under the hood that check for the existence of the file, logic to determine what to do if the file does not exist, how to find it, how to copy it, where to put it, what to do if a file copy fails, and so forth. This is probably the simplest use case and you can easily imagine how much more complicated it is to deal with any other task. Even a simplistic declarative system is going to be made from a myriad of imperative scripts that the administrator may never know about. Or it could be built by the administrator for very specific needs unique to the systems in question. The concept is flexible, but complex.</p>
			<p>In theory we can have declarative state management without going as far as to have infrastructure as code. As odd as it may sound, it is actually somewhat common for those starting out with state systems to do so almost manually, attempting to issue stateful commands in a nearly imperative way to inform the system of desired state without documenting all aspects in code before doing so.</p>
			<p>It is also common for the tools for these techniques to be deployed, and even used, but in a very incomplete way. This<a id="_idIndexMarker746"/> can be due to frustration, a lack of<a id="_idIndexMarker747"/> planning, internal corporate politics, you name it. Because working completely in this mode is so intensive and requires great planning up front it can be very difficult to obtain adequate time and buy-in from the powers that be to allow for complete implementations. Because of this we often see these tools being used to roll out basic, well-known functions using pre-built third party scripts, but complex configurations often unique to an organization still being done in a traditional way. The rush to get systems deployed will often drive this behavior.</p>
			<p>This brings us to what is really the key best practice around infrastructure as code. What matters most is getting broad organizational buy-in and investing in proper documentation and code completeness prior to workloads being put into production.</p>
			<p>Like any code that we would talk about in software engineering circles, our infrastructure as code requires testing before being used. Testing system administration code is often far easier than other types of applications, however. Creating our code for a new workload then attempting to deploy that workload from scratch is quite straightforward and can be attempting over and over again until results are perfect. Then system modifications, breaks, and other potential scenarios can be tested to ensure that the system will respond well under potential real-world problems.</p>
			<p>One of the great benefits of this type of testing is that the function of our code is to create infrastructure out of, roughly, nothing. So all we have to do is start with a blank slate and, if things are working correctly, our infrastructure will create itself and we can test from there.</p>
			<p>It is true that even if we cannot go as far as we would like by creating an entire infrastructure that is self-creating, healing, configuring, and destroying we can at least use these tools <a id="_idIndexMarker748"/>and<a id="_idIndexMarker749"/> techniques to go part of the way. Starting with simply building the infrastructure from scratch and not maintaining it or decommissioning it is an excellent step. </p>
			<h3>Infrastructure as code as disaster recovery</h3>
			<p>We have touched<a id="_idIndexMarker750"/> on this concept <a id="_idIndexMarker751"/>elsewhere and will really dig into it in the next chapter on Backups and Disaster Recovery, but it is so important that we have to discuss it whenever we talk about a constituent component of disaster recovery in modern systems. As we have talked about infrastructure as code we keep talking about the automated creation of systems where none existed previously.</p>
			<p>This is exactly what is needed in a disaster recovery scenario. Our systems are gone and we need to bring them back. Having our systems, their standard files, their configurations, and more all stored as code in a place where it can easily be recovered or even better, not lost at all during a normal disaster, means that we are ready to build a new system, anywhere that we want, at the drop of a proverbial hat.</p>
			<p>Because these types of build systems spend most of their time building workloads for testing and production deployment, they are easily justified for being fast, efficient, easy to use, well understood, and heavily tested. A system built in this way will be built identically during testing, the same in production, and the same during an emergency disaster recovery. We do all the hard work to make the system fast and repeatable up front so when something terrible happens we do not have to deviate from the established process.</p>
			<p>Traditional system restores after a disaster require building systems through a process that is unique and nothing like the process through which the systems were built initially. This is highly error prone for so many reasons. It is a process that rarely gets good testing or documentation. It is typically done without proper planning and under a high degree of stress. This is the worst possible time to be experiencing these conditions. There is so much to go wrong at a time when there is the highest pressure to get everything right.</p>
			<p>Avoiding those problems by creating a system that builds itself the first time, the second time, every time, almost instantly and completely identically is truly a big deal. If anything justifies the benefits of automation and infrastructure as code, it is this. Having the confidence that your systems are going to come back fast and correctly is something that most companies do not have today. The fear that backups are not going to work, that the knowledge of how to get a system up and running correctly configured for a workload is missing, that licenses or system details are not documented, or that necessary packages are not readily available is dramatic. Instead of overlooking those problems and hoping for the best we have a modern approach that makes these issues simply go away.</p>
			<p>Getting our organization to the level of truly implementing infrastructure as code is a tremendous step, but this is the future. Companies that do this have faster builds, faster recoveries from disaster, better security, are more agile, scale faster, and all of that means have a better chance of making more money. At the end of the day, our only job as system administrators is to do our job in such a way that we can increase the bottom line of the organization through our efforts, no matter how indirect and impossible to measure that they may be.</p>
			<p>Now that <a id="_idIndexMarker752"/>we<a id="_idIndexMarker753"/> know what the techniques are it is time to talk about the actual, already existing, ready to be tested tools that make things like state machines and infrastructure as code possible.</p>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor194"/>Modern tools of automation</h1>
			<p>All of this power <a id="_idIndexMarker754"/>comes primarily from modern tools that have been<a id="_idIndexMarker755"/> being introduced into the realm of system administration over the last fifteen to twenty years. The Linux world has been very fortunate to have been at the forefront of this movement since the very beginning. This comes naturally both because the Linux community tends to be one that thrives on and focuses on innovation, but also because the intrinsic nature of a system built around command line interfaces and simple text files for configuration and software repositories all make for vastly simpler automation. The design of Linux may not have been intentional to encourage automation, but nearly every major aspect of both the system implementation and the behavior of the ecosystem have led to it having the most ideal combination of factors to nearly always make it the leader in new automation<a id="_idIndexMarker756"/> tools<a id="_idIndexMarker757"/> and strategies.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor195"/>Configuration management systems</h2>
			<p>New tools are <a id="_idIndexMarker758"/>always arising and <a id="_idIndexMarker759"/>techniques do vary over time so making a definitive list here is not possible, but there are some important tools that have managed to make a name for themselves to a point that they are worth mentioning as starting points for an investigation into the tooling that will likely make sense for your environment. The biggest tools for this type of automation, for infrastructure as <a id="_idIndexMarker760"/>code, at <a id="_idIndexMarker761"/>the <a id="_idIndexMarker762"/>time <a id="_idIndexMarker763"/>of <a id="_idIndexMarker764"/>this <a id="_idIndexMarker765"/>writing <a id="_idIndexMarker766"/>include <strong class="bold">Chef</strong>, <strong class="bold">Puppet</strong>, <strong class="bold">SaltStack</strong>, <strong class="bold">CFEngine</strong>, <strong class="bold">Ansible</strong>, and <strong class="bold">Terraform</strong>. The oldest of these, <strong class="bold">CFEngine</strong>, is so old that it was first introduced in its earliest form less than two years after the first Linux kernel was introduced!</p>
			<p>All of these tools, often referred to as configuration management systems, share the common approach of allowing you to write code and configuration files to define your infrastructure environment and they manage the automation of the infrastructure based on that code. Most of them offer multiple functional behaviors such as the option to function either imperatively or declaratively. Most also offer the option to either pull configuration from the systems being managed or to push configuration from a central management location. So you get the range of functional options plus a range of product options.</p>
			<p>The biggest differences between these products really come down to the style of coding that is used to document your infrastructure and the cost, support, or licensing of the products. Most of the products in this space either started as or moved to being open source. Open source makes a lot of sense as the space has matured quickly and management tooling somewhat naturally tends towards open source as it is mostly made by end users or those that came from open source communities and is used only in technical circles. Closed source products naturally lend themselves towards more customer-visible products where managers, rather than engineers, are choosing them. A key strength to most infrastructure as code tools is that they are free, open source, and often included in the Linux distribution so engineering and administration teams can generally choose to test and deploy them even into production without management needing to approve or even be aware that they are doing so. Because of the licensing and use cases it is little different than needing to deploy OpenSSH or any other standard component used in day-to-day system administration.</p>
			<p>Of course the easiest approach here is to simply read about a few tools, download and install a few, and see what you like. Working with more than one is not a bad thing and most of the concepts will carry from one to another, even if the style of scripting and documentation vary. Many of these tools are cross platform and while they are generally designed with Linux as the primary platform both for deployment and to be managed, it is not uncommon for other platforms, especially BSD but also Windows, macOS, and others, may be able to be managed as well. Consider the possibility of choosing a platform that will be expandable to meet all of your organization's needs well into the future. If you are a heterogeneous shop you may want to invest your technical <a id="_idIndexMarker767"/>know-how into a <a id="_idIndexMarker768"/>platform that could be managing everything across the domains rather than only managing Linux, even if Linux is where you start.</p>
			<h3>Desktops are servers</h3>
			<p>Thinking of desktops<a id="_idIndexMarker769"/> as a form of server can be a bit confusing, but in a way, they are. They are simply one to one end user GUI servers that are generally deployed to desktops or homes rather than in the datacenter. This is a trivial bit of semantics until we start trying to understand how desktops may or may not fit into our greater support strategies.</p>
			<p>But when we consider that desktops are truly just a special class of servers it quickly becomes apparent that it makes sense to potentially group them in for common administration as well. Of course if we are using a Linux distribution for a desktop this is much more obvious than if we are using Windows, for example, but the truth remains the same. If we are going to potentially support Linux and Windows servers both using the same tooling, there is really no barrier to doing the same for desktops regardless of the operating system(s) deployed there.</p>
			<p>Traditionally, but for no reason of which I am aware, servers and desktops have been managed using very different toolsets. I can hypothesize many reasons why this might be. Typically, two separate teams provide this management and each chooses its own tooling independently. Servers grew up in one world while desktop support grew up in another. Vendors want to sell more tools and catering to hubris always makes sales easy. And the most likely factor: servers are typically administered from the command line and most desktop support teams expect to work purely from a GUI.</p>
			<p>When we step back and look at desktops (and other end user devices) as if they are servers it is easy to see that the same tools to manage, document, and monitor servers work equally well with desktops. There is no real reason to treat them differently. Of course desktop support teams will always require the ability to remotely see, and probably interact with, the end user's graphical desktop to be able to assist the end user's issues directly, but that is a different need entirely from administration duties. </p>
			<p>Desktops can be, or even should be, managed using the same advanced tools and techniques like infrastructure as code and state machines as we would with any other server. That is correct, when we understand that they are actually a server, then rules that apply to all servers also apply to desktops. Good semantics make everything easier to understand.</p>
			<p>In fact, there is even a good argument that end user devices are among the most valuable to apply these techniques to because they are the most likely to need to be rebuilt, modified, share profiles, undergo changes, be compromised, or need to be managed while not in communication with central services. This last point is especially valuable because <a id="_idIndexMarker770"/>state machines that keep functioning with their current state will continue to provide security and self-healing characteristics for machines that are off of the network and can enforce policies independently. This is possible to do in more traditional ways but is harder and less likely to be done.</p>
			<p>Because of the nature of how infrastructure as code systems tend to be deployed it is far more likely, as well, for these systems to keep functioning when a desktop or laptop is off of the corporate LAN compared to traditional management tooling that is generally built entirely around the LAN concept. Because end user devices traditionally have a high frequency of either moving on and off of or simply never existing on the LAN having tools that are not LAN-centric is typically much more important in the end user space than in the datacenter space already.</p>
			<p>In some ways, mobile device management, or MDM as it is generally known, is an attempt to make tools that work more likely infrastructure as code and state machines, but that are presented more like traditional tools and sold through more traditional channels with a focus solely on end user management. These tools have been successful, I feel, because they are copying much of the technology and common approaches from this space and once we work with these tools we typically find that mobile device management tools do not make sense unless we are lacking these capabilities in our organizations.</p>
			<p>Many of the benefits of any new system, of course, come from convention rather than strict definition. One of the largest conventions in the infrastructure as code space is the move from LAN-centric to network agnostic system management deployments. It is common, and nearly expected, that the tooling for infrastructure as code systems will be hosted in some public form whether on cloud, VPS, or colocation, and kept external to any LAN(s) that may exist for your organization. </p>
			<p>Hosting management infrastructure outside of the LAN means that any LAN-centric ties that we might otherwise make accidentally or casually are no longer possible unless we deploy a LAN extension technology like a VPN. This convention naturally moves us away from deploying technology that only works inside of a LAN or that uses the LAN boundaries as security features. Eliminating the LAN boundaries frees us to manage multiple sites, mobile users, even multiple organizations transparently from a single platform, as long as those systems use the Internet. Traditional systems break under so many<a id="_idIndexMarker771"/> circumstances, while also having common security vulnerabilities both because of the tendency to break easily or have gaps in utilization, and because LAN-centric thinking is not good security practice.</p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor196"/>Version control systems</h2>
			<p>The other <a id="_idIndexMarker772"/>category of tools that we should<a id="_idIndexMarker773"/> really address within the topic of automation is code repositories and version control systems. These are technically two separate things, but almost always go hand in hand.</p>
			<p>At their core, version control systems simply keep track of the changes made to documents that we have so that we can track essential data such as who made a change, when it was made, and what the document looked like before, at the time of, and at a later date than the change. This alone is quite powerful, but most any system that does this today also serves to distribute the code and the version control so that it can be used by multiple people, in multiple places. And in being able to do that, can also be used to populate a central repository which can be treated as a master location for storage so that no single person's endpoint needs to be considered vital for the purposes of data protection. That central location becomes a location for backups and recovery, as well!</p>
			<p>Version control systems influence many modern document systems and we covered both in our earlier chapter on documentation, but in this context we could look at real world products such as Google Docs, Microsoft Office Online, and Zoho Docs, all of which present traditional document file types or interfaces, but all provide version control of those documents. These are very clunky to use for the purpose of coding and code management, but all will serve in a pinch if you are looking to just get started quickly using what you already have deployed. These systems are essentially copying the mechanisms of traditional code version control systems and applying them to spreadsheets and word processing.</p>
			<p>Since these office document types are so well known, it is almost easier to picture these as the standard (they are not) and thinking of code version control systems as being a code-specific modification of those document tools (they are not, they came first.) These systems generally (and by generally, I mean every situation that I am aware of) work with standard text files and so can be used with any text editing tools whether you work with something basic like <em class="italic">vi</em> or <em class="italic">nano</em> directly on the Linux command line or you work with robust tools like <em class="italic">Atom </em>or <em class="italic">MS Visual Studio Code</em> that provide fully graphical coding environments with deep editing awareness and features, you can use version control systems. Some <a id="_idIndexMarker774"/>advanced environments will <a id="_idIndexMarker775"/>actually have version control integrated directly into the applications so that you can automate the entire process from a single place and make it look and feel much closer to the office style tools! </p>
			<p>In a practical sense, at this time, two protocols for version control have risen so far to the top that it almost feels like there are only two choices really left on the market: <em class="italic">git</em> and <em class="italic">mercurial.</em> In reality, there are many, but only these two require mentioning. Feel free to research other tools and protocols, but make sure that these two are included in any research short list that you might have. Both are free and work similarly and allow for all of the features that you often expect today including central repositories, copies on end user machines, automated deployments, version meta data, and so on.</p>
			<p>Beyond the protocols that are used, much of the power stemming from version control systems today come from the online repository services that power them. Of these there are more and you can run your own in house as well. The two key players are Microsoft's GitHub and the open source GitLab. Both are hosted services with extensive free offerings, and GitLab also offers their software for free that you can host yourself if this is a business or technical requirement for your environment. These two services, and others like them, provide central git and Mercurial repository locations, a centralized location for backups, a simple web GUI for code manipulation and management, and a raft of processes, tools, and services around code automation. Much of which is likely overkill or useless in a system administration environment, but much of it does have a potential use. You can certainly get the benefits that you need without these types of services, but it is far harder to do so and nearly all successful environments have been relying on them for years. They are almost always free for the needs of system administration, do not avoid them. As hosted services, their use is yet another means of <em class="italic">breaking free</em> from LAN thinking, as well.</p>
			<p>There is not much of best practices or even rules of thumb to discuss when talking about tools. Testing multiple tools, keeping up with the market as to what is available and what is nearly developed, evaluating the tools that make sense for your organization, and learning your <a id="_idIndexMarker776"/>chosen tools inside and out<a id="_idIndexMarker777"/> are all standard good approaches.</p>
			<p>Rules of Thumb: </p>
			<ul>
				<li>Management tools should almost always be open source. This is an area where security is of the absolute utmost importance and where licensing limitations create security risks on their own. So open-source matters more here than in most areas.</li>
				<li>Deploy management tools in a network-agnostic way. This means deploying them on the Internet in a place that is accessible to reasonably any machine located anywhere. Avoid any semblance of requiring or relying upon traditional LAN networking for connectivity or security unless absolutely necessary.</li>
			</ul>
			<p><strong class="bold">Best Practice</strong>: Keep<a id="_idIndexMarker778"/> code of all types under version control and in a repository.</p>
			<p>Now we have covered the techniques and talked briefly about a handful of real world tools that you can use to initiate your investigations into tools that you want to try to deploy and learn for your own environment.</p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor197"/>Summary</h1>
			<p>In this chapter we have looked at why automation is important. We investigated how we should approach automation and where to look to get started. We discussed maturity modeling. We delved into the rather complex topics of state machines and infrastructure as code. And finally we tackled actual tools that you can download and learn today to take your coding to a totally different level, entirely.</p>
			<p>In our next chapter we are going to be going into one of the absolutely most important, and most commonly avoided, topics in system administration: backups and disaster recovery. Do not try to skip over this coming chapter, if there is one thing that we need to get right in administration, it is our ability to avoid or recovery from a disaster.</p>
		</div>
	</div></body></html>