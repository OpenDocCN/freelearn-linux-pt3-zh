- en: '*Chapter 4*: Designing System Deployment Architectures'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第四章*：设计系统部署架构'
- en: How we deploy systems determines so much about how those systems will perform
    and how resilient they will be for years to come. A good understanding of design
    components and principles is necessary for us to understand in order to approach
    the design of the platforms that will carry our workloads. Remember, at the end
    of the day, only the applications running at the very top of the stack matter
    - everything beneath the applications, whether the operating system, hypervisor,
    storage, hardware, and others are just tools used to enable the final application-level
    workloads to do what they need to do best. It is easy to feel that these other
    components matter individually, but they do not. To put it another way, what matters
    is the results rather than the path taken to get to the results.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何部署系统决定了这些系统今后将表现如何以及它们的弹性将持续多久。了解设计组件和原则对我们理解如何设计将承载我们工作负载的平台至关重要。请记住，归根结底，只有位于堆栈顶端的应用程序才真正重要
    - 在应用程序之下的一切，无论是操作系统、虚拟化程序、存储、硬件还是其他工具，都只是用来实现最终应用级工作负载所需功能的工具。很容易感觉到这些其他组件的重要性，但实际上它们并不重要。换句话说，重要的是结果，而不是达到结果的途径。
- en: In this chapter, we are going to start by looking at the building blocks of
    systems (other than storage which we tackled extensively in our last chapter before
    taking all of those components as a whole and looking at them, to see how they
    can form robust carriers for our application workloads. Next, we will look at
    need analysis. Then finally we will move on to assembling those pieces into architectural
    designs to meet those needs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先查看系统的构建模块（除了我们在上一章中广泛讨论的存储外），然后将所有这些组件作为一个整体来查看它们，以了解它们如何形成我们应用工作负载的强大载体。接下来，我们将进行需求分析。最后，我们将继续将这些组件组装成满足这些需求的架构设计。
- en: By the end of this chapter, you should feel confident that, while Linux is potentially
    only one slice in the middle of our application stack, you are prepared to design
    the entire stack properly to meet workload goals. While technically much of this
    design is not strictly systems administration (or engineering) it most often falls
    to the system administrators to handle as only the rarest of organizations have
    highly skilled and end to end knowledgeable staff from other departments. The
    systems team sits at the nexus of all components and has the greatest single role
    visibility in both directions (up the stack to the applications and down the stack
    to hypervisors, hardware, and storage). It is natural that systems teams are tasked
    with the greater design tasks as there is no one else capable.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章的学习，您应该对此感到自信，尽管Linux可能只是我们应用程序堆栈中的一个片段，但您已准备好正确设计整个堆栈，以满足工作负载目标。尽管从技术上讲，这种设计大部分并不严格属于系统管理（或工程），但它通常由系统管理员来处理，因为只有极少数组织拥有来自其他部门高技能和全面知识的员工。系统团队处于所有组件的交汇点，并在两个方向上（向上到应用程序和向下到虚拟化程序、硬件和存储）具有最大的单一角色可见性。系统团队被赋予更大的设计任务是很自然的。
- en: 'In this chapter we are going to learn about the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: Virtualization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟化
- en: Containerization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器化
- en: Cloud and **Vitual Private Server** (**VPS**)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云和**虚拟专用服务器**（**VPS**）
- en: On premises, hosted, and hybrid hosting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自有、托管和混合托管
- en: System design architecture
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统设计架构
- en: Risk assessment and availability needs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风险评估和可用性需求
- en: Availability strategies
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用性策略
- en: Virtualization
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟化
- en: Twenty years ago, if you asked the average system administrator what virtualization
    was they would look at you with a blank stare. We have had virtualization technologies
    in IT since 1965 when IBM first introduced them in their mainframe computer systems,
    but for your average company these technologies were relatively rare and out of
    reach until vendors like *VMware* and *Xen* brought these to the mainstream market
    around the turn of the millennium. The enterprise space did have many of these
    technologies by the 1990s, but knowledge of them did not disseminate far.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 二十年前，如果你问一个普通的系统管理员什么是虚拟化，他们可能会一头雾水。自从IBM在1965年首次在其大型计算机系统中引入虚拟化技术以来，我们就已经在IT领域中拥有了这些技术，但对于普通公司来说，这些技术相对较少且难以获得，直到像*VMware*和*Xen*这样的供应商在千禧年之交将它们带入了主流市场。企业领域在20世纪90年代已经使用了许多这些技术，但对它们的了解并没有广泛传播开来。
- en: Times have changed. Since 2005, virtualization has been broadly available and
    widely understood, with options for every platform and at all price points, leaving
    no one with a need to avoid implementing the technology because it is out of technical
    or financial reach. At its core, virtualization is an abstraction layer that creates
    a computer *in software* (on top of the actual hardware) and presents a standard
    set of virtual hardware. Software that performs virtualization is called a **hypervisor**
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 时代已经变了。自2005年以来，虚拟化已经广泛普及并被广泛理解，每个平台和所有价格点都有选择，没有人因为技术或财务上的限制而需要避免实施这项技术。在其核心，虚拟化是创建一个*软件中的计算机*（在实际硬件的顶部）并呈现标准虚拟硬件集的抽象层。执行虚拟化的软件称为**hypervisor**
- en: In the last chapter we spoke repeatedly about interfaces and how something consumes
    or presents itself as a disk drive or file system, for example. A hypervisor is
    software that presents a *computer interface*, meaning it doesn't just present
    a hard drive, but it acts like an entire computer. If you have never used or thought
    about virtualization this might seem extraordinarily complex and confusing, but
    in reality, this is an abstraction that often makes computing far simpler and
    more reliable. Just like technologies that abstracted storage (like **Logical
    Volume Managers** and **RAID** systems) proved to be incredibly valuable once
    they were mature and understood, so has computer level virtualization.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们反复谈到接口以及某些东西如何消耗或呈现自己作为磁盘驱动器或文件系统，例如。hypervisor是一种呈现*计算机接口*的软件，这意味着它不仅仅呈现硬盘驱动器，而是像整个计算机一样运行。如果您从未使用过或思考过虚拟化，这可能看起来非常复杂和令人困惑，但实际上，这是一个通常使计算变得更简单和更可靠的抽象。就像抽象存储的技术（如**逻辑卷管理器**和**RAID**系统）一样，在其成熟和理解之后，计算机级虚拟化也被证明是非常有价值的。
- en: 'There are two types of hypervisors that we will talk about in this chapter,
    and they are simply known as Type 1 and Type 2 hypervisors. All hypervisors present
    the same thing: a *computer*. But what makes a Type 1 and a Type 2 hypervisor
    different is what they consume.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论两种类型的hypervisor，它们分别被称为Type 1和Type 2 hypervisor。所有的hypervisor都呈现同样的东西：一个*计算机*。但Type
    1和Type 2 hypervisor之间的区别在于它们消耗的内容。
- en: Type 1 hypervisor
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Type 1 hypervisor
- en: Sometimes called a *bare metal* hypervisor, Type 1 hypervisors are intended
    to run directly on the system hardware but, of course, can run on anything presenting
    itself as a capable piece of hardware (such as another hypervisor!) As such, a
    Type 1 Hypervisor is not an application and does not run on top of an operating
    system and so only needs to worry about hardware compatibility with the physical
    device on which it will be installed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有时称为*裸金属*hypervisor，Type 1 hypervisors旨在直接在系统硬件上运行，但当然也可以在任何表示自己为能够运行的硬件（如另一个hypervisor！）上运行。因此，Type
    1 Hypervisor不是应用程序，并且不运行在操作系统之上，因此只需要担心与将安装的物理设备的硬件兼容性。
- en: Type 1 hypervisors are generally the only type considered true ready for production
    because they install directly without any unnecessary software layers and so can
    be faster, smaller, and more reliable.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，Type 1 hypervisor被认为是真正适合生产的唯一类型，因为它们直接安装而无需任何不必要的软件层，因此可以更快、更小和更可靠。
- en: The Type 1 hypervisor was more difficult to initially engineer and so the earliest
    hypervisors were generally other types that could pass off work to the operating
    system. But effectively it was the introduction of the Type 1 hypervisor and enough
    vendors with disparate products to warrant a mature market designation that encouraged
    the extreme move to virtualization in the 2000s.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Type 1 hypervisor最初更难工程化，因此最早期的hypervisor通常是其他可以将工作传递给操作系统的类型。但事实上，是Type 1 hypervisor的引入和足够多的不同产品供应商使市场成熟，鼓励了在2000年代极端转向虚拟化。
- en: Type 2 hypervisor
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Type 2 hypervisor
- en: Unlike the bare-metal hypervisor, a Type 2 hypervisor is an application that
    you install onto an operating system. This means that the hypervisor has to wait
    for the operating system to give it resources, competes with other applications
    for resources, and requires that the operating system itself is stable, in addition
    to the hypervisor being stable, in order to keep workloads running on top of it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与裸金属hypervisor不同，Type 2 hypervisor是一个安装在操作系统上的应用程序。这意味着hypervisor必须等待操作系统提供资源，与其他应用程序竞争资源，并要求操作系统本身稳定，除了hypervisor稳定外，还需要使工作负载在其上运行。
- en: When virtualization was relatively new, especially in the microcomputer arena,
    Type 2 Hypervisors were much more common because they were cheaper and easier
    to make and have little need for hardware support to do what they do. A Type 2
    Hypervisor lets the bare metal operating system do the heavy lifting of supplying
    drivers and hardware detection, task scheduling, and so forth. So, in much of
    the 2000s we saw Type 2 Hypervisors taking a principal role in driving virtualization
    adoption. They are easy to deploy and very easy to understand and because they
    are just an application that gets deployed on top of an operating system anyone
    can just install one on an existing desktop or even laptop to try out virtualization
    for themselves.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当虚拟化技术相对较新时，尤其是在微型计算机领域，由于成本更低且更易于制造，并且几乎不需要硬件支持来完成其工作，因此类型 2 虚拟化监视程序更为常见。类型
    2 虚拟化监视程序让裸金属操作系统来处理供给驱动程序和硬件检测、任务调度等繁重工作。所以在 2000 年代的大部分时间里，我们看到类型 2 虚拟化监视程序在推动虚拟化技术采用中扮演了主要角色。它们易于部署，非常容易理解，因为它们只是一种在操作系统之上部署的应用程序，任何人都可以在现有的台式机甚至笔记本电脑上安装一个来尝试虚拟化技术。
- en: By the late 2000s, technology had changed rather significantly, the software
    had advanced and matured, and nearly all computers had gained some degree of hardware
    assistance for virtualization, allowing hypervisors to use less code while gaining
    much better performance. Type 1 hypervisors rapidly proliferated, and, before
    2010, the idea of using a Type 2 hypervisor in production was all but unthinkable.
    Type 1 hypervisors provide a single, standard operating system installation target,
    moving the heavy lifting away from the operating system and over to the hypervisor,
    where it is generally accepted to be better positioned. Because the hypervisor
    controls the bare metal, it is able to properly schedule system resources and
    eek maximum performance out of a system. Hypervisors are expected to be only a
    small fraction of the size of an operating system. This means little more than
    a shim between virtualized operating systems and the physical system (a tiny layer
    of code doing the bare minimum, and being essentially invisible to the operating
    system running on top of it). This minimizes bloat and features, while operating
    systems need to be large, complex, and feature-rich to do their jobs well in most
    cases.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到了 2000 年代末，技术发生了相当大的变化，软件得到了先进和成熟的发展，几乎所有计算机都获得了某种程度的虚拟化硬件支持，允许虚拟化监视程序使用更少的代码同时获得更好的性能。类型
    1 虚拟化监视程序迅速扩展，到了 2010 年之前，在生产环境中使用类型 2 虚拟化监视程序的想法几乎是不可想象的。类型 1 虚拟化监视程序提供了一个统一的标准操作系统安装目标，将大部分繁重工作从操作系统转移到监视程序，被普遍认为是更佳选择。因为监视程序控制裸金属，能够适当调度系统资源，并从系统中获得最大性能。监视程序预计只占操作系统大小的一小部分。这意味着它几乎只是虚拟化操作系统和物理系统之间的一个垫片（一层极少代码的基本功能层，对于运行在其上的操作系统基本不可见）。这最小化了膨胀和功能，而操作系统通常需要庞大、复杂和功能丰富以在大多数情况下完成其工作。
- en: Type 2 Hypervisors have proven to be useful in lab environments, especially
    for situations where testing or learning is best done from a personal computing
    environment such as a desktop or laptop or can be useful for special case temporary
    workloads where there is a need to completely disable or possibly even to remove
    the hypervisor when it is no longer needed. But for production server environments
    only Type 1 Hypervisors are really appropriate today.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 类型 2 虚拟化监视程序已被证明在实验环境中非常有用，尤其是在需要从个人计算环境（例如台式机或笔记本电脑）进行测试或学习的情况下，或者对于需要完全禁用或甚至在不再需要时可能移除监视程序的特殊临时工作负载也很有用。但对于生产服务器环境来说，只有类型
    1 虚拟化监视程序才真正合适。
- en: There are two best practices commonly associated with virtualization
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化技术通常与两项最佳实践相关联。
- en: Virtualize every system, unless a requirement makes you unable to do so. In
    practical terms, you will never realistically see a valid exception to this rule.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除非有要求使您无法这样做，否则应虚拟化每个系统。在实际操作中，您几乎永远不会真正看到这条规则的有效例外。
- en: Always use a Type 1 (Bare Metal) Hypervisor for servers.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器始终使用类型 1（裸金属）虚拟化监视程序。
- en: Hypervisor types are confusing
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟化监视程序的类型令人困惑。
- en: In the real world detecting what is and is not a Type 1 Hypervisor can be rather
    difficult. A hypervisor, by definition, really does not have any kind of end user
    interface of its own. This makes it something that we have to explain, but not
    something that we really see. Even a true operating system is hard to point to
    and say *see, there it is* because it is really a shell or a desktop environment
    running as an application on top of the operating system, rather than the operating
    system itself, that we see and touch. With a hypervisor, any interface that we
    see, of any sort, has to be being presented by something running on an operating
    system, not something running directly on the hypervisor.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，要检测什么是 Type 1 虚拟化管理程序可能相当困难。按定义，虚拟化管理程序实际上没有任何自己的最终用户界面。这使得我们必须解释它，但并不是我们真正看到的东西。即使是真正的操作系统也很难指出并说“看，它在这里”，因为我们真正看到和触摸的是作为应用程序在操作系统顶部运行的壳或桌面环境，而不是操作系统本身。对于虚拟化管理程序来说，我们看到的任何界面，无论是什么样的，都必须由运行在操作系统上的东西来呈现，而不是直接在虚拟化管理程序上运行的东西。
- en: Hypervisors, of course, need some sort of interface for us to interact with
    them. How they handle this varies wildly and not all hypervisors, even of the
    same type, are built the same. Under the hood, of course, they are always running
    on the bare metal, but they can use several different architectures to handle
    all of the functions that they need. Each different architecture has a different
    opportunity for how it will seem to appear to an end user – meaning that an end
    user sitting down to the system may experience wildly different interfaces that
    pretend to be things that they are or possibly are not.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，虚拟化管理程序需要某种接口来与我们互动。它们处理这个接口的方式差异很大，即使是同类型的虚拟化管理程序，构建方式也不尽相同。在幕后，它们总是在裸金属上运行，但它们可以使用几种不同的架构来处理它们所需的所有功能。每种不同的架构都有不同的机会，展示给最终用户的方式也不同
    - 这意味着一个坐到系统前的最终用户可能会体验到完全不同的界面，假装是它们或者可能并不是它们。
- en: In early Type 1 Hypervisors it was common to run a virtual machine (the name
    for a virtualized computer running on top of a hypervisor of any type) that had
    privileges to control the hypervisor given to it. This allows the hypervisor to
    be as lean as possible and allows big tasks like presenting a user interaction
    shell to be done using existing tools and no one had to reinvent the wheel. Using
    this approach meant that hypervisor engineering work was minimal in the early
    days allowing the virtualization itself to be the key focus.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期的 Type 1 虚拟化管理程序中，运行一个虚拟机（虚拟化管理程序顶部运行的虚拟计算机的名称）通常会赋予它控制虚拟化管理程序的权限。这使得虚拟化管理程序可以尽可能精简，并且可以使用现有工具完成像展示用户交互
    shell 这样的大任务，而无需重新发明轮子。采用这种方法意味着早期的虚拟化管理程序工程工作最小化，使得虚拟化本身成为关键关注点。
- en: As time has progressed, alternative approaches have begun to emerge. Running
    a full operating system in a virtualized environment on top of the hypervisor
    just to act as an interface for the end user felt like a waste of resources. Later
    hypervisors used creative ways to get around this making the hypervisor itself
    heavier but reducing the overall weight of the total system.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，开始出现了替代方法。在虚拟化管理程序顶部运行完整操作系统以充当最终用户接口似乎是资源的浪费。后来的虚拟化管理程序采用创造性方法来解决这个问题，使得虚拟化管理程序本身更重，但总体系统的重量减少。
- en: Virtualization tends to be confusing and vendors have little reason to want
    to expose the inner workings of their systems. It has therefore become commonplace
    to misuse terms or to suggest that hypervisors work differently than they do either
    for marketing reasons or to attempt to simplify the system for less knowledgeable
    customers. There are many hypervisors today and potentially more will arise in
    the future. In production enterprise environments, however, there are four that
    we expect to see with any regularity and we will briefly break down each one to
    explain how it works. No one approach is best, these are simply different ways
    to skin the same cat.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化往往令人困惑，供应商很少有理由想要揭示其系统的内部工作原理。因此，误用术语或建议虚拟化管理程序的工作方式与实际不同，无论出于营销原因还是为了试图简化对不太了解的客户的系统。今天有许多虚拟化管理程序，未来可能会有更多。然而，在生产企业环境中，我们预计会定期看到四种，我们将简要分析每种工作方式。没有一种方法是最佳的，这些只是解决同一个问题的不同方式。
- en: VMware ESXi
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VMware ESXi
- en: The market leader of virtualization today. VMware is one of the oldest virtualization
    products and has changed its design over time. Originally VMware followed the
    classic design of an extremely lean hypervisor and a *hidden* virtual machine
    that ran on top of it running a stripped down copy of Red Hat Enterprise Linux
    which provided the end user facing shell for interaction with the platform.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的虚拟化市场领导者。VMware 是最古老的虚拟化产品之一，随着时间的推移，其设计也有所变化。最初，VMware 遵循经典设计，采用了极简的超级监控程序和一个*隐藏*的虚拟机，其上运行了一个简化版的
    Red Hat Enterprise Linux，为用户提供了与平台交互的外壳界面。
- en: Today VMware ESXi instead builds a tiny shell into the hypervisor itself that
    provides only enough potential user interaction to handle the simplest of tasks
    such as detecting the IP address that is in use or setting the password. Everything
    else is handled through an API that is called from an external tool allowing for
    the heaviest portions of the user interface to be kept completely on the physical
    client workstation rather than on the hypervisor.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，VMware ESXi 将一个小型 shell 构建到超级监控程序本身中，仅提供足够的潜在用户交互能力来处理最简单的任务，如检测正在使用的 IP
    地址或设置密码。其他所有功能通过从外部工具调用的 API 处理，允许最重要的用户界面部分完全保留在物理客户工作站上，而不是在超级监控程序上。
- en: Microsoft Hyper-V
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Microsoft Hyper-V
- en: Although late to the enterprise Type 1 virtualization game, Microsoft opted
    for the classic approach and always runs a virtual machine in which is contained
    a stripped down copy of Windows which provides the graphical user interface that
    end users will see when having installed Hyper-V. This first virtual machine is
    installed automatically, by default requires no paid licensing, does not contain
    Windows branding, and is contrarily named the *physical* machine which together
    can make it appear that there is no VM at all, but rather a Type 2 hypervisor
    running on top of a Windows install, but this is not the case. It simply appears
    so based on tricky naming and the unnecessarily convoluted standard install processes.
    Doubt not, Hyper-V is a true Type 1 hypervisor running in the most classic way.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管晚于企业级 Type 1 虚拟化的竞争，微软选择了经典方法，始终运行一个虚拟机，其中包含一个简化版的 Windows，提供了安装 Hyper-V 后用户所见的图形用户界面。这个第一个虚拟机是默认自动安装的，不需要付费许可，不包含
    Windows 品牌，相反它被命名为*物理*机器，这使得它看起来好像根本没有虚拟机，而是在 Windows 安装之上运行的 Type 2 超级监控程序，但实际并非如此。这只是基于复杂的命名和不必要的混乱标准安装过程而看似如此。毋庸置疑，Hyper-V
    是一种真正的 Type 1 虚拟化监控程序，以最经典的方式运行。
- en: Xen
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Xen
- en: Coming from the same early era as VMware, Xen started with the classic approach,
    but, unlike Vmware, stuck with it over the years. However, unlike Hyper-V, Xen
    installations tend to be more manual, and the use of a first virtual machine for
    the purpose of providing end user interactions is not in any way hidden and, in
    fact, is completely exposed. What this means is that during the hypervisor installation
    process a virtual machine is created automatically (so it is always the first
    one) and that virtual machine is given special access to directly manipulate the
    console. So, what you see once it turns on is the console of the virtual machine
    itself as the hypervisor does not have one besides that.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与 VMware 同样来自早期时代的 Xen 也采用了经典方法，但与 VMware 不同的是，多年来一直保持这种方式。然而，与 Hyper-V 不同的是，Xen
    的安装过程更加手动化，用于提供用户交互的第一个虚拟机完全不被隐藏，事实上是完全暴露的。这意味着在超级监控程序安装过程中，一个虚拟机会被自动创建（因此它总是第一个），并且该虚拟机被特别授予直接操作控制台的特殊权限。因此，一旦启动，您看到的是虚拟机本身的控制台，因为超级监控程序除此之外没有其他控制台。
- en: You *can* even choose between different operating systems to use in the management
    virtual machine! In practice, however, Xen is always used with Linux as its control
    environment. Other operating systems are mostly theoretical. This exposure makes
    hidden classic systems, like Hyper-V, all the more confusing because in Xen it
    is so obvious how it all works.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*你*甚至可以选择在管理虚拟机中使用不同的操作系统！然而，在实践中，Xen 始终与 Linux 作为其控制环境一同使用。其他操作系统大多是理论上的。这种暴露使得像
    Hyper-V 这样的隐藏经典系统变得更加令人困惑，因为在 Xen 中，一切是如此显而易见。'
- en: Because Xen and Linux go together so tightly, it can be valuable for a Linux
    system administrator to have at least some knowledge of Xen, Xen management via
    Linux, and Xen architecture. This tight coupling does not ensure that systems
    and platform teams will become intertwined when using Xen, but it makes the likelihood
    higher.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Xen 与 Linux 紧密结合，对于 Linux 系统管理员来说，了解 Xen、通过 Linux 管理 Xen 和 Xen 架构至少有些知识可能很有价值。这种紧密结合并不意味着在使用
    Xen 时系统和平台团队会相互交织，但它增加了这种可能性。
- en: KVM
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KVM
- en: Finally, we come to the **Kernel-based Virtual Machine** (**KVM**). KVM is special
    for a few reasons. First because it uniquely takes the approach of merging the
    hypervisor into the operating system itself. And second because it does so with
    Linux. Unlike other hypervisors where you have a clear separation between the
    platform administration who manages the hypervisor and the system administrator
    who manages the operating system level. Here, the two roles must be merged into
    one because the two components have been merged into one. You cannot separate
    the operating system and the hypervisor when using KVM. KVM bakes the hypervisor
    right into the Linux kernel. It is simply part of the kernel itself and always
    there.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来谈谈**基于内核的虚拟机**（**KVM**）。KVM 之所以特别，有几个原因。首先，它独特地将虚拟化监视程序（hypervisor）合并到操作系统本身中。其次，它与
    Linux 一起工作。与其他虚拟化监视程序不同，其他程序中平台管理者负责管理虚拟化监视程序，系统管理员负责管理操作系统层面。而在这里，这两种角色必须合而为一，因为这两个组件已经合并为一个。在使用
    KVM 时，无法分开操作系统和虚拟化监视程序。KVM 将虚拟化监视程序直接嵌入 Linux 内核中。它只是内核的一部分，始终存在。
- en: This approach has clear benefits. It simplifies the entire system and provides
    most of the advantages of each different approach with relatively few caveats.
    There are caveats, of course, such as that there is a real risk of bloat in the
    hypervisor install which increases the potential attack surface. KVM's biggest
    benefit is probably that it leverages the ecosystem of Linux system administrators
    and existing knowledge so that some of the more complex aspects of managing a
    system that runs on bare metal such as filesystem and other storage architecture
    decisions, driver support, and hardware troubleshooting are all shared with Linux
    giving an enormous base platform and support network from which to begin.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法具有明显的好处。它简化了整个系统，并提供了几乎每种不同方法的优点，只有相对较少的注意事项。当然，也存在注意事项，例如虚拟化监视程序安装可能导致膨胀，增加潜在攻击面的真实风险。KVM
    最大的好处可能是利用 Linux 系统管理员和现有知识的生态系统，因此在管理运行在裸机上的系统的某些更复杂的方面，如文件系统和其他存储架构决策、驱动程序支持和硬件故障排除中，与
    Linux 共享，从而获得了一个庞大的基础平台和支持网络。
- en: Because of KVM's easy and well-known licensing (due to its inclusion in Linux),
    well known development potential, and broad availability of components it has
    become far and away the most popular means of building your own hypervisor platform
    when vendors want to create something of their own. Many large vendors in the
    cloud, hypervisor management, or hyperconvergence space have leveraged KVM as
    the base of their systems with customizations layered on top.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 KVM 的易用性和广为人知的许可（由于其包含在 Linux 中），以及广泛的组件可用性，当供应商希望创建自己的虚拟化监视程序平台时，KVM 已成为远远最受欢迎的选择。许多云端大型供应商、虚拟化监视程序管理或超融合空间已利用
    KVM 作为其系统的基础，并在其上进行了定制化。
- en: Is virtualization only for consolidation?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟化只是为了整合吗？
- en: 'Ask most people why you *bother* to virtualize and consistently the same answer
    is repeated: *Because it allows you to run several disparate workloads on a single
    physical device.* There is no doubt that consolidation is a massive benefit, when
    it applies, but stating this as the only, or even the key, benefit means we are
    missing the big picture.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人问你为什么*费心*虚拟化，答案总是相同的：*因为它允许你在单个物理设备上运行多个不同的工作负载。* 毫无疑问，当适用时，整合是巨大的好处，但将其陈述为唯一的甚至关键的好处意味着我们忽略了大局。
- en: The enduring myth that we virtualize to save money through consolidation is
    one that it seems no one is going to be able to dispel. The nature of virtualization
    is simply too complex for the average person, even the average IT professional,
    and what it really provides remains broadly misunderstood. The real value of virtualization
    is in the abstraction layer that it creates which provides a hedge against the
    unknown - a way to make system deployments more flexible, and more reliable, while
    incurring less overall effort. Virtualization gives you more options for that
    unknown event happening sometime in the future that you cannot plan for.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化为了节省成本通过合并的持久神话似乎没有人能够驱散。虚拟化的性质对于普通人，甚至是普通的IT专业人士来说都过于复杂，它真正提供的东西仍然被广泛误解。虚拟化的真正价值在于它创建的抽象层，这提供了对未知的避险方式
    - 一种使系统部署更灵活、更可靠，同时总体付出更少努力的方法。虚拟化为未来可能发生的未知事件提供了更多选项，这是你无法预计的。
- en: A core challenge for virtualization is that it offers simply too many benefits
    that do not always relate to one another. Most people want a simple, stable answer
    and do not want to understand how exactly virtualization works and why adding
    a layer of additional code actually makes systems simpler and more reliable. It
    is all a bit too much. The reality is that virtualization has many benefits, each
    of which is typically enough to justify always using it, and essentially no caveats.
    Virtualization has no cost, essentially no performance overhead, does not add
    management complexity (it does, but only in some areas while reducing it in others
    resulting in an overall reduction.)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于虚拟化的一个核心挑战是它提供了太多不一定相互关联的好处。大多数人希望得到一个简单、稳定的答案，不想理解虚拟化的工作原理以及为什么添加额外代码层实际上会使系统更简单、更可靠。这些都有点过于复杂。事实上，虚拟化有很多好处，每一个都通常足以证明始终使用它，并且基本没有任何限制。虚拟化几乎没有成本，基本没有性能开销，不会增加管理复杂性（确实会在某些领域增加，但在其他领域减少，从而总体减少）。
- en: Inevitably people will ask if special case workloads exist for which virtualization
    is not ideal. Of course special cases exist. But the next response from every
    IT shop on earth is to proclaim that they and they alone are unique in their server
    needs and that they are the one solitary case where virtualization does not make
    sense - and then they reliably state a stock and absolutely ideal workload for
    standard virtualization that applies to nearly everyone and is as far from a special
    case or an exemption from best practices as can be. Trust me, you are not the
    exception to this rule. Virtualize every workload, every time. No exceptions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不可避免地，人们会问是否存在虚拟化不适合的特殊工作负载。当然存在特殊情况。但地球上每个IT店铺的下一个反应是宣称他们独特且唯一，在他们的服务器需求中虚拟化不合理
    - 然后他们可靠地声明一种标准虚拟化的股票和绝对理想工作负载，适用于几乎每个人，并且远离特殊情况或免除最佳实践的例外。相信我，你不是这条规则的例外。每次都虚拟化每个工作负载。没有例外。
- en: Most examples that people give of why they avoid virtualization is normally
    examples of virtualization done wrong. Other, non-virtualization related mistakes,
    such as selecting a bad vendor, improperly sizing a server, or choosing a large
    overhead storage layer when something lean is needed could happen with or without
    virtualization. Everyone from the system administrator to the platform team to
    the hardware purchasers still have to do the same quality job that they would
    without virtualization. Virtualization is not a panacea but failing to be the
    silver bullet that removes the need to do our jobs well in no way excuses not
    using it every time. That is just bad logic.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人提到他们避免虚拟化的例子通常是虚拟化处理不当的例子。其他非虚拟化相关的错误，如选择不良供应商、错误大小的服务器或选择大幅超支的存储层而需要精简的情况，可能会发生，无论有无虚拟化。无论是系统管理员、平台团队还是硬件采购者，他们仍然必须像没有虚拟化时一样做同样高质量的工作。虚拟化并非万能药，但不能成为消除我们工作需要做好的银弹的借口。这只是错误的逻辑。
- en: Now we should have a good understanding of what virtualization really is, instead
    of simply a passing knowledge of its utility, and know why we use it for all production
    workloads. Virtualization should become second nature very quickly whether you
    are working in your home lab or running a giant production environment. Make it
    a foregone conclusion that you will virtualize and only worry about little details
    like storage provisioning and hypervisor selection or management tools. Next we
    look at virtualizations alternative and close cousin, containerization.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该对虚拟化有一个清晰的理解，而不仅仅是对其作用的浅显了解，并且知道为什么我们要将其用于所有生产工作负载。无论是在家庭实验室工作，还是在大型生产环境中运行，虚拟化应该很快成为第二天性。让虚拟化成为一个不言而喻的结论，只需要担心一些细节问题，比如存储配置、虚拟机监控程序选择或管理工具的使用。接下来，我们将看看虚拟化的替代方案及其近亲——容器化。
- en: Containerization
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器化
- en: Some people consider containers to be a form of virtualization, sometimes called
    Type-C virtualization or OS-level virtualization. In recent years, containers
    have taken on a life of their own and very specific container use cases have become
    such buzz-worthy topics that containers as a general concept have been all but
    lost. Containers, however, represent an extremely useful form of (or alternative
    to) traditional virtualization.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人认为容器是一种虚拟化形式，有时称为 C 类虚拟化或操作系统级虚拟化。近年来，容器已成为一种独立的存在，特定的容器使用场景成为了热门话题，以至于容器这一概念本身几乎被遗忘。然而，容器代表了一种极为有用的（或作为传统虚拟化的替代）形式。
- en: Container-based virtualization varies from traditional virtualization in that
    in traditional virtualization every aspect of system hardware is replicated in
    software by the hypervisor and exists uniquely to every instance or virtual machine
    (often called a Virtual Environment (VE) when talking about containers) running
    on top of it. There is nothing shared between the virtual machines and by definition
    any operating system that supports the hardware virtualized can run on it exactly
    as if it was running on bare metal.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基于容器的虚拟化与传统虚拟化不同，在传统虚拟化中，每个系统硬件的各个方面都由虚拟机监控程序以软件的形式进行复制，并且每个实例或虚拟机（在谈论容器时通常称为虚拟环境（VE））在其上运行时，都是唯一存在的。虚拟机之间没有任何共享的内容，按定义，任何支持虚拟化硬件的操作系统都可以在其上运行，就像它运行在裸机上一样。
- en: Container-based virtualization does not use a hypervisor at all, but rather
    is built from software that heavily isolates system resources in an operating
    system allowing individual virtual machines to be installed and operate as if
    they are fully unique instances, but behind the scenes all virtual machines running
    as containers share the host's kernel instance. Because of this, the ability to
    install any arbitrary operating system is limited as only operating systems capable
    of sharing the same kernel can be installed on a single platform.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于容器的虚拟化完全不使用虚拟机监控程序，而是由一款软件构建，这款软件在操作系统中进行强烈的资源隔离，允许安装并运行各个虚拟机，就好像它们是完全独立的实例一样，但在后台，所有作为容器运行的虚拟机共享主机的内核实例。由于这个原因，安装任何任意操作系统的能力受到限制，因为只能安装能够共享同一内核的操作系统到单个平台上。
- en: Because there is no hypervisor and only a single kernel shared between all systems,
    there is nearly zero overhead in most container systems making it perfect for
    many highly demanding tasks. Containers are especially popular in Linux where
    many container options exist today. The almost total lack of system overhead in
    container systems used to be a key feature of the approach, but as systems have
    moved from resource tight to having a surplus of power in many cases, the value
    of squeezing every last drop out of hardware has begun to wane in comparison to
    the greater flexibility and isolation of full virtualization. Because of this,
    what sounds like the greatest virtualization option ever is often overlooked or
    even forgotten about!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有虚拟机监控程序，且所有系统共享单一内核，大多数容器系统的开销几乎为零，这使其非常适合许多高要求的任务。容器在 Linux 环境中尤其受欢迎，今天有很多容器选项。容器系统几乎完全没有系统开销，曾经是这种方法的一个关键特性，但随着系统从资源紧张到在许多情况下具有丰富的计算能力，挤压硬件每一滴性能的价值开始减弱，相比之下，完整虚拟化所带来的更大灵活性和隔离性变得更加重要。因此，曾被认为是最伟大的虚拟化选项往往被忽视，甚至被遗忘！
- en: Containers do not require any special hardware support and are implemented completely
    in software allowing them to exist on a broader variety of platforms (it is easy
    to implement on old 32bit Intel hardware or a Raspberry Pi, for example) at lower
    cost. This made them important in the era before hardware acceleration was broadly
    available for full virtualization technologies.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 容器不需要任何特殊的硬件支持，完全通过软件实现，使得它们能够在更广泛的平台上运行（例如，很容易在老旧的 32 位 Intel 硬件或 Raspberry
    Pi 上实现），且成本更低。这使得它们在硬件加速广泛可用之前的时代变得非常重要。
- en: In the Linux world, which is what we care about, we can run many disparate Linux-based
    operating systems on a single container host because only the kernel needs to
    be shared. So, running virtual machines of Ubuntu, Debian, Red Hat Enterprise
    Linux, SUSE Tumbleweed, and Alpine Linux all on a single container host is no
    problem at all.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们关心的 Linux 世界中，多个不同的基于 Linux 的操作系统可以在同一个容器主机上运行，因为只需要共享内核。因此，在单个容器主机上运行 Ubuntu、Debian、Red
    Hat Enterprise Linux、SUSE Tumbleweed 和 Alpine Linux 等虚拟机毫无问题。
- en: Linux is blessed (and cursed) with a plethora of options for nearly every technology,
    and containerization is no exception. Multiple open source and commercial container
    products exist for Linux, but today the undisputed reigning champion is **LinuX
    Containers** (**LXC**). LXC is unique in that it is fully built into the Linux
    kernel so utilizing it is simply a matter of doing so, it does not require additional
    software or kernel modifications. If you are going to be implementing real containers
    on Linux, chances are it is going to be LXC. LXC is fully supported in nearly
    all Linux-based operating systems.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 拥有（也可能是个负担）几乎每个技术领域都具有大量的选项，容器化也不例外。Linux 上存在多个开源和商业容器产品，但今天无可争议的冠军是 **LinuX
    Containers**（**LXC**）。LXC 的独特之处在于它完全集成在 Linux 内核中，因此使用它其实只是简单地启用，不需要额外的软件或内核修改。如果你打算在
    Linux 上实现真正的容器，可能性很大就是 LXC。几乎所有基于 Linux 的操作系统都完全支持 LXC。
- en: A little history of containers
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器简史
- en: Full virtualization was introduced by IBM in the 1960s but proved to be complex
    and did not make it into general availability for mainstream servers for decades
    as high end hardware support and extensive special case software was necessary
    to make the magic happen until the very end of the 1990s.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 完全虚拟化由 IBM 在 1960 年代引入，但由于其复杂性，直到 1990 年代末期，借助高端硬件支持和大量特定的软件，才使其得以普及并进入主流服务器市场。
- en: Containers were first introduced in System 7 UNIX in 1979 using a mechanism
    called *chroot jails* which is rudimentary by today's standards, but is functionally
    pretty close to modern containers. In the UNIX world containers, of one type or
    another, have almost always been available. In 1999 what we might consider truly
    modern containers, starting with FreeBSD's Jails, were introduced and rapidly
    other UNIX platforms like Solaris with Zones and Linux with OpenVZ and then LXC
    began to emerge. By the mid-2000s containers were everywhere and quite popular
    before truly effective full virtualization had taken off.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 容器首次出现在 1979 年的 System 7 UNIX 中，使用的是名为 *chroot jails* 的机制，按今天的标准看起来相当简陋，但在功能上非常接近现代容器。在
    UNIX 世界中，容器（无论是哪种类型）几乎一直都有存在。1999 年，真正的现代容器可以说从 FreeBSD 的 Jails 开始引入，随后其他 UNIX
    平台如 Solaris 的 Zones 和 Linux 的 OpenVZ，最终 LXC 开始出现。到了 2000 年代中期，容器已经无处不在，并且在完全虚拟化技术真正起步之前就非常流行。
- en: Containers saw a Renaissance of sorts in 2013 with the introduction of Docker.
    Docker is not exactly a container, however, even though the term container has
    become more associated with Docker than with actual, true containers. Prior to
    Docker, containers were never really seen as being very sexy but rather basic
    process isolation workhorses doing a rudimentary security job for the operating
    system with the possible short lived exception of Solaris Zones which were heavily
    promoted for a short time in conjunction with the release of ZFS.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 容器在 2013 年经历了一次复兴，伴随着 Docker 的引入。尽管 Docker 并不完全是一个容器，但“容器”这个术语如今更多地与 Docker
    联系在一起，而非与真正的容器。早在 Docker 之前，容器从未被认为是特别吸引人，相反，它们更多是基本的进程隔离工具，承担着为操作系统执行初步安全工作的职责，唯一的例外可能是
    Solaris Zones，在 ZFS 发布时曾一度受到大力推广。
- en: Today, because of the popularity of Docker and its association with containers,
    the majority of people (even including system administrators!) think of Docker
    when someone mentions containers rather than true containers which have been around
    for decades.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，因 Docker 的流行及其与容器的紧密关联，大多数人（甚至包括系统管理员！）在提到容器时，会想到 Docker，而不是那些已经存在几十年的真正容器。
- en: We cannot talk about containers without mentioning Docker. Docker, today, is
    the name most associated with containers, and for good reason. Docker originated
    as a set of extensions built on top of LXC to provide for extreme process isolation
    with a *packaged* library environment for said applications. While Docker uses
    containers, first LXC and now their own container library, it itself is an application
    isolation environment providing a more limited range of services than something
    like LXC will provide. With LXC, you deploy an operating system (sans kernel)
    and treat it all but identically to traditional full virtualization. With Docker
    you are deploying an application or service, not an operating system. The scope
    is different and so Docker really finds itself more applicable to [*Chapter 5*](B16600_05_Final_ASB_ePub.xhtml#_idTextAnchor128),
    *Patch Management Strategies*. Because Docker is an application layer containerization,
    it would most often be run on top of a virtual machine in either full virtualization
    or containerization to provide its underlying operating system component.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能谈论容器而不提到 Docker。今天，Docker 是与容器最相关的名称，且这个称号当之无愧。Docker 最初作为一组构建在 LXC 之上的扩展而出现，为应用程序提供极端的进程隔离以及*打包*的库环境。尽管
    Docker 使用容器，最初是 LXC，现则是其自有的容器库，但 Docker 本身是一个应用隔离环境，提供的服务范围比 LXC 更为有限。使用 LXC，你可以部署一个操作系统（不包括内核），并且几乎将其视作传统的完全虚拟化。而
    Docker 则是部署应用或服务，而非操作系统。两者的范围不同，因此 Docker 更适用于[*第 5 章*](B16600_05_Final_ASB_ePub.xhtml#_idTextAnchor128)，*补丁管理策略*。由于
    Docker 是应用层容器化，它通常会运行在虚拟机上，无论是完全虚拟化还是容器化，以提供其底层操作系统组件。
- en: Containers, in general, represent a trusted, mature, and ultra-high performance
    virtualization option (or alternative.) While more limited in their capabilities
    (a Linux container host can only run Linux VEs, while FreeBSD VEs are not possible,
    for example) they are otherwise easier to maintain, faster, and generally more
    stable (there is simply less to go wrong.) They can be created faster, turned
    on or off faster, patched faster, are more flexible in their resource usage (they
    don't require the strict CPU and RAM assignments typically needed with full virtualization),
    need fewer skills, and have less overhead when running. The only significant caveats
    to containers are the inability to mix operating system workloads, or even kernel
    versions. If anything, that you do requires a specific kernel version (that is
    not uniform across the entire platform), or custom compilation of the kernel,
    a GUI, ISO or similar based full installs then containers simply are not flexible
    enough for you. But if you are dealing with a pure Linux environment where all
    workloads are Linux and can share the kernel, which is not that uncommon, then
    containers can be ideal for you.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，容器代表了一个受信任、成熟且超高性能的虚拟化选项（或替代方案）。尽管在能力上更为有限（例如，一个 Linux 容器主机只能运行 Linux VE，而
    FreeBSD VE 是不可能的），但它们更容易维护、更快速、通常也更稳定（因为出错的可能性更小）。容器可以更快地创建、启用或关闭、修补，资源使用上更灵活（它们不需要完全虚拟化所需的严格
    CPU 和内存分配），需要的技能较少，运行时开销更小。容器的唯一显著限制是无法混合操作系统工作负载，甚至内核版本。如果你所做的任何事情需要特定的内核版本（且该版本在整个平台中并不统一），或者需要自定义编译内核、GUI、ISO
    或类似的完整安装，那么容器显然无法满足你的需求。但如果你正在处理一个纯 Linux 环境，所有工作负载都是 Linux 且能够共享内核，这种情况并不罕见，那么容器将是理想选择。
- en: Containers, at least thus far, are not able to leverage the graphical interface
    of the operating system and so are relegated to server duties where pure text
    based (aka TTY) interfaces are used. Thus, they are not an option for graphical
    terminal servers or **virtual desktop instance** (**VDI**) deployments. Those
    kinds of workloads still need full virtualization until someone builds a workaround
    for that. But as that is generally not a heavily desired workload to support,
    there is little chance that someone is going to invest heavily in tackling that
    already easy to solve problem just to be able to say that they did it with containers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 至少目前为止，容器无法利用操作系统的图形界面，因此它们被限制用于纯文本界面（即 TTY）的服务器工作。因此，容器不是图形终端服务器或 **虚拟桌面实例**
    (**VDI**) 部署的可选方案。这类工作负载仍然需要完整虚拟化，直到有人为此开发出解决方法。但由于这通常不是一个高度渴望支持的工作负载，因此很少有人会在这个已经容易解决的问题上投入大量精力，只为了能够说他们用容器解决了它。
- en: Through the use of containers, you allow the operating system itself to act
    as a hypervisor, but one that is still the operating system as well and can be
    managed using all of the normal Linux tools and techniques because the system
    is still Linux. There is no separate hypervisor to learn or maintain. In this
    way the ability to leverage existing Linux skills and tools is very good.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用容器，你可以让操作系统本身充当虚拟机管理程序，但它仍然是操作系统，并且可以使用所有常规的 Linux 工具和技术进行管理，因为系统仍然是 Linux。你无需学习或维护单独的虚拟机管理程序。这样，利用现有的
    Linux 技能和工具的能力非常强大。
- en: It has to be noted that, because KVM and containers both use a standard, bare-metal
    Linux installation as their base, and because both are baked directly into the
    stock vanilla Linux kernel, it is not only possible but actually not uncommon
    for systems to run both full virtualization and containerization on the same host
    at the same time with vanilla Linux workloads running in containers. This ensures
    the lower overhead and extra flexibility and non-Linux (primarily Windows) or
    custom-kernel Linux systems running in full virtual machines on KVM. There is
    no reason to have to choose only one approach or the other if a blend is better
    for you. Container technology has become extremely popular and important today,
    but the use of containers in their traditional sense has dwindled heavily. This
    is mostly because of deep misunderstandings of the terminology and technology,
    which has caused it to be ignored even when it is highly appropriate. As a Linux
    system administrator especially, it may be very beneficial to consider containers
    instead of traditional virtualization in your environments. Having containers
    as another tool in your proverbial toolbelt makes you more flexible and effective.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 必须注意的是，由于 KVM 和容器都以标准的裸机 Linux 安装为基础，并且都直接集成在原生的 Linux 内核中，因此在同一主机上同时运行完整虚拟化和容器化并不罕见，且容器中运行原生的
    Linux 工作负载是完全可行的。这确保了较低的开销和更大的灵活性，并且非 Linux（主要是 Windows）或自定义内核的 Linux 系统可以在 KVM
    上作为完整虚拟机运行。如果你觉得混合使用更适合你的需求，就没有必要仅选择其中一种方法。容器技术如今已经变得极为流行且重要，但传统意义上对容器的使用已经大大减少。这主要是由于对术语和技术的误解，导致即使在非常合适的情况下，容器也常常被忽视。特别是作为
    Linux 系统管理员，考虑在你的环境中使用容器而非传统虚拟化，可能会非常有益。将容器作为你工具箱中的另一个工具，可以让你变得更加灵活和高效。
- en: Next, we will apply what we have learned about virtualization and containers,
    and add management, to learn about cloud computing.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将结合所学的虚拟化和容器知识，并加上管理方面的内容，来学习云计算。
- en: Cloud and VPS
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云与 VPS
- en: Any discussion of virtualization today is inevitably going to lead us to cloud.
    Cloud has become, that hot decade-long buzz-worthy concept that everyone wants,
    most people use, and no one has a clue what it is, what it means, or why anyone
    uses it. Few technologies are more totally misunderstood, yet widely talked about,
    than cloud. So we have a lot to cover here, much of it clearly up misconceptions
    and the misuse of terms.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 任何关于虚拟化的讨论，今天都不可避免地会引导我们进入云计算。云计算已经成为那个热衷了十年的流行词，人人都想要，大多数人都在使用，但却没有人知道它是什么，意味着什么，或者为什么要使用它。没有什么技术比云计算更加被误解，同时又被广泛谈论。因此，我们有很多内容需要讨论，其中大部分是为了澄清误解和滥用术语。
- en: The bizarre confusion of Cloud
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云的奇异困惑
- en: It is a rare combination of being vastly technical and non-applicable to normal
    business conversations while being constantly discussed as if it were a casual
    high level non-technical business decision at nearly all levels. Considering only
    a minuscule fraction of IT professionals have any serious grasp of what cloud
    is, and even fewer have a clear understanding of when to choose it, that the average
    non-technical mid-level manager will toss around the term as if they were discussing
    the price of postage stamps is mind-boggling. What do those people even think
    that they are discussing? No one truly knows. And I mean that, sincerely.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种罕见的组合，既是高度技术化的，又与正常的商业对话不相关，同时几乎在所有层级上都被讨论得像是一个随意的高层非技术性商业决策。考虑到只有极少数 IT
    专业人士对云有任何深入的理解，而且更少人能清晰地理解何时选择它，普通的非技术性中层经理人会把这个术语当作讨论邮票价格一样轻松地使用，真是让人难以理解。那些人到底以为自己在讨论什么？没人真正知道。我是说真的，真心的。
- en: Ask a group of people who have been throwing about the term cloud. Separate
    them so that they are not stealing each other's answers. Now, ask them to describe
    what cloud means to them. Mostly you will get gibberish, obviously. When you drill
    down, however, you will get a variety of descriptions and answers that are nothing
    like one another, and yet people listening to others discussing cloud will generally
    state that they believe that all of those people were meaning the same thing and
    often that *one thing* is something none of them actually meant, let alone more
    than one of them. Truly cloud means something different, random, and meaningless
    to nearly every human being with no rhyme or reason to it.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 问一群经常谈论云的人，把他们分开，避免彼此互相影响。现在，让他们描述云对他们来说意味着什么。你大多数时候会听到一些胡言乱语，显然如此。但当你深入探讨时，你会得到各种各样、彼此完全不同的描述和答案，而听别人讨论云的人通常会说，他们相信那些人说的都是在说同一件事，通常那个*一个东西*是他们都完全没有意思的，更别提是其中的一个人了。实际上，云对几乎每个人来说意味着不同、随机且毫无意义的东西，没有任何规律可言。
- en: If cloud as a term had a musical equivalent, it would be Alanis Morisette's
    Ironic, the song where the only thing ironic about it is the title. The term cloud
    is the same, everyone uses it, no one knows what it means. Just like ironic.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果云这个术语有一个音乐对应物，那就是阿拉尼斯·莫里塞特（Alanis Morissette）的《Ironic》，这首歌的唯一讽刺之处就是标题。云这个术语也是如此，人人都在用，没人知道它是什么意思。就像“讽刺”一样。
- en: For quite some years, cloud was commonly used to mean *hosted*. Simply replacing
    a long established, well known industry term with another one for no good reason.
    Of course, cloud means nothing of the sort. This horrific misinterpretation led
    to the meme of *there is no cloud, just someone else's computer*. Of course, even
    a passing knowledge of cloud would make one cringe to hear someone state something
    so profound while getting what cloud means so incredibly wrong.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，云通常被用来表示*托管*。简单地用另一个没有任何理由的新词替代一个已建立的、广为人知的行业术语。当然，云根本不意味着那样。这种可怕的误解导致了*没有云，只有别人的计算机*的迷因。当然，即使稍微了解一下云的人，听到有人说出这种话时，也会感到羞愧，因为他们完全误解了云的含义。
- en: Today, you are more likely to hear cloud used to mean *built with HTML*, I kid
    you not. Or sometimes it means *platform independent*. Other times it means *subscription
    pricing*. You name it, and cloud has been used to refer to it. The only thing
    you can be confident in is that no one, ever, actually means cloud. It could mean
    almost anything else, but it never means what the term actually is meant to refer
    to.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，你更有可能听到“云”用来表示*由 HTML 构建*，我不是在开玩笑。有时它意味着*平台独立*，其他时候它表示*订阅定价*。你可以列出所有可能的含义，而“云”已经被用来指代其中任何一个。你唯一可以确定的是，没人，永远不会，真正意味着云。它可能意味着几乎任何其他东西，但从来不意味着这个术语实际所指的内容。
- en: The only benefit to the mass hysteria around cloud definitions is that no alternative
    definition is used commonly enough to rise and overtake true cloud. The problem,
    however, is so bad that there is no reasonable way for you, as an IT professional,
    to use the term *cloud* with anyone except a truly well read and trusted technical
    colleague that you know actually knows what it means.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 围绕云定义的广泛狂热唯一的好处是，没有哪个替代定义足够普遍，能够崛起并超越真正的云。然而，问题已经严重到一个程度，你作为 IT 专业人士，除非和一个真正博学且值得信赖的技术同事交流，否则无法和任何人使用*云*这个术语，因为你知道他们真的知道它是什么意思。
- en: What is truly amazing is that the use of *cloud* has replaced terms like *synergy*
    as the default joke in business – that is, as a term only used by those who are
    truly lost. It is such common knowledge that *cloud* is complex and completely
    misunderstood that you can never use it to communicate an idea. It has become
    a standard example of a *marker* in the language for someone who is just spouting
    off management-speak without having any idea what they are saying and not realizing
    that everyone else is silently laughing at their ineptitude, and yet you hear
    it repeated almost constantly! No matter how much everyone knows that they are
    misusing it, somehow it remains addictive and in constant use.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important takeaways from this seeming rant (and rant it is)
    is that you cannot use the term cloud to any but the most elite professionals
    and you cannot explain cloud to any but well educated IT professionals. Avoid
    using the term because, no matter how much you think that you can use it in the
    same mistaken way that everyone else is using it, you cannot. There is no way
    to use cloud in a way that can be understood because everyone believes that they
    know what it means, even though they all think that it is something unique.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: When you absolutely must refer to cloud for some reason, use more complete terms
    like *cloud computing* or *cloud architecture* to clarify that you actually mean
    cloud and not just throwing out a word for the listener to interpret at will.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: In many ways, it is almost easier to describe cloud by what it is not, rather
    than what it is, because everyone thinks that it is one thing or another. Cloud
    has no association with hosting, none with the web, not even any with the Internet
    (the thing sometimes referred to as THE cloud, as opposed to A cloud.) We cannot
    here go into all details explaining every possible aspect of cloud computing,
    nor would much of it be applicable as the majority of cloud is not related to
    systems, and therefore not related to Linux. But we should address, to a small
    degree, what it means to the Linux system administrator, when it is applicable,
    and so forth.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: First, we must start with the **NIST** (the **National Institute of Standards
    and Technology** in the USA) cloud abstract definition which works from Amazon's
    original definition. Always keep in mind that cloud computing is a real, strict,
    technical term created by Amazon for a real-world architecture and therefore has
    a strict, non-fungible definition and no amount of misuse or misunderstanding
    or attempts to co-opt its use by others changes that it means an extremely specific
    thing. It is common for those who do not understand cloud to argue that it is
    a loose term that can mean what you want it to mean, but it is not. That is simply
    not the case, it is not a random English language word making its way into the
    lexicon organically, it was defined carefully within the industry before first
    use.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'The NIST definition is as follows: *Cloud computing is a model for enabling
    ubiquitous, convenient, on-demand network access to a shared pool of configurable
    computing resources (e.g., networks, servers, storage, applications, and services)
    that can be rapidly provisioned and released with minimal management effort or
    service provider interaction. This cloud model is composed of five essential characteristics,
    three service models, and four deployment models.*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: NIST的定义如下：*云计算是一种模型，用于启用无处不在、便捷的、按需的网络访问，以共享的可配置计算资源池（例如，网络、服务器、存储、应用程序和服务）为基础，这些资源可以在最小的管理工作或服务提供者交互下迅速配置和释放。这个云模型由五个基本特征、三种服务模型和四种部署模型组成。*
- en: The most important parts of this definition to us, as system administrators,
    are the parts that include *pool of resources*, with *rapid provisioning and release*.
    So shared resources (meaning servers, CPUs, RAM, storage, networking) that we
    can create and destroy access to quickly. While cloud certainly means more than
    that, those are the basics. If you immediately thought to yourself *that sounds
    a lot like what virtualization already does*, you are correct, there is an extreme
    degree of overlap, and virtualization is the key building block of cloud computing
    (both full virtualization and/or containers.) If you thought to yourself *wait,
    pooled resources that I can build AND destroy rapidly - those do not sound like
    useful characteristics to me or any environment I have worked in previously*,
    you are also correct. Cloud computing is not logically applicable to traditional
    workloads or environments, it is designed around extremely specific needs of purpose-built
    application architectures that few businesses are prepared to leverage on any
    scale.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们作为系统管理员来说，定义中最重要的部分是涉及*资源池*和*快速配置与释放*的部分。因此，我们可以快速创建和销毁的共享资源（意味着服务器、CPU、RAM、存储、网络）。虽然云计算的定义远不止这些，但这些是最基本的。如果你立刻想到“*这听起来像是虚拟化已经做的事情*”，你是对的，确实有很大的重叠，虚拟化是云计算的关键构建模块（包括完全虚拟化和/或容器）。如果你想到“*等一下，资源池我可以快速构建和销毁—这些特征对我或我以前工作过的任何环境来说都不太有用*”，你也是对的。云计算并不适用于传统工作负载或环境，它是围绕着特定用途的应用架构设计的，这种架构只有少数企业能够在任何规模上加以利用。
- en: What makes the use of *cloud* more confusing is that many vendors (and quite
    logically at that) use cloud themselves as a component of their products. So,
    when you ask your vendor if a product is cloud-based, they might be answering
    if they are providing you cloud itself as a product, or they might be answering
    if they use cloud in the building of the tool somewhere under the hood. The two
    are both applicable to how most people ask the question, and since no one knows
    what cloud really is, no one is sure what you are really asking or want to know.
    Let me give a contrived, but reasonable, example.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使得*云*的使用更加混淆的是，许多供应商（并且在逻辑上是合理的）自己将云作为其产品的一个组成部分。因此，当你问你的供应商某个产品是否基于云时，他们可能是在回答是否将云作为产品本身提供给你，或者他们可能是在回答是否在工具的构建过程中某个地方使用了云。这两种情况都适用于大多数人提问的方式，而且由于没人真正知道云是什么，没人确切知道你到底在问什么或想要了解什么。让我举一个牵强但合理的例子。
- en: If I were to purchase a legacy application, say a client-server application
    that uses MS SQL Server and an old Delphi (Objective Pascal) front end and then
    use a true cloud product to create a virtual machine and deploy the server-side
    components so that we can truly say that we built the solution on a cloud. Yet
    the resultant product is not cloud computing, in any sense. Just because one piece
    of an architecture is built on cloud does not imply that the final product is
    cloud. Cloud is a layer in the stack.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我购买一个遗留应用程序，比如一个使用MS SQL Server和旧版Delphi（Objective Pascal）前端的客户端-服务器应用程序，然后使用一个真正的云产品创建虚拟机并部署服务器端组件，这样我们就可以真正说我们是在云上构建了解决方案。然而，最终的产品从任何角度看都不是云计算。仅仅因为架构的某一部分是在云上构建的，并不意味着最终产品就是云。云只是堆栈中的一层。
- en: For us, as system administrators, we are concerned with the type of cloud knows
    as **Infrastructure as a Service** (**IaaS**). That's a fancy way of saying cloud-based
    virtual machines. Other types of cloud, like **Platform as a Service** (**PaaS**)
    and **Software as a Service** (**SaaS**), are very important in the cloud space
    but exist only when the system administrator is *somewhere else*. If we are the
    system administrator for PaaS or SaaS then, to us, cloud is the workload, and
    it is not cloud to us. If we are not the system administrator for a PaaS or SaaS
    system, then it is of no concern to us as system administrators to talk about
    those systems as they do not apply to our role.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们这些系统管理员来说，我们关心的是一种叫做**基础设施即服务**（**IaaS**）的云类型。这是一个高级的说法，实际上指的是基于云的虚拟机。其他类型的云，如**平台即服务**（**PaaS**）和**软件即服务**（**SaaS**），在云计算领域非常重要，但只有当系统管理员不在时才存在。如果我们是PaaS或SaaS的系统管理员，那么，对我们而言，云就是工作负载，而不是云。如果我们不是PaaS或SaaS系统的系统管理员，那么作为系统管理员，我们不需要讨论这些系统，因为它们与我们的角色无关。
- en: From our systems perspective, cloud is almost like an advanced, flexible virtualization
    layer. Of course, like virtualization, we may find ourselves tasked with being
    the ones to implement the cloud platform. That is a completely different animal
    and worthy of a book (or two) on its own. But in practical terms system administrators
    may consume platform resources from a hypervisor, a container engine, or either
    one orchestrated through a cloud interface. To us it is all the same - a mechanism
    on which we deploy an operating system. So, from a pure system administrator perspective,
    think of cloud computing no differently than any other virtualization because
    it is exactly the same. The only difference is how it is managed and handed off
    to us.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们系统的角度来看，云几乎就像是一个先进的、灵活的虚拟化层。当然，像虚拟化一样，我们可能会被要求实施云平台。这是完全不同的一回事，值得写一本书（甚至两本）。但从实际角度来看，系统管理员可能会从一个虚拟机监控器、容器引擎或通过云接口编排的任何一个平台中获取资源。对我们来说，这一切都是一样的——它们都是我们部署操作系统的机制。因此，从纯粹的系统管理员角度来看，可以把云计算看作与任何其他虚拟化技术没有区别，因为它们本质上是一样的。唯一的区别是它是如何管理的，以及如何交由我们处理。
- en: In practical terms, we likely have to be heavily involved in decision making
    around the recommendation for the use of cloud versus other paths to acquire virtualization.
    Like any business decision, this simply comes down to evaluating the performance
    and features offered at a price point and comparing to the performance and features
    at the same price point with other options. It is that simple. But, with cloud,
    due to all the reasons that we mentioned before, we are often fighting against
    a mountain of misinformation and a belief in magic. So, we need to talk a little
    about cloud in a way that we should not have to simply because these misunderstandings
    are so common and deeply rooted.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际操作来看，我们可能需要深度参与有关是否使用云计算与其他虚拟化获取路径的决策。像任何商业决策一样，这归根结底是评估在一个价格点下所提供的性能和特性，并将其与同一价格点下其他选项的性能和特性进行比较。就是这么简单。但是，鉴于我们之前提到的所有原因，在云计算方面，我们往往需要与大量的误解和对“魔法”的信仰作斗争。因此，我们需要稍微谈一谈云计算，尽管我们本不应该这么做，因为这些误解如此普遍且根深蒂固。
- en: First, there is a belief that cloud computing is cheap. And while in special
    cases cloud computing will potentially save a lot of money, this is rarely the
    case. Cloud computing is typically an extreme price premium product chosen because
    it allows a great degree of flexibility so that less is needed to be purchased
    overall to meet the same needs. Cloud computing is very expensive to provide and
    so vendors are forced to charge more than for other architectures in order to
    provide it to customers (keep in mind that the *vendor* might be in your internal
    cloud department, nothing about cloud implies an external vendor.)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，有一种观念认为云计算是便宜的。虽然在特殊情况下，云计算可能节省大量资金，但这很少发生。云计算通常是一种极其昂贵的产品，之所以被选择，是因为它提供了很大的灵活性，从而可以减少整体购买量以满足相同的需求。云计算的提供成本非常高，因此供应商被迫收取比其他架构更多的费用以向客户提供（请记住，*供应商*可能是你们内部的云部门，云计算并不意味着外部供应商）。
- en: Horizontally scalable elastic workloads
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 水平可扩展的弹性工作负载
- en: Attempting to describe what workloads cloud computing was built to address can
    be a challenge for those not already familiar with certain types of application
    architecture. We must take an aside and dive into some application concepts here
    to really understand how we related as the systems team and to see why different
    approaches play such a large role in our platform decision making at this level.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试描述云计算所应对的工作负载对于那些不熟悉某些类型应用架构的人来说可能是一项挑战。我们需要稍作停顿，深入探讨一些应用概念，才能真正理解我们作为系统团队是如何相关的，并看到为什么不同的方案在这个层次上对我们的平台决策起着如此重要的作用。
- en: In a traditional application design the expectation is that we will run the
    entire application on just one or maybe just a few operating system instances.
    Typically, one instance would be used as the database server and one as the application
    server. More roles might exist, and you could have a redundant database server
    or similar, but essentially the number of instances usable was quite limited and
    static. Once deployed, the number of instances would not change. In many cases
    the entire application would exist on a single operating system instance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的应用设计中，期望是将整个应用程序仅运行在一个或几个操作系统实例上。通常，一个实例会作为数据库服务器，另一个作为应用服务器。可能会有更多的角色，且你可能会有冗余的数据库服务器或类似的配置，但本质上，可用的实例数量是相当有限且静态的。一旦部署，实例的数量就不会再变化。在许多情况下，整个应用程序会存在于一个单一的操作系统实例中。
- en: Scaling a traditional application mostly focuses on increasing the power of
    a single operating instance. This might be done through some combination of faster
    CPUs, more CPUs, more CPU cores, more memory, more storage, or faster storage.
    Or as we would typically say, if you need your server to do more, you need a bigger
    server. This kind of performance improvement can go a really long way as top end
    servers are very powerful, and few companies need to run any workloads that exceed
    the performance capabilities of a single large server. This style of scaling is
    called vertical scaling, meaning that we improve the performance of the single
    thread or server *within the box*. This kind of scaling is by far the easiest
    to do and works for any kind of application no matter how it is designed (this
    is how you improve video game performance or any desktop workload).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展传统应用程序主要侧重于提高单个操作实例的能力。这可能通过更快的CPU、更多少量的CPU、更多的CPU核心、更大的内存、更大的存储或更快的存储来实现。或者，正如我们通常所说的，如果你需要服务器做更多事情，你就需要一台更强大的服务器。这种性能提升方式能够产生很大的效果，因为高端服务器非常强大，且很少有公司需要运行超出单个大型服务器性能能力的工作负载。这种扩展方式被称为垂直扩展，意味着我们在*框架内*提升单线程或单个服务器的性能。这种扩展方式无疑是最容易实现的，并且适用于任何类型的应用程序，无论其设计如何（这就是你提升视频游戏性能或任何桌面工作负载的方式）。
- en: For most people, workloads designed for vertical scaling are the only types
    of workloads that they know. Of course, for end users working on desktops, everything
    is vertical. Even system administrators almost exclusively have to oversee applications
    that are built for this kind of scaling only. Nearly all deploy in house applications
    assume that this is how you will scale and only recently do many developers know
    alternatives well and still many (potentially most) still do not, even though
    those that do are the more prominent in media.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对大多数人来说，专为垂直扩展设计的工作负载是他们唯一了解的工作负载类型。当然，对于在桌面上工作的终端用户而言，一切都是垂直的。即使是系统管理员，也几乎完全需要管理仅为这种扩展方式构建的应用程序。几乎所有内部部署的应用程序都假设这就是你扩展的方式，直到最近，许多开发人员才较为熟悉其他的替代方案，而且仍然有很多（可能大多数）人并不熟悉，尽管那些熟悉的人在媒体上更为突出。
- en: The alternative approach is to design applications that allow for the application
    to scale by adding additional, isolated operating instances. For example, running
    multiple database server instances (likely in a cluster) not just for resilience,
    but also performance. Running multiple application servers with the ability to
    simply add more operating system instances running the application while keeping
    each individual instance small. In a traditional application architecture, we
    might require a single application server with four high performance CPUs and
    1TB of RAM to handle our application workload. A horizontally scalable application
    might use sixteen smaller servers each with 64GB of RAM and a smaller CPU to handle
    the same load. Traditionally we would say that our systems *scaled up*, but in
    adding more instances we say that they *scale out*. Now, of course, you can always
    scale both *up and out* which would mean increasing the resources of each individual
    instance while also increasing the number of instances.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, few applications that we work with in the real world as
    end users or as system administrators are designed for or could leverage horizontal
    or *scale out* platforms effectively, if at all. It requires that the application
    architectures, analysts, and developers plan for this style of deployment from
    the very beginning. And no amount of planning or desire makes every workload capable
    of scaling in this manner.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Some applications like common web-based business processes, most websites, email
    systems, and so forth are very conducive to this type of design and you can easily
    find or make these kinds of applications to take advantage of these resources.
    Other applications like financial processing or inventory control systems may
    struggle with the design limitations and take far more work to be able to work
    in this way, if at all.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Just because a workload is designed by the development team to be horizontally
    scalable does not mean that the workload itself will, however. This is easiest
    with a simple example. You create a website that helps people choose a healthy
    breakfast. You market to the United States. From 6am Eastern until about 2PM eastern
    (when California is wrapping up breakfast) you are really busy, but outside of
    those hours your website is really slow. But another website helps people choose
    food for any meal and is marketed worldwide. This second site never gets quite
    as busy as the first but stays roughly as busy all day long. The first site can
    leverage scalability, the second site cannot because its resource needed never
    really change.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'The key advantages to horizontally scalable workloads are that they can be
    grown rapidly. Adding an additional operating system instance (or an additional
    one hundred!) is easy and non-disruptive. Adding more CPU or RAM to your existing
    server, is hard and slow by comparison. The next step of horizontal scaling is
    making it elastic. To be elastic, your system does not only have to scale out
    quickly but allow you to also scale back in quickly: that is to spin down and
    destroy unneeded operating system instances when capacity has changed. This is
    the unique proposition of cloud computing, to provide capacity on demand for elastic,
    horizontally scalable workloads so that you can use resources only when needed
    and stop using them when you do not.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Vertically scaled resources are far less expensive to provide than horizontally
    scaled ones. You can test this by trying to assemble several computers with roughly
    the same specifications. With only rare exceptions, it is a fraction of the cost
    to build a single large server than several smaller ones using real-world economics.
    A single system requires only a single operating system and application instance,
    but multiple systems require the overhead of the same operating system and applications
    loaded into memory in each case wasted many resources there, as well. It just
    takes less management power to oversee something that is *faster*, rather than
    many slower tasks. It is not unlike managing humans. It takes less overhead to
    manage one really fast, efficient employee than it does to manage several slower
    employees trying to coordinate doing the same work that the faster one was doing.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: So a horizontally scaled system, in order to make sense to choose, has to be
    able to leverage both scaling out, as well as in, have a workload use case that
    actually leverages this, and that does so to an extent large enough to overcome
    the lower cost, lower overhead, and great simplicity of traditional designs. Unless
    your workload meets all of those requirements, cloud computing should not be a
    consideration for you at all. It simply does not apply. And while contrary to
    how the media and trend-happy IT professionals want to portray cloud, it is only
    a small percentage of workloads that can effectively leverage cloud and only a
    small percentage of businesses have those rare workloads at all.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we are talking IaaS aspect of cloud. Other cloud aspects where only
    the application portion is exposed to the business will often be cloud based.
    But this is essentially unrelated and certainly a decision process totally different
    from anything we would be looking at in a tomb such as this.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Second, there is a belief that cloud computing is reliable. Absolutely nothing
    in cloud definitions or designs implies reliability in any way. In fact, this
    runs completely contrary to all standard cloud thinking. Cloud computing, because
    it is useful exclusively with scale out design, is built on the assumption that
    any redundancy or reliability is built into the application itself as scale out
    implies - as you have to have this inside the application itself in order for
    scale out to work properly. So including any redundancy at the system or platform
    level would be nonsensical and counterproductive. A basic understanding of cloud
    computing should, with any thought, make us surprised if anyone expected redundancy
    beyond the minimum at this level. In the real world, cloud computing resources
    tend to be far more fragile than traditional server resources for exactly this
    reason. Cloud computing assumes that either reliability is of trivial importance
    or that it is provided elsewhere in the stack. Cloud computing is just a building
    block of the resulting system, it is not in any way a complete solution by itself.
    Of course, theoretically, a high availability cloud provider could arise, but
    their cost and performance caveats would make it hard to compete in a marketplace
    driven almost entirely by price.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Third, there is a belief that cloud computing is broadly applicable, that every
    company should be using it, and that it is replacing all other architectures.
    This is not true at all. Cloud has been around now for more than fifteen years
    (at the time of writing) and it made its inroads quite quickly towards the beginning
    of that cycle. Today, cloud computing is mature and well known. Companies and
    workloads that are going to move to cloud (or design for it) have largely already
    done so, and new workloads are created on cloud at a roughly constant rate. The
    industry saturation rate for cloud computing has been more or less achieved. Some
    new workloads will go there as older ones or anti-cloud holdouts retire or give
    in to other pressures. Some will come back as overzealous cloud fanboys and buzz-word
    driven managers learn their lessons of having gone to cloud without any understanding
    or planning. Sending standard workloads to cloud computing without redesign is
    typically costly and risky. But by and large cloud computing has already settled
    into a known saturation rate and the computing world is as it will be until another
    exciting paradigm shift occurs. Basically, what we see today in advanced bespoke
    internal software and grand scale multi-customer software is ideal for the cloud
    paradigm, and traditional workloads for single customers remain the most beneficial
    on traditional paradigms. This is all as originally predicted when cloud computing
    was first announced long ago.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Using cloud does not require any specific skills or training, as many in the
    industry would like us to believe in order to sell certifications and training
    classes. In fact, just knowing what cloud truly is often enough to enable you
    to utilize it effectively. That said, individual cloud vendor platforms (such
    as **Amazon**'s **AWS** or **Microsoft**'s **Azure**) are so large and convoluted
    that there can be real value to getting vendor certifications and training to
    understand how to work with their product interfaces. But to be clear, the value
    in the training is learning how to work with the vendor in question, not in learning
    about cloud.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: That does not change the fact that most organizations seeking to get significant
    value out of cloud computing will most likely need to do so with deep vendor integrations
    that will almost certainly require an investment in specific vendor product knowledge.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Cloud is an amazing set of technologies that serves an incredibly important
    purpose. When your workload is right for cloud computing, nothing else can come
    close to it. Whether you build your own private cloud or use a public shared one,
    whether you host your cloud in house or let a hosting firm handle the data center
    components for you, cloud might be the right technology for some of your workloads.
    With your understanding here you should be able to evaluate your needs to know
    if cloud is likely to play any reasonable role, and be able to look at real work
    vendors and costs and make solid, math-based valuations of cloud in comparison
    to other options.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know cloud computing, we can step back and look at the older concept
    of virtual private servers and see why they are so closely tied with, but not
    actually related to, cloud computing today.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Virtual Private Servers (VPS)
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to and, on the Venn Diagram of things, nearly overlapping with IaaS
    cloud computing is the modern concept of virtual private servers or VPS. VPS actually
    predates cloud and comes from simpler virtualization (or containerization) allowing
    a vendor (which could be an internal department, of course) to carve out single
    virtual machines for customers from a larger, shared environment. Instead of needing
    to provide an entire server of their own, customers need only buy a small slice
    or set of slices of the vendor's server(s) to use for their needs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: As I mentioned, this sounds very similar to IaaS cloud that we just described,
    and certainly it is. So much so, that most people using IaaS cloud actually use
    a VPS aspect of it without realizing so. The idea behind VPS is to allow companies
    to purchase server-class resources at a fraction of the scale typically required
    to a single physical server. If you think back to our discussion on virtualization
    and how by using a hypervisor we might be able to take a single physical server
    and, for example, create one hundred virtual machines that run on top of it, each
    with their own operating system, then we could sell those resources to one hundred
    separate customers, each of which could run their own small server inside its
    own secure space. This allows small companies, or companies with small needs,
    to buy enterprise level datacenter and server hardware capacity and prices within
    any realistic budget.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we go further, we need to do a quick breakdown and comparison of VPS
    against IaaS cloud to see why VPS is so commonly confused with cloud computing
    and why they often compete:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '**First, the goals**: An IaaS Cloud''s goal is to provide rapid creation and
    destruction of resources on demand via automation - primarily used by the largest
    organization or workloads. The goal behind VPS is to carve up traditional server
    resources in such a way that they can be affordable to be used by small organizations
    and/or workloads. So, one goes after the biggest scale and the most complexity,
    the other after the smallest scale and least complexity. One expects custom engineering
    effort on both the application and infrastructure team''s sides where the other
    expects traditional applications and no special knowledge or accommodation from
    any team.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Second, the interface**: In cloud computing the expectation and purpose is
    for systems to be self-provisioning (and self-destroying.) Cloud is not designed
    for humans to have to interact manually to request resources, nor to configure
    them, nor to decide when more (or less) are needed, nor to destroy them when done.
    So, cloud''s focus is on APIs to allow software to handle provisioning. VPS is
    meant to work just like any normal virtual machine with a human initiating the
    build, installing the operating system, maybe configuring the operating system,
    and turning the VM off, and then deleting it when no longer needed. It is standard
    for cloud products to not offer any interaction directly with the virtualized
    hardware such as access to a console so any system requiring console level interaction
    (such as a GUI) are impossible. To qualify as a VPS console and GUI access are
    required to completely mimic a hardware device in a standard way. If you can use
    a normal server, you can use a VPS.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Third, the provisioning**: Cloud assumes a need for rapid provisioning. Of
    course, rapid is a relative term. But it is just expected that in a cloud ecosystem
    that systems must be able to go from first request to fully functional in minutes,
    and sometimes seconds. In the VPS world, while we always want everything available
    as quickly as possible, having to wait a few minutes before being given the access
    to begin a manual operating system install that could take potentially tens of
    minutes is common. We assume that a cloud instance will be created via software,
    but a VPS instance we assume will be created manually by a human.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fourth, the billing**: Because the value of cloud computing is assumed to
    come from its ability to be created and destroyed rapidly in order to keep costs
    managed it follows that billing must be granular to accomplish this. To this end
    billing it generally handled in increments of minutes or possibly hours, or in
    other extremely short measurements like processor cycles. VPS will sometimes charge
    in these short increments but may easily use longer intervals such as daily or
    monthly as it is not a rapid create and destroy intended service. (We can say
    that cloud leans towards stateless and VPS leans towards stateful.)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What often makes VPS and IaaS Cloud harder to distinguish is that today, VPS
    providers almost always use IaaS Cloud as their own mechanism for provisioning
    the VPS under the hood, and most IaaS Cloud providers have opted to offer VPS
    additionally. This was bound to happen for two reasons. First, VPS providers use
    cloud because it is a really logical way to build a VPS (if you think about the
    requirements that the VPS *vendor* would have, they would sound a lot like what
    cloud is meant to do) and because by being built on top of cloud computing, you
    can advertise the VPS as being cloud in a sense and be a sort of *simple interface
    to cloud resources*. It makes sense for cloud providers to offer VPS because the
    majority of customers who look for cloud have no idea what it is and only use
    it for political, not business or technical reasons, so offering something simple
    that allows them to purchase from you and use your resources (because cloud is
    so hard and complex) allows you to capture the majority of revenue.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Vendors like Amazon used to offer no VPS services and using their resources
    if you were not truly in need of cloud was difficult, at best. To address this,
    Amazon added LightSail as a VPS product layered on top of their cloud product.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Other cloud providers, such as *Digital Ocean*, *Linode*, and *Vultr* use VPS
    as their primary product offering and focus on it almost entirely while keeping
    their cloud interface quietly to the side so that customers truly looking for
    cloud can find it, but those seeking cloud when they intended to use VPS will
    be able to get what they need right away.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: VPS is one of the most popular and effective ways for real world companies to
    run workloads, especially smaller companies, but companies of any size can leverage
    them. Cloud is effective but primarily for special case workloads. The majority
    of companies talking about already leveraging cloud are actually using VPS and
    not even aware that they missed cloud computing entirely.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth noting that when we talk about rates of cloud adoption we have
    a fundamental problem: no one knows what cloud is, including people who think
    that they are or are not using it currently! Vendors like Amazon can tell you
    how many customers that they have, but they cannot tell you if their customers
    are using their products as cloud or just using cloud in some other way. In a
    survey about cloud adoption you have zero reasonable chance that the person being
    asked about their adoption, the person doing the asking, and the person reading
    about the adoption rates all understand enough about cloud to answer or ask meaningfully
    and, in reality, generally none of them know at all what is being asked. So any
    information about cloud adoption rates border on being totally meaningless. There
    is no honest mechanism by which any person or organization could possibly know
    what the cloud ecosystem really looks like. You would get just as meaningful data
    if a group of squirrels surveyed a bunch of hamsters about astrophysics and then
    handed the results to a bunch of hyper puppies to interpret. People love reports
    and data and rarely care if the survey in question was real in any way.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: You should, at this point, feel both overwhelmed and depressed about the state
    of cloud understanding within both IT and business, but you as the Linux system
    administrator should now be prepared to explain it, evaluate it, understand what
    is built upon it, and know when and how to choose to use it for your own workloads
    and when to look at VPS instead.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: On premises, hosted, and hybrid hosting
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have talked about so many aspects of the underlying components that
    are used to provide us with a platform on which to deploy an operating system,
    we can finally talk about where those systems should exist!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: This is, at least, the simplest of all our topics. Physical location is easy
    to explain, even if many businesses get confused about it in practice. Conceptually
    we really only think about two locations for a workload and that is as being either
    on premises or off premises. This can be a little convoluted, though, as companies
    own multiple locations so what is off premises to one site might be see as on
    premises to another. But we generally consider on premises to be all of a company's
    owned sites and off premises being any sites that are operated by a third party.
    Because of this we generally refer to off premises physicality as being hosted,
    as physical systems are being hosted on our behalf. However, there are reasons
    why this can prove to be very misleading.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Most people assume that when a system is kept on premises that that also implies
    that it will be being operated by an internal team. This is most often true, but
    having on premises systems managed by third party teams is not unheard of, especially
    in very high performance or high security environments. For example, if you required
    Amazon's specific range of cloud computing products, but could not allows for
    any off-premises hosting, you can have Amazon operate an AWS cloud instance on
    your own premises. This is anything but low cost or simple and requires housing
    at minimum a small, self-contained data center and all of its associated staff!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Hosting gets more complicated in practice, but the core issue remains the same:
    the demarcation points. When we decide to start having our systems be hosted off
    premises the questions rapidly become about defining what portions of the hosting
    will be provided by the hosting provider and which by us.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: In its most extreme (and impractical sense) you could rent a house, office,
    or storage unit and provide your own rack, servers, Internet, cooling, power,
    and so forth as needed, but if we did this one would rightfully argue that we
    had basically made the site our own premises. Touche.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Classically it was assumed that nearly all workloads should be run on premises.
    This was for the very simple reason that early business networks had no Internet
    connectivity so hosting elsewhere was effectively impossible or at least impractical.
    Follow that in the early days of the Internet wide area network links were slow
    and unreliable keeping remote servers almost unusable. And software was built
    around LAN networking characteristics, unlike today when enterprise software of
    any quality assumes that it needs to perform adequately over a long distance connection,
    most likely on the Internet.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Because of these old assumptions, the tribal knowledge that servers need to
    be local to an office where people work has been passed down by rote generation
    to generation without many people evaluating it. This information went from generally
    true to seldom true pretty quickly during the early 2000s.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Today most workloads work effectively over the Internet and so can be located
    almost anywhere. Using some form of off-premises hosting or centralized hosting
    that is not based at any specific company location is now the norm rather than
    the exception.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'In all on-premises and off-premises evaluations we have certain factors that
    are universal: who will access the data and from where, how does latency and bandwidth
    impact application performance, which people accessing the data have priority
    and what is the cost of performance issues at different locations.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: There is no hard and fast rules, we simply have to carefully consider as many
    factors as possible. On and Off Premises solutions are just locations and should
    be treated as such. The ability to use an enterprise datacenter off premises might
    be significant, especially if we do not have a real server room on premises. And
    disaster recovery might be better at an off premises location. But will the user
    experience be good enough if the server is far away? These questions are all situational
    and need to be answered not just about the status of the business infrastructure
    today but also for the near future.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Colocation
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a site provides the real estate, cooling, power, Internet, racking, networking,
    and so on but we provide our own physical servers it is called a colocation facility.
    Colocation is one of the most popular and effective ways that we, as system administrators,
    can acquire enterprise class datacenter services outside of our own premises while
    retaining the flexibility to use any hardware that makes sense for our organization
    and its workloads. Colocation is effective for very small businesses up to the
    most absolutely massive. No company outgrows the value that colocation may bring,
    nor does any government. It is a strategy that lacks a *top end* size.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'Colocation is one of the most useful and effective forms of moving IT equipment
    off premises because it allows the IT department to retain full control of hardware
    purchasing and configuration not just for systems but for networking and appliances
    as well. Only non-IT functions necessary to support the technology hardware is
    provided. This allows the colocation facility to focus on a strong facilities
    management skillset and IT to retain literally all IT functions and flexibility:
    basically doing remotely with a third party what you would hope you would be doing
    internally with your own teams assuming that you had enough volume to do so. It
    is expected that a colocation provider will also have *remote hands* to assist
    with bench tasks when necessary, such as changing or verifying cabling, power
    cycling devices, and things of that nature.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Flexibility is key with colocation. Whether it is because you want to custom
    build your own hardware, maintain legacy systems, use special case hardware (for
    example, IBM Power hardware), or what the freedom to do a lift and shift of an
    entire existing environment from on premises to the colocation facility you can
    do it all. Most colocation facilities allow for a range of scales as well, from
    housing a single 1U server on your behalf to fraction racks (tenth, quarter, half,
    full rack sizes are common) to providing cages that can house many racks to even
    renting entire floors of the datacenter!
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The biggest challenges that colocation faces is that there is no effective way
    to go extremely small because the smallest size you can reasonable host is a single
    server. If your needs are smaller than this, then colocation will generally struggle
    to be cost effective for you. But do not simply reject colocation with an assumption
    of it being expensive. I run these calculations often for companies who were ignoring
    colocation as too costly for them only to find out that it would be less than
    half of the cost of their alternative propositions while having more flexibility
    for growth without additional expenditures. Most people assume that servers are
    more expensive to buy than they really are and that colocation costs are higher
    than they really are. Colocation costs are often inappropriately associated with
    legacy systems, as if only twenty year old equipment can go into a datacenter
    today, and decades old cost models are often envisioned. Twenty years ago servers
    were much more expensive than they are today and had noticeably shorter operational
    lives and datacenter space was more costly than today as well. Like everything
    in IT, cost over time have come down and for workloads of any size colocation
    tends to be much less costly than most alternatives.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Colocation is just one approach to hosting systems off premises. Other approaches
    like public, hosted cloud and cloud-based VPS systems are standard alternatives.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The biggest challenges around locality are really all associated with understanding
    current marketing pricing for different approaches and being able to evaluate
    the benefits and caveats of hosting equipment on premises or off premises, and
    if that equipment should be dedicated or shared. You should now be ready to make
    that evaluation and choose appropriately for your deployments. Next, we dig into
    the much more complicated topic of platform level system design architectures.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: System Design Architecture
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the more challenging aspects of system administration is tackling the
    broad concept of system architecture. In some cases, we have it easy, our budget
    is so low or our needs so simplistic that we simply do not need to consider any
    but the most basic options. But for many systems, we have broader needs and a
    great number of factors to consider making system architecture potentially challenging
    in many ways.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: We now understand platform concepts, locality, and the range of services normally
    associated with providing an environment onto which we can install an operating
    system. Now we have to begin describing how we can combine these concepts into
    real world, usable designs. Most of system design is just common sense and practicality.
    Remember nothing should feel like magic or a black box and if we get services
    from a vendor, they are using the same potential range of technology and options
    that we are.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: We are going to talk about risk and availability in the next section, but before
    we do, we should mention here to make it more clear why system designs rely on
    this data, that any redundancy (whether for performance or risk reduction) that
    we add to our overall system can be done at different layers. Principally in the
    system layer (where we are looking now) or at the application layer (which we
    do not control.) So even in the most demanding of high availability workload situations,
    we may have no need for a robust underlying system design and have to consider
    this when thinking about design options.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: I am going to break down common design approaches so that we can understand
    how they best apply to different scenarios. These are physical system architectures
    that include both the storage and the compute. It is assumed that some sort of
    abstraction, meaning virtualization and/or containerization, is used in every
    case and so will not be mentioned case by case.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Standalone server, aka the snowflake
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You really cannot get more basic than this design. The simple server, the baseline
    against which all else must be measured. The self-contained, all in one server
    with storage and compute in a single chassis. No external dependencies, no clustering,
    no redundancy (external to the single box.) Of course, we assume standard hardware
    practices are followed such as minimums like RAID and dual, hot swappable power
    supplies.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Today, many IT people are going to frown on attempting to use a stand alone
    server, but they should not. The classic single server is a powerful, effective
    design appropriate for the majority of workloads. This should be the *go to* design,
    the default starting point, unless you have a specific need to do something else.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Because of its simplicity, single server designs have the best cost ratios to
    all other factors, are more robust than they appear, and have excellent performance.
    Many people think of servers as being rather fragile creatures, and years ago,
    they were; but that impression stems from servers of the 1980s and early 1990s.
    By the late 1990s server technology was becoming mature and reliable and today
    failure rates on well maintained servers are extremely low. The idea that a single
    server is a high risk is an antiquated one, but like many things in our industry
    old feelings often linger and get taught mentor to student without reevaluation
    to see if factors remain true (and in many cases without initial evaluation to
    know if they were ever true.)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Single servers benefit from having many fewer components and lowered complexity
    compared to any other approach and with fewer parts to fail and fewer configurations
    to get wrong it is that much easier to make really reliable: hence why we sometimes
    refer to this as the *brick* approach. Bricks are simple but effective, while
    they can fail, they rarely do. Emotionally it is common to associate complexity
    with robustness, but in practice simplicity is far more desirable. Complexity
    is its own enemy and an unnecessarily complex system takes on unnecessary risk
    (and cost.)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: While hard to measure for many, many reasons, we generally assume that a properly
    maintained and supported stand alone server can delivery average availability
    rates close to five nines (that is around one hour of downtime per year.) It is
    a rare workload in any business that cannot function well with that kind of downtime.
    What is difficult in stand alone servers is that this is an average only (of course)
    and we will have isolated systems experiencing much higher downtime, and others
    experiencing none.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Simplicity also brings us performance. By having fewer components in the path
    single servers have excellent performance. Attempting to gain total performance
    greater than what can be achieved using a single server is difficult. Single servers
    give us the lowest latency and nearly the best throughput of any approach.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to single server systems, use math and logic to explain why it
    may or may not make sense. Many people rely on emotions when it comes to system
    architecture and this should never happen. Our concerns with system design are
    about performance and availability and these are purely mathematical components.
    Emotions have no role here and are, in fact, our enemy (as they are the enemy
    of any business process.)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Single servers can scale far larger than most people assume. I often hear arguments
    that they cannot look at single servers because their needs are *so large*, but
    then deploy systems only a tiny fraction of the standard scale, let alone the
    maximum scale, that a single server can achieve. Remember that vertical scaling
    is highly effective compared to horizontal, and generally cost effective as well.
    The biggest single server systems can support hundreds of the most powerful CPUs,
    and many terabytes (or more) of RAM. The challenge is not finding a single server
    that is large enough for a task, but rather finding any workload that can leverage
    so much power usefully in a single location!
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: A big advantage to standalone servers is that each physical device can be scaled
    and custom designed to address the needs of its workload(s). So different servers
    can use different CPU to RAM to storage ratios, different servers can use different
    CPU generations or architectures, one system might use large hard drives while
    another uses small but screaming fast solid-state storage. Tuning is very easy.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Simple does not necessarily mean simple
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having a standalone server does not mean that we give up all of the options
    and flexibility that we might believe that we need from more complex designs.
    It simply means that we have to think about them differently. Many of the concerns
    that one may have about standalone servers likely stems from a pre-virtualization
    world with relatively slow networks. Today we have virtualization, fast storage,
    and fast networks and these can move the goal line by a bit.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: We refer to servers as being standalone in reference to their architecture,
    everything is self contained in a single piece of hardware. This does not mean
    that we do not have (or cannot have) more than one server. On the contrary, giant
    Fortune 100 firms will often have thousands of standalone servers. What makes
    then standalone is that they have no dependencies on each other. The complete
    failure (or theft) of one does not negatively impact another.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: A tiny organization might choose to rely on a single standalone server for their
    entire business and depend completely on backups and the ability to restore to
    replacement hardware should disaster strike. This is a totally valid approach
    and quite common.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: If your organization is larger, or workloads require more immediate protection
    against loss of availability, then it is standard to run multiple standard servers.
    This spreads load between physical hardware devices and, because of virtualization,
    provides an opportunity for there to be natural ways of mitigating hardware failure
    by rapidly rebuilding lost workloads on other hardware. If deployment density
    is too high, spare hardware is an option as well. With modern storage, networks,
    system management, and backup techniques restoring many workloads can be done
    in as little as minutes allowing even complete hardware failures to often carry
    only the tiniest of system impacts. In fact, keeping backups stored on other standalone
    nodes can allow for essentially instant recovery of lost systems while maintaining
    strong decoupling.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Standalone servers also do not imply that there is no form of unified management.
    Tools like ProxMox or VMware vSphere allow a consolidation of management while
    keeping system hardware independent. Modern tooling has made managing sprawling
    fleets of standalone servers very simple indeed.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Most every aspect of a stand-alone server can be improved by adding more to
    it and making it more complex, the two things that it always leads on are cost
    and simplicity. No other approach will reliably be able to keep our costs or our
    simplicity as low and in business, these are generally the factors that matter
    most.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Many to many servers and storage
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As companies grow there can be an opportunity to consolidate different aspects
    of the architecture in order to save money. Separating networking and storage
    is the most common approach to this. Creating a layer of compute nodes, and a
    layer of storage nodes allows for a lot of flexibility. The primary benefit is
    allowing for the easy movement of resources and better system utilization.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: For example, an organization maybe need fifteen physical compute nodes (traditional
    servers) but only half a dozen storage nodes (SAN or NAS) to support them. Each
    individual system can be easily custom scaled and does not need to match other
    systems in the pool. In this way this approach is not so different from the standalone
    server approach.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that when doing this, the storage layer is the greater risk,
    compared to the compute layer, for two key reasons. First, it is stateful where
    compute is stateless, which means that here we not only have to protect the availability
    (uptime) of the system, but this is also where we have to protect the data as
    it is stored so we have the risk of data loss as well - there is simply more to
    lose here. Second, storage is more complex than compute and equivalent hardware
    and software at both layers means that the storage layer is just more likely to
    fail due to complexity. This is all risk that also exists in the standalone server,
    but when combined into a single chassis it can be more difficult to understand
    where the risk is occurring, even if we know what the overall resultant risk is.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: In its simplest incarnation, we would have a single compute server node and
    a single storage node (typically a SAN array) and would connect them directly
    via a straight cable (Ethernet, eSATA, FC, and so on.) This is really more of
    a hypothetical scenario as it is so obviously bloated and illogical without any
    scale, but we can learn from the example to see how we take the single standalone
    server design and, without any benefits of scale, simply double the chassis to
    manage and increase the physical, as well as, logical complexity of the system
    design.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Typically, this type of design is leveraged most to consolidate heavily on storage,
    pushing as much storage into a single node as possible, while having many small
    to medium sized (one to two CPU) servers that allow workloads to move between
    them in order to best balance said workloads. This approach is flexible and generally
    cost-effective, and makes large scalability quite simple.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Moving past standalone servers means we start to inject dependencies that need
    to be discussed. At a minimum, when we move to a multi-nodal system, we have the
    complexities of the interconnections (which might be as simple as just a cable,
    or more complex like going through a switching fabric of some sort), any complexities
    that come from configuring the nodes to speak to each other, and the risks of
    the extra components that might fail.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: This kind of design really does nothing to address risk, and actually is far
    riskier than standard standalone servers. This is why it is important to use standalone
    servers as a baseline and discover risk variation from that point. In a *network*
    system design, there is no redundancy, so each workload has a full dependency
    on both its associated compute node and its associated storage node(s). This risk
    may be uniform across a compute node or each workload located there might have
    unique storage configurations so that risks may vary widely between different
    workloads on a single server. This is where risk becomes much more complicated
    to measure because we have to deal with the cumulative risks of the compute node,
    the storage node, the connection between the two, and the configuration! Each
    individual piece is extremely difficult to measure on its own – putting them together,
    we mostly have to look in relative terms only and understand that it is much riskier
    than a standalone server.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the world as a workload
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: System architecture is a *by the workload* task and there is no specific necessity
    for all workloads in your organization, or even all workloads running on a single
    compute node, to share architectures. Mixing and matching is totally doable and
    somewhat common. Each workload should be being evaluated as to its own needs,
    and then the overall architecture evaluated.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Often overlooked is the ability to use a complex and less reliable (but potentially
    less expensive at scale) option like network design for workloads that are less
    important, while on the same compute nodes also having local storage that is extremely
    fast and/or reliable for more critical workloads. Mixing and matching can be a
    strong strategy in a large environment where storage consolidation is considered
    necessary without endangering an isolated number of highly critical services in
    order to do so.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: In the same vein, each workload can have its own backup, replication, failover
    and other risk mitigation strategies for deal with disaster. Sharing a compute
    node generally dictates very little as to how reliability and availability from
    workload to workload must be handled. Of course typically all workloads are treated
    the same either out of a desire for standardization and simplicity, but also regularly
    out of a misunderstanding of the range of customization available for each individual
    workload. It is often assumed that choosing a system design is an all or nothing
    endeavour, but this is not the case.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: The main challenges of network system design is that any efficiency gained has
    to offset the additional cost created by needed more overall nodes (separating
    compute and storage means that additional hardware chassis and operating systems
    are necessary for the same tasks) and at any scale additional networking equipment
    is needed to handle the interconnects. Networking equipment can be as simple as
    a single Ethernet switch or as complex as clusters of Fiber Channel or Infiniband
    switches. Switches represent no only additional cost to purchase, but also additional
    points of failure both for hardware and, to a much lesser extent, configuration.
    Often redundant switches are purchased reducing hardware risk but increasing cost
    and configuration complexity. Even in extremely large environments this represents
    additional cost and risk that is very hard to overcome.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'The Inverted Pyramid of Doom: Clustered Compute with Risky Storage, aka the
    3-2-1'
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sadly the Inverted Pyramid of Doom (aka 3-2-1 or IPOD) has traditionally, for
    the majority of the 2000s and 2010s, been the most commonly deployed architecture
    in small and medium business and is also the prime example of the absolutely worst
    possible design decision for normal workload needs. It is also the design that
    maximizes profits for vendors and resellers, so it is what everyone wants you
    to buy.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'The IPOD design is differentiated from the network system design above in that
    the compute layer is clustered for high availability, but the storage layer is
    not. As we discussed in the last design storage is both more important to protect
    and more likely to fail. Typically the networking layer (the layer providing connectivity
    between compute and storage) is also clustered for high availability. This nodal
    count by layer creates the naming conventions used: 3-2-1 refers to the design
    having three (or more) compute nodes, connected to two redundant switches, all
    relying on a single storage device which is most typically a SAN.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: When viewed in an architectural drawing, the IPOD is a pyramid with the wide
    portion on top and everything balanced on the point. Hence the term *inverted
    pyramid*, this design is designed to be as costly and risky as possible, hence
    the moniker *of doom*.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Top-down redundancy
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Why is a design so obviously impractical as the IPOD so traditionally popular?
    The answer requires us to understand several factors. First, redundancy, risk,
    and system design are all areas that most businesses, and even most IT departments
    within those businesses, have received no training and are generally completely
    unaware and so represent an easy target for vendors to be manipulative.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'The real trick comes from two things: linguistics and the simplification enabled
    by top down viewing. The linguistic trick happens because the term *redundancy*
    does not mean what most people believe that it means and this system *has redundancy*,
    but in an all but meaningless way. So, when a customer says, *I need redundancy*
    they actually mean *I need high availability*, but this allows the vendor to state
    that there is redundancy and ignore actual needs. Semantics are super important
    in all business, and IT more than most.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: The top-down aspect of the system comes from how we view the architecture. As
    IT professionals, we know that we should view our architecture *from the side*,
    that is seeing the reliability as it stands layer by layer, knowing that compute
    is built on top of the network, and the network on top of the storage. But a vendor
    wanting to steer a customer to believe that there is strong redundancy will demonstrate
    the system *top down* showing a view that only sees the compute layer where there
    is redundancy. The other layers are overly complex and tend to be happily ignored
    by all parties as being *black boxes that do magic*. Ignoring the hard parts and
    just focusing on the trivial, easy part where redundancy is least important makes
    it really easy to mislead a customer.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if we really stop and think about it, what matters is the overall
    reliability of the entire system. Getting distracted by any single layer will
    simply lead us astray. We need to understand all of the layers, and how they interact
    with each other, in order to determine overall reliability, but there is a strong
    emotional drive to see one layer as being extremely reliable (as the compute layer
    here often is) and then feeling that the overall system must therefore be extremely
    reliable. But this is anything but true. The overall reliability of the system
    is driven primarily by the most fragile layer, not the most reliable. The system
    risk is, if you recall from earlier, cumulative. You combine all of the risks
    together because each layer depends one hundred percent on every other layer,
    if any layer fails everything fails. You can demonstrate this easily with a thought
    experiment... if one layer has a 100% chance of failure, and all other layers
    have a 0% chance of failure, the system will still fail 100% of the time. The
    impossibly reliable layers do literally nothing to offset the unreliable layer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Redundancy itself is a dangerous word to use. In general English usage, the
    word redundant simply means that you have multiple of something when fewer are
    needed. This can mean that one is a replacement or backup should the other fail,
    but that is not implied and often the term is used to mean something else. In
    RAID, for example, RAID 0 has multiple disks (redundant) but the more redundancy
    the higher the risk, not the lower. RAID 1 is the opposite. Redundancy is polar
    opposite there, even within a single context. This really shows the importance
    of semantics in IT (and business, or really, life in general.) People often use
    redundancy as a proxy work for reliability, but the two mean very different things.
    Use the term you mean and you will get far better information.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: The biggest problem with the IPOD design is one of practicality. If we were
    to look at it purely from a reliability standpoint we could state that it is safer
    than the network system design because at least some of the layers contain high
    availability measures, even if not all of them do. And this is totally correct,
    but tends to be misleading. Network system design is meant to trade high risk
    for cost savings versus the simple stand alone server design, using the idea of
    *safer than* something that is not even designed to be safe is not exactly wrong,
    but talking about it in that context is done to evoke an emotional response -
    to make the IPOD feel safe, which is not the same as *safer*. If we compare the
    reliability of an IPOD to the stand alone server, it feels quite unsafe and remember,
    we stated at the beginning, the low cost, simple, stand alone server is our baseline
    for comparisons. The problem with the IPOD is that the risk is extremely high,
    approaching the risk of the network system design, while its costs are much higher
    than the network system design and generally much higher than the stand alone
    server design all while having more complexity and effort for the IT team. It
    is the consistent combination of high risk and high cost that makes it problematic
    and generally accepted as the worst design to encounter in the real world.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Outside of production environments, the IPOD is often ideal for large lab environments
    where capacity matters most and reliability does not matter at all. The ability
    to flexibly scale compute with a single consolidated, low cost, highly unreliable
    storage layer can make sense to make large scale labs more affordable.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Layered high availability
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The logical system design derived from what we have already seen is to take
    the separate layers of the network system design, and the high availability clustering
    from the compute layer of the Inverted Pyramid of Doom and apply it to all layers
    giving us a high availability storage layer, a high availability networking layer,
    and a high availability compute layer. In this way we can have large scale compute,
    storage, and networking without any individual layer being a high level of concern.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: where each layer still depends on every other layer, that three highly available
    layers must still be evaluated with the risk of each layer added together. So,
    while we can almost certainly make any individual layer more, or even far more,
    reliable than a single standalone server would be when we accumulate the risk
    of each layer, and then add in the risk of the additional complexities from incorporating
    the layers together, it may or may not remain more reliable than the standalone
    server would be.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Reliability is relative
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When discussing reliability and these different architectures we have to remember
    to think in terms of apples to apples, not apples to oranges. When we say that
    a single server is a certain level of reliability, and that servers clustered
    with standard high availability technologies have a certain relatively higher
    reliability, we are assuming that all of those servers are roughly identical in
    their individual reliability. In most situations this is true. Whether we are
    talking compute nodes, networking hardware, or storage nodes, for roughly the
    same price range we get similar quality hardware and software with roughly similar
    failure characteristics. So, these different devices *of the same quality* are
    all about the same level of reliability with networking hardware being the most
    reliable (least complex) and storage nodes being the least reliable (because they
    are the most complex.)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: However we can manipulate this dramatically. A five thousand dollar server will
    generally be much less reliable (and performant) than a five hundred thousand
    dollar server. Yet each is an individual, stand alone server. So clearly we have
    to think in terms both of architectural reliability (the reliability of the system
    design that we make) and in terms of the individual components.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: A common problem found here is that *you get what you pay for* applies not at
    all and you can easily find extremely expensive single-chassis systems for both
    compute and storage nodes that are not highly available at all and may not even
    be as reliable as average devices! As reliability is hard to measure and even
    harder to prove, vendors have little incentive to tell us the truth. Vendors are
    highly incentivized to tell us whatever is likely to make us spend more money
    with them whether it is making us feel that traditional servers are more fragile
    than they really are, or by making wild high availability claims for devices that
    are essentially built from straw (and by pigs.)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: So we must be careful that we consider all of the factors. And we must understand
    that the ability to protect a single chassis (vertical reliability) compared to
    multiple chassis (horizontal reliability) is different. Single chassis reliability
    tends to be incredibly powerful for certain components (such as redundant power
    supplies, high quality components, and mirrored RAID for storage) but tends to
    be complex and problematic for others (CPU, RAM, Motherboards.) And single chassis
    systems, while easier to operate, cannot address some key concerns like physical
    damage (water, fire, forklift) in the same way that multiple chassis can.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: We must also be keenly aware that marketers and sales people often use confusion
    around reliability as a sales tactic and will push concepts such as *dual controller*
    systems as being essentially impossible to fail but without science or math to
    back it up. Dual controller systems are simply horizontally scaled systems inside
    a single chassis with all of the complexity of the former and the lack of physical
    protection of the later. And any product sold based on being misleading is that
    much more likely to be poorly made as it means that the vendor is unlikely to
    be being held accountable to quality design.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: It has become known, especially in the early 2010s, that server vendors were
    regularly pushing products branded as high availability or *cannot fail* that
    did not even begin to approach the baseline reliability of traditional servers.
    Since customers could not verify this for themselves, they often just take the
    vendor's word for it and if the businesses loses money, finger pointing is the
    natural recourse.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: This approach necessarily is the most expensive design we can reasonably assemble
    because we need multiple devices at each layer, as well as technology to create
    the clustering at each layer. This is best for very large systems where each layer
    is able to scale so large that cost benefits of scale come in at every point.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that essentially all cloud based systems run on this architecture
    due to their enormous scale. Certainly not all as cloud can run using any architecture,
    but this is far and away the most likely to be used in a large, public cloud implementation
    and is most generally what would be found even in a moderate scale private implementation.
    Many clouds do, however, run on stand alone servers even at massive scale.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Hyperconvergence
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last architectural type that we will look at is hyperconvergence and we
    have now come full circle from increasingly complex designs to one of the least.
    Hyperconvergence as an architecture is anything but new, but for the last few
    decades it has been almost completely ignored before having a sort of renaissance
    in the mid-2010s and is now, along with stand alone servers, the bulwark of system
    architectural design.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Hyperconvergence, also called HC or HCI, takes the compute and storage nodes
    of other, more complex architectures, and recombines them back into single servers
    (or you can view it as taking stand alone servers and adding high availability
    through engineering redundancy without adding unnecessary complexity.) Hyperconvergence
    gives us the best of both worlds, simplicity like stand alone servers, but options
    for high availability like Layered High Availability.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Hyperconvergence is both so incredibly simple, but so effective that it can
    be hard to explain. The key strategy is taking the existing power and cost savings
    of the stand alone approach and doing as little as possible while still being
    able to add high availability clustering. By having multiple stand alone nodes
    that are clustered together (are they still stand alone, then?) we make all of
    the pieces highly available, while also reducing how many pieces are needed in
    total.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: When done correctly, data can also be guaranteed to be kept locally to a compute
    node, even though storage is replicated between nodes to create storage high availability,
    which not only means that we can get the high performance for our storage like
    we can with stand alone servers, but also that we can avoid a cross-node dependency
    allowing any node to keep working on its own, even if all other nodes and/or the
    network connecting them fails! That means that unlike all other system designs,
    we are only adding more resilience on top of the stand alone server design! That
    is huge. All other designs must put in the bulk of their efforts to overcoming
    their own introduced fragilities and risk failing to adequately do so potentially
    leaving them riskier than they would have been had we done nothing.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Hyperconvergence, therefore, acts as the logical extension of the stand alone
    design and represents probably the most applicable system design for any large
    scale system. It is common to see hyperconvergence as being limited to small systems,
    but it is able to scale to the limits of the clustering technologies - the same
    limits affecting all design options. So all standard designs can go to roughly
    the same size, which is generally far larger than anyone would want to go in practical
    terms within the confines of a single system.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: High availability vs basic clustering
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: In this section we talk about clustering with the assumption that clustering
    (whether compute, storage, or networking) is done for the purpose of making the
    system highly available, at least within that one layer. High availability clustering
    is not the only kind of clustering, however. In all of these designs, including
    stand alone, we can add generic clustering as a management layer to manage many
    systems together. This can be confusing as the term clustering can be used to
    mean many things.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Best practices in System Design Architecture
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best practice for system design is to keep your architecture as simple as
    possible to meet your needs, but no simpler. Remember that simplicity is a benefit,
    not a caveat. Complexity should be avoided when possible as complexity brings
    cost and risk.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: In any assessment, start with the brick. Just one, single, solitary stand alone
    server. Simple and effective. Now evaluate, does this meet your needs? How could
    spending more money better meet your needs?
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: If high availability is needed, then assess hyperconvergence. Nothing can be
    more reliable, architecturally speaking.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: If you have special cases where you need cost savings at massive scale, other
    designs might be applicable. But remember, no matter how reliable it might feel
    emotionally or how much a sales person may push the solution, hyperconvergence
    is literally impossible to beat for reliability by design. Make sure that any
    design that is not one of these two starting points is being used with a comprehensive
    understanding of all of the risks and costs involved.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: That is a lot of material that we covered and a lot of turning conventional
    thinking on its ear. It is sad that here *conventional thinking* equates to *blindly
    ignoring needs and risk logic*, but it is what it is. This is a very difficult
    topic because it is so foreign to most people in any technical or business realm
    and such a specialty skill to master. And in many cases, you will get a lot of
    pushback from others who struggle to assess or communicate risks, and fail to
    turn risk information into actionable business decision making.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: At this point you have the tools and knowledge to design systems physically.
    This is a big topic, and it might be worth revisiting from time to time. This
    is very foundational and gives us the starting point to build reliable systems
    farther up the proverbial stack. And now that we know how to approach different
    designs for different purposes we will go on to look at risk itself and learn
    to ask what risk mitigation is right for us.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Risk assessment and availability needs
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the very core of what we do in designing a system architecture is taking
    business needs around performance and availability and applying our understanding
    of risk and, as with everything in business (and therefore IT) assessing against
    costs. In the last section we already talked about risk, a lot. We have to - risk
    and performance pretty much define everything (other than strict capabilities
    and features) for us during our design stages.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'If we ask our businesses about risk, we almost always receive one of two stock
    answers: *we are not willing to pay anything to mitigate risks* or *we cannot
    afford to go down, it is worth anything to be up one hundred percent of the time*.
    Both answers should be obviously seen as insane and have no reason to ever come
    out of the mouth of any business person or IT professional, and yet they are nearly
    the only answers that you will ever receive providing you with no guidance whatsoever.
    They represent management simply *blowing off* IT and leaving IT to take on all
    decision-making risks without management providing any guidance.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: We have some amount of basic guidance that we can almost always work with. On
    the *low* end of the spectrum the rule of thumb is that if data is valuable enough
    to have stored in the first place, then at a minimum it is worth backing up. This
    is the simplest aspect of data and availability protection, and if your business
    thinks that the data that they store is not worth even backing up, you should
    be asking yourself why you are there yourself. There are extremely special cases
    where storage data is truly ephemeral and does not need a backup, but this situation
    is so unique and rare that it can be safely ignored.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: On the other end of the spectrum no system, anywhere, ever is so important that
    it is worth anything to not have downtime. First of all, totally avoiding downtime
    is impossible. No one can do this with any amount of resources. We can make a
    system ridiculously reliable and easily recoverable from nearly endless potential
    scenarios but no government, military, secret cabal, alien species, investment
    bank, or otherwise can possibly meet requirements often demanded by small businesses
    without any discussion whatsoever. The theoretical maximum that can be invested
    into making systems reliable is the entire value of the company in question and
    even if every penny that the largest firms had was invested into reliability and
    nothing else, risk still remains, no matter how small.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Risk and diminishing returns
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Attempting to invest in risk mitigation technology is a tough thing to do because
    as systems become more reliable the cost of *moving the needle* significantly
    towards ever improved reliability becomes more and more costly. For example, getting
    a stand alone server that is well built may give us as much as five nines of availability
    without any special *high availability* features.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: We may find that we need much higher availability. Perhaps six or seven nines.
    To get those order of magnitude jumps in reliability will require, almost certainly,
    at least double the investment in hardware as the standalone server. This may
    be well justified by our needs, but the cost per workload just jumped significantly.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: If we want to move the needle again an order of magnitude beyond that, the price
    jumps yet again. We get less and less protection as we spend more and more money.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: And so, because our business will rarely be willing or able to clearly define
    for us what our risk aversion level truly is, it often falls to IT and within
    IT to systems administration to carry out this all-important task on behalf of
    management. This will generally take some math, a lot of interviews with many
    different parts of the company, some common sense, and of course, some guesswork.
    Working with risk requires maintaining a logical view while it is tempting to
    become emotional, which is often the mistake made in business. Business owners
    or managers tend to react emotionally either seeing money spent on protection
    as not generating revenue directly and therefore undesirable, or seeing their
    business as not justifying protection and so tending to spend too much to provide
    an impression that the company is seen as valuable because downtime would be such
    a terrible thing.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Of course good management will always been heavily involved in risk assessment
    tasks. This should not fall to IT. While IT has great insight and is a valuable
    contributor to any risk discussion it is the core management, operations, and
    financial departments that truly have the full risk picture necessary to create
    a corporate infrastructure risk strategy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: When looking at workloads, we must attempt to evaluate what downtime will truly
    cost our business. This is not straightforward in nearly any scenario, but it
    is what we need to understand to have any means of logically discussing risk.
    Most businesses want to simplify downtime cost into the simplest possible terms.
    So dollars per minute or hour is generally how downtime is discussed. For example,
    losing the company's primary line of business application will lose the business
    one thousand dollars per hour of downtime.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: While simple, almost no real world workload actually loses revenue evenly hour
    by hour. In the real world it is most common to see a complex curve. For example,
    in the first minutes, and possibly even hours, we might see almost zero revenue
    loss. But then commonly we see a spike as outages go long enough to first be perceived
    by customers, then to cause customer concern. Lack of confidence and lack of operations
    spikes tend to hit a peak relatively quickly. Then long term revenue loss tends
    to start to kick in after days or weeks as customers leave. But this curve is
    different for every business. Of course, making an entire curve graph of all downtime
    scenarios is difficult and probably impractical, but the business should be able
    to predict significant inflection points along a timeline that represent major
    changes in impact behavior.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: It is tempting to look at outages as being all or nothing. Basically, ignoring
    workloads and seeing the entire company as completely down, as if the zombie apocalypse
    is happening and all of the staff have been infected. It is a rare workload that
    is going to impact any business in that manner. For example, if a company loses
    an email workload there will likely be an impact, but as email is not real time,
    it might take hours or even days before there is an actual loss of revenue (but
    if email is used to win real time bids, losing even a few minutes might be very
    impactful - it just all depends.) But assuming email could not be restored for
    hours, or days, a normal business would immediately begin mitigating the loss
    of the email workload through other channels. Maybe employees talk to each other
    in person, or use the company's instant messaging products. Perhaps sales teams
    begin to call customers on the phone rather than emailing. Working around a lost
    workload is often far more possible and effective that one realizes until a triage
    process is performed to see what a real world recovery might really look like.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Workload interplay
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Something else that we need to understand is how workloads interact with each
    other. As systems administrators we might have excellent insight into technical
    dependencies such as that a key ERP or CRM system depends on another application,
    such as email or financial, to function and if one is down, the other is down,
    too. That is an important aspect of workload dependency, but one that is well
    known and understood just by mentioning it. What is much harder to understand
    is the human workflow interdependence of systems.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Some workloads may be exceptionally stand alone. Some may depend significantly
    on others. Others may overlap and provide risk mitigation unofficially.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Lets look at the third case first. In many an organization today there might
    be traditional telephones, email, a few types of video conferencing solutions,
    and a handful of instant messaging solutions. Even in a tiny organization it is
    easy to casually end up with several overlapping solutions simply because so many
    things come bundled with so much functionality. In a situation like this, losing
    email might matter little for a very long time as internal communications may
    move to instant messaging and customer communications to telephone or video conference.
    Most organizations have the ability to work around a system that is down by using
    other tools at their disposal.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: But the converse is also true. Two technically unrelated systems, again lets
    say CRM and email, might not connect together but the human workflow may require
    that both be used at the same time and the loss of either one of them might be
    functionally equivalent to losing them both. So we have to consider all use cases,
    and all mitigation possibilities.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: This interplay knowledge will help us to determine how it makes sense to deploy
    some workloads. For example, if email and instant messaging work to overlap during
    a crisis, it likely makes sense to decouple them as much as possible so that if
    the hardware or software for one was to fail that it would not take down the other.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: If we have systems, like our email and CRM example, where one is useless without
    the other, then combining the two workloads to share a failure domain might make
    total sense. Meaning, as an example, if we had two independent servers one running
    the CRM and one running email, then each individual server would carry its own
    risk of failing with near certainty that those failures would not happen at an
    overlapping time. Each workload has an equal amount of expected annual downtime
    of X. The total downtime expected for the combined workloads is 2X. Easy math.
    Combine the two workloads onto a single server as each retains an equal amount
    of annual downtime risk, still X, and the combined is still 2X. But the effective
    downtime in the first case is 2X (or 1.99999X as there is some tiny chance of
    the outages overlapping) but in the second case is just 1X. How did we do that?
    Not by reducing any individual risk, but by reducing the effective risk - that
    is the risk impacting the business as a final result. Under the hood, we did reduce
    risk physically as a single server has half the risk of downtime of two equal
    servers simply because there is half as many devices to fail.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Even a complete company shutdown is not necessary a total loss. Sending staff
    home for a surprise holiday might lower insurance costs and raise morale. Given
    a day or two to spend at home might reinvigorate workers who may be happy to return
    after systems have been restored and work more efficiently or maybe put in a little
    extra time to attempt to recoup lost business. We have to consider mitigation
    strategies when looking at losses from failed workloads. Some businesses may simply
    lose some efficiency, while others may lose customers.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Of course we have to consider the possibility of the opposite. What if you are
    a business that depends heavily on customer perception of high uptime and even
    a tiny outage has a sprawling impact? Maybe your entire business generates only
    one thousand dollars per hour, but loss of customer confidence from even a two
    hour outage (which we might assume could only, at maximum, lose us two thousand
    dollars in this case) resulted in the loss of customers resulting in tens or hundreds
    of thousands of dollars of losses!
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: All of these losses are just estimates. Even if an outage actually happens,
    there is no guaranteed way to know what revenue would have been without an outage
    having occurred. So if we cannot know this number for certain after something
    has happened then obviously we cannot know it with any certainty before the event
    that only might happen, has happened. Bottom line... estimating risk is very hard.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'In a large organization, consider playing *what if* games on a weekend with
    some staff from different departments. Run through scenarios of *X or Y has failed*
    and attempt to mitigate in a nearly real-world simulation on a small scale. Can
    you keep working with one tool or another? Which departments become dysfunctional,
    which keep humming along, how do your customers see the situation? This kind of
    *game* is best played with a combination of strong planners who are thinking about
    risk strategies and writing procedures as well as with a group of perceivers (for
    example: triage experts) who do not plan, but work tactically on the ground figuring
    out how to keep working with the tools at their disposal.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Defining high availability
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of my favorite quotes in all of IT comes from John Nicholson who said *High
    availability isn't something that you buy, it is something that you do*. It is
    so tempting to see high availability as an intrinsic need in IT, and then to see
    it as so complicated that we cannot know how to approach it, and so fall prey
    to vendors who slap the unverified name *high availability* onto products, or
    even just slip it into a product name, and act as if buying a product can deliver
    high availability when logically, this is impossible. Imagine buying a high availability
    airplane, as an example. While you can make one airplane must more reliable than
    another, almost all of your overall reliability and safety comes from the pilot,
    not the plane. The same is true in IT. The best made product does little if operated
    poorly. A million-dollar cluster without backups is likely not as safe as a desktop
    with good backups!
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: So first we need to establish a baseline for measurement. In our last section
    we said that stand alone server infrastructure serves as our baseline. This baseline
    has to represent what we will call *standard availability*. We now have two ways
    that we would hope that we can look at this availability. One is in absolute terms
    by giving a number such as a *nines* number and through industry evidence, it
    appears that well maintained, well-made stand alone servers can approach five
    nines of availability which roughly means six minutes, or less, of unplanned downtime
    per year (planned downtime for maintenance can represent a potential problem,
    but is not itself included in a reliability figure such as this.)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Now keep in mind, when we are talking about a server or a system design architecture,
    we are not including the final workload, only the platform providing an underlying
    system onto which a hypervisor will be installed. So essentially hardware availability.
    Any software running on top may have its own reliability concerns and no amount
    of platform stability will fix instability from bad code in the final workload,
    for example.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: The other, and generally more useful, way to look at reliability of system architecture
    is not in unmeasurable absolute terms, but in relative terms comparing different
    designs to one another. No one really knows what system reliability really is.
    It is not a big secret that server vendors are keeping from us, they simply do
    not know. Every little system configuration difference produces very different
    reliability numbers and, like we said about planes, the users operating the systems
    create the largest impact in terms of reliability. A company with a pristine datacenter
    and continuous onsite support that responds to alerts immediately and spare parts
    on hand or nearby might be able to squeeze very different reliability numbers
    out of the same server stuck in a closet without air conditioning, lots of dust,
    and generally ignored. There are simply too many factors involved. And even if
    we could somehow account for all of the potential variation, in order to get meaningful
    statistics on systems so complex with failure rates so low, we would need to operate
    thousands or tens of thousands of servers for more than a decade to collect useful
    numbers and then all of the data would be outdated by more than a decade. So,
    for all intents and purposes, it cannot be measured.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: So, our most important tool is not talking in terms of *nines*, that is a great
    marketing tool and something that managers steeped in big heavy processes like
    Six Sigma like to repeat, but it means nothing in this context, but rather looking
    at orders of magnitude of systems deviating from our baseline. A system that is
    significantly more available than our baseline can be classified as *high availability*
    and a system that is significantly less available than our baseline can be classified
    as *low availability* and any system that is roughly the same as baseline remains
    *standard availability*. Beyond these general terms it becomes all but impossible
    to discuss.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: A system design like hyperconvergence would be generally classified as *high
    availability* as it is the most reliable design approach. And an IPOD would generally
    be classified as *low availability* as it is closer to the least reliable design
    approach, which is the network system design. Layered clustering is generally
    considered high availability, but not *as high* as hyperconvergence. Of course,
    in this case we are only considering the availability of the system design and
    ignoring individual components. If we use extremely highly available individual
    components at every layer of an IPOD, we can theoretically get it back up to standard
    availability, but likely at great cost.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: It is far more valuable to think of reliability in relative terms, rather than
    absolute ones. It is almost trivial to look at a standalone server, an IPOD, and
    hyperconvergence and see how there is a clear *high*, *medium*, and *low* availability
    based on nothing but common sense and the location of risk, risk mitigation, and
    risk accumulation in the design. It requires no special training or math to see
    how dramatically each is separated from the next and how improving the overall
    quality of components moves the absolute reliability number, but the relative
    does not change. And at the end of the day, this is all that we can know.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: By knowing what downtime impact will look like financially for our business,
    even if it is only a very rough estimate, we have something to work with when
    attempting to decide on how to invest in risk mitigation. We should never invest
    more in risk mitigation than what calculated risk shows as our potential losses.
    This sounds obvious but is a common stumbling point for assessment in many firms.
    For example, if a possible outage might cost us one thousand dollars and protecting
    effectively against that outage would cost two thousand dollars, we should not
    at all entertain paying to mitigate that risk.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: We should think of risk mitigation as a form of outage itself, for mathematical
    reasons. This makes calculations easier to understand. With good risk mitigation
    we would incur a minimal financial penalty now (say spending one thousand dollars)
    to protect against a large potential outage (that might cost us one hundred thousand
    dollars.) The upfront cost is guaranteed, the future risk is only a possibility.
    So any risk mitigation must therefore be much smaller than the potential damage
    that it is meant to protect against.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'An analogy of my own that I have been using for years to describe paying more
    for risk mitigation than the potential damage of the outage itself is: *That''s
    like shooting yourself in the face today to avoid maybe getting a headache next
    year*.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'When comparing standard availability systems and high availability systems
    we might be talking about a difference of only several minutes per year, on average,
    of downtime. High availability, therefore, has to justify its cost and complexity
    very quickly. A massive public website where just a minute or two of being unavailable
    could cost millions in purchases or worse, erode customer confidence, therefore
    could easily justify a large expenditure in high availability systems even if
    the time saved seems trivial. But an internal system servicing employees where
    customer confidence is not a factor, and downtime does not lead users to turn
    to competitors (for example: email system, financial, CRM, and others) the lose
    of even several minutes a day, let alone a year, is likely to have no real financial
    impact whatsoever and investing heavily to protect those systems would be wasteful.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we apply all of this to best practices? The hard answer is that risk
    assessments and resulting system design are very hard things to do. Determining
    risk is a long process involving a lot of math, logic, and to some degree, guessing.
    It requires that we understand our businesses and our technology stacks deeply.
    It demands that we engage the business at all levels, and from all departments,
    and consolidate information that is generally siloed. It forces us to evaluate
    other risk assessments against logic and expected emotional reactions.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Rules of thumb tell us that the majority of systems that we deploy should be
    standalone servers, and nearly all remaining systems should be hyperconverged.
    These two standard patterns represent the near totality of what proper design
    will look like in the real world. All other designs are realistically relegated
    to extremely niche use cases with the IPOD being the ultimate *anti-pattern* of
    what not to do except for the most extreme of special cases.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: We have covered a lot of material in this chapter. But now we have an idea of
    how we make risk determinations, how we design our architectures based on that
    assessment. We understand how and why we use different kinds of virtualization,
    and why we always virtualize. And we know how to evaluate the use of cloud and
    locality for our deployments. Now to put all of this together! We use all of these
    tools in deciding the deployment of every workload! So many options, but that
    is what makes our careers challenging and fulfilling (and what makes us worth
    our salaries.)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In summary, system architecture is complex and requires us to really dig into
    business needs, how operations works, talk to key roles throughout the organization
    and elicit input, and take a broad view of technological building blocks to construct
    solutions that deliver the performance and reliability that our workloads need
    at the minimum cost.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: We looked at fundamental components with virtual machines and containers and
    should now be able to defend our use of them and choose properly between them,
    as well as be able to use traditional containers without becoming confused with
    more recent application containers. And we learned about locality. You should
    be able to navigate the complicated linguistic minefield that is managers attempting
    to talk about the placement and ownership of server resources, analyze costs and
    risks and find the right option for your organization. Colocation, cloud, traditional
    virtualization, on premises are all options that you understand.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: And finally, the big piece, system design and architecture. Taking the physical
    and logical components of our system and building a full functional platform that
    empowers our workloads rather than crippling them. This has been a long chapter
    and touches on a lot of topics that are very rarely taught individual, let alone
    together. These are some really hard topics, and it is probably worth covering
    a lot of this material again before moving on.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: For many of us in systems administration we might use the material in this chapter
    almost never. For others it might be nearly everyday skills. These topics are
    often ones that allow you to completely elevate your career by demonstrating a
    concrete ability to take seemingly mundane technical minutia and applying background
    system design decisions to key organizational needs. Of all of our topics in this
    book, this one is probably the one that should empower you more than any other
    to stand out among your peers and cross organizational boundaries.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: I hope that with the information presented here that you can filter through
    sales and marketing misinformation, apply solid logic and reasoning, and build
    on concepts that will remain timeless. Taking the time to really understand failure
    domains, additive risk, false redundancy, and more will make you better at nearly
    every aspect of your information technology journey whether your goals are purely
    technical, or you dream of sitting in the board room chairs.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we are going to return to the seemingly more pedestrian
    topic of system patching, and move from the high level system strategies to in
    the trenches security and stability warfare.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
