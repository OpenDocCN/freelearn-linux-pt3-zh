<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer304">
<h1 class="chapterNumber">23</h1>
<h1 class="chapterTitle" id="_idParaDest-302">Preventing Disasters</h1>
<p class="normal">In an enterprise network, a disaster can strike at any time. While, as administrators, we always do our best to design the most stable and fault-tolerant server implementations we possibly can, what matters most is how we are able to deal with disasters when they do happen. As stable as server hardware is, any component of a server can fail at any time. In the face of a disaster, we need a plan. How can you attempt to recover data from a failed disk? What do you do when your server all of a sudden decides it doesn’t want to boot? These are just some of the questions we’ll answer as we take a look at several ways we can prevent and recover from disasters. In our final chapter, we’ll cover the following topics:</p>
<ul>
<li class="bulletList">Preventing disasters</li>
<li class="bulletList">Utilizing Git for configuration management</li>
<li class="bulletList">Implementing a backup plan</li>
<li class="bulletList">Utilizing bootable recovery media</li>
</ul>
<p class="normal">We’ll start off our final chapter by looking at a few tips to help prevent disaster.</p>
<h1 class="heading-1" id="_idParaDest-303">Preventing disasters</h1>
<p class="normal">As we proceed through <a id="_idIndexMarker1326"/>this chapter, we’ll look at ways we can recover from disasters. However, if we can prevent a disaster from occurring in the first place, then that’s even better. We certainly can’t prevent every type of disaster that could possibly happen but having a good plan in place and following that plan will lessen the likelihood. A good disaster recovery plan will include a list of guidelines to be followed with regard to implementing new servers and managing current ones.</p>
<p class="normal">This plan may include information such as an approved list of hardware (such as hardware configurations known to work efficiently in an environment), as well as rules and regulations for users, a list of guidelines to ensure physical and software security, proper training for end users, and method change control. Some of these concepts we’ve touched on earlier in the book but are worth repeating from the standpoint of disaster prevention.</p>
<p class="normal">First, we talked about <a id="_idIndexMarker1327"/>the <strong class="keyWord">principle of least privilege</strong> back in <em class="chapterRef">Chapter 21</em>, <em class="italic">Securing Your Server</em>. The idea is to give your users as few permissions as possible. This is very important for security, as you want to ensure only those trained in their specific jobs are able to access and modify only the resources that they are required to. Accidental data deletion happens all the time. To take full advantage of this principle, create a set of groups as part of your overall security design. List departments and positions in your company and the types of activities each is required to perform. Create system groups that correspond to those activities. For example, create an accounting-ro and accounting-rw group to categorize users within your Accounting department that should have the ability to only read or read and write data. If you’re simply managing a home file server, be careful of open network shares where users have read and write access by default. By allowing users<a id="_idIndexMarker1328"/> to do as little as possible, you’ll prevent a great many disasters right away.</p>
<p class="normal">In <em class="chapterRef">Chapter 2</em>, <em class="italic">Managing Users and Permissions</em> (as well as <em class="chapterRef">Chapter 21</em>, <em class="italic">Securing Your Server</em>), we talked about best practices for the <code class="inlineCode">sudo</code> command. While the <code class="inlineCode">sudo</code> command is useful, it’s often misused. By default, anyone that’s a member of the <code class="inlineCode">sudo</code> group can use <code class="inlineCode">sudo</code> to do whatever they want. We talked about how to restrict <code class="inlineCode">sudo</code> access to particular commands, which is always recommended. Only trusted administrators should have full access to <code class="inlineCode">sudo</code>. Everyone else should have <code class="inlineCode">sudo</code> permissions only if they really need them, and even then, only when it comes to commands that are required for their job. A user with full access to <code class="inlineCode">sudo</code> can delete an entire filesystem, so it should never be taken lightly.</p>
<p class="normal">In regard to network shares, it’s always best to default to read-only whenever possible. This isn’t just because of the possibility of a user accidentally deleting data; it’s always possible for applications to malfunction and delete data as well. With a read-only share, the modification or deletion of files isn’t possible. Additional read-write shares can be created for those who need them, but if possible, always default to read-only.</p>
<p class="normal">Although I’ve spent a lot of time discussing security in a software sense, physical security is important too. For the purposes of this book, physical security doesn’t really enter the discussion much because our topic is specifically Ubuntu Server, and nothing you install on Ubuntu is going to increase the physical security of your servers. It’s worth noting, however, that physical security is every bit as important as securing your operating systems, applications, and data files.</p>
<p class="normal">All it would take is someone tripping over a network cable in a server room to disrupt an entire subnet or cause a production application to go offline. Server rooms should be locked, and only trusted administrators should be allowed to access your equipment. I’m sure this goes without saying and may sound obvious, but I’ve worked at several companies that did not secure their server room. Nothing good ever comes from placing important equipment within arm’s reach of unauthorized individuals.</p>
<p class="normal">In this section, I’ve mentioned <em class="chapterRef">Chapter 21</em>, <em class="italic">Securing Your Server</em>, a couple of times. A good majority of a disaster prevention plan includes a focus on security. This includes, but is not limited to, ensuring security updates are installed in a timely fashion, utilizing security applications such as failure monitors and firewalls, and ensuring secure settings for OpenSSH. I won’t go over these concepts again here since we’ve already covered them, but essentially security is a very important part of a disaster prevention plan. After all, users cannot break what they cannot access, and hackers will have a harder time penetrating your network if you designed it in a security-conscious way.</p>
<p class="normal">Effective disaster<a id="_idIndexMarker1329"/> prevention consists of a list of guidelines for things such as user management, server management, application installations, security, and procedure documents. A full walkthrough of proper disaster prevention would be an entire book in and of itself. My goal with this section is to provide you with some ideas you can use to begin developing your own plan. A disaster prevention plan is not something you’ll create all at once but is rather something you’ll create and refine indefinitely as you learn more about security and what types of things to watch out for.</p>
<p class="normal">The configuration files on your server determine the behavior of your services and applications, and backing up can enable you to recover their state. When it comes to managing the state of files, Git is a very powerful tool to do just that, and we’ll talk about it next.</p>
<h1 class="heading-1" id="_idParaDest-304">Utilizing Git for configuration management</h1>
<p class="normal">One of the most<a id="_idIndexMarker1330"/> valuable<a id="_idIndexMarker1331"/> assets on a server is its configuration. This is second only to the data the server stores. Often, when we implement new technology on a server, we’ll spend a great deal of time editing configuration files all over the server to make it work as best as we can. This can include any number of things, from Apache virtual host files to DHCP server configuration, DNS zone files, and more. If a server were to encounter a disaster from which the only recourse was to completely rebuild it, the last thing we’d want to do is re-engineer all of this configuration from scratch. This is where Git comes in.</p>
<p class="normal">In a typical development environment, an application being developed by a team of engineers can be managed by Git, each contributing to a repository that hosts the source code for their software. One of the things that makes Git so useful is how you’re able to go back to previous versions of a file in an instant, as it keeps a history of all changes made to the files within the repository.</p>
<p class="normal">Git isn’t just useful for software engineers, though. It’s also a really useful tool we can leverage for keeping track of configuration files on our servers. For our use case, we can use it to record changes to configuration files and push them to a central server for backup. When we make configuration changes, we push the changes back to our Git server. If for some reason we need to restore the configuration after a server fails, we can simply download our configuration files from Git back onto our new server. Another useful aspect of this approach is that if an administrator implements a change to a configuration file that breaks a service, we can simply revert to a known working commit and we’ll be immediately back up and running. You can even correlate changes in log files to<a id="_idIndexMarker1332"/> changes<a id="_idIndexMarker1333"/> made at around the same time in a Git repository, which makes it easier to narrow down the root cause of an issue.</p>
<p class="normal">Configuration management on servers is so important, in fact, I highly recommend that every Linux administrator takes advantage of version control for this purpose. Although it may seem a bit tricky at first, it’s actually really easy to get going once you practice with it. Once you’ve implemented Git for keeping track of all your server’s configuration files, you’ll wonder how you ever lived without it. We covered Git briefly in <em class="chapterRef">Chapter 15</em>, <em class="italic">Automating Server Configuration with Ansible,</em> where I walked you through creating a repository on GitHub to host Ansible configuration. However, using Github is not mandatory in order to benefit from version control. Sure, it’s definitely convenient – but not required. The only required component for a server to function as a Git server is the presence of the <code class="inlineCode">git</code> package:</p>
<pre class="programlisting con"><code class="hljs-con">sudo apt install git
</code></pre>
<p class="normal">And just like that, you now have a Git server. That may have seemed overly simplistic (and it is) but there aren’t a great deal of requirements in order to set up Git as a central resource on your network. Since Git uses OpenSSH by default, you’ll be able to store repositories on a server that has the <code class="inlineCode">git</code> package and is accessible to clients on your network.</p>
<p class="normal">If for some reason you don’t have an extra server to act as your Git server, you can install the <code class="inlineCode">git</code> package on another instance and add that functionality to an existing server. My personal preference is for each server to focus on one task (and perform that one task well) but sometimes the budget of the organization may not allow for a stand-alone server for every service.</p>
<p class="normal">Now, think of a configuration directory that’s important to you, and that you want to place into version control. A good example is the <code class="inlineCode">/etc/apache2</code> directory on a web server. That’s what I’ll use in my examples in this section. But you’re certainly not limited to that. Any configuration directory you would rather not lose is a good candidate. If you choose to use a different configuration path, change the paths I give you in my examples to that path.</p>
<p class="normal">On the server, create a directory to host your repositories. I’ll use <code class="inlineCode">/git</code> in my examples:</p>
<pre class="programlisting con"><code class="hljs-con">sudo mkdir /git
</code></pre>
<p class="normal">Next, you’ll want<a id="_idIndexMarker1334"/> to<a id="_idIndexMarker1335"/> modify this directory to be owned by the administrative user you use on your Ubuntu servers. Typically, this is the user that was created during the installation of the distribution. You can use any user you want actually, just make sure the user is allowed to use OpenSSH to access your Git server. Change the ownership of the <code class="inlineCode">/git</code> directory so it is owned by this user. My user on my Git server is <code class="inlineCode">jay</code>, so in my case, I would change the ownership with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">sudo chown jay:jay /git
</code></pre>
<p class="normal">Next, we’ll create our Git repository within the <code class="inlineCode">/git</code> directory. For Apache, I’ll create a bare repository for it within the <code class="inlineCode">/git</code> directory. A bare repository is basically a skeleton of a Git repository that doesn’t contain any useful data, just some default configuration to allow it to act as a Git folder. To create the bare repository, <code class="inlineCode">cd</code> into the <code class="inlineCode">/git</code> directory and execute:</p>
<pre class="programlisting con"><code class="hljs-con">git init --bare apache2
</code></pre>
<p class="normal">You should see the following output:</p>
<pre class="programlisting con"><code class="hljs-con">Initialized empty Git repository in /git/apache2/
</code></pre>
<p class="normal">That’s all we need to do on the server for now for the purposes of our Apache repository. On your client (the server that houses the configuration you want to place under version control), we’ll copy this bare repository by cloning it. To set that up, create a <code class="inlineCode">/git</code> directory on your Apache server (or whatever kind of server you’re backing up) just as we did before. Then, <code class="inlineCode">cd</code> into that directory and clone your repository with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">git clone 192.168.1.101:/git/apache2
</code></pre>
<p class="normal">For that command, replace the IP address with either the IP address of your Git server or its hostname if you’ve created a DNS entry for it. You should see the following output, warning us that we’ve cloned an empty repository:</p>
<pre class="programlisting con"><code class="hljs-con">warning: You appear to have cloned an empty repository
</code></pre>
<p class="normal">This is fine, we haven’t actually added anything to our repository yet. If you were to <code class="inlineCode">cd</code> into the directory we just cloned and list its storage, you’d see it as an empty directory. If you use <code class="inlineCode">ls -a</code> to view <a id="_idIndexMarker1336"/>hidden directories as well, you’ll <a id="_idIndexMarker1337"/>see a <code class="inlineCode">.git</code> directory inside. Inside the <code class="inlineCode">.git</code> directory, we’ll have configuration items for Git that allow this repository to function properly. For example, the config file in the <code class="inlineCode">.git</code> directory contains information on where the remote server is located. We won’t be manipulating this directory; I just wanted to give you a quick overview of what its purpose is.</p>
<p class="normal">Note that if you delete the <code class="inlineCode">.git</code> directory in your cloned repository, that basically removes version control from the directory and makes it a normal directory.</p>
<p class="normal">Anyway, let’s continue. We should first make a backup of our current <code class="inlineCode">/etc/apache2</code> directory on our web server, in case we make a mistake while converting it to being version controlled:</p>
<pre class="programlisting con"><code class="hljs-con">sudo cp -rp /etc/apache2 /etc/apache2.bak
</code></pre>
<p class="normal">Then, we can move all the contents of <code class="inlineCode">/etc/apache2</code> into our repository:</p>
<pre class="programlisting con"><code class="hljs-con">sudo mv /etc/apache2/* /git/apache2
</code></pre>
<p class="normal">The <code class="inlineCode">/etc/apache2</code> directory is now empty. Be careful not to restart Apache at this point; it won’t see its configuration files and will fail. Remove the (now empty) <code class="inlineCode">/etc/apache2</code> directory:</p>
<pre class="programlisting con"><code class="hljs-con">sudo rm /etc/apache2
</code></pre>
<p class="normal">Now, let’s make sure that Apache’s files are owned by <code class="inlineCode">root</code>. The problem though is if we use the <code class="inlineCode">chown</code> command, as we normally would to change ownership, we’ll also change the <code class="inlineCode">.git</code> directory to be owned by <code class="inlineCode">root</code> as well. We don’t want that, because the user responsible for pushing changes should be the owner of the <code class="inlineCode">.git</code> folder. The following command will change the ownership of the files to <code class="inlineCode">root</code>, but won’t touch hidden directories such as <code class="inlineCode">.git</code>:</p>
<pre class="programlisting con"><code class="hljs-con">sudo find /git/apache2 -name '.?*' -prune -o -exec chown root:root {} +
</code></pre>
<p class="normal">When you list the contents of your repository directory now, you should see that all files are owned by <code class="inlineCode">root</code>, except for the <code class="inlineCode">.git</code> directory, which should be owned by your administrative user account.</p>
<p class="normal">Next, create a <a id="_idIndexMarker1338"/>symbolic <a id="_idIndexMarker1339"/>link to your Git repository so the <code class="inlineCode">apache2</code> daemon can find it:</p>
<pre class="programlisting con"><code class="hljs-con">sudo ln -s /git/apache2 /etc/apache2
</code></pre>
<p class="normal">At this point, you should see a symbolic link for Apache, located at <code class="inlineCode">/etc/apache2</code>. If you list the contents of <code class="inlineCode">/etc</code> while grepping for <code class="inlineCode">apache2</code>, you should see it as a symbolic link:</p>
<pre class="programlisting con"><code class="hljs-con">ls -l /etc | grep apache2
</code></pre>
<p class="normal">The directory listing will look similar to the following:</p>
<pre class="programlisting con"><code class="hljs-con">lrwxrwxrwx 1 root root 37 2020-06-25 20:59 apache2 -&gt; /git/apache2
</code></pre>
<p class="normal">If you reload Apache, nothing should change and it should find the same configuration files as it did before, since its directory in <code class="inlineCode">/etc</code> maps to <code class="inlineCode">/git/apache2</code>, which includes the same files it did before:</p>
<pre class="programlisting con"><code class="hljs-con">sudo systemctl reload apache2
</code></pre>
<p class="normal">If you see no errors, you should be all set. Otherwise, make sure you created the symbolic link properly.</p>
<p class="normal">Next, we get to the main attraction. We’ve copied Apache’s files into our repository, but we didn’t actually push those changes back to our Git server yet. To set that up, we’ll need to associate the files within our <code class="inlineCode">/git/apache2</code> directory into version control. The reason for this is the files simply being in the <code class="inlineCode">git</code> repository folder isn’t enough for Git to care about them. We have to tell Git to pay attention to individual files. We can add every file within our Git repository for Apache by entering the following command from within that directory:</p>
<pre class="programlisting con"><code class="hljs-con">git add .
</code></pre>
<p class="normal">This basically tells Git to add everything in the directory to version control. You can actually do the following to add an individual file:</p>
<pre class="programlisting con"><code class="hljs-con">git add &lt;filename&gt;
</code></pre>
<p class="normal">In this case, we want to add everything, so we used a period in place of a directory name to add the entire current directory.</p>
<p class="normal">If you run the <code class="inlineCode">git status</code> command from within your Git repository, you should see output indicating <a id="_idIndexMarker1340"/>that <a id="_idIndexMarker1341"/>Git has new files that haven’t been committed yet. A <strong class="keyWord">Git commit</strong> simply<a id="_idIndexMarker1342"/> finalizes the changes locally. Basically, it packages up your current changes to prepare them for being copied to the server. To create a commit of all the files we’ve added so far, <code class="inlineCode">cd</code> into your <code class="inlineCode">/git/apache2</code> directory and run the following to stage a new commit:</p>
<pre class="programlisting con"><code class="hljs-con">git commit -a -m "My first commit."
</code></pre>
<p class="normal">With this command, the <code class="inlineCode">-a</code> option tells Git that you want to include anything that’s changed in your repository. The <code class="inlineCode">-m</code> option allows you to attach a message to the commit, which is actually required. If you don’t use the <code class="inlineCode">-m</code> option, it will open your default text editor and allow you to add a comment from there.</p>
<p class="normal">Finally, we can <code class="inlineCode">push</code> our changes back to the Git server:</p>
<pre class="programlisting con"><code class="hljs-con">git push origin master
</code></pre>
<p class="normal">By default, the <code class="inlineCode">git</code> suite of commands utilizes OpenSSH, so our <code class="inlineCode">git push</code> command should create an SSH connection back to our server and push the files there. You won’t be able to inspect the contents of the Git directory on your Git server, because it won’t contain the same file structure as your original directory. Whenever you pull a Git repository though, the resulting directory structure will be just as you left it.</p>
<p class="normal">From this point forward, if you need to restore a repository onto another server, all you should need to do is perform a Git clone. To clone the repository into your current working directory, execute the following:</p>
<pre class="programlisting con"><code class="hljs-con">git clone 192.168.1.101:/git/apache2
</code></pre>
<p class="normal">Now, each time you make changes to your configuration files, you can perform a <code class="inlineCode">git commit</code> and then push the changes up to the server to keep the content safe:</p>
<pre class="programlisting con"><code class="hljs-con">git commit -a -m "Updated config files."git push origin master
</code></pre>
<p class="normal">Now we know how to create a repository, push changes to a server, and pull the changes back down. Finally, we’ll need to know how to revert changes should our configuration get changed<a id="_idIndexMarker1343"/> with <a id="_idIndexMarker1344"/>non-working files. First, we’ll need to locate a known working commit. My favorite method is using the <code class="inlineCode">tig</code> command. The <code class="inlineCode">tig</code> package must be installed for this to work, but it’s a great utility to have:</p>
<pre class="programlisting con"><code class="hljs-con">sudo apt install tig
</code></pre>
<p class="normal">The <code class="inlineCode">tig</code> command (which is just <code class="inlineCode">git</code> backward) gives us a semi-graphical interface to browse through our Git commits. To use it, simply execute the <code class="inlineCode">tig</code> command from within a Git repository. In the following example screenshot, I’ve executed <code class="inlineCode">tig</code> from within a Git repository on one of my servers:</p>
<figure class="mediaobject"><img alt="" height="194" src="../Images/B18425_23_01.png" width="881"/></figure>
<p class="packt_figref">Figure 23.1: An example of the tig command, looking at a repository for example scripts</p>
<p class="normal">While using <code class="inlineCode">tig</code>, you’ll see a list of Git commits, along with their dates and comments that were entered with each. To inspect one, press the <em class="keystroke">up</em> and <em class="keystroke">down</em> arrows to change your selection, then press <em class="keystroke">Enter</em> on the one you want to view. You’ll see a new window, which will show you the <code class="inlineCode">commit hash</code> (which is a long string of alphanumeric characters), as well as an overview of which lines were added or removed from the files within the commit. To revert one, you’ll first need to find the commit you want to revert to and get its commit hash. The <code class="inlineCode">tig</code> command is great for finding this information. In most cases, the commit you’ll want to revert to is the one <em class="italic">before</em> the change took place. In my example screenshot, I fixed the syntax issue on 9/26/2020. If I want to restore that file, I should revert to the commit below that. I can get the commit hash by highlighting that entry and pressing <em class="keystroke">Enter</em>. It’s at the top of the window. Then, I can exit <code class="inlineCode">tig</code> by pressing <em class="keystroke">q</em>, and then revert to that commit:</p>
<pre class="programlisting con"><code class="hljs-con">git checkout &lt;commit hash&gt;
</code></pre>
<p class="normal">And just like that, the entire directory tree for the repository instantly changes to exactly what it was <a id="_idIndexMarker1345"/>before<a id="_idIndexMarker1346"/> the bad commit took place. I can then restart or reload the daemon for this repository, and it will be back to normal. </p>
<p class="normal">At this point, you’d want to test the application to make sure that the issue is completely fixed. After some time has passed and you’re finished testing, you can make the change permanent. First, we switch back to the most recent commit:</p>
<pre class="programlisting con"><code class="hljs-con">git checkout main
</code></pre>
<p class="normal">Then, we permanently switch back to the commit that was found to be working properly:</p>
<pre class="programlisting con"><code class="hljs-con">git revert --no-commit &lt;commit hash&gt;
</code></pre>
<p class="normal">Then, we can commit our reverted Git repository and push it back to the server:</p>
<pre class="programlisting con"><code class="hljs-con">git commit -a -m "The previous commit broke the application. Reverting."git push origin master
</code></pre>
<p class="normal">As you can see, Git is a very useful ally to utilize when managing configuration files on your servers. This benefits disaster recovery, because if a bad change is made that breaks a daemon, you can easily revert the change. If the server were to fail, you can recreate your configuration almost instantly by just cloning the repository again. There’s certainly a lot more to Git than what we’ve gone over in this section, so feel free to pick up a book about it if you wish to take your knowledge to the next level. But in regard to managing your configuration with Git, all you’ll need to know is how to place files into version control, update them, and clone them to new servers. Some services you run on a server may not be a good candidate for Git, however. For example, managing an entire MariaDB database via Git would be a nightmare, since there is too much overhead with such a use case, and database entries would likely change too rapidly for Git to keep up. Use your best judgment. If you have some configuration files that are only manipulated every once in a while, they’ll be a perfect candidate for Git.</p>
<p class="normal">Backups are one of <a id="_idIndexMarker1347"/>those <a id="_idIndexMarker1348"/>things that some people don’t seem to take seriously until it’s too late. Data loss can be a catastrophic event for an organization, so it’s imperative that you implement a solid backup plan. In the next section, we’ll look at what that entails.</p>
<h1 class="heading-1" id="_idParaDest-305">Implementing a backup plan</h1>
<p class="normal">Creating a solid <a id="_idIndexMarker1349"/>backup plan is one of the most important things you’ll ever do as a server administrator. Even if you’re only using Ubuntu Server at home as a personal file server, backups are critical. During my career, I’ve seen disks fail many times. I often hear arguments about which hard disk manufacturer beats others in terms of longevity, but I’ve seen disk failures so often, I don’t trust any of them. All disks will fail eventually, it’s just a matter of when. And when they do fail, they’ll usually fail hard with no easy way to recover data from them. A sound approach to managing data is that any disk or server can fail, and it won’t matter, since you’ll be able to regenerate your data from other sources, such as a backup or secondary server.</p>
<p class="normal">There’s no one best backup solution, since it all depends on what kind of data you need to secure, and what software and hardware resources are available to you. For example, if you manage a database that’s critical to your company, you should back it up regularly. If you have another server available, set up a replication secondary server so that your primary database isn’t a single point of failure. Not everyone has an extra server lying around, so sometimes you have to work with what you have available.</p>
<p class="normal">This may mean that you’ll need to make some compromises, such as creating regular snapshots of your database server’s storage volume or regularly dumping a backup of your important databases to an external storage device.</p>
<p class="normal">The <code class="inlineCode">rsync</code> utility is one of the most valuable pieces of software around to server administrators. It allows us to do some really wonderful things. In some cases, it can save us quite a bit of money. For example, online backup solutions are wonderful in the sense that we can use them to store off-site copies of our important files. However, depending on the volume of data, they can be quite expensive. With <code class="inlineCode">rsync</code>, we can back up our data in much the same way, with not only our current files copied over to a backup target but also differentials. If we have another server to send the backup to, even better.</p>
<p class="normal">At one company I managed servers for, they didn’t want to subscribe to an online backup solution. To work around that, a server was set up as a backup point for <code class="inlineCode">rsync</code>. We set up <code class="inlineCode">rsync</code> to back up to the secondary server, which housed quite a lot of files. Once the initial backup was complete, the secondary server was sent to one of our other offices in another state. From that point forward, we only needed to run <code class="inlineCode">rsync</code> weekly, to back up everything that had been changed since the last backup. Sending files via <code class="inlineCode">rsync</code> to the other site over the internet was rather slow, but since the initial backup was already complete before we sent the server there, all we needed to back up each week was differentials. Not only is this an example of how awesome <code class="inlineCode">rsync</code> is and how we can configure it to do pretty much what paid solutions do but the experience was also a good example of <a id="_idIndexMarker1350"/>utilizing what you have available to you.</p>
<p class="normal">Since we’ve already gone over <code class="inlineCode">rsync</code> in <em class="chapterRef">Chapter 12</em>, <em class="italic">Sharing and Transferring Files</em>, I won’t repeat too much of that information here. But since we’re on the subject of backing up, the <code class="inlineCode">--backup-dir</code> option is worth mentioning again. This option allows you to copy files that would normally be replaced to another location. As an example, here’s the <code class="inlineCode">rsync</code> command I mentioned in <em class="chapterRef">Chapter 12</em>, <em class="italic">Sharing and Transferring Files</em>:</p>
<pre class="programlisting con"><code class="hljs-con">CURDATE=$(date +%m-%d-%Y)
export $CURDATE
sudo rsync -avb --delete --backup-dir=/backup/incremental/$CURDATE /src /target  
</code></pre>
<p class="normal">This command was part of the topic of creating an <code class="inlineCode">rsync</code> backup script. The first command simply captures today’s date and stores it in a variable named <code class="inlineCode">$CURDATE</code>. In the actual <code class="inlineCode">rsync</code> command, we refer to this variable. The <code class="inlineCode">-b</code> option (part of the <code class="inlineCode">-avb</code> option string) tells <code class="inlineCode">rsync</code> to make a copy of any file that would normally be replaced. If <code class="inlineCode">rsync</code> is going to replace a file on the target with a new version, it will move the original file to a new name before overwriting it. The <code class="inlineCode">--backup-dir</code> option tells <code class="inlineCode">rsync</code> that when it’s about to overwrite a file, to put it somewhere else instead of copying it to a new name. We give the <code class="inlineCode">--backup-dir</code> option a path, where we want the files that would normally be replaced to be copied. In this case, the backup directory includes the <code class="inlineCode">$CURDATE</code> variable, which will be different every day. For example, a backup run on 8/16/2022 would have a backup directory of the following path, if we used the command I gave as an example:</p>
<pre class="programlisting con"><code class="hljs-con">/backup/incremental/8-16-2022
</code></pre>
<p class="normal">This essentially allows you to keep differentials. Files on <code class="inlineCode">/src</code> will still be copied to <code class="inlineCode">/target</code>, but the directory you identify as <code class="inlineCode">--backup-dir</code> will contain the original files before they were replaced that day.</p>
<p class="normal">On my servers, I use the <code class="inlineCode">--backup-dir</code> option with <code class="inlineCode">rsync</code> quite often. I’ll typically set up an external backup drive, with the following three folders:</p>
<ul>
<li class="bulletList"><code class="inlineCode">current</code></li>
<li class="bulletList"><code class="inlineCode">archive</code></li>
<li class="bulletList"><code class="inlineCode">logs</code></li>
</ul>
<p class="normal">The <code class="inlineCode">current</code> directory <a id="_idIndexMarker1351"/>always contains a current snapshot of the files on my server. The <code class="inlineCode">archive</code> directory on my backup disks is where I point the <code class="inlineCode">--backup-dir</code> option. Within that directory will be folders named with the dates that the backups were taken. The <code class="inlineCode">logs</code> directory contains log files from the backup. Basically, I redirect the output of my <code class="inlineCode">rsync</code> command to a log file within that directory, each log file being named with the same <code class="inlineCode">$CURDATE</code> variable so I’ll also have a backup log for each day the backup runs. I can easily look at any of the logs for which files were modified during that backup, and then traverse the archive folder to find an original copy of a file. I’ve found this approach to work very well. Of course, this backup is performed with multiple backup disks that are rotated every week, with one always off-site. It’s always crucial to keep a backup off-site in case of a situation that could compromise your entire local site.</p>
<p class="normal">The <code class="inlineCode">rsync</code> utility is just one of many you can utilize to create your own backup scheme. The plan you come up with will largely depend on what kind of data you’re wanting to protect and what kind of downtime you’re willing to endure.</p>
<p class="normal">Ideally, we would have an entire warm site with servers that are carbon copies of our production servers, ready to be put into production should any issues arise, but that’s also very expensive, and whether you can implement such a routine will depend on your budget. However, Ubuntu has many great utilities available that you can use to come up with your own system that works. If nothing else, utilize the power of <code class="inlineCode">rsync</code> to back up to external disks and/or external sites.</p>
<p class="normal">A tool that is very valuable when working to recover physical servers is USB recovery media, such as<a id="_idIndexMarker1352"/> flash drives with a bootable ISO image written to them. In the next section, we’ll take a look.</p>
<h1 class="heading-1" id="_idParaDest-306">Utilizing bootable recovery media</h1>
<p class="normal">The concept <a id="_idIndexMarker1353"/>of <strong class="keyWord">live media</strong> is a <a id="_idIndexMarker1354"/>wonderful thing, as we can boot into a completely different working environment from the operating system installed on our device and perform tasks without disrupting installed software on the host system. The desktop version of Ubuntu, for example, offers a complete computing environment we can use in order to not only test hardware and troubleshoot our systems but also browse the web just as we would on an installed system. In terms of recovering from disasters, live media becomes a saving grace.</p>
<p class="normal">As administrators, we run into one problem after another. This gives us our job security. Computers often seemingly have a mind of their own, failing when least expected (as well as seemingly every holiday). Our servers and desktops can encounter a fault at any time, and live media allows us to separate hardware issues from software issues, by troubleshooting from a known good working environment.</p>
<p class="normal">One of my favorites when it comes to live media is the desktop version of Ubuntu. Although geared primarily toward end users who wish to install Ubuntu on a laptop or desktop, as administrators we can use it to boot a machine that normally wouldn’t, or even recover data from failed disks. For example, I’ve used the Ubuntu live media to recover data from both failed Windows and Linux systems, by booting the machine with the live media and utilizing a network connection to move data from the bad machine to a network share. Often, when a computer or server fails to boot, the data on its disk is still accessible. Assuming the disk wasn’t encrypted during installation, you should have no problem accessing data on a server or workstation using live media such as Ubuntu live media.</p>
<p class="normal">Sometimes, certain levels of failure require us to use different tools. While Ubuntu’s live media is great, it doesn’t work for absolutely everything. One situation is a failing disk. Often, you’ll be able to recover data using Ubuntu’s live media from a failing disk, but if it’s too far gone, then the Ubuntu media will have difficulty accessing data from it as well. Tools such as Clonezilla specialize in working with hard disks and may be a better choice.</p>
<p class="normal">Live media can totally save the day. The Ubuntu live image in particular is a great boot disk to have available to you, as it gives you a very extensive environment you can use to troubleshoot systems and recover data.</p>
<p class="normal">One of the best aspects of using the Ubuntu live image is that you won’t have to deal with the underlying operating system and software set at all, you can bypass both by booting into a known working desktop, and then copy any important files from the drive right onto a network share. Another important feature of Ubuntu live media is the memory test option. Quite often, strange failures on a computer can be traced to defective memory. Other than simply letting you install Ubuntu, the live media is a Swiss Army knife of many tools you can use to recover a system from disaster. If nothing else, you can use live media to pinpoint whether a problem is software- or hardware-related. If a problem can only be reproduced in the installed environment but not in a live session, chances are a configuration problem is to blame. If a system also misbehaves in a live environment, it<a id="_idIndexMarker1355"/> may help you identify a hardware issue. Either way, every good administrator should have live media available to troubleshoot systems and recover data when the need arises.</p>
<h1 class="heading-1" id="_idParaDest-307">Summary</h1>
<p class="normal">In this chapter, we looked at several ways in which we can prevent and recover from disasters. Having a sound prevention and recovery plan in place is an important key to managing servers efficiently. We need to ensure we have backups of our most important data ready for whenever servers fail, and we should also keep backups of our most important configurations. Ideally, we’ll always have a warm site set up with preconfigured servers ready to go in a situation where our primary servers fail, but one of the benefits of open source software is that we have a plethora of tools available to us that we can use to create a sound recovery plan. In this chapter, we looked at leveraging <code class="inlineCode">rsync</code> as a useful utility for creating differential backups, and we also looked into setting up a Git server we can use for configuration management, which is also a crucial aspect of any sound prevention plan. We also talked about the importance of live media in diagnosing issues.</p>
<p class="normal">And with this chapter, this book comes to a close. Writing this book has been an extremely joyful experience. I was thrilled to write the first edition when Ubuntu 16.04 was in development, it was a fun project to write the second edition and update it to cover Ubuntu 18.04, the third edition for 20.04, and I’m even more thrilled to have had the opportunity to update this body of work yet again for 22.04 in this latest edition. I’d like to thank each and every one of you, my readers, for taking the time to read this book. In addition, I would like to thank all of the viewers of my YouTube channel, <em class="italic">Learn Linux TV</em> (<a href="https://www.learnlinux.tv"><span class="url">https://www.learnlinux.tv</span></a>), because I probably wouldn’t have had the opportunity to write this in the first place had it not been for my viewers helping make my channel so popular.</p>
<p class="normal">I’d also like to thank Packt Publishing for giving me the opportunity to write a book about one of my favorite technologies. Writing this book was definitely an honor. When I first started with Linux in 2002, I never thought I’d actually be an author, teaching the next generation of Linux administrators the tricks of the trade. I wish each of you the best of luck, and I hope this book is beneficial to you and your career.</p>
<p class="normal">For additional content, be sure to check out <a href="https://learnlinux.tv"><span class="url">https://learnlinux.tv</span></a> for even more content. I have quite a few training videos available there, free of charge.</p>
<h1 class="heading-1" id="_idParaDest-308">Further reading</h1>
<ul>
<li class="bulletList"><em class="italic">Pro Git</em> by Scott Chacon and Ben Straub: <a href="https://learnlinux.link/git-book"><span class="url">https://learnlinux.link/git-book</span></a></li>
<li class="bulletList"><em class="italic">Introduction to RAID terminology and concepts</em>: <a href="https://learnlinux.link/raid-concepts"><span class="url">https://learnlinux.link/raid-concepts</span></a></li>
</ul>
<h1 class="heading-1">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the author and other readers: </p>
<p class="normal"><a href="https://packt.link/LWaZ0"><span class="url">https://packt.link/LWaZ0</span></a></p>
<p class="normal"><img alt="" height="177" src="../Images/QR_Code50046724-1955875156.png" width="177"/></p>
</div>
</div></body></html>