<html><head></head><body>
		<div id="_idContainer025">
			<h1 id="_idParaDest-17" class="chapter-number"><a id="_idTextAnchor016"/>1</h1>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Block Storage – Learning How to Provision Block Storage on Red Hat Enterprise Linux</h1>
			<p>Block storage within <strong class="bold">Red Hat Enterprise Linux</strong> (<strong class="bold">RHEL</strong>) makes up the foundation of many core <a id="_idIndexMarker000"/>applications. You will use it for many things within the world of Linux, from application development to backups to deployments <a id="_idIndexMarker001"/>of infrastructure such as OpenStack using <strong class="bold">Internet Small Computer Systems Interface</strong> (<strong class="bold">iSCSI</strong>). Through understanding how and when to use block storage over other storage options and how to provision it through manual steps as well as automate it through Ansible, you will be able to comprehend and grasp the knowledge needed for your day-to-day work with Linux as well as ensuring you understand the building blocks required to meet the needs of the <em class="italic">EX358</em> exam. These lessons not only allow you to complete the <em class="italic">EX358</em> exam with success but also enable you to better understand why we use block storage over other filesystems in situations that dictate the use of this filesystem type in <span class="No-Break">real-world scenarios.</span></p>
			<p>This comes in handy when you are building your infrastructure at your company, in your home lab for learning purposes, or for that start-up you always wanted to create. At the end of this chapter, you will be able to provision block storage using Red Hat best practices both manually and through Ansible automation in order to meet the requirements of Red Hat. This will allow you to gain support from Red Hat if you have an active contract and also gain help from the community if you do not have Red Hat support in order to resolve any issues you may run into during your usage of <span class="No-Break">this technology.</span></p>
			<p>You will be able to configure iSCSI initiators, boot with them both manually and through Ansible automation, and safely tear down unused variations of the iSCSI block storage after you are done with this chapter. This will, in turn, ensure your full understanding of the overall life cycle and the effective nature of block storage in <span class="No-Break">your ecosystem.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>iSCSI block storage—overview of what it is and why we <span class="No-Break">need it</span></li>
				<li>iSCSI block storage—manual provisioning <span class="No-Break">and deployment</span></li>
				<li>iSCSI block storage—Ansible automation playbook creation <span class="No-Break">and usage</span></li>
			</ul>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>Technical requirements</h1>
			<p>Before we delve into the topics in detail, you will need to set a few things up. Let’s look at what <span class="No-Break">they are.</span></p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>Setting up GitHub access</h2>
			<p>You will need a free GitHub account in order to access some of the code that will be provided throughout <a id="_idIndexMarker002"/>this book. Please sign up for a free account at <a href="https://github.com/">https://github.com/</a>. We will be utilizing the code found in the following repository throughout the course of this book: <a href="https://github.com/PacktPublishing/Red-Hat-Certified-Specialist-in-Services-Management-and-Automation-EX358-">https://github.com/PacktPublishing/Red-Hat-Certified-Specialist-in-Services-Management-and-Automation-EX358-</a>. We will be utilizing the code snippets found in the <strong class="source-inline">ch1</strong> folder of this code repository (aka repo) for our iSCSI automation hands-on exercises, which can be found here: <a href="https://github.com/PacktPublishing/Red-Hat-Certified-Specialist-in-Services-Management-and-Automation-EX358-Exam-Guide/tree/main/Chapter01">https://github.com/PacktPublishing/Red-Hat-Certified-Specialist-in-Services-Management-and-Automation-EX358-Exam-Guide/tree/main/Chapter01</a>. The code placed here will allow you to check your work and ensure you are on the right track when writing your playbooks within Ansible. Please keep in mind these are one person’s way of writing tested playbooks that will meet the exam objectives; however, there are many ways of writing successful playbooks to meet <span class="No-Break">these objectives.</span></p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Setting up your lab environment</h2>
			<p>All of the <a id="_idIndexMarker003"/>demonstrations of VirtualBox and coding will be shown on macOS but can be performed on Windows as well as Linux OSs. We will be setting up some iSCSI block devices. First, you will need a machine that can run VirtualBox with enough memory to run your machine and three VMs that each have 2 GB of memory, one 10 GB hard drive, and one 5 GB hard drive, which equals 15 GB of required hard drive space per VM, as can be seen in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer006" class="IMG---Figure">
					<img src="image/Figure_1.01_B18607.jpg" alt="Figure 1.1 – Layout of the VirtualBox deployment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Layout of the VirtualBox deployment</p>
			<p>This is mainly <a id="_idIndexMarker004"/>for the storage hands-on labs, and you can revert to one 10 GB hard drive for exercises. RHEL 8.1 requires at least 9.37 GB of space to run. Using a Red Hat Developer account (<a href="https://developers.redhat.com/">https://developers.redhat.com/</a>), you can access real Red Hat software to develop your skills as well as the software in order to set <span class="No-Break">this up:</span></p>
			<div>
				<div id="_idContainer007" class="IMG---Figure">
					<img src="image/Figure_1.02_B18607.jpg" alt="Figure 1.2 – Signup is simple!"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Signup is simple!</p>
			<p>Because the <a id="_idIndexMarker005"/>exam is set for RHEL 8.1, I recommend using this version for your studying needs in order to get the most authentic exam-like infrastructure possible. In the following screenshot, the correct version you should download is the <span class="No-Break">first option:</span></p>
			<div>
				<div id="_idContainer008" class="IMG---Figure">
					<img src="image/Figure_1.03_B18607.jpg" alt="Figure 1.3 – The correct version for the exam and for you"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – The correct version for the exam and for you</p>
			<p>This will be <a id="_idIndexMarker006"/>true for the entirety of the book, including the comprehensive review and lab at the end. Before installing the OS, you can create a second hard drive in VirtualBox from the settings, as can be seen in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="image/Figure_1.04_B18607.jpg" alt="Figure 1.4 – Creating a second hard drive for your VM"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – Creating a second hard drive for your VM</p>
			<p>You also need to ensure that you choose <strong class="bold">Bridged Adapter</strong> mode for your network <strong class="bold">Attached to</strong> option. The <strong class="bold">Promiscuous Mode</strong> option is also allowed so that it can reach the internet and other adapters. One caveat to keep in mind is that bridged-over Wi-Fi does <a id="_idIndexMarker007"/>not always play nice, so try to ensure you have a wired connection if you are setting up your lab in <span class="No-Break">this manner:</span></p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/Figure_1.05_B18607.jpg" alt="Figure 1.5 – Bridged adapter with Promiscuous Mode option"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – Bridged adapter with Promiscuous Mode option</p>
			<p>From here, you can then mount the downloaded ISO and kick off <span class="No-Break">the installation:</span></p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/Figure_1.06_B18607.jpg" alt="Figure 1.6 – Mounting RHEL DVD ISO that was downloaded previously"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – Mounting RHEL DVD ISO that was downloaded previously</p>
			<p>There are <a id="_idIndexMarker008"/>some best practices you need to keep in mind. We will be installing the <strong class="bold">Server with the GUI</strong> option. Make sure to create yourself an administrator account as well as keeping your root account as you will want to do everything as sudo and not directly as root for security purposes and all-around good habits. The user creation screen, as follows, allows you to set up your root password and any users you would like <span class="No-Break">to create:</span></p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/Figure_1.07_B18607.jpg" alt="Figure 1.7 – Administrator accounts are best practices; sudo over root is always preferred"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – Administrator accounts are best practices; sudo over root is always preferred</p>
			<p>Next, you will <a id="_idIndexMarker009"/>need to use the login for your Red Hat Developer account and license the VMs using the account credentials. See the following screenshot for how to correctly apply a Red Hat <span class="No-Break">subscription license:</span></p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/Figure_1.08_B18607.jpg" alt="Figure 1.8 – Red Hat Developer credentials or an active Red Hat account needed"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – Red Hat Developer credentials or an active Red Hat account needed</p>
			<p>You can create <a id="_idIndexMarker010"/>one machine and then clone it into the other two you need. Make sure you choose to generate new MAC addresses and to make a full clone to ensure that no overlap causes network or storage issues, as shown in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/Figure_1.09_B18607.jpg" alt="Figure 1.9 – Full clones with new MAC generation and a new name for the VM"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.9 – Full clones with new MAC generation and a new name for the VM</p>
			<p>Next, we will <a id="_idIndexMarker011"/>set up the host file with the domain of <strong class="source-inline">example.com</strong> in order to route correctly to and from the different systems. You will need to do the following on the servers in a command line to get your IP addresses and then go to each device and set up the host file with the <span class="No-Break">same information:</span></p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/Figure_1.10_B18607.jpg" alt="Figure 1.10 – Hostname and IP of rhel1.example.com system"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.10 – Hostname and IP of rhel1.example.com system</p>
			<p>Next, let’s gather <a id="_idIndexMarker012"/>the hostnames or change them to what you would like them to be using the following commands and review the output in this case, which <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">rhel1.example.com</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
[emcleroy@rhel1 ~]$ sudo hostnamectl set-hostname rhel1.example.com
[emcleroy@rhel1 ~]$ hostname
 rhel1.example.com</pre>
			<p>Use the hostnames and the IP addresses to build the inventory for the host file. After you do this, make sure that you shut down the system for it to save the changes permanently. Next, you’re going to want to add these as noted to the host file on all three VMs using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ sudo vi /etc/hosts</pre>
			<p>Here is <a id="_idIndexMarker013"/>an example of the completed <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">etc/hosts</strong></span><span class="No-Break"> file:</span></p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/Figure_1.11_B18607.jpg" alt="Figure 1.11 – Finished /etc/hosts file"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.11 – Finished /etc/hosts file</p>
			<p>Keep in mind your <strong class="source-inline">/etc/hosts</strong> file will look different based on your IPs. You should now be able to ping via the hostname and IP of all of the different VMs from one <span class="No-Break">to another:</span></p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/Figure_1.12_B18607.jpg" alt="Figure 1.12 – Example of working networking environment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.12 – Example of working networking environment</p>
			<p>Next, for ease <a id="_idIndexMarker014"/>of use, let’s set up passwordless sudo for our user account, which in my case would <span class="No-Break">be </span><span class="No-Break"><strong class="source-inline">emcleroy</strong></span><span class="No-Break">.</span></p>
			<p>We will start by running the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ sudo visudo</pre>
			<p>Next, we will locate the lines of code highlighted in the following screenshot and add the highlighted lines of text. Also, note that if you are allowing administrators, you can simply uncomment <strong class="source-inline">#</strong> in front of the <strong class="source-inline">%wheel</strong> line <span class="No-Break">as well:</span></p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/Figure_1.13_B18607.jpg" alt="Figure 1.13 – Highlighted lines of text to be added, substituting your username for mine"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.13 – Highlighted lines of text to be added, substituting your username for mine</p>
			<p>You will need <a id="_idIndexMarker015"/>to do this for all three of <span class="No-Break">the servers.</span></p>
			<p>Finally, we will add SSH keys across the servers to allow for fast connectivity so that we do not have to type passwords every time we need to log in from one server to another. Start by generating SSH keys with the following command on your <span class="No-Break"><strong class="source-inline">rhel1</strong></span><span class="No-Break"> VM:</span></p>
			<pre class="console">
$ ssh-keygen</pre>
			<p>Just leave the defaults and keep hitting <em class="italic">Enter</em>, and then once that is generated, you will want to do <span class="No-Break">the following:</span></p>
			<pre class="console">
$ ssh-copy-id -i ~/.ssh/id_rsa.pub username@server</pre>
			<p>This will push the keys to the servers and allow all the servers to talk bi-directionally. You will want to do that for all three servers, so you will do the following (including for the server you are currently on to ensure that the keys are pushed to the known host file for all of <span class="No-Break">the servers):</span></p>
			<pre class="console">
$ ssh-copy-id -i ~/.ssh/id_rsa.pub emcleroy@rhel1
$ ssh-copy-id -i ~/.ssh/id_rsa.pub emcleroy@rhel2
$ ssh-copy-id -i ~/.ssh/id_rsa.pub emcleroy@rhel3</pre>
			<p>From here, you have full access to a three-VM lab running RHEL 8.1 with secondary HDDs for use <a id="_idIndexMarker016"/>with this iSCSI hands-on exercise. The only minor differences will come up in the networking hands-on labs where we will go over adding <a id="_idIndexMarker017"/>additional <strong class="bold">network interface controllers</strong> (<strong class="bold">NICs</strong>) for network teaming. This will be another topic that you need to understand in order to ace the <span class="No-Break"><em class="italic">EX358</em></span><span class="No-Break"> exam.</span></p>
			<p>Congratulations! You have now successfully set up your lab environment. Pat yourself on the back and take a break. We will now be talking about the manual steps to build out iSCSI block devices and use them. This will be followed by putting that lab environment you just built to the test and getting hands-on experience with <span class="No-Break">the technology.</span></p>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/>iSCSI block storage – overview of what it is and why we need it</h1>
			<p>There are a <a id="_idIndexMarker018"/>number of things you need to know about <a id="_idIndexMarker019"/>block storage and, in this case, iSCSI. It is a <strong class="bold">storage area network</strong> (<strong class="bold">SAN</strong>) protocol that allows for devices or parts of devices to be seen as block storage by an end device. SAN is how iSCSI connects to the network and gives the ability <a id="_idIndexMarker020"/>to provide network <strong class="bold">logical unit numbers</strong> (<strong class="bold">LUNs</strong>). This allows systems to use these block devices as if they were physical hard drives in the system that they can boot from, save files to, or use like any hard drive that you have in your normal computer. With this in mind, we have to take a few things <span class="No-Break">into account.</span></p>
			<p>First, you have to ensure that your network can handle the connectivity without congestion as this will cause your systems to slow down and possibly lag behind what you are doing, causing users to become frustrated. Knowing this, you have to plan out your SAN extremely well and properly network out your block storage onto a normally non-encrypted network setup that meets the minimum speeds of 10 GB but can go much higher in a lot of cases. This allows smooth usage of your storage without the headaches you will run into as a system administrator. If you were to put this on the same network as your LAN traffic and expect your streaming (don’t do this while at work!) users are watching videos while trying to also do their jobs from a machine that is hosted from a SAN iSCSI block storage device. Other things to keep in mind are you need to ensure proper <strong class="source-inline">firewalld</strong> syntax is utilized and SELinux protocols are followed to allow connectivity at startup or you will have a giant paperweight without <span class="No-Break">much happening.</span></p>
			<p>There are <a id="_idIndexMarker021"/>some main items you have to take into account when you are looking at iSCSI using <strong class="source-inline">targetcli</strong>, and I will get to more details about <strong class="source-inline">targetcli</strong> as that is the toolset we will utilize to allow us to use iSCSI in our RHEL 8.1 environment. The main things you need to know are the <strong class="bold">initiator</strong>, <strong class="bold">target</strong>, <strong class="bold">Portal</strong>, <strong class="bold">LUN</strong>, <strong class="bold">Access Control List</strong> (<strong class="bold">ACL</strong>), and <strong class="bold">Target Portal Group</strong> (<strong class="bold">TPG</strong>). These items make up iSCSI storage and lead to a lot of misconceptions. Let’s test your knowledge before we dig deeper into the systems and how they work together to provide block storage over the network to <span class="No-Break">remote servers.</span></p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Testing your knowledge</h2>
			<p>Answer <a id="_idIndexMarker022"/>the <span class="No-Break">following questions:</span></p>
			<ol>
				<li>What is an iSCSI storage source on an <span class="No-Break">iSCSI server?</span><ol><li><span class="No-Break">Target</span></li><li><span class="No-Break">LUN</span></li><li><strong class="bold">iSCSI Qualified </strong><span class="No-Break"><strong class="bold">Name</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">IQN</strong></span><span class="No-Break">)</span></li><li><span class="No-Break">ACL</span></li></ol></li>
				<li>What is a unique worldwide name used to identify both initiators <span class="No-Break">and targets?</span><ol><li><span class="No-Break">Target</span></li><li><span class="No-Break">LUN</span></li><li><span class="No-Break">IQN</span></li><li><span class="No-Break">ACL</span></li></ol></li>
				<li>An iSCSI client that is typically software-based is known <span class="No-Break">as a:</span><ol><li><span class="No-Break">TPG</span></li><li><span class="No-Break">Portal</span></li><li><span class="No-Break">IQN</span></li><li><span class="No-Break">Initiator</span></li></ol></li>
				<li>Which of the following is an access restriction using <span class="No-Break">the IQN?</span><ol><li><span class="No-Break">Target</span></li><li><span class="No-Break">LUN</span></li><li><span class="No-Break">IQN</span></li><li><span class="No-Break">ACL</span></li></ol></li>
				<li>What is the <a id="_idIndexMarker023"/>most commonly used software for setting up RHEL 8.1 iSCSI <span class="No-Break">block storage?</span><ol><li><span class="No-Break"><strong class="source-inline">firewalld</strong></span></li><li><span class="No-Break">SELinux</span></li><li><span class="No-Break"><strong class="source-inline">targetcli</strong></span></li><li><span class="No-Break"><strong class="source-inline">networkd</strong></span></li></ol></li>
				<li>Which service or port do you need to allow for iSCSI to work through <span class="No-Break">your firewall?</span><ol><li><span class="No-Break"><strong class="source-inline">iSCSI-target</strong></span></li><li><span class="No-Break"><strong class="source-inline">3260/UDP</strong></span></li><li><span class="No-Break"><strong class="source-inline">iSCSI</strong></span></li><li><span class="No-Break"><strong class="source-inline">targetcli</strong></span></li></ol></li>
				<li>What includes the named <span class="No-Break">item </span><span class="No-Break"><strong class="source-inline">2020-06.com.mcleroy.www</strong></span><span class="No-Break">?</span><ol><li><span class="No-Break">Target</span></li><li><span class="No-Break">LUN</span></li><li><span class="No-Break">IQN</span></li><li><span class="No-Break">ACL</span></li></ol></li>
				<li>Which system <a id="_idIndexMarker024"/>do you need to enable to ensure iSCSI will start <span class="No-Break">at boot?</span><ol><li><span class="No-Break"><strong class="source-inline">firewalld</strong></span></li><li><span class="No-Break">Target</span></li><li><span class="No-Break"><strong class="source-inline">targetcli</strong></span></li><li><span class="No-Break"><strong class="source-inline">networkd</strong></span></li></ol></li>
			</ol>
			<p><span class="No-Break">Answers:</span></p>
			<ol>
				<li value="1"><span class="No-Break">A. Target</span></li>
				<li><span class="No-Break">C. IQN</span></li>
				<li><span class="No-Break">D. Initiator</span></li>
				<li><span class="No-Break">D. ACL</span></li>
				<li><span class="No-Break">C. </span><span class="No-Break"><strong class="source-inline">targetcli</strong></span></li>
				<li><span class="No-Break">A.</span><span class="No-Break"><strong class="source-inline">iSCSI-target</strong></span></li>
				<li><span class="No-Break">C. IQN</span></li>
				<li><span class="No-Break">B. Target</span></li>
			</ol>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor023"/>iSCSI block storage – manual provisioning and deployment</h1>
			<p>We will <a id="_idIndexMarker025"/>start by installing <strong class="source-inline">targetcli</strong> and using that to set up iSCSI to provide block-based storage to other systems for file usage, boot systems, and so on. This will showcase the wide range of uses that come with iSCSI <a id="_idIndexMarker026"/>block storage implemented with RHEL 8.1. We will then show how to decommission the storage device and clean up the systems after utilizing <span class="No-Break">the resources.</span></p>
			<p>First, we will inst<a id="_idTextAnchor024"/>all <strong class="source-inline">targetcli</strong> in order to utilize the iSCSI systems <span class="No-Break">on </span><span class="No-Break"><strong class="source-inline">rhel1</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ sudo dnf install targetcli -y</pre>
			<p>We will follow that by enabling the system to start up the iSCSI block storage. When the system boots or has an issue that causes the target system to need to restart, it will reload the service in order to keep the storage up <span class="No-Break">and operational:</span></p>
			<pre class="console">
$ sudo systemctl enable target</pre>
			<p>After that, we will allow <strong class="source-inline">iscsi-initiator</strong> through <strong class="source-inline">firewalld</strong> in order to ensure that the other servers are able to access the block storage without issue. We will also reload the firewall, or the opening you just made will not <span class="No-Break">be there:</span></p>
			<pre class="console">
$ sudo firewall-cmd --permanent --add-service=iscsi-target
$ sudo firewall-cmd –reload</pre>
			<p>After that, we will be utilizing the new service we just installed—<strong class="source-inline">targetcli</strong>—to create network block storage in order to share it <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">rhel2.example.com</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/Figure_1.14_B18607.jpg" alt="Figure 1.14 – targetcli initiated for the first time"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.14 – targetcli initiated for the first time</p>
			<p>We will <a id="_idIndexMarker027"/>now create backstores for the physical disk partitioning. We will be creating backstores of the type <strong class="source-inline">block</strong> in order to meet the <a id="_idIndexMarker028"/>needs of the iSCSI system today. This will allow the persistent filesystems and us to set up how we would like the LUNs to be in terms of size <span class="No-Break">and security:</span></p>
			<pre class="console">
/&gt; cd /backstores/block
/backstores/block&gt; create blockstorage1 /dev/sdb
Created block storage object blockstorage1 using /dev/sdb.</pre>
			<p>Next, we will create an IQN in the <strong class="source-inline">/iscsi</strong> directory in order to provide a target and destination for the block <span class="No-Break">storage device:</span></p>
			<pre class="console">
/backstores/block&gt; cd /iscsi
/iscsi&gt; create iqn.2022-05.com.example:rhel1
Created target iqn.2022-05.com.example:rhel1.
Created TPG 1.
Global pref auto_add_default_portal=true
Created default portal listening on all IPs (0.0.0.0), port 3260.
/iscsi&gt; ls
o- iscsi ..................................... [Targets: 1]
  o- iqn.2022-05.com.example:rhel1 .............. [TPGs: 1]
    o- tpg1 ........................ [no-gen-acls, no-auth]
      o- acls ................................... [ACLs: 0]
      o- luns ................................... [LUNs: 0]
      o- portals ............................. [Portals: 1]
        o- 0.0.0.0:3260 .............................. [OK]</pre>
			<p>As you can <a id="_idIndexMarker029"/>see in the preceding code snippet, a default portal was created on port <strong class="source-inline">3260</strong> for connectivity to the block storage backstores <a id="_idIndexMarker030"/>using the <strong class="source-inline">create</strong> command for the IQN. Next, we will create a LUN for physically supporting the storage needs of the iSCSI <span class="No-Break">block storage:</span></p>
			<pre class="console">
/iscsi&gt; cd /iscsi/iqn.2022-05.com.example:rhel1/tpg1/luns
/iscsi/iqn.20…sk1/tpg1/luns&gt; create  /backstores/block/blockstorage1
Created LUN 0.</pre>
			<p>The next thing we need for iSCSI is an ACL to allow traffic to reach our storage devices successfully. We will need to exit out of <strong class="source-inline">targetcli</strong> temporarily to view the Red Hat name for the initiator’s IQN. It can be found <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">/etc/iscsi/initiatorname.iscsi</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
Global pref auto_save_on_exit=true
Configuration saved to /etc/
[emcleroy@rhel1 ~]$ vi /etc/iscsi/initiatorname.iscsi</pre>
			<p>Here is <a id="_idIndexMarker031"/>an example of the initiator name that is <a id="_idIndexMarker032"/>currently being used on the <span class="No-Break">next image:</span></p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/Figure_1.15_B18607.jpg" alt="Figure 1.15 – initiatorname.iscsi"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.15 – initiatorname.iscsi</p>
			<p>We will go back into <strong class="source-inline">targetcli</strong> and finish up the system preparations, setting up the system to use an ACL of our choosing for the system that will be utilizing the <span class="No-Break">block storage:</span></p>
			<pre class="console">
[emcleroy@rhel1 ~]$ sudo targetcli
targetcli shell version 2.1.53
Copyright 2011-2013 by Datera, Inc and others.
For help on commands, type 'help'.
&gt; cd /iscsi/iqn.2022-05.com.example:rhel1/tpg1/acls
/iscsi/iqn.20...sk1/tpg1/acls&gt; create iqn.2022-05.com.example:rhel2
Created Node ACL for iqn.2022-05.com.example:rhel2
Created mapped LUN 0.</pre>
			<p>Next, we will <a id="_idIndexMarker033"/>remove the default portal and create <a id="_idIndexMarker034"/>one on the specific IP address of <span class="No-Break">our server:</span></p>
			<pre class="source-code">
&gt; cd /iscsi/iqn.2022-05.com.example:rhel1/tpg1/portals
/iscsi/iqn.20.../tpg1/portals&gt; delete 0.0.0.0 3260
Deleted network portal 0.0.0.0:3260
/iscsi/iqn.20.../tpg1/portals&gt; create 192.168.1.198 3260
Using default IP port 3260
Created network portal 192.168.1.198:3260.</pre>
			<p>Finally, the following is your completed block <span class="No-Break">storage target:</span></p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/Figure_1.16_B18607.jpg" alt="Figure 1.16 – iSCSI block storage target"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.16 – iSCSI block storage target</p>
			<p>We have <a id="_idIndexMarker035"/>just shown how to provision iSCSI block <a id="_idIndexMarker036"/>storage for consumption. Now, we will showcase how to utilize that block storage for actual usage in your systems. We will connect from <strong class="source-inline">rhel1.example.com</strong> to <strong class="source-inline">rhel2.example.com</strong> and mount it, provision it, and utilize it to move and store files, as one of the examples of how we can use these systems is to increase the storage capacity of remote servers without needing to increase space, power, or cooling directly for the rack the server is <span class="No-Break">housed within.</span></p>
			<p>The first thing we will need to do is install the iSCSI utilities, as on the exam you may not have the installation of the <strong class="bold">Server with </strong><span class="No-Break"><strong class="bold">the GUI</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ sudo dnf install iscsi-initiator-utils targetcli -y</pre>
			<p>This allows us to ingest the iSCSI block storage that we created previously. Next, we are going to look up the configured target on <strong class="source-inline">rhel1</strong> (<strong class="source-inline">192.168.1.198</strong>) (please note: this might be a different IP for you in your lab) and log in to it to ensure connectivity. From here, we need to set the login information on the <strong class="source-inline">/etc/iscsi/iscsid.conf</strong> file in order to pass the correct login information so that we can log in to the <span class="No-Break">storage device:</span></p>
			<pre class="source-code">
$ sudo getent hosts rhel1</pre>
			<p>Now, we will set the <strong class="source-inline">InitiatorName</strong> field so that we can pass a known entry to the connecting server using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
[emcleroy@rhel1 ~]$ sudo vi /etc/iscsi/initiatorname.iscsi
InitiatorName=iqn.2022-05.com.example:rhel1
[emcleroy@rhel1 ~]$ sudo systemctl restart iscsid.service</pre>
			<p>Please note <a id="_idIndexMarker037"/>you can use the manual page to <a id="_idIndexMarker038"/>gain further insight into the <strong class="source-inline">iscsiadm</strong> command set with the <strong class="source-inline">man iscsiadm</strong> command. On <strong class="source-inline">rhel2</strong>, we will do a discovery of available block devices using the <strong class="source-inline">iscsiadm</strong> command. The <strong class="source-inline">–m</strong> flag specifies the mode—in this case, <strong class="source-inline">discovery</strong>. The <strong class="source-inline">–t</strong> flag specifies the type of target—in our case, <strong class="source-inline">st</strong>, which is <strong class="source-inline">sendtargets</strong>, which tells the server to send a list of iSCSI targets. The <strong class="source-inline">–p</strong> flag specifies which portal to use, which is a combination of IP address and port. If no port is passed, it will default <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">3260</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
[emcleroy@rhel2 ~]$ sudo iscsiadm -m discovery -t st -p 192.168.1.198:3260</pre>
			<p>Please note the output from the preceding command will be <span class="No-Break">as follows:</span></p>
			<pre class="console">
 192.168.1.198:3260,1 iqn.2022-05.com.example:rhel1</pre>
			<p>As you can see here, we have a block device that is showing <span class="No-Break">as available.</span></p>
			<p>We will try to log in to the device, and you can see we have logged in and it is showing the device connected, <span class="No-Break">as follows:</span></p>
			<pre class="console">
[emcleroy@rhel2 ~]$ sudo iscsiadm -m node -T iqn.2022-05.com.example:rhel1  -p 192.168.1.198 -l</pre>
			<p>In the preceding code, we are using the <strong class="source-inline">–m</strong> flag to choose <strong class="source-inline">node</strong> mode. We are using the <strong class="source-inline">–T</strong> flag to specify the target name. We are again using the <strong class="source-inline">–p</strong> flag for the portal, which defaults to port <strong class="source-inline">3260</strong>. Finally, we are using the <strong class="source-inline">–l</strong> flag to tell <strong class="source-inline">iscsiadm</strong> to log in to <span class="No-Break">the target.</span></p>
			<p>Next, we are going to use the <strong class="source-inline">–m</strong> mode flag to check the session and <strong class="source-inline">–P</strong> to print the information level <span class="No-Break">of 3:</span></p>
			<pre class="console">
[emcleroy@rhel2 ~]$ sudo iscsiadm -m session -P 3
iSCSI Transport Class version 2.0-870
version 6.2.1.4-1
Target: iqn.2022-05.com.example:rhel1 (non-flash)
     Current Portal: 192.168.1.198:3260,1
     Persistent Portal: 192.168.1.198:3260,1</pre>
			<p>You can <a id="_idIndexMarker039"/>see that we have <strong class="source-inline">sdb</strong>, which is the second <a id="_idIndexMarker040"/>drive on <strong class="source-inline">rhel2</strong>, and now we have <strong class="source-inline">sdc</strong> <span class="No-Break">as well:</span></p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/Figure_1.17_B18607.jpg" alt="Figure 1.17 – sdc drive is now showing up"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.17 – sdc drive is now showing up</p>
			<p>Next, we are going to partition the drive and format it with <strong class="source-inline">xfs</strong>. This will allow us to mount the system on boot as well as to save persistent files. This can be used for many things from <a id="_idIndexMarker041"/>file storage to OS or databases. First, we <a id="_idIndexMarker042"/>are going to format the drive <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">xfs</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
[emcleroy@rhel2 ~]$ sudo mkfs.xfs /dev/sdc
meta-data=/dev/sdc               isize=512    agcount=4, agsize=327680 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=1310720, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0</pre>
			<p>Then, we are going to use the following command to get the UUID to use in <strong class="source-inline">fstab</strong> to make it a persistent mount that will automatically mount <span class="No-Break">at startup:</span></p>
			<pre class="console">
[emcleroy@rhel2 ~]$ lsblk -f /dev/sdc
NAME FSTYPE LABEL UUID                                 MOUNTPOINT
sdc  xfs          38505868-00de-4269-88d8-3357a22f2101
[emcleroy@rhel2 ~]$ sudo vi /etc/fstab</pre>
			<p>Here, we can see an example of the added value highlighted <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">fstab</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/Figure_1.18_B18607.jpg" alt="Figure 1.18 – Updated fstab after adding the iSCSI block storage device"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.18 – Updated fstab after adding the iSCSI block storage device</p>
			<p>Here are the <a id="_idIndexMarker043"/>lines where we added the new iSCSI <a id="_idIndexMarker044"/>drive to <strong class="source-inline">fstab</strong>. Please note that for network devices, we pass the <strong class="source-inline">_netdev</strong> option. Next, we are going to mount the system in order to use it for moving <span class="No-Break">files around:</span></p>
			<pre class="console">
[emcleroy@rhel2 ~]$ sudo mkdir -p /data
[emcleroy@rhel2 ~]$ sudo mount /data
[emcleroy@rhel2 ~]$ df /data
Filesystem     1K-blocks  Used Available Use% Mounted on
/dev/sdc         5232640 69616   5163024   2% /home/emcleroy/data
[emcleroy@rhel2 ~]$ cd /data</pre>
			<p>After it is mounted, we are going to move into the new drive, create a folder and a test <strong class="source-inline">.txt</strong> file, and ensure it saves, which it does by using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
[emcleroy@rhel2 ~]$ sudo mkdir test
[emcleroy@rhel2 ~]$ cd test/
[emcleroy@rhel2 ~]$ sudo vi test.txt</pre>
			<p>Next, we are <a id="_idIndexMarker045"/>going to remove the mount, log out <a id="_idIndexMarker046"/>of the connection, and delete <span class="No-Break">the leftovers:</span></p>
			<pre class="console">
[emcleroy@rhel2 ~]$ cd
[emcleroy@rhel2 ~]$ sudo umount /data
[emcleroy@rhel2 ~]$ sudo iscsiadm -m node -T iqn.2022-05.com.example:rhel1 -p 192.168.1.198 -u
Logging out of session [sid: 8, target: iqn.2022-05.com.example:rhel1, portal: 192.168.1.198,3260]
Logout of [sid: 8, target: iqn.2022-05.com.example:rhel1, portal: 192.168.1.198,3260] successful.
[emcleroy@rhel2 ~]$ sudo iscsiadm -m node -T iqn.2022-05.com.example:rhel1 -p 192.168.1.198 -o delete</pre>
			<p>This wraps up the section on manually setting up iSCSI. Next is automating it. We will go into more detail in the hands-on review and the quiz at the end of the book. I hope you are enjoying this journey as much as <span class="No-Break">I am.</span></p>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor025"/>iSCSI block storage – Ansible automation playbook creation and usage</h1>
			<p>We will <a id="_idIndexMarker047"/>start the automation <a id="_idIndexMarker048"/>portion of working with iSCSI block storage by first installing and configuring the use of Ansible core 2.9 as that is what is used in the <em class="italic">EX358</em> exam. I will not be using the <strong class="bold">fully qualified collection name</strong> (<strong class="bold">FQCN</strong>) as that can sometimes cause errors in a 2.9 environment, which <a id="_idIndexMarker049"/>could lead to issues while taking the exam. This we want to avoid at all costs, so we will be using the classic module names, and I will explain the differences to a degree so that you can understand what you will need to use in future versions <span class="No-Break">of Ansible.</span></p>
			<p>First, let’s start by installing Ansible 2.9 on server <strong class="source-inline">rhel3</strong> as that is going to be what we consider the workstation server from our <strong class="source-inline">yum</strong> repository. Depending on your personal preferences, you can make <strong class="source-inline">rhel1</strong> your classroom server and <strong class="source-inline">rhel2</strong> and <strong class="source-inline">rhel3</strong> your test servers, but in our case, we have already set up <strong class="source-inline">rhel1</strong> with iSCSI <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">rhel2</strong></span><span class="No-Break">.</span></p>
			<p>First, we <a id="_idIndexMarker050"/>will enable the <a id="_idIndexMarker051"/><span class="No-Break">needed repos:</span></p>
			<pre class="console">
[emcleroy@rhel3 ~]$ sudo subscription-manager repos --enable ansible-2.9-for-rhel-8-x86_64-rpms
Repository 'ansible-2.9-for-rhel-8-x86_64-rpms' is enabled for this system.</pre>
			<p>Next, we will install <span class="No-Break">Python 3:</span></p>
			<pre class="console">
[emcleroy@rhel3 ~]$ sudo dnf install python3 -y</pre>
			<p>Then, we will install <span class="No-Break">Ansible 2.9:</span></p>
			<pre class="console">
[emcleroy@rhel3 ~]$ sudo dnf install ansible -y</pre>
			<p>Let’s check and ensure that the right version of Ansible <span class="No-Break">is installed:</span></p>
			<pre class="console">
[emcleroy@rhel3 ~]$ ansible --version
ansible 2.9.27
  config file = /etc/ansible/ansible.cfg
  configured module search path = ['/home/emcleroy/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']
  ansible python module location = /usr/lib/python3.6/site-packages/ansible
  executable location = /usr/bin/ansible
  python version = 3.6.8 (default, Oct 11 2019, 15:04:54) [GCC 8.3.1 20190507 (Red Hat 8.3.1-4)]</pre>
			<p>Next, we are <a id="_idIndexMarker052"/>going to start writing a playbook using the <strong class="bold">Yet Another Markup Language</strong> (<strong class="bold">YAML</strong>) Ansible language. This is a simple module-based function that will allow you to write up a playbook that will accomplish your task quickly and efficiently. I recommend a good editor when writing up these <a id="_idIndexMarker053"/>playbooks. JetBrains' PyCharm is my go-to and is what you will see me write my playbooks in when you see <a id="_idIndexMarker054"/>example screenshots of the finished results. Do also note that the finished playbooks can be found in the GitHub repository of this book, as mentioned in the <em class="italic">Technical requirements</em> section for <span class="No-Break">each chapter.</span></p>
			<p>The first thing you will want to create is a directory where you want to run the <span class="No-Break">playbooks from:</span></p>
			<pre class="console">
[emcleroy@rhel3 ~]$ mkdir iscsi_mount</pre>
			<p>Once in the directory, we will create an inventory file with a default group that will have both the <strong class="source-inline">rhel1</strong> and <strong class="source-inline">rhel2</strong> servers <span class="No-Break">in them:</span></p>
			<pre class="console">
[emcleroy@rhel3 ~]$ cd iscsi_mount
[emcleroy@rhel3 ~]$ vi inventory
[defaults]
rhel1 ansible_host=192.168.1.198
rhel2 ansible_host=192.168.1.133
[iscsi_block]
rhel1 ansible_host=192.168.1.198
[iscsi_user]
rhel2 ansible_host=192.168.1.133</pre>
			<p>As you can see, I added <strong class="source-inline">ansible_host</strong> and the IP address. This is in case there is no host file set up or the name is not DNS routable. I added the default group with all of the hosts, and there are two additional groups that allow me to limit what my playbooks make changes to. That way, I can tell my playbook to mount the storage on <strong class="source-inline">rhel2</strong> using the <span class="No-Break"><strong class="source-inline">iscsi_user</strong></span><span class="No-Break"> group.</span></p>
			<p>Next, we are <a id="_idIndexMarker055"/>going to write the <a id="_idIndexMarker056"/>block storage playbook named <strong class="source-inline">mount_iscsi.yml</strong>, and I will break it down after showing you what that playbook <span class="No-Break">looks like:</span></p>
			<pre class="console">
---
- name: Ensure /data is mounted from rhel1 iSCSI target that was created manually onto rhel2
  hosts: iscsi_user
  become: true
  become_method: sudo
  tasks:
    - name: the targetcli package is installed
      yum:
        name: targetcli
        state: present
    - name: the IQN is set for the initiator
      template:
        dest: /etc/iscsi/initiatorname.iscsi
        src: templates/initiatorname.iscsi.j2
        mode: '644'
        owner: root
        group: root
    - name: Create mount directory for /data
      file:
        path: /data
        state: directory
        mode: '0755'
    - name: Restart iscsiadm
      command:
        cmd: systemctl restart iscsid.service
    - name: Mount new drive
      command:
  cmd: iscsiadm -m node –T iqn.2022-05.com.example:rhel1  -p 192.168.1.198 -l</pre>
			<p>The module <a id="_idIndexMarker057"/>name for this instance is <strong class="source-inline">yum</strong>, and that is used to install the <strong class="source-inline">iscsi-initiator-utils</strong> package <a id="_idIndexMarker058"/>that will install the utilities. Next, we have the different flags of the modules, such as <strong class="source-inline">dest:</strong> for the destination of the source file that is in your playbook’s <strong class="source-inline">templates</strong> folder. In the template folder location within your playbook directory, you will have the file<strong class="source-inline">/templates/initiatorname.iscsi.j2</strong>, which contains the initiator name to pass to the playbook. It will contain the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
InitiatorName=iqn.2022-05.com.example:rhel1</pre>
			<p>You can find out more about each module that you’re using by looking at the equivalent of a man page, <span class="No-Break">as follows:</span></p>
			<pre class="console">
[emcleroy@rhel3 ~]$ ansible-doc yum</pre>
			<p>You can also list the files with the following command, but keep in mind there are thousands of modules, so try to grep the names <span class="No-Break">if possible:</span></p>
			<pre class="console">
[emcleroy@rhel3 ~]$ ansible-doc –-list</pre>
			<p>The following screenshot shows what a normal <strong class="source-inline">ansible-doc</strong> page looks like for the <span class="No-Break">different modules:</span></p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/Figure_1.19_B18607.jpg" alt="Figure 1.19 – Example of the yum module documentation page"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.19 – Example of the yum module documentation page</p>
			<p>We will <a id="_idIndexMarker059"/>use the following <a id="_idIndexMarker060"/>command to run the <strong class="source-inline">ansible-playbook -i inventory mount_iscsi.yml -u emcleroy -k --ask-become –v</strong> playbook. This will be executed from the <strong class="source-inline">rhel3</strong> server and make changes to the <strong class="source-inline">rhel2</strong> server. This concludes our automated approach to mounting a LUN for iSCSI block storage. We learned a little about Ansible and how it works, from modules to templates. We will learn a lot more about Ansible and all of its inner workings in the upcoming chapters in greater detail, so <span class="No-Break">stick around.</span></p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor026"/>Summary</h1>
			<p>This brings us to the end of the first chapter, where we went into details about RHEL block storage, setting up a hands-on environment for testing purposes, and getting the first taste of Ansible. In the coming chapters, we will be digging deeper into how to use Ansible with further examples and more hands-on exercises that will help hone your abilities as a systems admin and help ensure you pass the <em class="italic">EX358</em> exam. In the next chapter, we will be continuing our journey into network storage, talking about network file storage and how we can use that to share information across our organizations and make our jobs faster. Please join me as we continue our road to gaining the <em class="italic">EX358</em> certification that you want to achieve and that I want to help <span class="No-Break">you obtain.</span></p>
		</div>
	</body></html>