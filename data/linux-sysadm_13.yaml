- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: High Availability
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用性
- en: All computer hardware has limits regarding its performance and reliability,
    so systems that must process requests from large numbers of users without interruptions
    are always composed of multiple individual worker machines and dedicated load-balancing
    nodes that spread the load among those workers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 所有计算机硬件在性能和可靠性方面都有一定的限制，因此必须能够在不间断的情况下处理大量用户请求的系统，通常由多个独立的工作机器和专用的负载均衡节点组成，这些节点将负载分配到各个工作机器上。
- en: Linux includes functionality for load balancing and redundancy in the kernel,
    and multiple user-space daemons manage that built-in functionality and implement
    additional protocols and features.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 在内核中包含负载均衡和冗余的功能，多个用户空间守护进程管理这些内置功能并实现额外的协议和特性。
- en: 'In this chapter, we will learn about the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: Different types of redundancy and load balancing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的冗余和负载均衡
- en: Link and network layer redundancy mechanisms in Linux
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux 中的链路层和网络层冗余机制
- en: Transport layer load balancing with **Linux Virtual** **Server** (**LVS**)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 **Linux Virtual** **Server** (**LVS**) 进行传输层负载均衡
- en: Using Keepalived to share a virtual IP address between multiple nodes and automate
    LVS configuration
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keepalived 在多个节点之间共享虚拟 IP 地址并自动化 LVS 配置
- en: Application layer load-balancing solutions, using HAProxy as an example
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用层负载均衡解决方案，以 HAProxy 为例
- en: Types of redundancy and load balancing
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 冗余和负载均衡的类型
- en: Before we delve into specific high-availability features and their configuration,
    let’s discuss possible types of redundancy and load balancing, their advantages,
    and their limitations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论具体的高可用性特性及其配置之前，让我们先讨论可能的冗余和负载均衡类型、它们的优点及其局限性。
- en: 'First of all, we need to remember that the modern TCP/IP networking stack is
    *layered*. Multiple layering models include different numbers of layers but the
    idea is the same: protocols at the upper layer are unaware of the protocols at
    any lower levels and vice versa. The most commonly used models are the seven-layer
    **Open Systems Interconnection** (**OSI**) model and the four-level DoD model
    (developed by the United States Department of Defense). We have summarized them
    in the following table:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要记住，现代的 TCP/IP 网络栈是 *分层的*。多种分层模型包括不同数量的层，但思路是相同的：上层的协议对任何下层的协议都不了解，反之亦然。最常用的模型是七层
    **开放系统互联** (**OSI**) 模型和四层 DoD 模型（由美国国防部开发）。我们已在下表中对它们进行了总结：
- en: '| **OSI model** | **DoD model** | **Purpose** | **Examples** |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **OSI 模型** | **DoD 模型** | **目的** | **示例** |'
- en: '| Physical | Link | Transmission of electrical/optical signals that represent
    bit streams | Ethernet, Wi-Fi |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 物理 | 链路 | 传输代表比特流的电气/光学信号 | Ethernet, Wi-Fi |'
- en: '| Data link |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 数据链路 |'
- en: '| Network | Internet | Transmission of packets in segmented, routed networks
    | IPv4, IPv6 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 网络 | 互联网 | 在分段、路由的网络中传输数据包 | IPv4, IPv6 |'
- en: '| Transport | Transport | Reliable transmission of data segments (integrity
    checking, acknowledgment, congestion control, and more) | TCP, UDP, SSTP |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 传输 | 传输 | 数据段的可靠传输（完整性检查、确认、拥塞控制等） | TCP, UDP, SSTP |'
- en: '| Session | Application | Transmission of application-specific data | HTTP,
    SMTP, SSH |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 会话 | 应用 | 传输特定应用数据 | HTTP, SMTP, SSH |'
- en: '| Presentation |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 表示层 |'
- en: '| Application |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 应用层 |'
- en: Table 13.1 — OSI and DoD network stack models
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13.1 — OSI 和 DoD 网络栈模型
- en: Since the network stack is layered, to make a network resistant to different
    types of failures, redundancy can and should be implemented at multiple levels.
    For example, connecting a single server to the network with two cables rather
    than one protects it from a single broken cable or a malfunctioning network card
    but will not protect the users from failures of the server software – in that
    case, the server will remain connected to the network but unable to serve any
    requests.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于网络栈是分层的，为了让网络抵御不同类型的故障，冗余可以并且应该在多个层级上实现。例如，通过两根电缆而不是一根将单个服务器连接到网络，可以保护它免受单根电缆断开或网络卡故障的影响，但不会保护用户免受服务器软件故障的影响——在这种情况下，服务器仍然会连接到网络，但无法处理任何请求。
- en: 'This problem can usually be solved by setting up multiple servers and introducing
    a dedicated load-balancer node to the network, which acts as an intermediary:
    it receives connections from users and distributes the load across all those servers.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题通常可以通过设置多个服务器并引入专用的负载均衡节点来解决，该节点充当中介：它接收用户的连接并将负载分配到所有这些服务器上。
- en: Having a load balancer adds redundancy at the transport or application layer
    since the system remains able to serve requests, so long as at least one server
    is available. It also increases the total service capacity beyond the performance
    limit of a single server.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 引入负载均衡器可以在传输层或应用层增加冗余，因为只要至少有一个服务器可用，系统就能继续提供服务。它还增加了超出单个服务器性能极限的总服务能力。
- en: 'However, the load balancer itself becomes a single point of failure – if its
    software fails or it ends up disconnected from the network, the entire service
    becomes unavailable. Additionally, it becomes subject to the greatest network
    traffic load compared to any individual server. This makes link layer and network
    layer redundancy especially relevant. Finally, to make sure that requests that
    users send to the IP address of the load balancer are always accepted, the public
    address is often shared between multiple physical load-balancing servers in a
    cluster using the **First Hop Redundancy** **Protocol** (**FHRP**):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，负载均衡器本身成为了单点故障——如果其软件故障或与网络断开连接，整个服务将变得不可用。此外，它相比任何单独的服务器，还将承受最大的网络流量负载。这使得链路层和网络层冗余尤其重要。最后，为了确保用户发送到负载均衡器IP地址的请求始终被接受，公共地址通常在多个物理负载均衡服务器之间共享，形成一个集群，并使用**首跳冗余协议**（**FHRP**）：
- en: '![Figure 13.1 — Typical high-availability setup](img/B18575_13_01.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图13.1 — 典型的高可用性设置](img/B18575_13_01.jpg)'
- en: Figure 13.1 — Typical high-availability setup
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 — 典型的高可用性设置
- en: '*Figure 13**.1* shows a fully redundant setup with three application servers
    and two load balancers that are protected from cable or network card failures
    with aggregated Ethernet links.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.1* 显示了一个完全冗余的设置，包括三个应用服务器和两个负载均衡器，且通过聚合以太网链接保护免受电缆或网卡故障的影响。'
- en: Before we learn about different redundancy types and their implementations in
    Linux, we should review the terminology. Different protocols and technologies
    use different names for node roles, and some of them still use terminology that
    is inaccurate and may be offensive, but it’s important to know it to understand
    what their documentation is talking about.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们学习Linux中不同冗余类型及其实现之前，我们应该回顾一下术语。不同的协议和技术使用不同的节点角色名称，其中一些仍使用不准确且可能冒犯的术语，但了解这些术语对于理解它们的文档至关重要。
- en: Notes on terminology
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 术语说明
- en: 'To describe redundant setups, we will use *active*/*standby* terminology by
    default: only one *active* node performs any work at a time and one or more additional
    *standby* nodes are waiting to take its place if it fails.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描述冗余设置，我们默认使用*活动*/*备用*（*active*/*standby*）术语：只有一个*活动*节点在任何时刻执行工作，且一个或多个额外的*备用*节点在等待接管它的任务（如果它发生故障）。
- en: A lot of older literature, including protocol standards, configuration files,
    and official documentation for high-availability solutions, may use *master*/*slave*
    terminology instead. That terminology is getting phased out by many projects due
    to its associations with human slavery and also because it is misleading since
    in most protocols, the active node does not have any control over standby nodes.
    We will use that terminology when we discuss protocols and software that still
    use it, for consistency with their documentation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 许多较早的文献，包括协议标准、配置文件以及高可用性解决方案的官方文档，可能仍使用*主/从*（*master*/*slave*）术语。由于这些术语与人类奴隶制的关联以及其误导性——因为在大多数协议中，活动节点并没有控制备用节点——许多项目正在逐步淘汰这些术语。我们在讨论仍使用这些术语的协议和软件时，会为了与它们的文档一致而使用这些术语。
- en: Link layer redundancy
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链路层冗余
- en: Broken cables and Ethernet switch ports are quite common, especially in outdoor
    installations and industrial networks. In those situations, it is very useful
    to have more than one link layer connection. However, simply connecting two different
    network cards of a Linux machine to different ports of the same switch does not
    make them work as a single connection. The user needs to explicitly set up those
    two network cards so that they work together.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 电缆断开和以太网交换机端口故障是非常常见的，尤其是在户外安装和工业网络中。在这些情况下，拥有多个链路层连接非常有用。然而，单纯将一台Linux机器的两个不同网络卡连接到同一交换机的不同端口并不能使它们作为单一连接工作。用户需要显式地设置这两个网络卡，使它们协同工作。
- en: 'Luckily, Linux supports multiple ways to use several network cards together
    – both in active/standby and load-balancing configurations. Some of them do not
    require any support from the Ethernet switch and will work even with very basic
    unmanaged switches. Other modes require that the switch supports either the older
    EtherChannel protocol (designed by Cisco Systems) or the newer and vendor-neutral
    IEEE 802.3ad **Link Aggregation and Control Protocol** (**LACP**), and the ports
    must be configured explicitly to enable those protocols. We can summarize all
    these methods in the following table:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Linux 支持多种方法将多个网卡一起使用——包括主动/备用模式和负载均衡配置。其中一些方法不需要以太网交换机的支持，甚至可以在非常基础的非托管交换机上使用。其他模式则要求交换机支持较旧的
    EtherChannel 协议（由思科系统设计）或较新的、厂商中立的 IEEE 802.3ad **链路聚合控制协议** (**LACP**)，且端口必须显式配置以启用这些协议。我们可以将所有这些方法总结在以下表格中：
- en: '| **Type** | **Operation** | **Switch Requirements** |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **类型** | **操作** | **交换机要求** |'
- en: '| `active-backup` (`1`) | One network card remains disabled while the other
    is up | None; will work for any switch (even unmanaged) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| `active-backup` (`1`) | 一个网络卡保持禁用，而另一个处于工作状态 | 无；适用于任何交换机（即使是非托管交换机） |'
- en: '| `802.3ad` (`4`) | Frames are balanced across all ports | Requires 803.3ad
    LACP support |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| `802.3ad` (`4`) | 帧在所有端口间平衡分配 | 需要 803.3ad LACP 支持 |'
- en: '| `balance-xor` (`2`) and `broadcast` (`3`) | Requires EtherChannel support
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| `balance-xor` (`2`) 和 `broadcast` (`3`) | 需要 EtherChannel 支持 |'
- en: '| `balance-tlb` (`5`) and `balance-alb` (`6`) | None |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| `balance-tlb` (`5`) 和 `balance-alb` (`6`) | 无 |'
- en: Table 13.2 — Link layer redundancy methods in Linux
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13.2 — Linux 中的链路层冗余方法
- en: 'The simplest mode is active-backup, which requires no special setup on the
    Ethernet switch and can even work with the simplest and cheapest unmanaged switches.
    Unlike modes such as 802.3ad LACP, it only provides active-standby redundancy
    rather than load balancing. Using the following command, you can join the `eth0`
    and `eth1` network interfaces to a single `bond0` interface using the active-backup
    method, on a system that uses NetworkManager for configuration:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的模式是 active-backup，它不需要对以太网交换机进行特殊设置，甚至可以与最简单、最便宜的非托管交换机一起使用。与 802.3ad LACP
    等模式不同，它仅提供主动-备用冗余，而非负载均衡。使用以下命令，你可以将 `eth0` 和 `eth1` 网络接口通过 active-backup 方法连接到一个单一的
    `bond0` 接口，在使用 NetworkManager 配置的系统上：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, if either `eth0` or `eth1` are physically disconnected from the switch,
    link layer connectivity will be preserved.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果 `eth0` 或 `eth1` 中的任何一个与交换机物理断开，链路层连接将保持。
- en: The cost of that configuration simplicity and low requirements for the Ethernet
    switch is wasted bandwidth. Whenever possible, high-performance servers should
    be connected to Ethernet networks using the current industry-standard 802.3ad
    LACP protocol, which allows them to benefit from the combined bandwidth of multiple
    links and also automatically exclude failed links to provide redundancy.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种配置简单且对以太网交换机的要求低，但代价是浪费带宽。尽可能地，高性能服务器应使用当前行业标准的 802.3ad LACP 协议连接到以太网网络，这样它们可以利用多个链路的合并带宽，并自动排除故障链路，以提供冗余。
- en: Network layer redundancy and load balancing
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络层冗余和负载均衡
- en: If a system has multiple independent connections to the internet or an internal
    network, it is possible to either provide a backup route or balance IP packets
    across multiple routes. However, in practice, network layer redundancy is only
    used by routers rather than hosts, and its simplest forms are only applicable
    to networks with public, globally routed addresses.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个系统有多个独立的互联网或内部网络连接，可以提供备用路由或在多个路由间平衡 IP 数据包。然而，实际上，网络层冗余通常仅被路由器使用，而非主机，其最简单的形式仅适用于拥有公共、全球路由地址的网络。
- en: Suppose your Linux system is connected to two different routers, one with IPv4
    address 192.0.2.1, and the other with 203.0.113.1\. If you are fine with one connection
    being completely unused and acting purely as a standby, you can create two default
    routes with different *metrics* and assign a higher metric to the standby connection.
    The metric’s value determines the route’s priority, and if multiple routes with
    different metrics exist, the kernel will always use the route with the lowest
    metric. When that route disappears (for example, due to a network card going down),
    the kernel will switch to using the route with the next lowest metric of those
    still available.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的 Linux 系统连接了两个不同的路由器，一个具有 IPv4 地址 192.0.2.1，另一个具有 203.0.113.1。如果你希望其中一个连接完全不使用，仅作为备用连接，你可以创建两个具有不同*度量值*的默认路由，并将备用连接的度量值设置为更高。度量值的大小决定了路由的优先级，如果存在多个不同度量值的路由，内核将始终使用度量值最小的路由。当该路由消失时（例如，由于网络卡故障），内核将切换到使用仍可用的、度量值次小的路由。
- en: 'For example, you can use the following commands if you want `192.0.2.1` to
    be the backup router:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你希望`192.0.2.1`成为备用路由器，你可以使用以下命令：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The advantage of this method is that it is compatible with **Network Address
    Translation** (**NAT**) set up on the same system. If you want to create a load-balancing
    configuration instead, many more issues come into play because network layer load
    balancing is per-packet and unaware of any concept of a connection.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是，它与同一系统上设置的**网络地址转换**(**NAT**)兼容。如果你想创建负载均衡配置，则涉及更多的问题，因为网络层负载均衡是按数据包进行的，并且无法感知连接的概念。
- en: 'On the surface, the configuration for multi-path routes is quite simple. You
    can specify as many gateway addresses as you want and, optionally, assign weights
    to them to direct more traffic to faster links. For example, if you wanted twice
    as much traffic to flow through `203.0.113.1`, you could achieve this with the
    following command:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从表面上看，多路径路由的配置相当简单。你可以指定任意数量的网关地址，并且可以选择性地为它们分配权重，以将更多流量导向更快的链路。例如，如果你希望`203.0.113.1`的流量是其他链接的两倍，你可以使用以下命令来实现：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The problem is that this configuration, by itself, is incompatible with NAT
    because it will send packets that belong to the same TCP connection or a UDP stream
    to different gateways. If you have a publicly routed network, that is considered
    normal and even inevitable. However, if you only have a single external address
    from each provider and have to use NAT to map a private network to that single
    outgoing address, packets that belong to a single connection must always flow
    through the same gateway for the setup to work as expected. There are ways to
    set up per-connection load balancing using policy-based routing but that is outside
    the scope of this book. If you are interested, you can find more information in
    other sources, such as *Policy Routing* *With* *Linux*, by *Matthew G. Marsh*,
    which is freely available online.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，这种配置本身与 NAT 不兼容，因为它会将属于同一 TCP 连接或 UDP 流的包发送到不同的网关。如果你有一个公开路由的网络，这是正常且不可避免的。然而，如果你只有来自每个提供商的单个外部地址，并且必须使用
    NAT 将私有网络映射到该单一的外发地址，则属于同一连接的包必须始终通过相同的网关流动，才能按预期工作。有一些方法可以使用基于策略的路由来设置每连接负载均衡，但这超出了本书的范围。如果你感兴趣，可以在其他来源中找到更多信息，例如*Policy
    Routing* *With* *Linux*，作者是*Matthew G. Marsh*，该书可以在线免费获取。
- en: Transport layer load balancing with LVS
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 LVS 进行传输层负载均衡
- en: The main disadvantage of all network layer mechanisms is that the network layer
    operates with individual packets and has no concept of connections. Many network
    services are connection-oriented so at the very least, all packets that belong
    to the same connection must always be sent to the same server. While the NAT implementation
    in Linux is smart enough to detect packets from the same connection, simple load
    balancing with one-to-many NAT is still too simplistic for many use cases. For
    example, it does not provide an easy way to track how many connections each server
    gets and cannot preferentially send new connections to the least loaded servers
    (that is, to servers that are handling the smallest number of existing connections
    at the moment).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所有网络层机制的主要缺点是网络层是以单个数据包为单位进行操作的，并没有连接的概念。许多网络服务是面向连接的，因此至少，属于同一连接的所有数据包必须始终发送到同一台服务器。虽然Linux中的NAT实现足够智能，可以检测同一连接的多个数据包，但简单的负载均衡（基于一对多的NAT）对于许多使用场景来说仍然过于简化。例如，它没有提供一种简单的方法来跟踪每台服务器接收到多少连接，也无法优先将新连接发送到负载最轻的服务器（即处理现有连接数最少的服务器）。
- en: To account for this use case, Linux includes the `ipvsadm`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这种使用场景，Linux包含了`ipvsadm`。
- en: The key concepts of the LVS framework are *virtual servers* and *real servers*.
    Virtual servers are Linux machines that provide the public address of the service,
    accept connections to it, and then distribute those connections to multiple real
    servers. Real servers can run any OS and software and can be unaware of the virtual
    server’s existence.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: LVS框架的关键概念是*虚拟服务器*和*真实服务器*。虚拟服务器是提供服务公共地址的Linux机器，接受连接并将这些连接分发到多个真实服务器。真实服务器可以运行任何操作系统和软件，并且可以不知道虚拟服务器的存在。
- en: LVS is a flexible framework that provides multiple load-scheduling algorithms,
    load-balancing mechanisms, and configuration options, all with their advantages
    and disadvantages. Let’s examine them in detail.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: LVS是一个灵活的框架，提供多种负载调度算法、负载均衡机制和配置选项，具有各自的优缺点。我们将详细探讨它们。
- en: Scheduling algorithms
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调度算法
- en: 'There are multiple ways to distribute the load between multiple servers, each
    with its advantages and disadvantages. We can summarize them in the following
    table:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以在多个服务器之间分配负载，每种方法都有其优缺点。我们可以通过以下表格进行总结：
- en: '| **Algorithm** | **Description** |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| **算法** | **描述** |'
- en: '| Round Robin (`rr`) | Distributes a connection across all servers equally.
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 轮询（`rr`） | 将连接均匀地分配到所有服务器。 |'
- en: '| Weighted Round Robin (`wrr`) | This is similar to Round Robin but allows
    you to send more connections to certain servers by assigning a higher weight value
    to them. |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 加权轮询（`wrr`） | 这类似于轮询，但通过为某些服务器分配更高的权重值，允许将更多连接发送到这些服务器。 |'
- en: '| Least Connection (`lc`) | Preferentially sends new connections to the server
    with the least number of current connections. |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 最少连接（`lc`） | 优先将新连接发送到当前连接数最少的服务器。 |'
- en: '| Weighted Least Connection (`wlc`) | The default scheduling algorithm. This
    is similar to Least Connection but allows you to assign weights to servers. |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 加权最少连接（`wlc`） | 默认调度算法。这类似于最少连接，但允许为服务器分配权重。 |'
- en: '| Locality-Based Least-Connection (`lblc`) | Sends new connections with the
    same destination IP address to the same server, and switches to the next server
    if the first one is unavailable or overloaded. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 基于位置的最少连接（`lblc`） | 将具有相同目标IP地址的新连接发送到同一服务器，如果第一台服务器不可用或超载，则切换到下一台服务器。 |'
- en: '| Locality-Based Least-Connection with Replication (`lblcr`) | Sends new connections
    with the same destination IP address to the same server, if it is not overloaded.
    Otherwise, it sends them to the server with the least connections. |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 基于位置的最少连接与复制（`lblcr`） | 将具有相同目标IP地址的新连接发送到同一服务器，如果该服务器未超载。否则，将它们发送到连接数最少的服务器。
    |'
- en: '| Destination Hashing (`dh`) | Creates a hash table that maps destination IP
    addresses to servers. |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 目标地址哈希（`dh`） | 创建一个哈希表，将目标IP地址映射到服务器。 |'
- en: '| Source Hashing (`sh`) | Creates a hash table that maps source IP addresses
    to servers. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 源地址哈希（`sh`） | 创建一个哈希表，将源IP地址映射到服务器。 |'
- en: '| Shortest Expected Delay (`sed`) | Sends new connections to the server with
    the shortest expected delay. |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 最短预期延迟（`sed`） | 将新连接发送到具有最短预期延迟的服务器。 |'
- en: '| Never Queue (`nq`) | Sends new connections to the first idle servers, and
    switches to Shortest Expected Delay if there are no idle servers. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 永不排队 (`nq`) | 将新连接发送到第一个空闲服务器，并在没有空闲服务器时切换到最短预期延迟。 |'
- en: Table 13.3 – LVS scheduling algorithms
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13.3 – LVS 调度算法
- en: The right choice of scheduling algorithm depends on the type of service; none
    of them is inherently better than others for all use cases. For example, Round
    Robin and Weighted Round Robin work best for services with short-lived connections,
    such as web servers that serve static pages or files (such as content delivery
    networks).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的调度算法取决于服务类型；对于所有用例，没有一个算法本质上比其他算法更好。例如，轮询和加权轮询最适合服务于短连接的服务，例如提供静态页面或文件的
    Web 服务器（例如内容分发网络）。
- en: Services that use very long-lived, persistent connections, such as online game
    servers, can benefit from Least Connection algorithms instead. Using Round Robin
    methods for such services can be counter-productive because if new connections
    are relatively infrequent but resource consumption per connection is high, it
    can overload some of the servers or create a very unequal load distribution. Least
    Connection algorithms that keep track of the number of active connections to each
    server were designed to counter that problem.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用非常长寿命、持久连接的服务，如在线游戏服务器，可以从最少连接算法中受益。对于这类服务，使用轮询方法可能是适得其反的，因为新连接相对不频繁，但每个连接的资源消耗较高，可能会导致某些服务器过载或创建非常不平衡的负载分布。跟踪每个服务器活动连接数的最少连接算法旨在解决这个问题。
- en: Finally, if response latency is a big factor in the quality of service, the
    Shorted Expected Delay and Never Queue algorithms can improve it, while Round
    Robin and Least Connection do not take response time into account at all.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果响应延迟是服务质量的重要因素，则最短预期延迟和永不排队算法可以改善它，而轮询和最少连接则完全不考虑响应时间。
- en: LVS load-balancing methods
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LVS 负载平衡方法
- en: 'First, we will examine the load-balancing methods that LVS provides. It supports
    three methods: direct routing, IP tunneling, and NAT. We will summarize the differences
    between them and their advantages and disadvantages in a table, then examine them
    in detail with configuration examples:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将审视 LVS 提供的负载平衡方法。它支持三种方法：直接路由、IP 隧道和 NAT。我们将总结它们之间的区别及其优缺点，并通过配置示例详细审查它们：
- en: '| **Mechanism** | **Implementation** | **Advantages** | **Disadvantages** |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| **机制** | **实施** | **优势** | **劣势** |'
- en: '| Direct routing | Replaces the destination MAC address | Best performance;
    real servers send replies directly to clients | All servers must be on the same
    networkIt has difficulties with ARP |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 直接路由 | 替换目标 MAC 地址 | 最佳性能；真实服务器直接向客户端发送回复 | 所有服务器必须在同一网络上，对 ARP 有困难 |'
- en: '| IP tunneling | Sends client requests encapsulated in a tunneling protocol
    | Real servers send replies directly to clientsReal servers can be on any network
    | Real servers must support IPIP tunneling and must have tunnels to the virtual
    serverThe return packets may be rejected as spoofed |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| IP 隧道 | 将客户端请求封装在隧道协议中发送 | 真实服务器直接向客户端发送回复，真实服务器可以在任何网络上 | 真实服务器必须支持 IPIP
    隧道并且必须具有到虚拟服务器的隧道，返回数据包可能会被拒绝作为伪造 |'
- en: '| NAT | Creates NAT rules behind the scenes | Real servers don’t need public
    addresses or any special configuration | Relatively resource-intensiveAll traffic
    goes through the virtual serverThe best method in practice despite its drawbacks
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| NAT | 在幕后创建 NAT 规则 | 真实服务器不需要公共地址或任何特殊配置 | 相对资源密集，所有流量经过虚拟服务器，在实践中是最佳方法，尽管存在缺点
    |'
- en: Table 13.4 – LVS load-balancing methods
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13.4 – LVS 负载平衡方法
- en: Let’s examine these load-balancing mechanisms in detail, starting with NAT.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细分析这些负载平衡机制，首先从 NAT 开始。
- en: NAT
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NAT
- en: 'NAT is the most practical load-balancing method of LVS because of two factors:
    real servers do not need to have publicly routable IP addresses and also do not
    need to be aware of the virtual server or specially configured to work with it.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于两个因素，NAT 是 LVS 中最实用的负载平衡方法：真实服务器不需要具有公共可路由的 IP 地址，也不需要知道虚拟服务器或专门配置以与其配合。
- en: The ability to use non-public internal addresses is especially important in
    IPv4 networks, considering the shortage of IPv4 addresses. The lack of special
    configuration requirements on the real servers also makes it possible to use any
    OS on them, and it simplifies the configuration process as well.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在 IPv4 网络中，能够使用非公开的内部地址尤为重要，因为 IPv4 地址有限。真实服务器无需特殊配置，这也使得可以在其上使用任何操作系统，并简化了配置过程。
- en: An additional advantage of this method is that TCP or UDP ports do not have
    to be the same on the virtual server and real servers since the virtual server
    performs translation anyway rather than forwarding unmodified IP packets.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的另一个优势是，TCP 或 UDP 端口不需要在虚拟服务器和真实服务器上相同，因为虚拟服务器执行的是地址转换，而不是转发未经修改的 IP 数据包。
- en: 'We will set up the virtual server to listen for HTTP requests on `192.168.56.100:80`
    and forward those requests to port `8000` of real servers:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将设置虚拟服务器，以便在`192.168.56.100:80`上监听 HTTP 请求，并将这些请求转发到真实服务器的端口`8000`：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The first command creates a virtual server instance. The second command adds
    a real server to forward packets to – in our case, `10.20.30.2:8000`. Finally,
    the `--masquearding (-m)` option tells it to use the NAT method when sending connections
    to that server.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令创建一个虚拟服务器实例。第二个命令添加一个真实服务器以转发数据包——在我们的例子中是`10.20.30.2:8000`。最后，`--masquearding
    (-m)`选项告诉它在发送连接到该服务器时使用 NAT 方法。
- en: 'We used the long versions of all the `ipvsadm` command-line options here but
    the command could also be written in short form (with the Round-Robin scheduling
    algorithm specified, `-``s rr`):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用了`ipvsadm`命令行选项的长版本，但该命令也可以用短格式写出（指定轮询调度算法，`-s rr`）：
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we can ensure that the virtual server is configured using the `ipvsadm
    –list` or `ipvsadm -``l` command:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过`ipvsadm –list`或`ipvsadm -l`命令来确保虚拟服务器已正确配置：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, if we run `wget http://192.168.56.100:80` on the client machine and run
    a traffic capture on the real server, we will see the following output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们在客户端机器上运行`wget http://192.168.56.100:80`并在真实服务器上运行流量捕获工具，我们将看到以下输出：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'On the virtual server, we will see a notably different output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在虚拟服务器上，我们将看到一个显著不同的输出：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, the virtual server completely takes over the communication between
    the client and the real server. Theoretically, this is a disadvantage because
    it greatly increases the amount of traffic that flows through the virtual server.
    In practice, Linux network performance is pretty good even on modest hardware,
    so it is not a serious issue. Besides, application-specific load-balancing solutions
    also proxy all traffic through the server, so this is no worse than using a service
    such as HAProxy. Since packet forwarding and port/address translation happen in
    the kernel space, this method offers better performance than user-space load-balancing
    applications.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，虚拟服务器完全接管了客户端和真实服务器之间的通信。从理论上讲，这是一个缺点，因为它大大增加了通过虚拟服务器传输的流量。但实际上，即使在普通硬件上，Linux
    的网络性能也相当不错，所以这不是一个严重的问题。而且，特定应用的负载均衡解决方案也通过服务器代理所有流量，因此这与使用像 HAProxy 这样的服务并无太大区别。由于数据包转发和端口/地址转换发生在内核空间，这种方法的性能优于用户空间的负载均衡应用。
- en: We will briefly examine the other load-balancing mechanisms, but for a variety
    of reasons, they are much less practical than NAT and normally need not be used.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要介绍其他负载均衡机制，但由于种种原因，它们比 NAT 实现起来要不太实用，通常无需使用。
- en: Direct routing
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直接路由
- en: 'To set up LVS for direct routing, we need to use the `--gatewaying (-g)` option
    when we add a real server:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要为直接路由设置 LVS，我们需要在添加真实服务器时使用`--gatewaying (-g)`选项：
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With this setup, when the virtual server receives a request on `10.20.30.1:8000`,
    it will simply change the MAC address in that packet to the MAC address of the
    `10.20.30.2` real server and re-send it to the Ethernet network for the real server
    to receive. The real server will then reply directly to the client without creating
    any additional load on the virtual server.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此配置，当虚拟服务器接收到`10.20.30.1:8000`的请求时，它将简单地将该数据包中的 MAC 地址更改为`10.20.30.2`真实服务器的
    MAC 地址，并将其重新发送到以太网网络，供真实服务器接收。然后，真实服务器将直接回复客户端，而不会对虚拟服务器造成额外负载。
- en: While this method is theoretically the most performant and conceptually simplest,
    in reality, it places the hardest requirements on the real servers. The minimal
    requirement is that all real servers must be in the same broadcast network segment.
    The other requirement is that all real servers must also be able to respond to
    packets from the same virtual IP as the service IP, usually by having the virtual
    service IP assigned as an alias.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管理论上这种方法是性能最优且概念上最简单的，但在实际操作中，它对真实服务器提出了最严格的要求。最小的要求是所有真实服务器必须在同一广播网络段内。另一个要求是所有真实服务器还必须能够响应来自与服务IP相同的虚拟IP的数据包，通常是通过将虚拟服务IP分配为别名来实现。
- en: 'However, assigning the same IP address to multiple hosts creates an address
    conflict. To make the network function properly in the presence of an address
    conflict, all nodes except the virtual server must be made to ignore ARP requests
    for the virtual IP. This can be done, for example, with the `arptables` tool:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将相同的IP地址分配给多个主机会导致地址冲突。为了在存在地址冲突的情况下使网络正常工作，必须让所有节点（除了虚拟服务器）忽略虚拟IP的ARP请求。例如，可以使用`arptables`工具来实现这一点：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To truly avoid this conflict and ensure that no real server answers an ARP request
    for the virtual IP, those rules need to be inserted before the address is assigned.
    This fact makes it difficult or even impossible to correctly configure real servers
    for this scheme using the usual network configuration methods, such as distribution-specific
    scripts or NetworkManager.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了真正避免这种冲突并确保没有真实服务器响应虚拟IP的ARP请求，必须在分配地址之前插入这些规则。这个事实使得使用通常的网络配置方法（如分发特定脚本或NetworkManager）正确配置真实服务器变得困难，甚至不可能。
- en: This fact makes this scheme impractical to implement in most networks, despite
    its theoretical advantages.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个事实使得尽管从理论上讲具有优势，但在大多数网络中实施这一方案变得不切实际。
- en: Tunneling
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隧道
- en: 'To set up a virtual server for tunneling, we need to use the `--ipip (-i)`
    option when we add a real server:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置一个用于隧道的虚拟服务器，我们在添加真实服务器时需要使用`--ipip (-i)`选项：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we need to set up an IPIP tunnel on the real server so that it can handle
    incoming tunneled traffic from the virtual server and assign the virtual server
    IP to it:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要在真实服务器上设置一个IPIP隧道，以便它能够处理来自虚拟服务器的传入隧道流量，并为其分配虚拟服务器IP：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, if we make an HTTP request to the virtual server and run a traffic capture
    on the real server, we will see incoming IPIP packets with requests for the virtual
    IP inside:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们向虚拟服务器发起一个HTTP请求，并在真实服务器上进行流量捕获，我们将看到带有虚拟IP请求的传入IPIP数据包：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: While this approach theoretically enables real servers to be in any network,
    it comes with several difficulties in practice. First, the real server OS must
    support IPIP tunneling. This can be a serious difficulty even with Linux systems
    if they run in containers and do not have permission to create tunnels, even if
    the host system kernel is built with IPIP support. Second, since replies are supposed
    to be sent directly to the client rather than back through the tunnel, this scheme
    falls apart in networks that take measures against source IP spoofing – as they
    should.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管理论上这种方法使真实服务器可以位于任何网络，但在实践中存在若干困难。首先，真实服务器的操作系统必须支持IPIP隧道。即使在Linux系统中，如果它们运行在容器内且没有创建隧道的权限，这也可能是一个严重的问题，即使主机系统内核已构建了IPIP支持。其次，由于应直接将回复发送给客户端，而不是通过隧道返回，因此在采取源IP伪造防护措施的网络中，这种方案会失败——而这些防护措施应该是存在的。
- en: Saving and restoring LVS configurations
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存和恢复LVS配置
- en: 'It is possible to export the current LVS configuration in a format that it
    can load from standard input:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 可以以一种格式导出当前的LVS配置，以便它能够从标准输入加载：
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can save the output to a file and then feed it to `ipvsadm –restore`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将输出保存到文件中，然后将其输入到`ipvsadm –restore`：
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: However, in practice, it is better to automate LVS configuration with Keepalived
    or another user-space daemon, as we will learn later in this chapter.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，最好通过Keepalived或其他用户空间守护进程自动化LVS配置，正如我们将在本章稍后学习的那样。
- en: Additional LVS options
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外的LVS选项
- en: In addition to scheduling algorithms and balancing between real servers, LVS
    offers a few additional features and options.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 除了调度算法和真实服务器之间的负载均衡，LVS还提供了一些额外的功能和选项。
- en: Connection persistence
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接持久性
- en: 'By default, LVS balances connections from clients across all servers and does
    not match clients with specific servers. This approach works well for serving
    web pages over HTTP, for example. However, some services use long-lasting and
    stateful connections and would not work well without persistence. One extreme
    example is remote desktop connections: if such connections are balanced between
    multiple servers, sending a user to a different server after a disconnect will
    create a completely new session rather than get the user back to their already
    running applications.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，LVS将客户端的连接均衡到所有服务器，而不会将客户端与特定的服务器匹配。例如，这种方法适用于通过HTTP提供网页服务。然而，一些服务使用持久且有状态的连接，如果没有持久性机制，效果会很差。一个极端的例子是远程桌面连接：如果这种连接在多个服务器之间均衡，断开连接后将用户发送到另一台服务器，会创建一个全新的会话，而不是让用户回到已经运行的应用程序。
- en: 'To make LVS remember client-to-server mappings and send new connections from
    the same client to the same server, you need to specify `--persistent` and, optionally,
    specify a persistence timeout:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让LVS记住客户端与服务器的映射，并将来自同一客户端的新连接发送到同一服务器，你需要指定`--persistent`，并可选地指定持久性超时：
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This preceding command creates a server that remembers client-to-server associations
    for `600` seconds.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个前述的命令会创建一个服务器，记住客户端到服务器的关联，持续`600`秒。
- en: Connection state synchronization
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接状态同步
- en: One notable feature of LVS is its connection state synchronization daemon. In
    that case, the word *daemon* is partially a misnomer since it is implemented in
    the kernel and is not a user-space process. Connection synchronization is unidirectional,
    with dedicated primary (master) and replica (backup) nodes.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: LVS的一个显著特点是其连接状态同步守护进程。在这种情况下，*守护进程*这个词部分上是个误用，因为它是在内核中实现的，并不是用户空间的进程。连接同步是单向的，具有专用的主（主控）节点和副本（备份）节点。
- en: 'There is no explicit peer configuration. Instead, connection states are sent
    to peers using IP multicast. It is possible to specify the network interface to
    use for synchronization messages:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 没有明确的对等配置。相反，连接状态通过IP组播发送到对等方。可以指定用于同步消息的网络接口：
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: However, connection state synchronization by itself is useless, unless there’s
    also a failover mechanism that allows you to transfer the virtual IP to the backup
    node if the primary load-balancer node fails.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅通过连接状态同步是没有用的，除非还存在一个故障转移机制，它可以在主负载均衡器节点失败时，将虚拟IP转移到备份节点。
- en: In the next section, we will learn how to configure failover using the Keepalived
    daemon for VRRP.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将学习如何使用Keepalived守护进程为VRRP配置故障转移。
- en: Active/backup configurations and load balancing with Keepalived
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keepalived的主动/备份配置和负载均衡
- en: A Linux server that is set up as a load balancer for multiple worker servers
    and keeps the service available, even if any of those workers fail. However, the
    load balancer itself becomes a single point of failure in that scheme, unless
    the administrator also takes care to provide a failover mechanism for multiple
    balancers.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一台被配置为负载均衡器的Linux服务器，为多个工作服务器提供服务，即使其中任何一个工作服务器失败，服务也能保持可用。然而，在这种方案中，负载均衡器本身成为了单点故障，除非管理员还采取措施为多个负载均衡器提供故障转移机制。
- en: The usual way to achieve failover is by using a floating *virtual IP address*.
    Suppose `www.example.com` is configured to point at `192.0.2.100`. If you assign
    that address directly to a load-balancing server in a `192.0.2.0/24` network,
    it becomes a single point of failure. However, if you set up two servers with
    primary addresses from that network (say, `192.0.2.10` and `192.0.2.20`), you
    can use a special failover protocol to allow two or more servers to decide which
    one will hold the virtual `192.0.2.100` address and automatically transfer it
    to a different server if the primary server fails.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 实现故障转移的常见方法是使用一个漂浮的*虚拟IP地址*。假设`www.example.com`被配置为指向`192.0.2.100`。如果你将该地址直接分配给`192.0.2.0/24`网络中的一个负载均衡服务器，它就成为了单点故障。然而，如果你设置了两个来自该网络的主地址的服务器（例如`192.0.2.10`和`192.0.2.20`），你可以使用一种特殊的故障转移协议，让两台或更多服务器决定哪个服务器持有虚拟`192.0.2.100`地址，并在主服务器失败时自动将其转移到其他服务器。
- en: The most popular protocol for that purpose is called **Virtual Router Redundancy
    Protocol** (**VRRP**). Despite its name, machines that use VRRP do not have to
    be routers – even though it was originally implemented by router OSs, now, its
    use is much wider.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为此目的，最流行的协议叫做**虚拟路由冗余协议**（**VRRP**）。尽管它的名字中有“路由器”一词，但使用VRRP的机器不一定是路由器——尽管它最初是由路由操作系统实现的，但现在它的应用范围更广泛。
- en: The most popular VRRP implementation for Linux is the Keepalived project. Apart
    from VRRP, it also implements a configuration frontend for LVS, so it is possible
    to write a configuration file for both failover and load balancing, without setting
    up LVS by hand with `ipvsadm`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Linux上最流行的VRRP实现是Keepalived项目。除了VRRP外，它还实现了LVS的配置前端，因此可以编写同时处理故障转移和负载均衡的配置文件，而无需手动使用`ipvsadm`设置LVS。
- en: Installing Keepalived
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Keepalived
- en: 'Most Linux distributions have Keepalived in their repositories, so installing
    it is a straightforward process. On Fedora, RHEL, and its community derivatives
    such as Rocky Linux, you can install it using the following command:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数Linux发行版都在其软件库中提供了Keepalived，因此安装它是一个简单的过程。在Fedora、RHEL及其社区衍生版本如Rocky Linux上，可以使用以下命令安装它：
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'On Debian, Ubuntu, and other distributions that use APT, run the following
    command:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在Debian、Ubuntu和其他使用APT的发行版上，运行以下命令：
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now that we have installed Keepalived, let’s look at the basics of the VRRP
    protocol.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了Keepalived，让我们来看看VRRP协议的基础知识。
- en: Basics of the VRRP protocol operation
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VRRP协议操作的基础知识
- en: VRRP and similar protocols, such as the older **Hot Standby Router Protocol**
    (**HSRP**) and the community-developed **Common Address Redundancy Protocol**
    (**CARP**), are based on the idea of electing the primary node and continually
    checking its status by listening to its keepalive packets. Collectively, such
    protocols are known as **First Hop Redundancy** **Protocols** (**FHRPs**).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: VRRP和类似协议（例如较旧的**热备份路由器协议**（**HSRP**）和社区开发的**通用地址冗余协议**（**CARP**））基于选举主节点的思想，并通过监听其保持活动数据包来持续检查其状态。这些协议统称为**第一跳冗余协议**（**FHRPs**）。
- en: Initially, every node assumes that it may be the primary node and starts transmitting
    keepalive packets (named *advertisements* in the VRRP terminology) that include
    a unique identifier of the VRRP instance and a priority value. At the same time,
    they all start listening to incoming VRRP advertisement packets. If a node receives
    a packet with a priority value higher than its own, it assumes the backup role
    and stops transmitting keepalive packets. The node with the highest priority becomes
    the primary node and assigns the virtual address to itself.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，每个节点都假设可能是主节点，并开始传输保持活动的数据包（在VRRP术语中称为*广告*），其中包括VRRP实例的唯一标识符和优先级值。同时，它们都开始监听传入的VRRP广告数据包。如果一个节点接收到优先级值比自己更高的数据包，则该节点承担备份角色并停止传输保持活动的数据包。优先级最高的节点成为主节点，并分配虚拟地址给自己。
- en: The elected primary node keeps sending VRRP advertisement packets at regular
    intervals to signal that it is functional. Other nodes remain in the backup state,
    so long as they receive those packets. If the original primary node ceases to
    transmit VRRP packets, a new election is initiated.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当选主节点继续定期发送VRRP广告数据包以表明其功能时，其他节点保持备份状态。如果原始主节点停止传输VRRP数据包，则会启动新的选举。
- en: If the original primary node reappears after a failure, there are two possible
    scenarios. By default, in the Keepalived implementation, the highest priority
    node will always preempt and the node that assumed its role during its downtime
    will go back to the backup state. This is usually a good idea because it keeps
    the primary router predictable under normal circumstances. However, preemption
    also causes an additional failover event that may lead to dropped connections
    and brief service interruptions. If such interruptions are undesirable, it is
    possible to disable preemption.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果原始的主节点在故障后重新出现，有两种可能的情况。默认情况下，在Keepalived实现中，优先级最高的节点将始终抢占，并且在其宕机期间承担其角色的节点将返回备份状态。通常这是一个好主意，因为它可以在正常情况下保持主路由器的可预测性。然而，抢占也会导致额外的故障转移事件，可能会导致连接中断和短暂的服务中断。如果不希望发生这样的中断，可以禁用抢占。
- en: Configuring VRRP
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置VRRP
- en: 'Let’s look at a simple example of VRRP configuration and then examine its options
    in detail:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的VRRP配置示例，然后详细查看其选项：
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You will need to save that configuration to the Keepalived configuration file
    – typically, to `/etc/keepalived/keepalived.conf`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要将该配置保存到Keepalived配置文件中 - 通常是`/etc/keepalived/keepalived.conf`。
- en: The Keepalived configuration file may include one or more VRRP instances. Their
    names are purely informational and can be arbitrary, so long as they are unique
    within the configuration file.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Keepalived配置文件可能包含一个或多个VRRP实例。它们的名称仅供信息参考，可以是任意的，只要它们在配置文件中是唯一的。
- en: The `state` option defines the initial state of the router. It is safe to specify
    `BACKUP` on all routers because they will elect the active router automatically,
    even if none of them has the `MASTER` state in its configuration.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`state`选项定义了路由器的初始状态。将`BACKUP`指定给所有路由器是安全的，因为它们会自动选举活动路由器，即使它们的配置中没有任何一个具有`MASTER`状态。'
- en: VRRP instances are bound to network interfaces and exist in a single broadcast
    domain only, so we need to specify the network interface from which VRRP advertisements
    will originate. In that example, it is `interface eth1`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: VRRP 实例绑定到网络接口并且只存在于一个广播域中，因此我们需要指定 VRRP 广告包将从哪个网络接口发送。在这个示例中，它是`interface eth1`。
- en: The `virtual_router_id 100`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`virtual_router_id 100`。'
- en: The next two parameters are optional. The default VRRP router priority is 100
    unless specified otherwise. If you want to specify router priorities manually,
    you can use numbers from 1 to 254 – priority numbers `0` and `255` are reserved
    and cannot be used. A higher priority value means that the router is more likely
    to be elected as an active (master) router.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个参数是可选的。默认的 VRRP 路由器优先级是 100，除非另有指定。如果你希望手动指定路由器优先级，可以使用 1 到 254 之间的数字——优先级数字
    `0` 和 `255` 是保留的，不能使用。较高的优先级值意味着路由器更有可能被选为活动（主）路由器。
- en: The advertisement packet transmission interval (`advertise_interval`) defaults
    to one second and for most installations, it is a sensible setting. VRRP does
    not create much traffic, so there are no strong reasons to make the interval longer.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 广告包传输间隔（`advertise_interval`）默认为 1 秒，对于大多数安装来说，这是一个合理的设置。VRRP 不会产生太多流量，因此没有强烈的理由将间隔设置得更长。
- en: Finally, we specified a single virtual address, `10.20.30.100/24`. It is possible
    to specify up to 20 virtual addresses, separated by spaces. One thing to note
    is that all virtual addresses do not have to belong to the same network and do
    not have to be in the same network as the permanent, non-floating address of the
    network interface where the VRRP instance is running. It may even be possible
    to create redundant internet connections by assigning private IPv4 addresses to
    the WAN interfaces of two routers and setting up the public IPv4 addresses allocated
    by the internet service provider as virtual addresses.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们指定了一个虚拟地址，`10.20.30.100/24`。最多可以指定 20 个虚拟地址，地址之间用空格分隔。需要注意的一点是，所有虚拟地址不必属于同一网络，也不必与
    VRRP 实例运行的网络接口的永久非浮动地址位于同一网络。通过将私有 IPv4 地址分配给两台路由器的 WAN 接口，并将互联网服务提供商分配的公共 IPv4
    地址设置为虚拟地址，甚至可以创建冗余的互联网连接。
- en: Verifying VRRP’s status
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证 VRRP 的状态
- en: When you save the sample configuration to `/etc/keepalived/keepalived.conf`
    and start the process with `sudo systemctl start keepalived.service` (on Linux
    distributions with systemd), your server will become the active (master) node
    and assign the virtual address to its network interface, until and unless you
    add a second server with a higher priority to the same network.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将示例配置保存到`/etc/keepalived/keepalived.conf`并使用`sudo systemctl start keepalived.service`启动进程（在带有
    systemd 的 Linux 发行版中），你的服务器将成为活动（主）节点，并将虚拟地址分配给其网络接口，直到你将一个优先级更高的第二台服务器添加到同一网络。
- en: 'The simplest way to verify this is to view IP addresses for the interface that
    we configured VRRP to run on:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 验证此操作的最简单方法是查看我们配置 VRRP 运行的接口的 IP 地址：
- en: '[PRE20]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You can also use traffic capture tools such as `tcpdump` to verify that the
    server is indeed sending VRRP advertisement packets:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用流量捕获工具，如`tcpdump`，验证服务器是否确实正在发送 VRRP 广告包：
- en: '[PRE21]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: However, there is also a way to request VRRP’s status data directly from Keepalived.
    Unlike some other services, Keepalived (as of its 2.2.7 release) does not include
    a socket interface or a command-line utility for interacting with it and uses
    POSIX signals to trigger state file creation. This is less convenient than a dedicated
    utility would be.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有一种方法可以直接从 Keepalived 请求 VRRP 的状态数据。与一些其他服务不同，Keepalived（自 2.2.7 版本起）不包含套接字接口或命令行工具来与其交互，而是使用
    POSIX 信号来触发状态文件的创建。这比专用工具要不太方便。
- en: First, you need to look up the identifier (PID) of the Keepalived process. The
    best way to retrieve it is to read its PID file, most often located at `/run/keepalived.pid`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要查找 Keepalived 进程的标识符（PID）。获取它的最佳方法是读取其 PID 文件，通常位于`/run/keepalived.pid`。
- en: 'Sending the `SIGUSR1` signal to the process with `kill -USR1 <PID>` will produce
    a data file at `/tmp/keepalived.data`. This file contains multiple sections, and
    the section of immediate interest for us to find out the status of our VRRP instance
    is named **VRRP Topology**:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向进程发送 `SIGUSR1` 信号（`kill -USR1 <PID>`），会在 `/tmp/keepalived.data` 生成一个数据文件。此文件包含多个部分，我们立即感兴趣的部分是名为
    **VRRP 拓扑** 的部分，用来查找 VRRP 实例的状态：
- en: '[PRE22]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'It is also possible to request a statistics file (`/tmp/keepalived.stats`)
    by sending the Keepalived process the `SIGUSR2` signal instead:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以通过向 Keepalived 进程发送 `SIGUSR2` 信号，要求其生成统计文件（`/tmp/keepalived.stats`）：
- en: '[PRE23]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: While the information method is somewhat unwieldy at the moment, you can glean
    a lot of information about your VRRP instances from those data files.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然目前信息获取方式稍显笨拙，但您可以从这些数据文件中获取大量关于 VRRP 实例的信息。
- en: Configuring virtual servers
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置虚拟服务器
- en: As we already said, Keepalived can also create and maintain LVS configurations.
    The advantage over configuring LVS manually is that Keepalived is easy to start
    at boot time since it always comes with service management integration (typically,
    a systemd unit), while LVS is a kernel component that does not have a configuration
    persistence mechanism. Additionally, Keepalived can perform health checks and
    reconfigure the LVS subsystem when servers fail.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说，Keepalived 也可以创建和维护 LVS 配置。与手动配置 LVS 相比，它的优势在于，Keepalived 很容易在启动时启动，因为它总是与服务管理集成（通常是
    systemd 单元），而 LVS 是内核组件，缺乏配置持久化机制。此外，Keepalived 还可以执行健康检查，并在服务器故障时重新配置 LVS 子系统。
- en: 'For demonstration purposes, let’s consider a minimal load-balancing configuration
    with a Weighted Round Robin balancing algorithm, NAT as the load-balancing method,
    and two real servers with equal weights:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，我们假设一个最小的负载均衡配置，使用加权轮询（Weighted Round Robin）负载均衡算法，NAT 作为负载均衡方法，两个具有相等权重的真实服务器：
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Every load-balancing algorithm that we discussed in the *Transport layer load
    balancing with LVS* section can be specified in the `lb_algo` option, so it could
    be `lb_algo wlc` (Weighted Least Connection), for example.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *LVS 传输层负载均衡* 部分讨论的每个负载均衡算法都可以在 `lb_algo` 选项中指定，因此它可以是 `lb_algo wlc`（加权最少连接）等。
- en: 'If you save that configuration to `/etc/keepalived/keepalived.conf` and restart
    the daemon with `systemctl restart keepalived`, you can verify that it created
    an LVS configuration:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将该配置保存到 `/etc/keepalived/keepalived.conf` 并使用 `systemctl restart keepalived`
    重启守护进程，您可以验证它是否创建了 LVS 配置：
- en: '[PRE25]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now that we know how to make a basic virtual server configuration, let’s learn
    how to monitor the status of real servers and exclude them if they fail.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道如何进行基本的虚拟服务器配置，接下来让我们学习如何监控真实服务器的状态，并在服务器失败时将其排除。
- en: Server health tracking
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务器健康监控
- en: LVS by itself is purely a load-balancing solution and it does not include a
    server health monitoring component. However, in real-world installations, prompt
    exclusion of servers that are not functioning correctly or are under scheduled
    maintenance is an essential task, since directing user requests to non-functional
    servers defeats the purpose of a high-availability configuration. Keepalived includes
    monitoring capabilities so that it can detect and remove servers that fail health
    checks.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: LVS 本身纯粹是一个负载均衡解决方案，它不包含服务器健康监控组件。然而，在实际安装中，及时排除无法正常工作或正在进行计划维护的服务器是一项至关重要的任务，因为将用户请求指向无法正常工作的服务器会破坏高可用性配置的目标。Keepalived
    包含监控功能，可以检测并移除未通过健康检查的服务器。
- en: Health checks are configured separately for each real server, although in most
    real-world installations, they should logically be the same for all servers, and
    using different health check settings for different servers is usually a bad idea.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 每个真实服务器都单独配置健康检查，尽管在大多数实际安装中，它们的配置应逻辑上保持一致，通常不建议为不同的服务器使用不同的健康检查设置。
- en: TCP and UDP connection checks
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TCP 和 UDP 连接检查
- en: 'The simplest but the least specific health check type is a simple connection
    check. It exists in two variants – `UDP_CHECK` and `TCP_CHECK` for UDP and TCP
    protocols, respectively. Here is a configuration example for that check type:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单但最不具体的健康检查类型是简单的连接检查。它有两种变体——`UDP_CHECK` 和 `TCP_CHECK`，分别适用于 UDP 和 TCP 协议。以下是该检查类型的配置示例：
- en: '[PRE26]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As you can see, there is no need to specify the TCP port for connection checks
    explicitly: Keepalived will use the port specified in the server address configuration
    (port `80` in this case).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，不需要显式地指定用于连接检查的TCP端口：Keepalived会使用服务器地址配置中指定的端口（在本例中为端口`80`）。
- en: 'When you start Keepalived with that configuration, it will activate the health-checking
    subsystem and begin connection checks. If there is no running web server on `192.168.56.101`
    listening on port `80`, Keepalived will remove that server from the LVS configuration
    once its check fails three times (as defined by the `retry` option). You will
    see the following in the system log (which you can view, for example, with `sudo
    journalctl -``u keepalived`):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用该配置启动Keepalived时，它将激活健康检查子系统并开始连接检查。如果`192.168.56.101`上没有运行Web服务器并且端口`80`没有监听，Keepalived将在检查失败三次后（根据`retry`选项定义）将该服务器从LVS配置中移除。你将在系统日志中看到如下信息（例如，你可以使用`sudo
    journalctl -u keepalived`查看）：
- en: '[PRE27]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The advantage of this simple TCP check is that it works for any TCP-based service,
    no matter what its application layer protocol is: you can use it for web applications,
    as well as SMTP servers or any custom protocols. However, the fact that a server
    responds to TCP connections by itself does not always mean that it is also functioning
    correctly. For example, a web server may respond to TCP connections but reply
    to every request with a **500 Internal Server** **Error** result.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单的TCP检查的优点是它适用于任何基于TCP的服务，无论其应用层协议是什么：你可以用它来检查Web应用程序，也可以用于SMTP服务器或任何自定义协议。然而，服务器对TCP连接的响应本身并不总是意味着它也正常运行。例如，Web服务器可能会响应TCP连接，但每次请求都返回**500
    内部服务器错误**的结果。
- en: If you want perfect, fine-grained control over the check logic, Keepalived gives
    you that option in the form of the `MISC_CHECK` method.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望对检查逻辑进行完美、精细的控制，Keepalived提供了`MISC_CHECK`方法来满足这个需求。
- en: Misc (arbitrary script) check
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任意脚本检查
- en: 'The most universal check is `MISC_CHECK`, which does not have any built-in
    checking logic and relies on an external script instead. For example, this is
    how you can make Keepalived execute the `/tmp/my_check.sh` script and consider
    the server unavailable if that script returns a non-zero exit code:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最通用的检查是`MISC_CHECK`，它没有任何内建的检查逻辑，而是依赖于外部脚本。例如，你可以让Keepalived执行`/tmp/my_check.sh`脚本，并在该脚本返回非零退出码时认为服务器不可用：
- en: '[PRE28]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: With this type of health check, you can monitor any kind of server, although
    the disadvantage is that you have to implement all the checking logic yourself
    in a script.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种类型的健康检查，你可以监控任何类型的服务器，尽管缺点是你必须在脚本中实现所有的检查逻辑。
- en: HTTP and HTTPS checks
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: HTTP和HTTPS检查
- en: While `MISC_CHECK` gives you total control, it is also overkill in most cases.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`MISC_CHECK`给你完全的控制权，但在大多数情况下，它也是一种过度设计。
- en: As a compromise between specificity and flexibility, you can also use protocol-specific
    checks. For example, there is the `HTTP_GET` check, which makes an HTTP request
    to a URL and can check the hash sum of the response, or its HTTPS equivalent named
    `SSL_CHECK`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 作为具体性和灵活性之间的折衷，你还可以使用特定协议的检查。例如，`HTTP_GET`检查会向URL发起HTTP请求，并检查响应的哈希值，或其HTTPS等价物`SSL_CHECK`。
- en: 'For example, suppose you want to serve a simple static page. In that case,
    you can calculate an MD5 hash sum from that page by hand using the `md5sum` command:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你想提供一个简单的静态页面。在这种情况下，你可以通过使用`md5sum`命令手动计算该页面的MD5哈希值：
- en: '[PRE29]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To calculate the expected hash sum of a dynamically generated page, you can
    use the `genhash` utility that comes with Keepalived. If you run it with `--verbose`,
    it will show you detailed information about the HTTP request it performs:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算动态生成页面的预期哈希值，你可以使用Keepalived自带的`genhash`工具。如果你用`--verbose`运行它，它将显示有关其执行的HTTP请求的详细信息：
- en: '[PRE30]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: However, it only calculates the hash sum of the HTTP response body rather than
    the complete response with headers, so you do not have to use it – you can retrieve
    the response body with any other HTTP request utility if you prefer.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它只计算HTTP响应体的哈希值，而不是包括头部的完整响应，因此你不必非得使用它——如果你愿意，你可以通过任何其他HTTP请求工具来获取响应体。
- en: 'Once you have the expected response hash sum, you can configure the `HTTP_GET`
    check to periodically perform a request and check its response body against the
    given MD5 sum:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了预期的响应哈希值，你可以配置`HTTP_GET`检查定期发起请求并将其响应体与给定的MD5哈希值进行对比：
- en: '[PRE31]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Since normal, user-visible pages can change at any time, it is better to create
    a special page whose content stays constant if you want to use the hash sum check.
    Otherwise, the hash sum will change when its content changes, and the check will
    start failing.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 由于普通的、用户可见的页面可能随时变化，因此如果您希望使用哈希值检查，最好创建一个内容保持不变的特殊页面。否则，页面内容变化时哈希值会发生变化，从而导致检查失败。
- en: Email notifications
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 邮件通知
- en: It is also possible to configure Keepalived to send email notifications to one
    or more addresses when any status changes occur – that is, when VRRP transitions
    from master to backup or the other way around, or when real servers become unavailable
    and fail checks, or pass checks that were failing earlier and are added back to
    the LVS configuration in the kernel.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以配置 Keepalived，当任何状态变化发生时，将邮件通知发送到一个或多个地址——即 VRRP 从主节点切换到备节点或反之，或者当实际服务器变得不可用并且失败检查时，或者通过之前失败的检查并重新添加到内核中的
    LVS 配置时。
- en: 'Here is a configuration example:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个配置示例：
- en: '[PRE32]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Unfortunately, there is no support for SMTP authentication, so if you choose
    to use the built-in email notification mechanism, you need to configure a server
    as an open relay and take appropriate measures to ensure that only the servers
    running Keepalived can send messages through it – for example, by limiting access
    to it to your private network using firewall rules.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，LVS 不支持 SMTP 验证，因此如果选择使用内置的邮件通知机制，您需要将服务器配置为开放转发，并采取适当措施确保只有运行 Keepalived
    的服务器能够通过它发送邮件——例如，通过防火墙规则限制仅允许您的私人网络访问它。
- en: Application layer load balancing
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用层负载均衡
- en: LVS is a flexible framework for load balancing and the fact that it is implemented
    within the kernel makes it a high-performance solution since it does not require
    context switches and data transfer between user-space programs and the kernel.
    The fact that it works at the TCP or UDP protocol level also makes it application-agnostic
    and allows you to use it with any application service.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: LVS 是一个灵活的负载均衡框架，且由于它是在内核中实现的，因此它是一种高性能的解决方案，因为它不需要上下文切换，也不需要在用户空间程序与内核之间进行数据传输。它在
    TCP 或 UDP 协议层工作，使其不依赖于应用程序，能够与任何应用服务一起使用。
- en: However, its lack of application protocol awareness is also its greatest weakness
    because it means that it cannot perform any protocol-specific optimizations. For
    example, one obvious way to improve performance for applications that may return
    the same reply to multiple users is to cache replies. LVS operates with TCP connections
    or UDP streams, so it has no way to know what a request or a reply looks like
    in any application layer protocol – it simply does not inspect TCP or UDP payloads
    at all.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它缺乏对应用协议的意识也是其最大弱点，因为这意味着它无法执行任何特定于协议的优化。例如，对于那些可能向多个用户返回相同回复的应用程序，一个显而易见的提高性能的方法是缓存回复。LVS
    仅在 TCP 连接或 UDP 流上操作，因此它无法知道任何应用层协议中的请求或回复是什么样子的——它根本不检查 TCP 或 UDP 负载。
- en: Additionally, many modern application layer protocols are encrypted, so it is
    impossible to look inside the payload of a connection that the server does not
    initiate or terminate.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多现代应用层协议都进行了加密，因此无法查看服务器未启动或终止的连接的负载内容。
- en: There are more potential disadvantages to forwarding connections directly from
    users to real servers. For example, it exposes servers to TCP-based attacks such
    as SYN flood and requires appropriate security measures on all servers or a dedicated
    firewall setup at the entry point.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 直接将连接从用户转发到实际服务器也有更多潜在的缺点。例如，这使得服务器暴露于基于 TCP 的攻击，如 SYN 洪水攻击，并且要求所有服务器或入口点的专用防火墙设置采取适当的安全措施。
- en: One way to solve these issues is to use a user-space daemon that implements
    the protocol of the service you are running, terminates TCP connections, and forwards
    application layer protocol requests to target servers.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题的一种方法是使用一个用户空间的守护进程，它实现您运行的服务协议，终止 TCP 连接，并将应用层协议请求转发到目标服务器。
- en: Since most applications in the world are currently web applications, most such
    solutions target HTTP and HTTPS. They provide in-memory response caching to speed
    up replies, terminate SSL connections, and manage certificates, and can optionally
    provide security features as well. HAProxy and Varnish are prominent examples
    of web application load-balancing servers, although there are other solutions
    for that purpose as well.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 由于世界上大多数应用程序目前都是 Web 应用程序，因此大多数此类解决方案都针对 HTTP 和 HTTPS。它们提供内存中的响应缓存来加速回复，终止 SSL
    连接，管理证书，并可以选择提供安全功能。HAProxy 和 Varnish 是 Web 应用负载均衡服务器的典型例子，尽管也有其他类似的解决方案。
- en: There are also solutions for other protocols that include high availability
    and load balancing. For example, OpenSIPS and FreeSWITCH can provide load balancing
    for **Voice over Internet Protocol** (**VoIP**) calls made using the SIP protocol.
    Such solutions are beyond the scope of this book, however. We will take a quick
    look at HAProxy as one of the most popular high-availability solutions for web
    applications.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 也有针对其他协议的解决方案，提供高可用性和负载均衡。例如，OpenSIPS 和 FreeSWITCH 可以为使用 SIP 协议的 **互联网语音协议**
    (**VoIP**) 通话提供负载均衡。然而，此类解决方案超出了本书的范围。我们将快速了解 HAProxy 作为 Web 应用的高可用性解决方案之一。
- en: Web application load balancing with HAProxy
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 HAProxy 进行 Web 应用负载均衡
- en: 'HAProxy configuration is a large subject since it includes a lot of functionality.
    We will examine a simple configuration example to get a sense of its capabilities:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy 配置是一个庞大的主题，因为它包含了许多功能。我们将通过一个简单的配置示例来了解它的能力：
- en: '[PRE33]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As you can see, at its core, any HAProxy configuration maps frontends (that
    is, load-balancing instances) with backends – sets of actual application servers.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，其核心功能是将前端（即负载均衡实例）与后端——实际应用服务器集合进行映射。
- en: 'In this case, a single frontend is mapped to two backends: a single server
    specially for serving static files and two application servers. This is only possible
    for HAProxy because it handles HTTP requests itself, sends new requests to its
    backends, and prepares a reply to the user, instead of simply balancing connections.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一个前端映射到两个后端：一个专门用于提供静态文件的服务器和两个应用服务器。这仅在 HAProxy 中可行，因为它自己处理 HTTP 请求，发送新请求到其后端，并准备回复用户，而不是仅仅平衡连接。
- en: Summary
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we learned about the concepts of high availability: redundancy,
    failover, and load balancing. We also learned how to configure link-layer redundancy
    by creating bonding interfaces, as well as how to set up redundant routes at the
    network layer. To ensure transport layer redundancy, we learned how to configure
    the LVS subsystem by hand with `ipvsadm` or using Keepalived and also learned
    how to provide failover for load-balancing nodes using VRRP. Finally, we took
    a brief look at HAProxy as an application layer load-balancing solution for web
    servers.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了高可用性的概念：冗余、故障转移和负载均衡。我们还学习了如何通过创建绑定接口来配置链路层冗余，以及如何在网络层设置冗余路由。为了确保传输层冗余，我们学习了如何手动使用
    `ipvsadm` 配置 LVS 子系统，或者使用 Keepalived，并学习了如何使用 VRRP 为负载均衡节点提供故障转移。最后，我们简要了解了 HAProxy
    作为 Web 服务器的应用层负载均衡解决方案。
- en: In the next chapter, we will learn about managing Linux systems with configuration
    automation tools.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何使用配置自动化工具管理 Linux 系统。
- en: Further reading
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 若想了解更多本章涉及的内容，请查看以下资源：
- en: '*Policy Routing* *With* *Linux*, by Matthew G. Marsh: [https://web.archive.org/web/20230322065520/http://www.policyrouting.org/PolicyRoutingBook/ONLINE/TOC.xhtml](https://web.archive.org/web/20230322065520/http://www.policyrouting.org/PolicyRoutingBook/ONLINE/TOC.xhtml)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*策略路由* *与* *Linux*，作者：Matthew G. Marsh：[https://web.archive.org/web/20230322065520/http://www.policyrouting.org/PolicyRoutingBook/ONLINE/TOC.xhtml](https://web.archive.org/web/20230322065520/http://www.policyrouting.org/PolicyRoutingBook/ONLINE/TOC.xhtml)'
- en: 'Keepalived documentation: [https://keepalived.readthedocs.io/en/latest/](https://keepalived.readthedocs.io/en/latest/%0D)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keepalived 文档：[https://keepalived.readthedocs.io/en/latest/](https://keepalived.readthedocs.io/en/latest/%0D)
- en: 'HAProxy: [http://www.haproxy.org/](http://www.haproxy.org/)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'HAProxy: [http://www.haproxy.org/](http://www.haproxy.org/)'
