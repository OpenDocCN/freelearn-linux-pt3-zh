- en: '*Chapter 4*: Designing System Deployment Architectures'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第四章*：设计系统部署架构'
- en: How we deploy systems determines so much about how those systems will perform
    and how resilient they will be for years to come. A good understanding of design
    components and principles is necessary for us to understand in order to approach
    the design of the platforms that will carry our workloads. Remember, at the end
    of the day, only the applications running at the very top of the stack matter
    - everything beneath the applications, whether the operating system, hypervisor,
    storage, hardware, and others are just tools used to enable the final application-level
    workloads to do what they need to do best. It is easy to feel that these other
    components matter individually, but they do not. To put it another way, what matters
    is the results rather than the path taken to get to the results.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何部署系统决定了这些系统今后将表现如何以及它们的弹性将持续多久。了解设计组件和原则对我们理解如何设计将承载我们工作负载的平台至关重要。请记住，归根结底，只有位于堆栈顶端的应用程序才真正重要
    - 在应用程序之下的一切，无论是操作系统、虚拟化程序、存储、硬件还是其他工具，都只是用来实现最终应用级工作负载所需功能的工具。很容易感觉到这些其他组件的重要性，但实际上它们并不重要。换句话说，重要的是结果，而不是达到结果的途径。
- en: In this chapter, we are going to start by looking at the building blocks of
    systems (other than storage which we tackled extensively in our last chapter before
    taking all of those components as a whole and looking at them, to see how they
    can form robust carriers for our application workloads. Next, we will look at
    need analysis. Then finally we will move on to assembling those pieces into architectural
    designs to meet those needs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先查看系统的构建模块（除了我们在上一章中广泛讨论的存储外），然后将所有这些组件作为一个整体来查看它们，以了解它们如何形成我们应用工作负载的强大载体。接下来，我们将进行需求分析。最后，我们将继续将这些组件组装成满足这些需求的架构设计。
- en: By the end of this chapter, you should feel confident that, while Linux is potentially
    only one slice in the middle of our application stack, you are prepared to design
    the entire stack properly to meet workload goals. While technically much of this
    design is not strictly systems administration (or engineering) it most often falls
    to the system administrators to handle as only the rarest of organizations have
    highly skilled and end to end knowledgeable staff from other departments. The
    systems team sits at the nexus of all components and has the greatest single role
    visibility in both directions (up the stack to the applications and down the stack
    to hypervisors, hardware, and storage). It is natural that systems teams are tasked
    with the greater design tasks as there is no one else capable.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章的学习，您应该对此感到自信，尽管Linux可能只是我们应用程序堆栈中的一个片段，但您已准备好正确设计整个堆栈，以满足工作负载目标。尽管从技术上讲，这种设计大部分并不严格属于系统管理（或工程），但它通常由系统管理员来处理，因为只有极少数组织拥有来自其他部门高技能和全面知识的员工。系统团队处于所有组件的交汇点，并在两个方向上（向上到应用程序和向下到虚拟化程序、硬件和存储）具有最大的单一角色可见性。系统团队被赋予更大的设计任务是很自然的。
- en: 'In this chapter we are going to learn about the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: Virtualization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟化
- en: Containerization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器化
- en: Cloud and **Vitual Private Server** (**VPS**)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云和**虚拟专用服务器**（**VPS**）
- en: On premises, hosted, and hybrid hosting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自有、托管和混合托管
- en: System design architecture
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统设计架构
- en: Risk assessment and availability needs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风险评估和可用性需求
- en: Availability strategies
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用性策略
- en: Virtualization
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟化
- en: Twenty years ago, if you asked the average system administrator what virtualization
    was they would look at you with a blank stare. We have had virtualization technologies
    in IT since 1965 when IBM first introduced them in their mainframe computer systems,
    but for your average company these technologies were relatively rare and out of
    reach until vendors like *VMware* and *Xen* brought these to the mainstream market
    around the turn of the millennium. The enterprise space did have many of these
    technologies by the 1990s, but knowledge of them did not disseminate far.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 二十年前，如果你问一个普通的系统管理员什么是虚拟化，他们可能会一头雾水。自从IBM在1965年首次在其大型计算机系统中引入虚拟化技术以来，我们就已经在IT领域中拥有了这些技术，但对于普通公司来说，这些技术相对较少且难以获得，直到像*VMware*和*Xen*这样的供应商在千禧年之交将它们带入了主流市场。企业领域在20世纪90年代已经使用了许多这些技术，但对它们的了解并没有广泛传播开来。
- en: Times have changed. Since 2005, virtualization has been broadly available and
    widely understood, with options for every platform and at all price points, leaving
    no one with a need to avoid implementing the technology because it is out of technical
    or financial reach. At its core, virtualization is an abstraction layer that creates
    a computer *in software* (on top of the actual hardware) and presents a standard
    set of virtual hardware. Software that performs virtualization is called a **hypervisor**
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 时代已经变了。自2005年以来，虚拟化已经广泛普及并被广泛理解，每个平台和所有价格点都有选择，没有人因为技术或财务上的限制而需要避免实施这项技术。在其核心，虚拟化是创建一个*软件中的计算机*（在实际硬件的顶部）并呈现标准虚拟硬件集的抽象层。执行虚拟化的软件称为**hypervisor**
- en: In the last chapter we spoke repeatedly about interfaces and how something consumes
    or presents itself as a disk drive or file system, for example. A hypervisor is
    software that presents a *computer interface*, meaning it doesn't just present
    a hard drive, but it acts like an entire computer. If you have never used or thought
    about virtualization this might seem extraordinarily complex and confusing, but
    in reality, this is an abstraction that often makes computing far simpler and
    more reliable. Just like technologies that abstracted storage (like **Logical
    Volume Managers** and **RAID** systems) proved to be incredibly valuable once
    they were mature and understood, so has computer level virtualization.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们反复谈到接口以及某些东西如何消耗或呈现自己作为磁盘驱动器或文件系统，例如。hypervisor是一种呈现*计算机接口*的软件，这意味着它不仅仅呈现硬盘驱动器，而是像整个计算机一样运行。如果您从未使用过或思考过虚拟化，这可能看起来非常复杂和令人困惑，但实际上，这是一个通常使计算变得更简单和更可靠的抽象。就像抽象存储的技术（如**逻辑卷管理器**和**RAID**系统）一样，在其成熟和理解之后，计算机级虚拟化也被证明是非常有价值的。
- en: 'There are two types of hypervisors that we will talk about in this chapter,
    and they are simply known as Type 1 and Type 2 hypervisors. All hypervisors present
    the same thing: a *computer*. But what makes a Type 1 and a Type 2 hypervisor
    different is what they consume.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论两种类型的hypervisor，它们分别被称为Type 1和Type 2 hypervisor。所有的hypervisor都呈现同样的东西：一个*计算机*。但Type
    1和Type 2 hypervisor之间的区别在于它们消耗的内容。
- en: Type 1 hypervisor
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Type 1 hypervisor
- en: Sometimes called a *bare metal* hypervisor, Type 1 hypervisors are intended
    to run directly on the system hardware but, of course, can run on anything presenting
    itself as a capable piece of hardware (such as another hypervisor!) As such, a
    Type 1 Hypervisor is not an application and does not run on top of an operating
    system and so only needs to worry about hardware compatibility with the physical
    device on which it will be installed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有时称为*裸金属*hypervisor，Type 1 hypervisors旨在直接在系统硬件上运行，但当然也可以在任何表示自己为能够运行的硬件（如另一个hypervisor！）上运行。因此，Type
    1 Hypervisor不是应用程序，并且不运行在操作系统之上，因此只需要担心与将安装的物理设备的硬件兼容性。
- en: Type 1 hypervisors are generally the only type considered true ready for production
    because they install directly without any unnecessary software layers and so can
    be faster, smaller, and more reliable.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，Type 1 hypervisor被认为是真正适合生产的唯一类型，因为它们直接安装而无需任何不必要的软件层，因此可以更快、更小和更可靠。
- en: The Type 1 hypervisor was more difficult to initially engineer and so the earliest
    hypervisors were generally other types that could pass off work to the operating
    system. But effectively it was the introduction of the Type 1 hypervisor and enough
    vendors with disparate products to warrant a mature market designation that encouraged
    the extreme move to virtualization in the 2000s.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Type 1 hypervisor最初更难工程化，因此最早期的hypervisor通常是其他可以将工作传递给操作系统的类型。但事实上，是Type 1 hypervisor的引入和足够多的不同产品供应商使市场成熟，鼓励了在2000年代极端转向虚拟化。
- en: Type 2 hypervisor
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Type 2 hypervisor
- en: Unlike the bare-metal hypervisor, a Type 2 hypervisor is an application that
    you install onto an operating system. This means that the hypervisor has to wait
    for the operating system to give it resources, competes with other applications
    for resources, and requires that the operating system itself is stable, in addition
    to the hypervisor being stable, in order to keep workloads running on top of it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与裸金属hypervisor不同，Type 2 hypervisor是一个安装在操作系统上的应用程序。这意味着hypervisor必须等待操作系统提供资源，与其他应用程序竞争资源，并要求操作系统本身稳定，除了hypervisor稳定外，还需要使工作负载在其上运行。
- en: When virtualization was relatively new, especially in the microcomputer arena,
    Type 2 Hypervisors were much more common because they were cheaper and easier
    to make and have little need for hardware support to do what they do. A Type 2
    Hypervisor lets the bare metal operating system do the heavy lifting of supplying
    drivers and hardware detection, task scheduling, and so forth. So, in much of
    the 2000s we saw Type 2 Hypervisors taking a principal role in driving virtualization
    adoption. They are easy to deploy and very easy to understand and because they
    are just an application that gets deployed on top of an operating system anyone
    can just install one on an existing desktop or even laptop to try out virtualization
    for themselves.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当虚拟化技术相对较新时，尤其是在微型计算机领域，由于成本更低且更易于制造，并且几乎不需要硬件支持来完成其工作，因此类型 2 虚拟化监视程序更为常见。类型
    2 虚拟化监视程序让裸金属操作系统来处理供给驱动程序和硬件检测、任务调度等繁重工作。所以在 2000 年代的大部分时间里，我们看到类型 2 虚拟化监视程序在推动虚拟化技术采用中扮演了主要角色。它们易于部署，非常容易理解，因为它们只是一种在操作系统之上部署的应用程序，任何人都可以在现有的台式机甚至笔记本电脑上安装一个来尝试虚拟化技术。
- en: By the late 2000s, technology had changed rather significantly, the software
    had advanced and matured, and nearly all computers had gained some degree of hardware
    assistance for virtualization, allowing hypervisors to use less code while gaining
    much better performance. Type 1 hypervisors rapidly proliferated, and, before
    2010, the idea of using a Type 2 hypervisor in production was all but unthinkable.
    Type 1 hypervisors provide a single, standard operating system installation target,
    moving the heavy lifting away from the operating system and over to the hypervisor,
    where it is generally accepted to be better positioned. Because the hypervisor
    controls the bare metal, it is able to properly schedule system resources and
    eek maximum performance out of a system. Hypervisors are expected to be only a
    small fraction of the size of an operating system. This means little more than
    a shim between virtualized operating systems and the physical system (a tiny layer
    of code doing the bare minimum, and being essentially invisible to the operating
    system running on top of it). This minimizes bloat and features, while operating
    systems need to be large, complex, and feature-rich to do their jobs well in most
    cases.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到了 2000 年代末，技术发生了相当大的变化，软件得到了先进和成熟的发展，几乎所有计算机都获得了某种程度的虚拟化硬件支持，允许虚拟化监视程序使用更少的代码同时获得更好的性能。类型
    1 虚拟化监视程序迅速扩展，到了 2010 年之前，在生产环境中使用类型 2 虚拟化监视程序的想法几乎是不可想象的。类型 1 虚拟化监视程序提供了一个统一的标准操作系统安装目标，将大部分繁重工作从操作系统转移到监视程序，被普遍认为是更佳选择。因为监视程序控制裸金属，能够适当调度系统资源，并从系统中获得最大性能。监视程序预计只占操作系统大小的一小部分。这意味着它几乎只是虚拟化操作系统和物理系统之间的一个垫片（一层极少代码的基本功能层，对于运行在其上的操作系统基本不可见）。这最小化了膨胀和功能，而操作系统通常需要庞大、复杂和功能丰富以在大多数情况下完成其工作。
- en: Type 2 Hypervisors have proven to be useful in lab environments, especially
    for situations where testing or learning is best done from a personal computing
    environment such as a desktop or laptop or can be useful for special case temporary
    workloads where there is a need to completely disable or possibly even to remove
    the hypervisor when it is no longer needed. But for production server environments
    only Type 1 Hypervisors are really appropriate today.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 类型 2 虚拟化监视程序已被证明在实验环境中非常有用，尤其是在需要从个人计算环境（例如台式机或笔记本电脑）进行测试或学习的情况下，或者对于需要完全禁用或甚至在不再需要时可能移除监视程序的特殊临时工作负载也很有用。但对于生产服务器环境来说，只有类型
    1 虚拟化监视程序才真正合适。
- en: There are two best practices commonly associated with virtualization
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化技术通常与两项最佳实践相关联。
- en: Virtualize every system, unless a requirement makes you unable to do so. In
    practical terms, you will never realistically see a valid exception to this rule.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除非有要求使您无法这样做，否则应虚拟化每个系统。在实际操作中，您几乎永远不会真正看到这条规则的有效例外。
- en: Always use a Type 1 (Bare Metal) Hypervisor for servers.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器始终使用类型 1（裸金属）虚拟化监视程序。
- en: Hypervisor types are confusing
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟化监视程序的类型令人困惑。
- en: In the real world detecting what is and is not a Type 1 Hypervisor can be rather
    difficult. A hypervisor, by definition, really does not have any kind of end user
    interface of its own. This makes it something that we have to explain, but not
    something that we really see. Even a true operating system is hard to point to
    and say *see, there it is* because it is really a shell or a desktop environment
    running as an application on top of the operating system, rather than the operating
    system itself, that we see and touch. With a hypervisor, any interface that we
    see, of any sort, has to be being presented by something running on an operating
    system, not something running directly on the hypervisor.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，要检测什么是 Type 1 虚拟化管理程序可能相当困难。按定义，虚拟化管理程序实际上没有任何自己的最终用户界面。这使得我们必须解释它，但并不是我们真正看到的东西。即使是真正的操作系统也很难指出并说“看，它在这里”，因为我们真正看到和触摸的是作为应用程序在操作系统顶部运行的壳或桌面环境，而不是操作系统本身。对于虚拟化管理程序来说，我们看到的任何界面，无论是什么样的，都必须由运行在操作系统上的东西来呈现，而不是直接在虚拟化管理程序上运行的东西。
- en: Hypervisors, of course, need some sort of interface for us to interact with
    them. How they handle this varies wildly and not all hypervisors, even of the
    same type, are built the same. Under the hood, of course, they are always running
    on the bare metal, but they can use several different architectures to handle
    all of the functions that they need. Each different architecture has a different
    opportunity for how it will seem to appear to an end user – meaning that an end
    user sitting down to the system may experience wildly different interfaces that
    pretend to be things that they are or possibly are not.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，虚拟化管理程序需要某种接口来与我们互动。它们处理这个接口的方式差异很大，即使是同类型的虚拟化管理程序，构建方式也不尽相同。在幕后，它们总是在裸金属上运行，但它们可以使用几种不同的架构来处理它们所需的所有功能。每种不同的架构都有不同的机会，展示给最终用户的方式也不同
    - 这意味着一个坐到系统前的最终用户可能会体验到完全不同的界面，假装是它们或者可能并不是它们。
- en: In early Type 1 Hypervisors it was common to run a virtual machine (the name
    for a virtualized computer running on top of a hypervisor of any type) that had
    privileges to control the hypervisor given to it. This allows the hypervisor to
    be as lean as possible and allows big tasks like presenting a user interaction
    shell to be done using existing tools and no one had to reinvent the wheel. Using
    this approach meant that hypervisor engineering work was minimal in the early
    days allowing the virtualization itself to be the key focus.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期的 Type 1 虚拟化管理程序中，运行一个虚拟机（虚拟化管理程序顶部运行的虚拟计算机的名称）通常会赋予它控制虚拟化管理程序的权限。这使得虚拟化管理程序可以尽可能精简，并且可以使用现有工具完成像展示用户交互
    shell 这样的大任务，而无需重新发明轮子。采用这种方法意味着早期的虚拟化管理程序工程工作最小化，使得虚拟化本身成为关键关注点。
- en: As time has progressed, alternative approaches have begun to emerge. Running
    a full operating system in a virtualized environment on top of the hypervisor
    just to act as an interface for the end user felt like a waste of resources. Later
    hypervisors used creative ways to get around this making the hypervisor itself
    heavier but reducing the overall weight of the total system.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，开始出现了替代方法。在虚拟化管理程序顶部运行完整操作系统以充当最终用户接口似乎是资源的浪费。后来的虚拟化管理程序采用创造性方法来解决这个问题，使得虚拟化管理程序本身更重，但总体系统的重量减少。
- en: Virtualization tends to be confusing and vendors have little reason to want
    to expose the inner workings of their systems. It has therefore become commonplace
    to misuse terms or to suggest that hypervisors work differently than they do either
    for marketing reasons or to attempt to simplify the system for less knowledgeable
    customers. There are many hypervisors today and potentially more will arise in
    the future. In production enterprise environments, however, there are four that
    we expect to see with any regularity and we will briefly break down each one to
    explain how it works. No one approach is best, these are simply different ways
    to skin the same cat.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化往往令人困惑，供应商很少有理由想要揭示其系统的内部工作原理。因此，误用术语或建议虚拟化管理程序的工作方式与实际不同，无论出于营销原因还是为了试图简化对不太了解的客户的系统。今天有许多虚拟化管理程序，未来可能会有更多。然而，在生产企业环境中，我们预计会定期看到四种，我们将简要分析每种工作方式。没有一种方法是最佳的，这些只是解决同一个问题的不同方式。
- en: VMware ESXi
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VMware ESXi
- en: The market leader of virtualization today. VMware is one of the oldest virtualization
    products and has changed its design over time. Originally VMware followed the
    classic design of an extremely lean hypervisor and a *hidden* virtual machine
    that ran on top of it running a stripped down copy of Red Hat Enterprise Linux
    which provided the end user facing shell for interaction with the platform.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的虚拟化市场领导者。VMware 是最古老的虚拟化产品之一，随着时间的推移，其设计也有所变化。最初，VMware 遵循经典设计，采用了极简的超级监控程序和一个*隐藏*的虚拟机，其上运行了一个简化版的
    Red Hat Enterprise Linux，为用户提供了与平台交互的外壳界面。
- en: Today VMware ESXi instead builds a tiny shell into the hypervisor itself that
    provides only enough potential user interaction to handle the simplest of tasks
    such as detecting the IP address that is in use or setting the password. Everything
    else is handled through an API that is called from an external tool allowing for
    the heaviest portions of the user interface to be kept completely on the physical
    client workstation rather than on the hypervisor.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，VMware ESXi 将一个小型 shell 构建到超级监控程序本身中，仅提供足够的潜在用户交互能力来处理最简单的任务，如检测正在使用的 IP
    地址或设置密码。其他所有功能通过从外部工具调用的 API 处理，允许最重要的用户界面部分完全保留在物理客户工作站上，而不是在超级监控程序上。
- en: Microsoft Hyper-V
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Microsoft Hyper-V
- en: Although late to the enterprise Type 1 virtualization game, Microsoft opted
    for the classic approach and always runs a virtual machine in which is contained
    a stripped down copy of Windows which provides the graphical user interface that
    end users will see when having installed Hyper-V. This first virtual machine is
    installed automatically, by default requires no paid licensing, does not contain
    Windows branding, and is contrarily named the *physical* machine which together
    can make it appear that there is no VM at all, but rather a Type 2 hypervisor
    running on top of a Windows install, but this is not the case. It simply appears
    so based on tricky naming and the unnecessarily convoluted standard install processes.
    Doubt not, Hyper-V is a true Type 1 hypervisor running in the most classic way.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管晚于企业级 Type 1 虚拟化的竞争，微软选择了经典方法，始终运行一个虚拟机，其中包含一个简化版的 Windows，提供了安装 Hyper-V 后用户所见的图形用户界面。这个第一个虚拟机是默认自动安装的，不需要付费许可，不包含
    Windows 品牌，相反它被命名为*物理*机器，这使得它看起来好像根本没有虚拟机，而是在 Windows 安装之上运行的 Type 2 超级监控程序，但实际并非如此。这只是基于复杂的命名和不必要的混乱标准安装过程而看似如此。毋庸置疑，Hyper-V
    是一种真正的 Type 1 虚拟化监控程序，以最经典的方式运行。
- en: Xen
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Xen
- en: Coming from the same early era as VMware, Xen started with the classic approach,
    but, unlike Vmware, stuck with it over the years. However, unlike Hyper-V, Xen
    installations tend to be more manual, and the use of a first virtual machine for
    the purpose of providing end user interactions is not in any way hidden and, in
    fact, is completely exposed. What this means is that during the hypervisor installation
    process a virtual machine is created automatically (so it is always the first
    one) and that virtual machine is given special access to directly manipulate the
    console. So, what you see once it turns on is the console of the virtual machine
    itself as the hypervisor does not have one besides that.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与 VMware 同样来自早期时代的 Xen 也采用了经典方法，但与 VMware 不同的是，多年来一直保持这种方式。然而，与 Hyper-V 不同的是，Xen
    的安装过程更加手动化，用于提供用户交互的第一个虚拟机完全不被隐藏，事实上是完全暴露的。这意味着在超级监控程序安装过程中，一个虚拟机会被自动创建（因此它总是第一个），并且该虚拟机被特别授予直接操作控制台的特殊权限。因此，一旦启动，您看到的是虚拟机本身的控制台，因为超级监控程序除此之外没有其他控制台。
- en: You *can* even choose between different operating systems to use in the management
    virtual machine! In practice, however, Xen is always used with Linux as its control
    environment. Other operating systems are mostly theoretical. This exposure makes
    hidden classic systems, like Hyper-V, all the more confusing because in Xen it
    is so obvious how it all works.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*你*甚至可以选择在管理虚拟机中使用不同的操作系统！然而，在实践中，Xen 始终与 Linux 作为其控制环境一同使用。其他操作系统大多是理论上的。这种暴露使得像
    Hyper-V 这样的隐藏经典系统变得更加令人困惑，因为在 Xen 中，一切是如此显而易见。'
- en: Because Xen and Linux go together so tightly, it can be valuable for a Linux
    system administrator to have at least some knowledge of Xen, Xen management via
    Linux, and Xen architecture. This tight coupling does not ensure that systems
    and platform teams will become intertwined when using Xen, but it makes the likelihood
    higher.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Xen 与 Linux 紧密结合，对于 Linux 系统管理员来说，了解 Xen、通过 Linux 管理 Xen 和 Xen 架构至少有些知识可能很有价值。这种紧密结合并不意味着在使用
    Xen 时系统和平台团队会相互交织，但它增加了这种可能性。
- en: KVM
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KVM
- en: Finally, we come to the **Kernel-based Virtual Machine** (**KVM**). KVM is special
    for a few reasons. First because it uniquely takes the approach of merging the
    hypervisor into the operating system itself. And second because it does so with
    Linux. Unlike other hypervisors where you have a clear separation between the
    platform administration who manages the hypervisor and the system administrator
    who manages the operating system level. Here, the two roles must be merged into
    one because the two components have been merged into one. You cannot separate
    the operating system and the hypervisor when using KVM. KVM bakes the hypervisor
    right into the Linux kernel. It is simply part of the kernel itself and always
    there.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来谈谈**基于内核的虚拟机**（**KVM**）。KVM 之所以特别，有几个原因。首先，它独特地将虚拟化监视程序（hypervisor）合并到操作系统本身中。其次，它与
    Linux 一起工作。与其他虚拟化监视程序不同，其他程序中平台管理者负责管理虚拟化监视程序，系统管理员负责管理操作系统层面。而在这里，这两种角色必须合而为一，因为这两个组件已经合并为一个。在使用
    KVM 时，无法分开操作系统和虚拟化监视程序。KVM 将虚拟化监视程序直接嵌入 Linux 内核中。它只是内核的一部分，始终存在。
- en: This approach has clear benefits. It simplifies the entire system and provides
    most of the advantages of each different approach with relatively few caveats.
    There are caveats, of course, such as that there is a real risk of bloat in the
    hypervisor install which increases the potential attack surface. KVM's biggest
    benefit is probably that it leverages the ecosystem of Linux system administrators
    and existing knowledge so that some of the more complex aspects of managing a
    system that runs on bare metal such as filesystem and other storage architecture
    decisions, driver support, and hardware troubleshooting are all shared with Linux
    giving an enormous base platform and support network from which to begin.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法具有明显的好处。它简化了整个系统，并提供了几乎每种不同方法的优点，只有相对较少的注意事项。当然，也存在注意事项，例如虚拟化监视程序安装可能导致膨胀，增加潜在攻击面的真实风险。KVM
    最大的好处可能是利用 Linux 系统管理员和现有知识的生态系统，因此在管理运行在裸机上的系统的某些更复杂的方面，如文件系统和其他存储架构决策、驱动程序支持和硬件故障排除中，与
    Linux 共享，从而获得了一个庞大的基础平台和支持网络。
- en: Because of KVM's easy and well-known licensing (due to its inclusion in Linux),
    well known development potential, and broad availability of components it has
    become far and away the most popular means of building your own hypervisor platform
    when vendors want to create something of their own. Many large vendors in the
    cloud, hypervisor management, or hyperconvergence space have leveraged KVM as
    the base of their systems with customizations layered on top.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 KVM 的易用性和广为人知的许可（由于其包含在 Linux 中），以及广泛的组件可用性，当供应商希望创建自己的虚拟化监视程序平台时，KVM 已成为远远最受欢迎的选择。许多云端大型供应商、虚拟化监视程序管理或超融合空间已利用
    KVM 作为其系统的基础，并在其上进行了定制化。
- en: Is virtualization only for consolidation?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟化只是为了整合吗？
- en: 'Ask most people why you *bother* to virtualize and consistently the same answer
    is repeated: *Because it allows you to run several disparate workloads on a single
    physical device.* There is no doubt that consolidation is a massive benefit, when
    it applies, but stating this as the only, or even the key, benefit means we are
    missing the big picture.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人问你为什么*费心*虚拟化，答案总是相同的：*因为它允许你在单个物理设备上运行多个不同的工作负载。* 毫无疑问，当适用时，整合是巨大的好处，但将其陈述为唯一的甚至关键的好处意味着我们忽略了大局。
- en: The enduring myth that we virtualize to save money through consolidation is
    one that it seems no one is going to be able to dispel. The nature of virtualization
    is simply too complex for the average person, even the average IT professional,
    and what it really provides remains broadly misunderstood. The real value of virtualization
    is in the abstraction layer that it creates which provides a hedge against the
    unknown - a way to make system deployments more flexible, and more reliable, while
    incurring less overall effort. Virtualization gives you more options for that
    unknown event happening sometime in the future that you cannot plan for.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化为了节省成本通过合并的持久神话似乎没有人能够驱散。虚拟化的性质对于普通人，甚至是普通的IT专业人士来说都过于复杂，它真正提供的东西仍然被广泛误解。虚拟化的真正价值在于它创建的抽象层，这提供了对未知的避险方式
    - 一种使系统部署更灵活、更可靠，同时总体付出更少努力的方法。虚拟化为未来可能发生的未知事件提供了更多选项，这是你无法预计的。
- en: A core challenge for virtualization is that it offers simply too many benefits
    that do not always relate to one another. Most people want a simple, stable answer
    and do not want to understand how exactly virtualization works and why adding
    a layer of additional code actually makes systems simpler and more reliable. It
    is all a bit too much. The reality is that virtualization has many benefits, each
    of which is typically enough to justify always using it, and essentially no caveats.
    Virtualization has no cost, essentially no performance overhead, does not add
    management complexity (it does, but only in some areas while reducing it in others
    resulting in an overall reduction.)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于虚拟化的一个核心挑战是它提供了太多不一定相互关联的好处。大多数人希望得到一个简单、稳定的答案，不想理解虚拟化的工作原理以及为什么添加额外代码层实际上会使系统更简单、更可靠。这些都有点过于复杂。事实上，虚拟化有很多好处，每一个都通常足以证明始终使用它，并且基本没有任何限制。虚拟化几乎没有成本，基本没有性能开销，不会增加管理复杂性（确实会在某些领域增加，但在其他领域减少，从而总体减少）。
- en: Inevitably people will ask if special case workloads exist for which virtualization
    is not ideal. Of course special cases exist. But the next response from every
    IT shop on earth is to proclaim that they and they alone are unique in their server
    needs and that they are the one solitary case where virtualization does not make
    sense - and then they reliably state a stock and absolutely ideal workload for
    standard virtualization that applies to nearly everyone and is as far from a special
    case or an exemption from best practices as can be. Trust me, you are not the
    exception to this rule. Virtualize every workload, every time. No exceptions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不可避免地，人们会问是否存在虚拟化不适合的特殊工作负载。当然存在特殊情况。但地球上每个IT店铺的下一个反应是宣称他们独特且唯一，在他们的服务器需求中虚拟化不合理
    - 然后他们可靠地声明一种标准虚拟化的股票和绝对理想工作负载，适用于几乎每个人，并且远离特殊情况或免除最佳实践的例外。相信我，你不是这条规则的例外。每次都虚拟化每个工作负载。没有例外。
- en: Most examples that people give of why they avoid virtualization is normally
    examples of virtualization done wrong. Other, non-virtualization related mistakes,
    such as selecting a bad vendor, improperly sizing a server, or choosing a large
    overhead storage layer when something lean is needed could happen with or without
    virtualization. Everyone from the system administrator to the platform team to
    the hardware purchasers still have to do the same quality job that they would
    without virtualization. Virtualization is not a panacea but failing to be the
    silver bullet that removes the need to do our jobs well in no way excuses not
    using it every time. That is just bad logic.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人提到他们避免虚拟化的例子通常是虚拟化处理不当的例子。其他非虚拟化相关的错误，如选择不良供应商、错误大小的服务器或选择大幅超支的存储层而需要精简的情况，可能会发生，无论有无虚拟化。无论是系统管理员、平台团队还是硬件采购者，他们仍然必须像没有虚拟化时一样做同样高质量的工作。虚拟化并非万能药，但不能成为消除我们工作需要做好的银弹的借口。这只是错误的逻辑。
- en: Now we should have a good understanding of what virtualization really is, instead
    of simply a passing knowledge of its utility, and know why we use it for all production
    workloads. Virtualization should become second nature very quickly whether you
    are working in your home lab or running a giant production environment. Make it
    a foregone conclusion that you will virtualize and only worry about little details
    like storage provisioning and hypervisor selection or management tools. Next we
    look at virtualizations alternative and close cousin, containerization.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该对虚拟化有一个清晰的理解，而不仅仅是对其作用的浅显了解，并且知道为什么我们要将其用于所有生产工作负载。无论是在家庭实验室工作，还是在大型生产环境中运行，虚拟化应该很快成为第二天性。让虚拟化成为一个不言而喻的结论，只需要担心一些细节问题，比如存储配置、虚拟机监控程序选择或管理工具的使用。接下来，我们将看看虚拟化的替代方案及其近亲——容器化。
- en: Containerization
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器化
- en: Some people consider containers to be a form of virtualization, sometimes called
    Type-C virtualization or OS-level virtualization. In recent years, containers
    have taken on a life of their own and very specific container use cases have become
    such buzz-worthy topics that containers as a general concept have been all but
    lost. Containers, however, represent an extremely useful form of (or alternative
    to) traditional virtualization.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人认为容器是一种虚拟化形式，有时称为 C 类虚拟化或操作系统级虚拟化。近年来，容器已成为一种独立的存在，特定的容器使用场景成为了热门话题，以至于容器这一概念本身几乎被遗忘。然而，容器代表了一种极为有用的（或作为传统虚拟化的替代）形式。
- en: Container-based virtualization varies from traditional virtualization in that
    in traditional virtualization every aspect of system hardware is replicated in
    software by the hypervisor and exists uniquely to every instance or virtual machine
    (often called a Virtual Environment (VE) when talking about containers) running
    on top of it. There is nothing shared between the virtual machines and by definition
    any operating system that supports the hardware virtualized can run on it exactly
    as if it was running on bare metal.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基于容器的虚拟化与传统虚拟化不同，在传统虚拟化中，每个系统硬件的各个方面都由虚拟机监控程序以软件的形式进行复制，并且每个实例或虚拟机（在谈论容器时通常称为虚拟环境（VE））在其上运行时，都是唯一存在的。虚拟机之间没有任何共享的内容，按定义，任何支持虚拟化硬件的操作系统都可以在其上运行，就像它运行在裸机上一样。
- en: Container-based virtualization does not use a hypervisor at all, but rather
    is built from software that heavily isolates system resources in an operating
    system allowing individual virtual machines to be installed and operate as if
    they are fully unique instances, but behind the scenes all virtual machines running
    as containers share the host's kernel instance. Because of this, the ability to
    install any arbitrary operating system is limited as only operating systems capable
    of sharing the same kernel can be installed on a single platform.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于容器的虚拟化完全不使用虚拟机监控程序，而是由一款软件构建，这款软件在操作系统中进行强烈的资源隔离，允许安装并运行各个虚拟机，就好像它们是完全独立的实例一样，但在后台，所有作为容器运行的虚拟机共享主机的内核实例。由于这个原因，安装任何任意操作系统的能力受到限制，因为只能安装能够共享同一内核的操作系统到单个平台上。
- en: Because there is no hypervisor and only a single kernel shared between all systems,
    there is nearly zero overhead in most container systems making it perfect for
    many highly demanding tasks. Containers are especially popular in Linux where
    many container options exist today. The almost total lack of system overhead in
    container systems used to be a key feature of the approach, but as systems have
    moved from resource tight to having a surplus of power in many cases, the value
    of squeezing every last drop out of hardware has begun to wane in comparison to
    the greater flexibility and isolation of full virtualization. Because of this,
    what sounds like the greatest virtualization option ever is often overlooked or
    even forgotten about!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有虚拟机监控程序，且所有系统共享单一内核，大多数容器系统的开销几乎为零，这使其非常适合许多高要求的任务。容器在 Linux 环境中尤其受欢迎，今天有很多容器选项。容器系统几乎完全没有系统开销，曾经是这种方法的一个关键特性，但随着系统从资源紧张到在许多情况下具有丰富的计算能力，挤压硬件每一滴性能的价值开始减弱，相比之下，完整虚拟化所带来的更大灵活性和隔离性变得更加重要。因此，曾被认为是最伟大的虚拟化选项往往被忽视，甚至被遗忘！
- en: Containers do not require any special hardware support and are implemented completely
    in software allowing them to exist on a broader variety of platforms (it is easy
    to implement on old 32bit Intel hardware or a Raspberry Pi, for example) at lower
    cost. This made them important in the era before hardware acceleration was broadly
    available for full virtualization technologies.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 容器不需要任何特殊的硬件支持，完全通过软件实现，使得它们能够在更广泛的平台上运行（例如，很容易在老旧的 32 位 Intel 硬件或 Raspberry
    Pi 上实现），且成本更低。这使得它们在硬件加速广泛可用之前的时代变得非常重要。
- en: In the Linux world, which is what we care about, we can run many disparate Linux-based
    operating systems on a single container host because only the kernel needs to
    be shared. So, running virtual machines of Ubuntu, Debian, Red Hat Enterprise
    Linux, SUSE Tumbleweed, and Alpine Linux all on a single container host is no
    problem at all.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们关心的 Linux 世界中，多个不同的基于 Linux 的操作系统可以在同一个容器主机上运行，因为只需要共享内核。因此，在单个容器主机上运行 Ubuntu、Debian、Red
    Hat Enterprise Linux、SUSE Tumbleweed 和 Alpine Linux 等虚拟机毫无问题。
- en: Linux is blessed (and cursed) with a plethora of options for nearly every technology,
    and containerization is no exception. Multiple open source and commercial container
    products exist for Linux, but today the undisputed reigning champion is **LinuX
    Containers** (**LXC**). LXC is unique in that it is fully built into the Linux
    kernel so utilizing it is simply a matter of doing so, it does not require additional
    software or kernel modifications. If you are going to be implementing real containers
    on Linux, chances are it is going to be LXC. LXC is fully supported in nearly
    all Linux-based operating systems.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 拥有（也可能是个负担）几乎每个技术领域都具有大量的选项，容器化也不例外。Linux 上存在多个开源和商业容器产品，但今天无可争议的冠军是 **LinuX
    Containers**（**LXC**）。LXC 的独特之处在于它完全集成在 Linux 内核中，因此使用它其实只是简单地启用，不需要额外的软件或内核修改。如果你打算在
    Linux 上实现真正的容器，可能性很大就是 LXC。几乎所有基于 Linux 的操作系统都完全支持 LXC。
- en: A little history of containers
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器简史
- en: Full virtualization was introduced by IBM in the 1960s but proved to be complex
    and did not make it into general availability for mainstream servers for decades
    as high end hardware support and extensive special case software was necessary
    to make the magic happen until the very end of the 1990s.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 完全虚拟化由 IBM 在 1960 年代引入，但由于其复杂性，直到 1990 年代末期，借助高端硬件支持和大量特定的软件，才使其得以普及并进入主流服务器市场。
- en: Containers were first introduced in System 7 UNIX in 1979 using a mechanism
    called *chroot jails* which is rudimentary by today's standards, but is functionally
    pretty close to modern containers. In the UNIX world containers, of one type or
    another, have almost always been available. In 1999 what we might consider truly
    modern containers, starting with FreeBSD's Jails, were introduced and rapidly
    other UNIX platforms like Solaris with Zones and Linux with OpenVZ and then LXC
    began to emerge. By the mid-2000s containers were everywhere and quite popular
    before truly effective full virtualization had taken off.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 容器首次出现在 1979 年的 System 7 UNIX 中，使用的是名为 *chroot jails* 的机制，按今天的标准看起来相当简陋，但在功能上非常接近现代容器。在
    UNIX 世界中，容器（无论是哪种类型）几乎一直都有存在。1999 年，真正的现代容器可以说从 FreeBSD 的 Jails 开始引入，随后其他 UNIX
    平台如 Solaris 的 Zones 和 Linux 的 OpenVZ，最终 LXC 开始出现。到了 2000 年代中期，容器已经无处不在，并且在完全虚拟化技术真正起步之前就非常流行。
- en: Containers saw a Renaissance of sorts in 2013 with the introduction of Docker.
    Docker is not exactly a container, however, even though the term container has
    become more associated with Docker than with actual, true containers. Prior to
    Docker, containers were never really seen as being very sexy but rather basic
    process isolation workhorses doing a rudimentary security job for the operating
    system with the possible short lived exception of Solaris Zones which were heavily
    promoted for a short time in conjunction with the release of ZFS.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 容器在 2013 年经历了一次复兴，伴随着 Docker 的引入。尽管 Docker 并不完全是一个容器，但“容器”这个术语如今更多地与 Docker
    联系在一起，而非与真正的容器。早在 Docker 之前，容器从未被认为是特别吸引人，相反，它们更多是基本的进程隔离工具，承担着为操作系统执行初步安全工作的职责，唯一的例外可能是
    Solaris Zones，在 ZFS 发布时曾一度受到大力推广。
- en: Today, because of the popularity of Docker and its association with containers,
    the majority of people (even including system administrators!) think of Docker
    when someone mentions containers rather than true containers which have been around
    for decades.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，因 Docker 的流行及其与容器的紧密关联，大多数人（甚至包括系统管理员！）在提到容器时，会想到 Docker，而不是那些已经存在几十年的真正容器。
- en: We cannot talk about containers without mentioning Docker. Docker, today, is
    the name most associated with containers, and for good reason. Docker originated
    as a set of extensions built on top of LXC to provide for extreme process isolation
    with a *packaged* library environment for said applications. While Docker uses
    containers, first LXC and now their own container library, it itself is an application
    isolation environment providing a more limited range of services than something
    like LXC will provide. With LXC, you deploy an operating system (sans kernel)
    and treat it all but identically to traditional full virtualization. With Docker
    you are deploying an application or service, not an operating system. The scope
    is different and so Docker really finds itself more applicable to [*Chapter 5*](B16600_05_Final_ASB_ePub.xhtml#_idTextAnchor128),
    *Patch Management Strategies*. Because Docker is an application layer containerization,
    it would most often be run on top of a virtual machine in either full virtualization
    or containerization to provide its underlying operating system component.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能谈论容器而不提到 Docker。今天，Docker 是与容器最相关的名称，且这个称号当之无愧。Docker 最初作为一组构建在 LXC 之上的扩展而出现，为应用程序提供极端的进程隔离以及*打包*的库环境。尽管
    Docker 使用容器，最初是 LXC，现则是其自有的容器库，但 Docker 本身是一个应用隔离环境，提供的服务范围比 LXC 更为有限。使用 LXC，你可以部署一个操作系统（不包括内核），并且几乎将其视作传统的完全虚拟化。而
    Docker 则是部署应用或服务，而非操作系统。两者的范围不同，因此 Docker 更适用于[*第 5 章*](B16600_05_Final_ASB_ePub.xhtml#_idTextAnchor128)，*补丁管理策略*。由于
    Docker 是应用层容器化，它通常会运行在虚拟机上，无论是完全虚拟化还是容器化，以提供其底层操作系统组件。
- en: Containers, in general, represent a trusted, mature, and ultra-high performance
    virtualization option (or alternative.) While more limited in their capabilities
    (a Linux container host can only run Linux VEs, while FreeBSD VEs are not possible,
    for example) they are otherwise easier to maintain, faster, and generally more
    stable (there is simply less to go wrong.) They can be created faster, turned
    on or off faster, patched faster, are more flexible in their resource usage (they
    don't require the strict CPU and RAM assignments typically needed with full virtualization),
    need fewer skills, and have less overhead when running. The only significant caveats
    to containers are the inability to mix operating system workloads, or even kernel
    versions. If anything, that you do requires a specific kernel version (that is
    not uniform across the entire platform), or custom compilation of the kernel,
    a GUI, ISO or similar based full installs then containers simply are not flexible
    enough for you. But if you are dealing with a pure Linux environment where all
    workloads are Linux and can share the kernel, which is not that uncommon, then
    containers can be ideal for you.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，容器代表了一个受信任、成熟且超高性能的虚拟化选项（或替代方案）。尽管在能力上更为有限（例如，一个 Linux 容器主机只能运行 Linux VE，而
    FreeBSD VE 是不可能的），但它们更容易维护、更快速、通常也更稳定（因为出错的可能性更小）。容器可以更快地创建、启用或关闭、修补，资源使用上更灵活（它们不需要完全虚拟化所需的严格
    CPU 和内存分配），需要的技能较少，运行时开销更小。容器的唯一显著限制是无法混合操作系统工作负载，甚至内核版本。如果你所做的任何事情需要特定的内核版本（且该版本在整个平台中并不统一），或者需要自定义编译内核、GUI、ISO
    或类似的完整安装，那么容器显然无法满足你的需求。但如果你正在处理一个纯 Linux 环境，所有工作负载都是 Linux 且能够共享内核，这种情况并不罕见，那么容器将是理想选择。
- en: Containers, at least thus far, are not able to leverage the graphical interface
    of the operating system and so are relegated to server duties where pure text
    based (aka TTY) interfaces are used. Thus, they are not an option for graphical
    terminal servers or **virtual desktop instance** (**VDI**) deployments. Those
    kinds of workloads still need full virtualization until someone builds a workaround
    for that. But as that is generally not a heavily desired workload to support,
    there is little chance that someone is going to invest heavily in tackling that
    already easy to solve problem just to be able to say that they did it with containers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 至少目前为止，容器无法利用操作系统的图形界面，因此它们被限制用于纯文本界面（即 TTY）的服务器工作。因此，容器不是图形终端服务器或 **虚拟桌面实例**
    (**VDI**) 部署的可选方案。这类工作负载仍然需要完整虚拟化，直到有人为此开发出解决方法。但由于这通常不是一个高度渴望支持的工作负载，因此很少有人会在这个已经容易解决的问题上投入大量精力，只为了能够说他们用容器解决了它。
- en: Through the use of containers, you allow the operating system itself to act
    as a hypervisor, but one that is still the operating system as well and can be
    managed using all of the normal Linux tools and techniques because the system
    is still Linux. There is no separate hypervisor to learn or maintain. In this
    way the ability to leverage existing Linux skills and tools is very good.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用容器，你可以让操作系统本身充当虚拟机管理程序，但它仍然是操作系统，并且可以使用所有常规的 Linux 工具和技术进行管理，因为系统仍然是 Linux。你无需学习或维护单独的虚拟机管理程序。这样，利用现有的
    Linux 技能和工具的能力非常强大。
- en: It has to be noted that, because KVM and containers both use a standard, bare-metal
    Linux installation as their base, and because both are baked directly into the
    stock vanilla Linux kernel, it is not only possible but actually not uncommon
    for systems to run both full virtualization and containerization on the same host
    at the same time with vanilla Linux workloads running in containers. This ensures
    the lower overhead and extra flexibility and non-Linux (primarily Windows) or
    custom-kernel Linux systems running in full virtual machines on KVM. There is
    no reason to have to choose only one approach or the other if a blend is better
    for you. Container technology has become extremely popular and important today,
    but the use of containers in their traditional sense has dwindled heavily. This
    is mostly because of deep misunderstandings of the terminology and technology,
    which has caused it to be ignored even when it is highly appropriate. As a Linux
    system administrator especially, it may be very beneficial to consider containers
    instead of traditional virtualization in your environments. Having containers
    as another tool in your proverbial toolbelt makes you more flexible and effective.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 必须注意的是，由于 KVM 和容器都以标准的裸机 Linux 安装为基础，并且都直接集成在原生的 Linux 内核中，因此在同一主机上同时运行完整虚拟化和容器化并不罕见，且容器中运行原生的
    Linux 工作负载是完全可行的。这确保了较低的开销和更大的灵活性，并且非 Linux（主要是 Windows）或自定义内核的 Linux 系统可以在 KVM
    上作为完整虚拟机运行。如果你觉得混合使用更适合你的需求，就没有必要仅选择其中一种方法。容器技术如今已经变得极为流行且重要，但传统意义上对容器的使用已经大大减少。这主要是由于对术语和技术的误解，导致即使在非常合适的情况下，容器也常常被忽视。特别是作为
    Linux 系统管理员，考虑在你的环境中使用容器而非传统虚拟化，可能会非常有益。将容器作为你工具箱中的另一个工具，可以让你变得更加灵活和高效。
- en: Next, we will apply what we have learned about virtualization and containers,
    and add management, to learn about cloud computing.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将结合所学的虚拟化和容器知识，并加上管理方面的内容，来学习云计算。
- en: Cloud and VPS
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云与 VPS
- en: Any discussion of virtualization today is inevitably going to lead us to cloud.
    Cloud has become, that hot decade-long buzz-worthy concept that everyone wants,
    most people use, and no one has a clue what it is, what it means, or why anyone
    uses it. Few technologies are more totally misunderstood, yet widely talked about,
    than cloud. So we have a lot to cover here, much of it clearly up misconceptions
    and the misuse of terms.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 任何关于虚拟化的讨论，今天都不可避免地会引导我们进入云计算。云计算已经成为那个热衷了十年的流行词，人人都想要，大多数人都在使用，但却没有人知道它是什么，意味着什么，或者为什么要使用它。没有什么技术比云计算更加被误解，同时又被广泛谈论。因此，我们有很多内容需要讨论，其中大部分是为了澄清误解和滥用术语。
- en: The bizarre confusion of Cloud
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云的奇异困惑
- en: It is a rare combination of being vastly technical and non-applicable to normal
    business conversations while being constantly discussed as if it were a casual
    high level non-technical business decision at nearly all levels. Considering only
    a minuscule fraction of IT professionals have any serious grasp of what cloud
    is, and even fewer have a clear understanding of when to choose it, that the average
    non-technical mid-level manager will toss around the term as if they were discussing
    the price of postage stamps is mind-boggling. What do those people even think
    that they are discussing? No one truly knows. And I mean that, sincerely.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种罕见的组合，既是高度技术化的，又与正常的商业对话不相关，同时几乎在所有层级上都被讨论得像是一个随意的高层非技术性商业决策。考虑到只有极少数 IT
    专业人士对云有任何深入的理解，而且更少人能清晰地理解何时选择它，普通的非技术性中层经理人会把这个术语当作讨论邮票价格一样轻松地使用，真是让人难以理解。那些人到底以为自己在讨论什么？没人真正知道。我是说真的，真心的。
- en: Ask a group of people who have been throwing about the term cloud. Separate
    them so that they are not stealing each other's answers. Now, ask them to describe
    what cloud means to them. Mostly you will get gibberish, obviously. When you drill
    down, however, you will get a variety of descriptions and answers that are nothing
    like one another, and yet people listening to others discussing cloud will generally
    state that they believe that all of those people were meaning the same thing and
    often that *one thing* is something none of them actually meant, let alone more
    than one of them. Truly cloud means something different, random, and meaningless
    to nearly every human being with no rhyme or reason to it.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 问一群经常谈论云的人，把他们分开，避免彼此互相影响。现在，让他们描述云对他们来说意味着什么。你大多数时候会听到一些胡言乱语，显然如此。但当你深入探讨时，你会得到各种各样、彼此完全不同的描述和答案，而听别人讨论云的人通常会说，他们相信那些人说的都是在说同一件事，通常那个*一个东西*是他们都完全没有意思的，更别提是其中的一个人了。实际上，云对几乎每个人来说意味着不同、随机且毫无意义的东西，没有任何规律可言。
- en: If cloud as a term had a musical equivalent, it would be Alanis Morisette's
    Ironic, the song where the only thing ironic about it is the title. The term cloud
    is the same, everyone uses it, no one knows what it means. Just like ironic.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果云这个术语有一个音乐对应物，那就是阿拉尼斯·莫里塞特（Alanis Morissette）的《Ironic》，这首歌的唯一讽刺之处就是标题。云这个术语也是如此，人人都在用，没人知道它是什么意思。就像“讽刺”一样。
- en: For quite some years, cloud was commonly used to mean *hosted*. Simply replacing
    a long established, well known industry term with another one for no good reason.
    Of course, cloud means nothing of the sort. This horrific misinterpretation led
    to the meme of *there is no cloud, just someone else's computer*. Of course, even
    a passing knowledge of cloud would make one cringe to hear someone state something
    so profound while getting what cloud means so incredibly wrong.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，云通常被用来表示*托管*。简单地用另一个没有任何理由的新词替代一个已建立的、广为人知的行业术语。当然，云根本不意味着那样。这种可怕的误解导致了*没有云，只有别人的计算机*的迷因。当然，即使稍微了解一下云的人，听到有人说出这种话时，也会感到羞愧，因为他们完全误解了云的含义。
- en: Today, you are more likely to hear cloud used to mean *built with HTML*, I kid
    you not. Or sometimes it means *platform independent*. Other times it means *subscription
    pricing*. You name it, and cloud has been used to refer to it. The only thing
    you can be confident in is that no one, ever, actually means cloud. It could mean
    almost anything else, but it never means what the term actually is meant to refer
    to.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，你更有可能听到“云”用来表示*由 HTML 构建*，我不是在开玩笑。有时它意味着*平台独立*，其他时候它表示*订阅定价*。你可以列出所有可能的含义，而“云”已经被用来指代其中任何一个。你唯一可以确定的是，没人，永远不会，真正意味着云。它可能意味着几乎任何其他东西，但从来不意味着这个术语实际所指的内容。
- en: The only benefit to the mass hysteria around cloud definitions is that no alternative
    definition is used commonly enough to rise and overtake true cloud. The problem,
    however, is so bad that there is no reasonable way for you, as an IT professional,
    to use the term *cloud* with anyone except a truly well read and trusted technical
    colleague that you know actually knows what it means.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 围绕云定义的广泛狂热唯一的好处是，没有哪个替代定义足够普遍，能够崛起并超越真正的云。然而，问题已经严重到一个程度，你作为 IT 专业人士，除非和一个真正博学且值得信赖的技术同事交流，否则无法和任何人使用*云*这个术语，因为你知道他们真的知道它是什么意思。
- en: What is truly amazing is that the use of *cloud* has replaced terms like *synergy*
    as the default joke in business – that is, as a term only used by those who are
    truly lost. It is such common knowledge that *cloud* is complex and completely
    misunderstood that you can never use it to communicate an idea. It has become
    a standard example of a *marker* in the language for someone who is just spouting
    off management-speak without having any idea what they are saying and not realizing
    that everyone else is silently laughing at their ineptitude, and yet you hear
    it repeated almost constantly! No matter how much everyone knows that they are
    misusing it, somehow it remains addictive and in constant use.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 真正令人惊讶的是，*云*的使用已经取代了像*协同*这样的术语，成为商业中默认的笑话——也就是说，只有那些真正迷失的人才会使用这个术语。*云*复杂且被完全误解，已经成为如此常识，以至于你无法用它来传达一个想法。它已经成为一种语言中的*标志*，用来指代那些仅仅是在说管理术语，却完全不明白自己在说什么的人，而他们没有意识到其他人正默默地嘲笑他们的无能，但你几乎能听到它不断地被重复！无论每个人都知道他们在误用它，*云*依然具有一种上瘾的特性，并持续不断地被使用。
- en: One of the most important takeaways from this seeming rant (and rant it is)
    is that you cannot use the term cloud to any but the most elite professionals
    and you cannot explain cloud to any but well educated IT professionals. Avoid
    using the term because, no matter how much you think that you can use it in the
    same mistaken way that everyone else is using it, you cannot. There is no way
    to use cloud in a way that can be understood because everyone believes that they
    know what it means, even though they all think that it is something unique.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从这番看似抱怨的话（确实是抱怨）中，你应该记住的最重要的一点是，只有最顶尖的专业人士才能理解“云”这个术语，而只有受过良好教育的IT专业人士才能理解云计算。避免使用这个术语，因为无论你认为自己能像别人一样错误地使用它，你都不能。没有任何方式能正确使用云这个词，因为每个人都认为自己知道它是什么意思，尽管他们都认为它是某种独特的东西。
- en: When you absolutely must refer to cloud for some reason, use more complete terms
    like *cloud computing* or *cloud architecture* to clarify that you actually mean
    cloud and not just throwing out a word for the listener to interpret at will.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当你必须出于某种原因提到云计算时，使用更完整的术语，比如*云计算*或*云架构*，以澄清你实际上是指云，而不是随便抛出一个词让听者自行解读。
- en: In many ways, it is almost easier to describe cloud by what it is not, rather
    than what it is, because everyone thinks that it is one thing or another. Cloud
    has no association with hosting, none with the web, not even any with the Internet
    (the thing sometimes referred to as THE cloud, as opposed to A cloud.) We cannot
    here go into all details explaining every possible aspect of cloud computing,
    nor would much of it be applicable as the majority of cloud is not related to
    systems, and therefore not related to Linux. But we should address, to a small
    degree, what it means to the Linux system administrator, when it is applicable,
    and so forth.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从很多方面来看，描述云计算几乎比描述它是什么更容易，尤其是因为每个人都认为它是某种东西。云计算与托管无关，与网页无关，甚至与互联网无关（有时互联网被称为“云”，而不是“一种云”）。我们不能在这里详细解释云计算的每一个可能方面，也没有必要，因为大多数云计算与系统无关，因此也与Linux无关。但我们应该在一定程度上讨论它对Linux系统管理员的意义，何时适用等问题。
- en: First, we must start with the **NIST** (the **National Institute of Standards
    and Technology** in the USA) cloud abstract definition which works from Amazon's
    original definition. Always keep in mind that cloud computing is a real, strict,
    technical term created by Amazon for a real-world architecture and therefore has
    a strict, non-fungible definition and no amount of misuse or misunderstanding
    or attempts to co-opt its use by others changes that it means an extremely specific
    thing. It is common for those who do not understand cloud to argue that it is
    a loose term that can mean what you want it to mean, but it is not. That is simply
    not the case, it is not a random English language word making its way into the
    lexicon organically, it was defined carefully within the industry before first
    use.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须从**NIST**（美国**国家标准与技术研究院**）的云计算抽象定义开始，这一定义基于亚马逊最初的定义。始终牢记，云计算是一个真实的、严格的技术术语，由亚马逊为真实世界的架构所创造，因此它有一个严格、不可替代的定义，任何误用、误解或其他人试图篡改它的使用都不会改变它所指代的极其具体的事物。那些不了解云计算的人常常争辩说它是一个可以按自己理解的方式使用的宽泛术语，但事实并非如此。它不是一个偶然进入词汇表的随机英语单词，它在首次使用前就在行业内被精确地定义过。
- en: 'The NIST definition is as follows: *Cloud computing is a model for enabling
    ubiquitous, convenient, on-demand network access to a shared pool of configurable
    computing resources (e.g., networks, servers, storage, applications, and services)
    that can be rapidly provisioned and released with minimal management effort or
    service provider interaction. This cloud model is composed of five essential characteristics,
    three service models, and four deployment models.*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: NIST的定义如下：*云计算是一种模型，用于启用无处不在、便捷的、按需的网络访问，以共享的可配置计算资源池（例如，网络、服务器、存储、应用程序和服务）为基础，这些资源可以在最小的管理工作或服务提供者交互下迅速配置和释放。这个云模型由五个基本特征、三种服务模型和四种部署模型组成。*
- en: The most important parts of this definition to us, as system administrators,
    are the parts that include *pool of resources*, with *rapid provisioning and release*.
    So shared resources (meaning servers, CPUs, RAM, storage, networking) that we
    can create and destroy access to quickly. While cloud certainly means more than
    that, those are the basics. If you immediately thought to yourself *that sounds
    a lot like what virtualization already does*, you are correct, there is an extreme
    degree of overlap, and virtualization is the key building block of cloud computing
    (both full virtualization and/or containers.) If you thought to yourself *wait,
    pooled resources that I can build AND destroy rapidly - those do not sound like
    useful characteristics to me or any environment I have worked in previously*,
    you are also correct. Cloud computing is not logically applicable to traditional
    workloads or environments, it is designed around extremely specific needs of purpose-built
    application architectures that few businesses are prepared to leverage on any
    scale.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们作为系统管理员来说，定义中最重要的部分是涉及*资源池*和*快速配置与释放*的部分。因此，我们可以快速创建和销毁的共享资源（意味着服务器、CPU、RAM、存储、网络）。虽然云计算的定义远不止这些，但这些是最基本的。如果你立刻想到“*这听起来像是虚拟化已经做的事情*”，你是对的，确实有很大的重叠，虚拟化是云计算的关键构建模块（包括完全虚拟化和/或容器）。如果你想到“*等一下，资源池我可以快速构建和销毁—这些特征对我或我以前工作过的任何环境来说都不太有用*”，你也是对的。云计算并不适用于传统工作负载或环境，它是围绕着特定用途的应用架构设计的，这种架构只有少数企业能够在任何规模上加以利用。
- en: What makes the use of *cloud* more confusing is that many vendors (and quite
    logically at that) use cloud themselves as a component of their products. So,
    when you ask your vendor if a product is cloud-based, they might be answering
    if they are providing you cloud itself as a product, or they might be answering
    if they use cloud in the building of the tool somewhere under the hood. The two
    are both applicable to how most people ask the question, and since no one knows
    what cloud really is, no one is sure what you are really asking or want to know.
    Let me give a contrived, but reasonable, example.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使得*云*的使用更加混淆的是，许多供应商（并且在逻辑上是合理的）自己将云作为其产品的一个组成部分。因此，当你问你的供应商某个产品是否基于云时，他们可能是在回答是否将云作为产品本身提供给你，或者他们可能是在回答是否在工具的构建过程中某个地方使用了云。这两种情况都适用于大多数人提问的方式，而且由于没人真正知道云是什么，没人确切知道你到底在问什么或想要了解什么。让我举一个牵强但合理的例子。
- en: If I were to purchase a legacy application, say a client-server application
    that uses MS SQL Server and an old Delphi (Objective Pascal) front end and then
    use a true cloud product to create a virtual machine and deploy the server-side
    components so that we can truly say that we built the solution on a cloud. Yet
    the resultant product is not cloud computing, in any sense. Just because one piece
    of an architecture is built on cloud does not imply that the final product is
    cloud. Cloud is a layer in the stack.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我购买一个遗留应用程序，比如一个使用MS SQL Server和旧版Delphi（Objective Pascal）前端的客户端-服务器应用程序，然后使用一个真正的云产品创建虚拟机并部署服务器端组件，这样我们就可以真正说我们是在云上构建了解决方案。然而，最终的产品从任何角度看都不是云计算。仅仅因为架构的某一部分是在云上构建的，并不意味着最终产品就是云。云只是堆栈中的一层。
- en: For us, as system administrators, we are concerned with the type of cloud knows
    as **Infrastructure as a Service** (**IaaS**). That's a fancy way of saying cloud-based
    virtual machines. Other types of cloud, like **Platform as a Service** (**PaaS**)
    and **Software as a Service** (**SaaS**), are very important in the cloud space
    but exist only when the system administrator is *somewhere else*. If we are the
    system administrator for PaaS or SaaS then, to us, cloud is the workload, and
    it is not cloud to us. If we are not the system administrator for a PaaS or SaaS
    system, then it is of no concern to us as system administrators to talk about
    those systems as they do not apply to our role.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们这些系统管理员来说，我们关心的是一种叫做**基础设施即服务**（**IaaS**）的云类型。这是一个高级的说法，实际上指的是基于云的虚拟机。其他类型的云，如**平台即服务**（**PaaS**）和**软件即服务**（**SaaS**），在云计算领域非常重要，但只有当系统管理员不在时才存在。如果我们是PaaS或SaaS的系统管理员，那么，对我们而言，云就是工作负载，而不是云。如果我们不是PaaS或SaaS系统的系统管理员，那么作为系统管理员，我们不需要讨论这些系统，因为它们与我们的角色无关。
- en: From our systems perspective, cloud is almost like an advanced, flexible virtualization
    layer. Of course, like virtualization, we may find ourselves tasked with being
    the ones to implement the cloud platform. That is a completely different animal
    and worthy of a book (or two) on its own. But in practical terms system administrators
    may consume platform resources from a hypervisor, a container engine, or either
    one orchestrated through a cloud interface. To us it is all the same - a mechanism
    on which we deploy an operating system. So, from a pure system administrator perspective,
    think of cloud computing no differently than any other virtualization because
    it is exactly the same. The only difference is how it is managed and handed off
    to us.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们系统的角度来看，云几乎就像是一个先进的、灵活的虚拟化层。当然，像虚拟化一样，我们可能会被要求实施云平台。这是完全不同的一回事，值得写一本书（甚至两本）。但从实际角度来看，系统管理员可能会从一个虚拟机监控器、容器引擎或通过云接口编排的任何一个平台中获取资源。对我们来说，这一切都是一样的——它们都是我们部署操作系统的机制。因此，从纯粹的系统管理员角度来看，可以把云计算看作与任何其他虚拟化技术没有区别，因为它们本质上是一样的。唯一的区别是它是如何管理的，以及如何交由我们处理。
- en: In practical terms, we likely have to be heavily involved in decision making
    around the recommendation for the use of cloud versus other paths to acquire virtualization.
    Like any business decision, this simply comes down to evaluating the performance
    and features offered at a price point and comparing to the performance and features
    at the same price point with other options. It is that simple. But, with cloud,
    due to all the reasons that we mentioned before, we are often fighting against
    a mountain of misinformation and a belief in magic. So, we need to talk a little
    about cloud in a way that we should not have to simply because these misunderstandings
    are so common and deeply rooted.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际操作来看，我们可能需要深度参与有关是否使用云计算与其他虚拟化获取路径的决策。像任何商业决策一样，这归根结底是评估在一个价格点下所提供的性能和特性，并将其与同一价格点下其他选项的性能和特性进行比较。就是这么简单。但是，鉴于我们之前提到的所有原因，在云计算方面，我们往往需要与大量的误解和对“魔法”的信仰作斗争。因此，我们需要稍微谈一谈云计算，尽管我们本不应该这么做，因为这些误解如此普遍且根深蒂固。
- en: First, there is a belief that cloud computing is cheap. And while in special
    cases cloud computing will potentially save a lot of money, this is rarely the
    case. Cloud computing is typically an extreme price premium product chosen because
    it allows a great degree of flexibility so that less is needed to be purchased
    overall to meet the same needs. Cloud computing is very expensive to provide and
    so vendors are forced to charge more than for other architectures in order to
    provide it to customers (keep in mind that the *vendor* might be in your internal
    cloud department, nothing about cloud implies an external vendor.)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，有一种观念认为云计算是便宜的。虽然在特殊情况下，云计算可能节省大量资金，但这很少发生。云计算通常是一种极其昂贵的产品，之所以被选择，是因为它提供了很大的灵活性，从而可以减少整体购买量以满足相同的需求。云计算的提供成本非常高，因此供应商被迫收取比其他架构更多的费用以向客户提供（请记住，*供应商*可能是你们内部的云部门，云计算并不意味着外部供应商）。
- en: Horizontally scalable elastic workloads
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 水平可扩展的弹性工作负载
- en: Attempting to describe what workloads cloud computing was built to address can
    be a challenge for those not already familiar with certain types of application
    architecture. We must take an aside and dive into some application concepts here
    to really understand how we related as the systems team and to see why different
    approaches play such a large role in our platform decision making at this level.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试描述云计算所应对的工作负载对于那些不熟悉某些类型应用架构的人来说可能是一项挑战。我们需要稍作停顿，深入探讨一些应用概念，才能真正理解我们作为系统团队是如何相关的，并看到为什么不同的方案在这个层次上对我们的平台决策起着如此重要的作用。
- en: In a traditional application design the expectation is that we will run the
    entire application on just one or maybe just a few operating system instances.
    Typically, one instance would be used as the database server and one as the application
    server. More roles might exist, and you could have a redundant database server
    or similar, but essentially the number of instances usable was quite limited and
    static. Once deployed, the number of instances would not change. In many cases
    the entire application would exist on a single operating system instance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的应用设计中，期望是将整个应用程序仅运行在一个或几个操作系统实例上。通常，一个实例会作为数据库服务器，另一个作为应用服务器。可能会有更多的角色，且你可能会有冗余的数据库服务器或类似的配置，但本质上，可用的实例数量是相当有限且静态的。一旦部署，实例的数量就不会再变化。在许多情况下，整个应用程序会存在于一个单一的操作系统实例中。
- en: Scaling a traditional application mostly focuses on increasing the power of
    a single operating instance. This might be done through some combination of faster
    CPUs, more CPUs, more CPU cores, more memory, more storage, or faster storage.
    Or as we would typically say, if you need your server to do more, you need a bigger
    server. This kind of performance improvement can go a really long way as top end
    servers are very powerful, and few companies need to run any workloads that exceed
    the performance capabilities of a single large server. This style of scaling is
    called vertical scaling, meaning that we improve the performance of the single
    thread or server *within the box*. This kind of scaling is by far the easiest
    to do and works for any kind of application no matter how it is designed (this
    is how you improve video game performance or any desktop workload).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展传统应用程序主要侧重于提高单个操作实例的能力。这可能通过更快的CPU、更多少量的CPU、更多的CPU核心、更大的内存、更大的存储或更快的存储来实现。或者，正如我们通常所说的，如果你需要服务器做更多事情，你就需要一台更强大的服务器。这种性能提升方式能够产生很大的效果，因为高端服务器非常强大，且很少有公司需要运行超出单个大型服务器性能能力的工作负载。这种扩展方式被称为垂直扩展，意味着我们在*框架内*提升单线程或单个服务器的性能。这种扩展方式无疑是最容易实现的，并且适用于任何类型的应用程序，无论其设计如何（这就是你提升视频游戏性能或任何桌面工作负载的方式）。
- en: For most people, workloads designed for vertical scaling are the only types
    of workloads that they know. Of course, for end users working on desktops, everything
    is vertical. Even system administrators almost exclusively have to oversee applications
    that are built for this kind of scaling only. Nearly all deploy in house applications
    assume that this is how you will scale and only recently do many developers know
    alternatives well and still many (potentially most) still do not, even though
    those that do are the more prominent in media.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对大多数人来说，专为垂直扩展设计的工作负载是他们唯一了解的工作负载类型。当然，对于在桌面上工作的终端用户而言，一切都是垂直的。即使是系统管理员，也几乎完全需要管理仅为这种扩展方式构建的应用程序。几乎所有内部部署的应用程序都假设这就是你扩展的方式，直到最近，许多开发人员才较为熟悉其他的替代方案，而且仍然有很多（可能大多数）人并不熟悉，尽管那些熟悉的人在媒体上更为突出。
- en: The alternative approach is to design applications that allow for the application
    to scale by adding additional, isolated operating instances. For example, running
    multiple database server instances (likely in a cluster) not just for resilience,
    but also performance. Running multiple application servers with the ability to
    simply add more operating system instances running the application while keeping
    each individual instance small. In a traditional application architecture, we
    might require a single application server with four high performance CPUs and
    1TB of RAM to handle our application workload. A horizontally scalable application
    might use sixteen smaller servers each with 64GB of RAM and a smaller CPU to handle
    the same load. Traditionally we would say that our systems *scaled up*, but in
    adding more instances we say that they *scale out*. Now, of course, you can always
    scale both *up and out* which would mean increasing the resources of each individual
    instance while also increasing the number of instances.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是设计能够通过添加额外、独立的操作实例来扩展的应用程序。例如，运行多个数据库服务器实例（可能在一个集群中）不仅仅是为了提高弹性，同时也为了提高性能。运行多个应用程序服务器，可以通过简单地添加更多的操作系统实例来运行应用程序，同时保持每个单独实例的规模较小。在传统的应用程序架构中，我们可能需要一台拥有四个高性能CPU和1TB内存的应用服务器来处理我们的应用负载。一个水平可扩展的应用程序可能会使用16台较小的服务器，每台服务器有64GB内存和较小的CPU来处理相同的负载。传统上，我们会说我们的系统是*向上扩展*，但通过添加更多实例，我们说它们是*向外扩展*。当然，你总是可以同时进行*向上和向外扩展*，这意味着增加每个单独实例的资源，同时也增加实例的数量。
- en: As you can imagine, few applications that we work with in the real world as
    end users or as system administrators are designed for or could leverage horizontal
    or *scale out* platforms effectively, if at all. It requires that the application
    architectures, analysts, and developers plan for this style of deployment from
    the very beginning. And no amount of planning or desire makes every workload capable
    of scaling in this manner.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可以想象的那样，我们在现实世界中作为终端用户或系统管理员使用的少数应用程序设计上并没有考虑或能够有效利用水平扩展（*scale out*）平台，即使能做到也是有限的。这要求应用程序架构师、分析师和开发人员从一开始就为这种部署方式进行规划。而再多的规划或期望也不能使每个工作负载都能够以这种方式扩展。
- en: Some applications like common web-based business processes, most websites, email
    systems, and so forth are very conducive to this type of design and you can easily
    find or make these kinds of applications to take advantage of these resources.
    Other applications like financial processing or inventory control systems may
    struggle with the design limitations and take far more work to be able to work
    in this way, if at all.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一些应用程序，如常见的基于Web的业务流程、大多数网站、电子邮件系统等，非常适合这种设计，你可以轻松地找到或制作这些类型的应用程序，以利用这些资源。其他应用程序，如财务处理或库存控制系统，可能会因为设计上的局限性而面临困难，要能够实现这种扩展可能需要更多的工作，甚至有可能根本无法做到。
- en: Just because a workload is designed by the development team to be horizontally
    scalable does not mean that the workload itself will, however. This is easiest
    with a simple example. You create a website that helps people choose a healthy
    breakfast. You market to the United States. From 6am Eastern until about 2PM eastern
    (when California is wrapping up breakfast) you are really busy, but outside of
    those hours your website is really slow. But another website helps people choose
    food for any meal and is marketed worldwide. This second site never gets quite
    as busy as the first but stays roughly as busy all day long. The first site can
    leverage scalability, the second site cannot because its resource needed never
    really change.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅因为开发团队设计了一个工作负载，使其具备水平扩展性，并不意味着该工作负载本身就能实现这一点。这可以通过一个简单的例子来说明。你创建了一个帮助人们选择健康早餐的网站，你将其市场定位于美国。从东部时间早上6点到下午2点（即加州人吃完早餐时），网站非常繁忙，但在这些时间之外，网站运行非常缓慢。另一方面，另一个网站帮助人们选择任何一餐的食物，并且面向全球市场。这个第二个网站的访问量虽然没有第一个网站那么大，但全天保持着相对一致的流量。第一个网站可以利用扩展性，而第二个网站无法做到这一点，因为其所需资源几乎没有变化。
- en: 'The key advantages to horizontally scalable workloads are that they can be
    grown rapidly. Adding an additional operating system instance (or an additional
    one hundred!) is easy and non-disruptive. Adding more CPU or RAM to your existing
    server, is hard and slow by comparison. The next step of horizontal scaling is
    making it elastic. To be elastic, your system does not only have to scale out
    quickly but allow you to also scale back in quickly: that is to spin down and
    destroy unneeded operating system instances when capacity has changed. This is
    the unique proposition of cloud computing, to provide capacity on demand for elastic,
    horizontally scalable workloads so that you can use resources only when needed
    and stop using them when you do not.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 水平可扩展工作负载的关键优势在于它们可以迅速增长。添加一个额外的操作系统实例（或者额外的一百个！）是简单且不具破坏性的。相比之下，向现有服务器添加更多的CPU或RAM则既困难又缓慢。水平扩展的下一步是让它变得具有弹性。为了具备弹性，系统不仅要能快速扩展，还要能够在容量发生变化时迅速收缩：即在不需要时关闭并销毁不必要的操作系统实例。这正是云计算的独特优势，为弹性、水平可扩展的工作负载提供按需容量，使你可以在需要时使用资源，不需要时停止使用。
- en: Vertically scaled resources are far less expensive to provide than horizontally
    scaled ones. You can test this by trying to assemble several computers with roughly
    the same specifications. With only rare exceptions, it is a fraction of the cost
    to build a single large server than several smaller ones using real-world economics.
    A single system requires only a single operating system and application instance,
    but multiple systems require the overhead of the same operating system and applications
    loaded into memory in each case wasted many resources there, as well. It just
    takes less management power to oversee something that is *faster*, rather than
    many slower tasks. It is not unlike managing humans. It takes less overhead to
    manage one really fast, efficient employee than it does to manage several slower
    employees trying to coordinate doing the same work that the faster one was doing.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直扩展资源提供的成本远低于水平扩展资源。你可以通过尝试组装几台规格大致相同的计算机来验证这一点。除非在极少数情况下，构建一台大型服务器的成本是几台小型服务器的一小部分，这在实际经济学中是如此。单一系统只需要一个操作系统和应用实例，而多个系统则需要每个系统都加载相同操作系统和应用程序到内存中，浪费了许多资源。此外，管理一个*更快*的系统比管理多个较慢的系统要少得多。管理人力也是如此。管理一个高效、快速的员工比管理多个较慢的员工要少得多，后者需要协调完成同样的工作。
- en: So a horizontally scaled system, in order to make sense to choose, has to be
    able to leverage both scaling out, as well as in, have a workload use case that
    actually leverages this, and that does so to an extent large enough to overcome
    the lower cost, lower overhead, and great simplicity of traditional designs. Unless
    your workload meets all of those requirements, cloud computing should not be a
    consideration for you at all. It simply does not apply. And while contrary to
    how the media and trend-happy IT professionals want to portray cloud, it is only
    a small percentage of workloads that can effectively leverage cloud and only a
    small percentage of businesses have those rare workloads at all.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要使水平扩展系统成为一个合理的选择，它必须能够利用扩展和收缩两种方式，拥有能够有效利用这一点的工作负载使用场景，并且这一点要足够大，以克服传统设计的低成本、低开销和高度简化。除非你的工作负载满足所有这些要求，否则云计算根本不应成为你的考虑对象。它根本不适用。尽管媒体和趋势驱动的IT专业人士常常会误导人们认为云计算无所不在，但实际上，只有少数工作负载能够有效地利用云计算，而且只有少数企业拥有这些稀有的工作负载。
- en: Of course, we are talking IaaS aspect of cloud. Other cloud aspects where only
    the application portion is exposed to the business will often be cloud based.
    But this is essentially unrelated and certainly a decision process totally different
    from anything we would be looking at in a tomb such as this.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们谈论的是云计算的IaaS方面。在其他云计算领域，只有应用部分暴露给业务，通常是基于云的。但这与我们在这种场景中讨论的内容基本无关，决策过程也完全不同。
- en: Second, there is a belief that cloud computing is reliable. Absolutely nothing
    in cloud definitions or designs implies reliability in any way. In fact, this
    runs completely contrary to all standard cloud thinking. Cloud computing, because
    it is useful exclusively with scale out design, is built on the assumption that
    any redundancy or reliability is built into the application itself as scale out
    implies - as you have to have this inside the application itself in order for
    scale out to work properly. So including any redundancy at the system or platform
    level would be nonsensical and counterproductive. A basic understanding of cloud
    computing should, with any thought, make us surprised if anyone expected redundancy
    beyond the minimum at this level. In the real world, cloud computing resources
    tend to be far more fragile than traditional server resources for exactly this
    reason. Cloud computing assumes that either reliability is of trivial importance
    or that it is provided elsewhere in the stack. Cloud computing is just a building
    block of the resulting system, it is not in any way a complete solution by itself.
    Of course, theoretically, a high availability cloud provider could arise, but
    their cost and performance caveats would make it hard to compete in a marketplace
    driven almost entirely by price.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，存在一种观点认为云计算是可靠的。云计算的定义或设计中完全没有任何东西暗示它具有可靠性。事实上，这与所有标准的云计算思维完全相悖。云计算，由于它是专门为扩展设计而构建的，因此假设任何冗余或可靠性都必须内建于应用程序本身，因为扩展要求——你必须将这一点包含在应用程序内部，以便扩展能够正常工作。因此，在系统或平台层级包含冗余是没有意义且适得其反的。对于云计算的基本理解，应该使我们感到惊讶，如果有人期望在这一层级上提供超出最低要求的冗余。在现实世界中，云计算资源通常比传统的服务器资源更加脆弱，正是因为这个原因。云计算假设可靠性不是非常重要，或者它在技术栈的其他地方得到提供。云计算只是最终系统的一个构建块，绝不是一个完整的解决方案。当然，理论上，可能会出现一个高可用性的云计算服务商，但他们的成本和性能限制将使其很难在几乎完全由价格驱动的市场中竞争。
- en: Third, there is a belief that cloud computing is broadly applicable, that every
    company should be using it, and that it is replacing all other architectures.
    This is not true at all. Cloud has been around now for more than fifteen years
    (at the time of writing) and it made its inroads quite quickly towards the beginning
    of that cycle. Today, cloud computing is mature and well known. Companies and
    workloads that are going to move to cloud (or design for it) have largely already
    done so, and new workloads are created on cloud at a roughly constant rate. The
    industry saturation rate for cloud computing has been more or less achieved. Some
    new workloads will go there as older ones or anti-cloud holdouts retire or give
    in to other pressures. Some will come back as overzealous cloud fanboys and buzz-word
    driven managers learn their lessons of having gone to cloud without any understanding
    or planning. Sending standard workloads to cloud computing without redesign is
    typically costly and risky. But by and large cloud computing has already settled
    into a known saturation rate and the computing world is as it will be until another
    exciting paradigm shift occurs. Basically, what we see today in advanced bespoke
    internal software and grand scale multi-customer software is ideal for the cloud
    paradigm, and traditional workloads for single customers remain the most beneficial
    on traditional paradigms. This is all as originally predicted when cloud computing
    was first announced long ago.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，存在一种观点认为云计算具有广泛的适用性，认为每个公司都应该使用它，并且它正在取代所有其他架构。实际上，这完全不正确。云计算从现在开始已经存在超过十五年（在写作时），并且在那个周期的初期它就迅速渗透了市场。今天，云计算已经成熟并广为人知。那些将要迁移到云端（或为之设计）的大多数公司和工作负载已经完成了迁移，新的工作负载也在以大致稳定的速度在云端创建。云计算的行业饱和率或多或少已经实现。随着旧的工作负载退休或在其他压力下屈服，一些新的工作负载将会迁移到云端。有些工作负载会回流，因为一些过于热衷的云计算爱好者和受流行词驱动的管理者会从云端迁移回来，吸取没有理解或规划的教训。将标准工作负载迁移到云计算而不进行重新设计通常会带来高昂的成本和风险。但总体来说，云计算已经稳定在一个已知的饱和点，计算世界将保持现状，直到另一个激动人心的范式转变发生。基本上，我们今天在高级定制内部软件和大规模多客户软件中看到的东西非常适合云计算范式，而传统的单客户工作负载仍然在传统范式下最为有利。这一切都正如当初云计算首次宣布时所预测的那样。
- en: Using cloud does not require any specific skills or training, as many in the
    industry would like us to believe in order to sell certifications and training
    classes. In fact, just knowing what cloud truly is often enough to enable you
    to utilize it effectively. That said, individual cloud vendor platforms (such
    as **Amazon**'s **AWS** or **Microsoft**'s **Azure**) are so large and convoluted
    that there can be real value to getting vendor certifications and training to
    understand how to work with their product interfaces. But to be clear, the value
    in the training is learning how to work with the vendor in question, not in learning
    about cloud.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用云计算并不需要任何特定的技能或培训，尽管许多行业人士为了销售认证和培训课程，可能希望我们相信这一点。事实上，了解云计算的真正含义通常足以让你有效地使用它。也就是说，个别云供应商平台（例如**Amazon**的**AWS**或**Microsoft**的**Azure**）如此庞大且复杂，以至于获得供应商认证和培训在了解如何与他们的产品接口交互时确实具有实际价值。但需要明确的是，培训的价值在于学习如何与特定供应商合作，而不是学习云计算本身。
- en: That does not change the fact that most organizations seeking to get significant
    value out of cloud computing will most likely need to do so with deep vendor integrations
    that will almost certainly require an investment in specific vendor product knowledge.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不改变这样一个事实：大多数寻求从云计算中获得重大价值的组织，很可能需要通过深度的供应商集成来实现，而这几乎肯定需要对特定供应商的产品知识进行投资。
- en: Cloud is an amazing set of technologies that serves an incredibly important
    purpose. When your workload is right for cloud computing, nothing else can come
    close to it. Whether you build your own private cloud or use a public shared one,
    whether you host your cloud in house or let a hosting firm handle the data center
    components for you, cloud might be the right technology for some of your workloads.
    With your understanding here you should be able to evaluate your needs to know
    if cloud is likely to play any reasonable role, and be able to look at real work
    vendors and costs and make solid, math-based valuations of cloud in comparison
    to other options.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算是一组令人惊叹的技术，服务于一个极其重要的目的。当你的工作负载适合云计算时，其他任何东西都无法与之媲美。无论你是建立自己的私有云还是使用公共共享云，无论你是将云托管在内部还是让托管公司处理数据中心组件，云计算可能是某些工作负载的正确技术。通过你在这里的理解，你应该能够评估自己的需求，判断云计算是否可能发挥任何合理作用，并能够查看实际供应商和成本，进行与其他选项的比较，并做出基于数学的合理评估。
- en: Now that we know cloud computing, we can step back and look at the older concept
    of virtual private servers and see why they are so closely tied with, but not
    actually related to, cloud computing today.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了云计算，我们可以回顾一下虚拟专用服务器的旧概念，并探讨它们为何与今天的云计算紧密相关，但实际上并不相同。
- en: Virtual Private Servers (VPS)
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟专用服务器（VPS）
- en: Similar to and, on the Venn Diagram of things, nearly overlapping with IaaS
    cloud computing is the modern concept of virtual private servers or VPS. VPS actually
    predates cloud and comes from simpler virtualization (or containerization) allowing
    a vendor (which could be an internal department, of course) to carve out single
    virtual machines for customers from a larger, shared environment. Instead of needing
    to provide an entire server of their own, customers need only buy a small slice
    or set of slices of the vendor's server(s) to use for their needs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 与IaaS云计算类似，甚至在事物的维恩图中几乎重叠的现代概念是虚拟专用服务器（VPS）。VPS实际上早于云计算，源于更简单的虚拟化（或容器化）技术，允许供应商（当然也可以是内部部门）从更大的共享环境中为客户划分单独的虚拟机。客户无需提供完整的服务器，而只需购买供应商服务器的一个小片段或几个片段来满足他们的需求。
- en: As I mentioned, this sounds very similar to IaaS cloud that we just described,
    and certainly it is. So much so, that most people using IaaS cloud actually use
    a VPS aspect of it without realizing so. The idea behind VPS is to allow companies
    to purchase server-class resources at a fraction of the scale typically required
    to a single physical server. If you think back to our discussion on virtualization
    and how by using a hypervisor we might be able to take a single physical server
    and, for example, create one hundred virtual machines that run on top of it, each
    with their own operating system, then we could sell those resources to one hundred
    separate customers, each of which could run their own small server inside its
    own secure space. This allows small companies, or companies with small needs,
    to buy enterprise level datacenter and server hardware capacity and prices within
    any realistic budget.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我提到的，这听起来与我们刚才描述的IaaS云非常相似，实际上它确实是如此。实际上，许多人使用IaaS云时，实际上是在不自知的情况下使用其VPS方面。VPS背后的理念是允许公司以通常所需的物理服务器规模的很小一部分来购买服务器级别的资源。如果你回想一下我们讨论虚拟化时提到的，如何通过使用虚拟机监控程序（hypervisor），我们可能将一台物理服务器转换成一百台虚拟机，每台虚拟机运行自己独立的操作系统，那么我们可以将这些资源卖给一百个不同的客户，每个客户都能在自己的安全空间内运行一台小型服务器。这使得小公司或有小需求的公司能够以任何现实的预算购买企业级数据中心和服务器硬件的容量与价格。
- en: 'Before we go further, we need to do a quick breakdown and comparison of VPS
    against IaaS cloud to see why VPS is so commonly confused with cloud computing
    and why they often compete:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们需要快速进行一次VPS与IaaS云的对比分析，以了解为什么VPS与云计算如此常被混淆，并且它们为何经常相互竞争：
- en: '**First, the goals**: An IaaS Cloud''s goal is to provide rapid creation and
    destruction of resources on demand via automation - primarily used by the largest
    organization or workloads. The goal behind VPS is to carve up traditional server
    resources in such a way that they can be affordable to be used by small organizations
    and/or workloads. So, one goes after the biggest scale and the most complexity,
    the other after the smallest scale and least complexity. One expects custom engineering
    effort on both the application and infrastructure team''s sides where the other
    expects traditional applications and no special knowledge or accommodation from
    any team.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**首先，目标**：IaaS云的目标是通过自动化提供按需快速创建和销毁资源的能力——主要供最大型的组织或工作负载使用。VPS背后的目标是将传统服务器资源划分，以便小型组织和/或工作负载能够负担得起。一个追求的是最大的规模和最复杂的系统，另一个追求的是最小的规模和最少的复杂性。一个预计应用程序和基础设施团队双方都需要进行定制化工程工作，而另一个则期望传统应用程序，并且任何团队无需特殊知识或额外配合。'
- en: '**Second, the interface**: In cloud computing the expectation and purpose is
    for systems to be self-provisioning (and self-destroying.) Cloud is not designed
    for humans to have to interact manually to request resources, nor to configure
    them, nor to decide when more (or less) are needed, nor to destroy them when done.
    So, cloud''s focus is on APIs to allow software to handle provisioning. VPS is
    meant to work just like any normal virtual machine with a human initiating the
    build, installing the operating system, maybe configuring the operating system,
    and turning the VM off, and then deleting it when no longer needed. It is standard
    for cloud products to not offer any interaction directly with the virtualized
    hardware such as access to a console so any system requiring console level interaction
    (such as a GUI) are impossible. To qualify as a VPS console and GUI access are
    required to completely mimic a hardware device in a standard way. If you can use
    a normal server, you can use a VPS.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**其次，接口**：在云计算中，期望和目标是系统能够自我配置（并且自我销毁）。云并不是为了让人类手动交互来请求资源，也不是为了配置它们，也不是为了决定何时需要更多（或更少）的资源，也不是为了在完成后销毁它们。因此，云的重点是提供API，使软件能够处理资源配置。VPS的设计目标是像任何正常的虚拟机一样工作，由人工启动构建，安装操作系统，可能进行操作系统配置，并关闭虚拟机，最后在不再需要时删除它。云产品通常不提供与虚拟化硬件的任何直接交互，例如访问控制台，因此需要控制台级别交互（如图形用户界面）的一些系统是无法实现的。要成为VPS，必须提供控制台和图形用户界面访问，才能以标准方式完全模拟硬件设备。如果你能使用普通服务器，那么你就能使用VPS。'
- en: '**Third, the provisioning**: Cloud assumes a need for rapid provisioning. Of
    course, rapid is a relative term. But it is just expected that in a cloud ecosystem
    that systems must be able to go from first request to fully functional in minutes,
    and sometimes seconds. In the VPS world, while we always want everything available
    as quickly as possible, having to wait a few minutes before being given the access
    to begin a manual operating system install that could take potentially tens of
    minutes is common. We assume that a cloud instance will be created via software,
    but a VPS instance we assume will be created manually by a human.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第三，资源配置**：云服务假设需要快速配置。当然，“快速”是一个相对的概念。但在云生态系统中，系统必须能够在几分钟甚至几秒钟内从首次请求到完全功能的状态，这几乎是理所当然的。在VPS的世界里，虽然我们总是希望一切能尽快准备好，但在开始手动安装操作系统之前，等待几分钟的时间是很常见的，甚至可能需要几十分钟。我们假设云实例是通过软件创建的，而VPS实例则是由人工手动创建的。'
- en: '**Fourth, the billing**: Because the value of cloud computing is assumed to
    come from its ability to be created and destroyed rapidly in order to keep costs
    managed it follows that billing must be granular to accomplish this. To this end
    billing it generally handled in increments of minutes or possibly hours, or in
    other extremely short measurements like processor cycles. VPS will sometimes charge
    in these short increments but may easily use longer intervals such as daily or
    monthly as it is not a rapid create and destroy intended service. (We can say
    that cloud leans towards stateless and VPS leans towards stateful.)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第四，计费**：由于云计算的价值假设来源于其能够快速创建和销毁的特性，从而保持成本的可控性，因此计费必须是细化的才能实现这一点。为此，计费通常按分钟或可能是小时的增量来处理，或者以其他极短的单位如处理器周期来计算。VPS有时也会按这些短的时间增量收费，但也可以轻松使用较长的时间间隔，如按天或按月收费，因为它不是一个快速创建和销毁的服务。（我们可以说，云服务倾向于无状态，而VPS则倾向于有状态。）'
- en: What often makes VPS and IaaS Cloud harder to distinguish is that today, VPS
    providers almost always use IaaS Cloud as their own mechanism for provisioning
    the VPS under the hood, and most IaaS Cloud providers have opted to offer VPS
    additionally. This was bound to happen for two reasons. First, VPS providers use
    cloud because it is a really logical way to build a VPS (if you think about the
    requirements that the VPS *vendor* would have, they would sound a lot like what
    cloud is meant to do) and because by being built on top of cloud computing, you
    can advertise the VPS as being cloud in a sense and be a sort of *simple interface
    to cloud resources*. It makes sense for cloud providers to offer VPS because the
    majority of customers who look for cloud have no idea what it is and only use
    it for political, not business or technical reasons, so offering something simple
    that allows them to purchase from you and use your resources (because cloud is
    so hard and complex) allows you to capture the majority of revenue.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使VPS和IaaS云服务更难区分的一个原因是，今天几乎所有的VPS提供商都使用IaaS云服务作为其VPS配置的底层机制，而大多数IaaS云服务提供商也选择额外提供VPS。这种情况注定会发生，原因有两个。首先，VPS提供商使用云服务，因为这是一种非常合理的方式来构建VPS（如果你考虑一下VPS*供应商*的需求，它们听起来与云服务的目标非常相似），并且由于VPS是建立在云计算之上的，你可以将它作为云的一部分来宣传，并成为一种*简化的云资源接口*。对于云服务提供商来说，提供VPS也是有道理的，因为大多数寻找云服务的客户根本不知道它是什么，只是出于政治而非商业或技术原因使用它，因此提供一个简单的服务让他们可以从你这里购买并使用你的资源（因为云服务非常复杂难懂）能够帮助你捕获大多数的收入。
- en: Vendors like Amazon used to offer no VPS services and using their resources
    if you were not truly in need of cloud was difficult, at best. To address this,
    Amazon added LightSail as a VPS product layered on top of their cloud product.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 像Amazon这样的供应商曾经没有提供VPS服务，如果你并非真正需要云服务，使用他们的资源是非常困难的。为了解决这个问题，Amazon将LightSail作为VPS产品加入到他们的云产品之上。
- en: Other cloud providers, such as *Digital Ocean*, *Linode*, and *Vultr* use VPS
    as their primary product offering and focus on it almost entirely while keeping
    their cloud interface quietly to the side so that customers truly looking for
    cloud can find it, but those seeking cloud when they intended to use VPS will
    be able to get what they need right away.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其他云服务提供商，如*Digital Ocean*、*Linode*和*Vultr*，将VPS作为其主要产品，并几乎完全专注于它，同时将其云界面静静地放在一旁，让真正需要云服务的客户能够找到它，而那些原本打算使用VPS的人则能立即获得所需的服务。
- en: VPS is one of the most popular and effective ways for real world companies to
    run workloads, especially smaller companies, but companies of any size can leverage
    them. Cloud is effective but primarily for special case workloads. The majority
    of companies talking about already leveraging cloud are actually using VPS and
    not even aware that they missed cloud computing entirely.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: VPS是现实世界中企业运行工作负载的最受欢迎且最有效的方式之一，尤其是对小型公司而言，但任何规模的公司都可以利用它们。云计算是有效的，但主要适用于特殊情况的工作负载。大多数宣称正在利用云计算的公司实际上正在使用VPS，并且甚至没有意识到他们完全忽视了云计算。
- en: 'It is worth noting that when we talk about rates of cloud adoption we have
    a fundamental problem: no one knows what cloud is, including people who think
    that they are or are not using it currently! Vendors like Amazon can tell you
    how many customers that they have, but they cannot tell you if their customers
    are using their products as cloud or just using cloud in some other way. In a
    survey about cloud adoption you have zero reasonable chance that the person being
    asked about their adoption, the person doing the asking, and the person reading
    about the adoption rates all understand enough about cloud to answer or ask meaningfully
    and, in reality, generally none of them know at all what is being asked. So any
    information about cloud adoption rates border on being totally meaningless. There
    is no honest mechanism by which any person or organization could possibly know
    what the cloud ecosystem really looks like. You would get just as meaningful data
    if a group of squirrels surveyed a bunch of hamsters about astrophysics and then
    handed the results to a bunch of hyper puppies to interpret. People love reports
    and data and rarely care if the survey in question was real in any way.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，当我们谈论云采用率时，我们面临一个根本性的问题：没有人知道云计算是什么，包括那些认为自己当前在使用或没有在使用云计算的人！像亚马逊这样的供应商可以告诉你他们有多少客户，但他们不能告诉你这些客户是在将他们的产品作为云使用，还是以其他方式使用云计算。在关于云采用的调查中，几乎没有合理的机会，能够确保被询问的人、问卷调查的人以及阅读采纳率报告的人，三者对云计算的理解足够深入，能够有意义地回答或提问，实际上，通常他们都根本不知道被问到的问题是什么。所以，关于云采用率的任何信息都接近完全没有意义。没有任何真实的机制可以让任何个人或组织真正了解云计算生态系统的真实面貌。如果一群松鼠对一堆仓鼠进行天体物理学调查，然后把结果交给一群超级活跃的小狗来解读，你会得到同样有意义的数据。人们喜欢报告和数据，很少关心调查本身是否有任何实际意义。
- en: You should, at this point, feel both overwhelmed and depressed about the state
    of cloud understanding within both IT and business, but you as the Linux system
    administrator should now be prepared to explain it, evaluate it, understand what
    is built upon it, and know when and how to choose to use it for your own workloads
    and when to look at VPS instead.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能会对IT和业务领域中对云计算的理解状况感到既不知所措又沮丧，但作为Linux系统管理员的你，现在应该准备好去解释它，评估它，了解它所构建的基础，并知道何时以及如何选择将其用于自己的工作负载，何时则应考虑选择VPS。
- en: On premises, hosted, and hybrid hosting
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地部署、托管和混合托管
- en: Now that we have talked about so many aspects of the underlying components that
    are used to provide us with a platform on which to deploy an operating system,
    we can finally talk about where those systems should exist!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了这么多构成我们平台的底层组件，用以部署操作系统的基础，接下来我们终于可以讨论这些系统应该存在在哪里！
- en: This is, at least, the simplest of all our topics. Physical location is easy
    to explain, even if many businesses get confused about it in practice. Conceptually
    we really only think about two locations for a workload and that is as being either
    on premises or off premises. This can be a little convoluted, though, as companies
    own multiple locations so what is off premises to one site might be see as on
    premises to another. But we generally consider on premises to be all of a company's
    owned sites and off premises being any sites that are operated by a third party.
    Because of this we generally refer to off premises physicality as being hosted,
    as physical systems are being hosted on our behalf. However, there are reasons
    why this can prove to be very misleading.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，这些是我们讨论的最简单的主题。物理位置容易解释，尽管许多企业在实践中对此感到困惑。从概念上讲，我们通常只考虑两种工作负载位置，即本地部署和非本地部署。尽管如此，这可能有点复杂，因为公司拥有多个地点，所以对某一个地点来说是非本地的，在另一个地点可能就被视为本地的。然而，我们通常认为本地部署是指公司所有的站点，而非本地部署则指由第三方运营的任何站点。因此，我们通常将非本地物理性称为托管，因为物理系统是由我们委托托管的。然而，也有一些原因，这种说法可能会误导人。
- en: Most people assume that when a system is kept on premises that that also implies
    that it will be being operated by an internal team. This is most often true, but
    having on premises systems managed by third party teams is not unheard of, especially
    in very high performance or high security environments. For example, if you required
    Amazon's specific range of cloud computing products, but could not allows for
    any off-premises hosting, you can have Amazon operate an AWS cloud instance on
    your own premises. This is anything but low cost or simple and requires housing
    at minimum a small, self-contained data center and all of its associated staff!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人认为，当系统托管在本地时，这也意味着它将由内部团队操作。这通常是对的，但由第三方团队管理本地系统并不鲜见，尤其是在高性能或高安全性环境中。例如，如果你需要亚马逊特定范围的云计算产品，但又无法允许任何外部托管，你可以让亚马逊在你的本地运行AWS云实例。这远非低成本或简单，至少需要一个小型的自包含数据中心及其所有相关人员！
- en: 'Hosting gets more complicated in practice, but the core issue remains the same:
    the demarcation points. When we decide to start having our systems be hosted off
    premises the questions rapidly become about defining what portions of the hosting
    will be provided by the hosting provider and which by us.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 托管在实践中变得更加复杂，但核心问题始终如一：划分点。当我们决定将系统托管到外部时，问题迅速变成了定义哪些部分由托管服务提供商提供，哪些由我们自己提供。
- en: In its most extreme (and impractical sense) you could rent a house, office,
    or storage unit and provide your own rack, servers, Internet, cooling, power,
    and so forth as needed, but if we did this one would rightfully argue that we
    had basically made the site our own premises. Touche.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在最极端（也是不切实际）的情况下，你可以租一座房子、办公室或储藏室，并根据需要提供自己的机架、服务器、互联网、冷却、供电等，但如果我们这样做，可以说我们实际上已经将这个地方变成了我们的本地场所。说得好。
- en: Classically it was assumed that nearly all workloads should be run on premises.
    This was for the very simple reason that early business networks had no Internet
    connectivity so hosting elsewhere was effectively impossible or at least impractical.
    Follow that in the early days of the Internet wide area network links were slow
    and unreliable keeping remote servers almost unusable. And software was built
    around LAN networking characteristics, unlike today when enterprise software of
    any quality assumes that it needs to perform adequately over a long distance connection,
    most likely on the Internet.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，人们认为几乎所有工作负载都应该在本地运行。这是因为一个非常简单的原因：早期的商业网络没有互联网连接，因此将其托管到其他地方几乎是不可能的，或者至少不实际。再加上在互联网初期，广域网连接速度慢且不稳定，使得远程服务器几乎无法使用。而且，软件是围绕局域网网络特性构建的，与今天不同的是，今天任何优质的企业软件都假定它需要在长距离连接上（最有可能是互联网）有效地运行。
- en: Because of these old assumptions, the tribal knowledge that servers need to
    be local to an office where people work has been passed down by rote generation
    to generation without many people evaluating it. This information went from generally
    true to seldom true pretty quickly during the early 2000s.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些旧的假设，服务器需要位于员工工作的办公室这一“部落知识”被代代相传，很多人未曾对此进行评估。随着2000年代初期的到来，这一信息从普遍正确转变为几乎不再正确。
- en: Today most workloads work effectively over the Internet and so can be located
    almost anywhere. Using some form of off-premises hosting or centralized hosting
    that is not based at any specific company location is now the norm rather than
    the exception.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，大多数工作负载通过互联网有效运行，因此可以几乎位于任何地方。使用某种形式的外部托管或不基于任何特定公司位置的集中式托管如今已成为常态，而不是例外。
- en: 'In all on-premises and off-premises evaluations we have certain factors that
    are universal: who will access the data and from where, how does latency and bandwidth
    impact application performance, which people accessing the data have priority
    and what is the cost of performance issues at different locations.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有本地和外部托管的评估中，我们有一些普遍的因素：谁会访问数据，以及从哪里访问，延迟和带宽如何影响应用性能，哪些访问数据的人具有优先权，以及在不同位置的性能问题成本如何。
- en: There is no hard and fast rules, we simply have to carefully consider as many
    factors as possible. On and Off Premises solutions are just locations and should
    be treated as such. The ability to use an enterprise datacenter off premises might
    be significant, especially if we do not have a real server room on premises. And
    disaster recovery might be better at an off premises location. But will the user
    experience be good enough if the server is far away? These questions are all situational
    and need to be answered not just about the status of the business infrastructure
    today but also for the near future.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 没有固定的规则，我们只是需要尽可能仔细地考虑尽可能多的因素。场内和场外解决方案仅仅是位置，应当视作如此。使用企业级场外数据中心可能非常重要，特别是如果我们在场内没有真正的服务器机房。而且灾难恢复在场外位置可能会更好。但如果服务器离得很远，用户体验是否足够好呢？这些问题都是情境性的问题，需要从当前的业务基础设施状况以及未来短期的角度来回答。
- en: Colocation
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合租机房
- en: When a site provides the real estate, cooling, power, Internet, racking, networking,
    and so on but we provide our own physical servers it is called a colocation facility.
    Colocation is one of the most popular and effective ways that we, as system administrators,
    can acquire enterprise class datacenter services outside of our own premises while
    retaining the flexibility to use any hardware that makes sense for our organization
    and its workloads. Colocation is effective for very small businesses up to the
    most absolutely massive. No company outgrows the value that colocation may bring,
    nor does any government. It is a strategy that lacks a *top end* size.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个站点提供了房地产、冷却、电力、互联网、机架、网络等服务，而我们提供自己的物理服务器时，这被称为合租机房。合租机房是我们作为系统管理员，在保留使用任何对我们的组织和工作负载有意义的硬件灵活性的同时，获得企业级数据中心服务的最流行和有效的方式之一。合租机房对从非常小的企业到绝对巨大的企业都有效。没有任何公司会超越合租机房可能带来的价值，政府也一样。这是一种没有*上限*的策略。
- en: 'Colocation is one of the most useful and effective forms of moving IT equipment
    off premises because it allows the IT department to retain full control of hardware
    purchasing and configuration not just for systems but for networking and appliances
    as well. Only non-IT functions necessary to support the technology hardware is
    provided. This allows the colocation facility to focus on a strong facilities
    management skillset and IT to retain literally all IT functions and flexibility:
    basically doing remotely with a third party what you would hope you would be doing
    internally with your own teams assuming that you had enough volume to do so. It
    is expected that a colocation provider will also have *remote hands* to assist
    with bench tasks when necessary, such as changing or verifying cabling, power
    cycling devices, and things of that nature.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 合租机房是将IT设备移出场地的最有用和有效的形式之一，因为它允许IT部门保留对硬件采购和配置的完全控制，不仅仅是对系统，还有网络和设备的控制。只有支持技术硬件的非IT职能是由合租机房提供的。这使得合租机房能够专注于强大的设施管理技能，而IT部门则保留所有IT职能和灵活性：基本上，你可以通过第三方远程完成原本希望自己内部团队做的事情，假设你有足够的业务量来这么做。预计合租机房提供商也会有*远程操作人员*，在必要时帮助完成一些基础工作任务，比如更换或验证电缆、重启设备等。
- en: Flexibility is key with colocation. Whether it is because you want to custom
    build your own hardware, maintain legacy systems, use special case hardware (for
    example, IBM Power hardware), or what the freedom to do a lift and shift of an
    entire existing environment from on premises to the colocation facility you can
    do it all. Most colocation facilities allow for a range of scales as well, from
    housing a single 1U server on your behalf to fraction racks (tenth, quarter, half,
    full rack sizes are common) to providing cages that can house many racks to even
    renting entire floors of the datacenter!
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 灵活性是合租机房的关键。不论是因为你希望定制自己的硬件、维护遗留系统、使用特殊硬件（例如IBM Power硬件），还是希望将现有的整个环境从场内迁移到合租机房，合租机房都能提供这种自由。大多数合租机房设施也允许不同规模的部署，从为你托管一台1U服务器，到为你提供部分机架（通常有十分之一、四分之一、半个、全机架等规格），甚至提供能够容纳多个机架的笼子，甚至出租整个数据中心楼层！
- en: The biggest challenges that colocation faces is that there is no effective way
    to go extremely small because the smallest size you can reasonable host is a single
    server. If your needs are smaller than this, then colocation will generally struggle
    to be cost effective for you. But do not simply reject colocation with an assumption
    of it being expensive. I run these calculations often for companies who were ignoring
    colocation as too costly for them only to find out that it would be less than
    half of the cost of their alternative propositions while having more flexibility
    for growth without additional expenditures. Most people assume that servers are
    more expensive to buy than they really are and that colocation costs are higher
    than they really are. Colocation costs are often inappropriately associated with
    legacy systems, as if only twenty year old equipment can go into a datacenter
    today, and decades old cost models are often envisioned. Twenty years ago servers
    were much more expensive than they are today and had noticeably shorter operational
    lives and datacenter space was more costly than today as well. Like everything
    in IT, cost over time have come down and for workloads of any size colocation
    tends to be much less costly than most alternatives.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 合作托管面临的最大挑战是没有有效的方法去实现极小规模，因为你可以合理托管的最小规模是单个服务器。如果你的需求小于这个规模，那么合作托管通常难以为你提供具有成本效益的解决方案。但不要仅仅因为认为它昂贵就拒绝合作托管。我经常为那些认为合作托管太昂贵的公司做这些计算，结果发现合作托管的成本不到他们其他提案成本的一半，同时还能提供更多的增长灵活性，而无需额外支出。大多数人认为服务器比实际更贵，合作托管的费用比实际更高。合作托管的费用常常不恰当地与传统系统挂钩，好像只有二十年的旧设备才能进入数据中心，而人们往往设想的是几十年前的成本模型。二十年前，服务器比现在贵得多，使用寿命也明显更短，数据中心空间的成本也比今天高。就像IT中的一切一样，成本随着时间的推移已经下降，对于任何规模的工作负载，合作托管通常比大多数替代方案便宜得多。
- en: Colocation is just one approach to hosting systems off premises. Other approaches
    like public, hosted cloud and cloud-based VPS systems are standard alternatives.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 合作托管只是将系统托管到远程的其中一种方法。其他方法如公共云、托管云和基于云的VPS系统是标准的替代方案。
- en: The biggest challenges around locality are really all associated with understanding
    current marketing pricing for different approaches and being able to evaluate
    the benefits and caveats of hosting equipment on premises or off premises, and
    if that equipment should be dedicated or shared. You should now be ready to make
    that evaluation and choose appropriately for your deployments. Next, we dig into
    the much more complicated topic of platform level system design architectures.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 关于本地性的最大挑战，实际上都是与理解当前市场上不同方法的定价相关，并能够评估在本地或远程托管设备的好处和注意事项，以及这些设备应该是专用的还是共享的。现在你应该准备好做出评估，并根据你的部署情况做出适当的选择。接下来，我们将深入探讨平台级系统设计架构这一更复杂的话题。
- en: System Design Architecture
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统设计架构
- en: One of the more challenging aspects of system administration is tackling the
    broad concept of system architecture. In some cases, we have it easy, our budget
    is so low or our needs so simplistic that we simply do not need to consider any
    but the most basic options. But for many systems, we have broader needs and a
    great number of factors to consider making system architecture potentially challenging
    in many ways.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 系统管理中最具挑战性的一部分就是处理系统架构这个广泛的概念。在某些情况下，我们的预算非常低，或者我们的需求非常简单，以至于我们根本不需要考虑除了最基本的选项之外的任何事情。但对于许多系统来说，我们有更广泛的需求，且需要考虑的因素非常多，使得系统架构在许多方面都可能变得具有挑战性。
- en: We now understand platform concepts, locality, and the range of services normally
    associated with providing an environment onto which we can install an operating
    system. Now we have to begin describing how we can combine these concepts into
    real world, usable designs. Most of system design is just common sense and practicality.
    Remember nothing should feel like magic or a black box and if we get services
    from a vendor, they are using the same potential range of technology and options
    that we are.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在理解了平台概念、本地性以及通常与提供一个可以安装操作系统的环境相关联的服务范围。接下来，我们需要开始描述如何将这些概念结合到现实世界中，形成可用的设计。大多数系统设计其实就是常识和实用性。记住，任何事情都不应像魔法或黑匣子一样神秘，如果我们从供应商那里获得服务，他们使用的技术和选项范围与我们是一样的。
- en: We are going to talk about risk and availability in the next section, but before
    we do, we should mention here to make it more clear why system designs rely on
    this data, that any redundancy (whether for performance or risk reduction) that
    we add to our overall system can be done at different layers. Principally in the
    system layer (where we are looking now) or at the application layer (which we
    do not control.) So even in the most demanding of high availability workload situations,
    we may have no need for a robust underlying system design and have to consider
    this when thinking about design options.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一部分讨论风险和可用性，但在此之前，我们应该提到这一点，以便更清晰地说明为什么系统设计依赖这些数据：我们为整体系统增加的任何冗余（无论是为了性能还是风险降低）都可以在不同层次上进行。主要是在系统层（我们当前讨论的层面），或者在应用层（我们无法控制的层面）。因此，即使在需求最苛刻的高可用性工作负载情况下，我们也可能不需要一个强大的底层系统设计，在思考设计选项时必须考虑这一点。
- en: I am going to break down common design approaches so that we can understand
    how they best apply to different scenarios. These are physical system architectures
    that include both the storage and the compute. It is assumed that some sort of
    abstraction, meaning virtualization and/or containerization, is used in every
    case and so will not be mentioned case by case.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我将分析常见的设计方法，以便我们理解它们如何最好地应用于不同的场景。这些是包括存储和计算的物理系统架构。假设在每种情况下都使用某种抽象技术，即虚拟化和/或容器化，因此在每个案例中不会逐一提及。
- en: Standalone server, aka the snowflake
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独立服务器，也叫“雪花服务器”
- en: You really cannot get more basic than this design. The simple server, the baseline
    against which all else must be measured. The self-contained, all in one server
    with storage and compute in a single chassis. No external dependencies, no clustering,
    no redundancy (external to the single box.) Of course, we assume standard hardware
    practices are followed such as minimums like RAID and dual, hot swappable power
    supplies.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设计可以说是最基础不过的了。简单的服务器，作为所有其他设计的基准。自包含的“一体化”服务器，将存储和计算都集成在一个机箱内。没有外部依赖，没有集群，也没有冗余（单一机箱外部的冗余）。当然，我们假设遵循了标准硬件实践，比如最基本的RAID和双热插拔电源供应等。
- en: Today, many IT people are going to frown on attempting to use a stand alone
    server, but they should not. The classic single server is a powerful, effective
    design appropriate for the majority of workloads. This should be the *go to* design,
    the default starting point, unless you have a specific need to do something else.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，许多IT人员会对尝试使用独立服务器表示反对，但他们不应该这么做。经典的单一服务器是一种强大、有效的设计，适合大多数工作负载。这应该是*首选*设计，默认的起点，除非你有特殊需求做其他事情。
- en: Because of its simplicity, single server designs have the best cost ratios to
    all other factors, are more robust than they appear, and have excellent performance.
    Many people think of servers as being rather fragile creatures, and years ago,
    they were; but that impression stems from servers of the 1980s and early 1990s.
    By the late 1990s server technology was becoming mature and reliable and today
    failure rates on well maintained servers are extremely low. The idea that a single
    server is a high risk is an antiquated one, but like many things in our industry
    old feelings often linger and get taught mentor to student without reevaluation
    to see if factors remain true (and in many cases without initial evaluation to
    know if they were ever true.)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其简单性，单服务器设计具有最好的成本比，较其他因素相比，它们比看起来更强大，且性能优秀。许多人认为服务器是一种脆弱的设备，而多年前它们确实是这样；但这种印象源自1980年代和1990年代初的服务器。到了1990年代末，服务器技术逐渐成熟且可靠，今天在良好维护的服务器上，故障率极低。认为单一服务器风险很高的观点已经过时了，但像我们行业中的许多事情一样，过时的观念常常残留下来，并在导师与学员之间传递，而没有重新评估这些因素是否仍然成立（而且在许多情况下甚至没有最初的评估，根本不知道这些观点是否曾经成立）。
- en: 'Single servers benefit from having many fewer components and lowered complexity
    compared to any other approach and with fewer parts to fail and fewer configurations
    to get wrong it is that much easier to make really reliable: hence why we sometimes
    refer to this as the *brick* approach. Bricks are simple but effective, while
    they can fail, they rarely do. Emotionally it is common to associate complexity
    with robustness, but in practice simplicity is far more desirable. Complexity
    is its own enemy and an unnecessarily complex system takes on unnecessary risk
    (and cost.)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他方法相比，单个服务器受益于拥有更少的组件和更低的复杂性，并且由于部件失败较少且配置错误较少，使其更加容易实现真正的可靠性：这就是为什么我们有时将其称为*砖块*方法。砖块简单而有效，虽然它们可能失败，但很少失败。情感上常常将复杂性与健壮性联系在一起，但实际上简单性更加可取。复杂性本身就是其敌人，不必要的复杂系统会带来不必要的风险（和成本）。
- en: While hard to measure for many, many reasons, we generally assume that a properly
    maintained and supported stand alone server can delivery average availability
    rates close to five nines (that is around one hour of downtime per year.) It is
    a rare workload in any business that cannot function well with that kind of downtime.
    What is difficult in stand alone servers is that this is an average only (of course)
    and we will have isolated systems experiencing much higher downtime, and others
    experiencing none.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管由于许多原因难以衡量，我们通常假设一个良好维护和支持的独立服务器可以提供接近五个九的平均可用性（即每年约一小时的停机时间）。在任何业务中，很少有工作负载不能很好地应对这种停机时间。独立服务器面临的困难是这只是一个平均值（当然），我们将有孤立系统经历更高的停机时间，而其他系统则完全没有停机时间。
- en: Simplicity also brings us performance. By having fewer components in the path
    single servers have excellent performance. Attempting to gain total performance
    greater than what can be achieved using a single server is difficult. Single servers
    give us the lowest latency and nearly the best throughput of any approach.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 简单性还带来了性能。通过路径中减少组件，单个服务器具有出色的性能。试图获得超过单个服务器所能实现的总体性能是困难的。单个服务器为我们提供了几乎任何方法中最低的延迟和几乎最佳的吞吐量。
- en: When it comes to single server systems, use math and logic to explain why it
    may or may not make sense. Many people rely on emotions when it comes to system
    architecture and this should never happen. Our concerns with system design are
    about performance and availability and these are purely mathematical components.
    Emotions have no role here and are, in fact, our enemy (as they are the enemy
    of any business process.)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到单个服务器系统时，使用数学和逻辑来解释它是否合理是很重要的。许多人在系统架构方面依赖情感，这是不应该发生的。我们对系统设计的关注点是性能和可用性，这些纯粹是数学组成部分。情感在这里没有任何作用，事实上，它们是我们的敌人（就像它们是任何业务流程的敌人一样）。
- en: Single servers can scale far larger than most people assume. I often hear arguments
    that they cannot look at single servers because their needs are *so large*, but
    then deploy systems only a tiny fraction of the standard scale, let alone the
    maximum scale, that a single server can achieve. Remember that vertical scaling
    is highly effective compared to horizontal, and generally cost effective as well.
    The biggest single server systems can support hundreds of the most powerful CPUs,
    and many terabytes (or more) of RAM. The challenge is not finding a single server
    that is large enough for a task, but rather finding any workload that can leverage
    so much power usefully in a single location!
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 单个服务器的扩展能力远远超出大多数人的想象。我经常听到的论点是，他们无法考虑单个服务器，因为他们的需求*如此巨大*，但随后部署的系统仅仅是标准规模的一小部分，更不用说单个服务器可以达到的最大规模了。请记住，与横向扩展相比，纵向扩展效果显著，并且通常也具有成本效益。最大的单个服务器系统可以支持数百个最强大的CPU和许多TB（甚至更多）的RAM。挑战不在于找到一个足够大的单个服务器来完成任务，而是找到任何能够有效利用如此强大能力的工作负载的任务位置！
- en: A big advantage to standalone servers is that each physical device can be scaled
    and custom designed to address the needs of its workload(s). So different servers
    can use different CPU to RAM to storage ratios, different servers can use different
    CPU generations or architectures, one system might use large hard drives while
    another uses small but screaming fast solid-state storage. Tuning is very easy.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 独立服务器的一个重要优势是，每台物理设备可以根据其工作负载的需求进行扩展和定制设计。因此，不同的服务器可以使用不同的CPU到RAM到存储比例，不同的服务器可以使用不同的CPU代数或架构，一个系统可能使用大容量硬盘，而另一个则使用小巧但速度极快的固态存储。调整非常简单。
- en: Simple does not necessarily mean simple
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单并不一定意味着简单
- en: Having a standalone server does not mean that we give up all of the options
    and flexibility that we might believe that we need from more complex designs.
    It simply means that we have to think about them differently. Many of the concerns
    that one may have about standalone servers likely stems from a pre-virtualization
    world with relatively slow networks. Today we have virtualization, fast storage,
    and fast networks and these can move the goal line by a bit.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有独立的服务器并不意味着我们放弃所有我们可能认为需要从更复杂设计中得到的选择和灵活性。这只是意味着我们必须以不同的方式考虑它们。关于独立服务器的许多关切很可能源于一个虚拟化技术尚未普及，网络相对较慢的时代。今天我们拥有虚拟化、快速存储和快速网络，这些可以在一定程度上改变游戏规则。
- en: We refer to servers as being standalone in reference to their architecture,
    everything is self contained in a single piece of hardware. This does not mean
    that we do not have (or cannot have) more than one server. On the contrary, giant
    Fortune 100 firms will often have thousands of standalone servers. What makes
    then standalone is that they have no dependencies on each other. The complete
    failure (or theft) of one does not negatively impact another.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将服务器称为独立的，是指它们的架构，一切都是自包含在一块硬件中的。这并不意味着我们没有（或不能有）多个服务器。相反，像世界五百强公司经常会拥有数千台独立的服务器。使它们独立的是它们彼此之间没有依赖。其中一台的完全失效（或被盗）不会对另一台产生负面影响。
- en: A tiny organization might choose to rely on a single standalone server for their
    entire business and depend completely on backups and the ability to restore to
    replacement hardware should disaster strike. This is a totally valid approach
    and quite common.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小型组织可能会选择依赖单个独立服务器来支持他们的整个业务，并完全依赖备份和恢复到替换硬件的能力，如果灾难发生。这是一种完全有效且相当常见的方法。
- en: If your organization is larger, or workloads require more immediate protection
    against loss of availability, then it is standard to run multiple standard servers.
    This spreads load between physical hardware devices and, because of virtualization,
    provides an opportunity for there to be natural ways of mitigating hardware failure
    by rapidly rebuilding lost workloads on other hardware. If deployment density
    is too high, spare hardware is an option as well. With modern storage, networks,
    system management, and backup techniques restoring many workloads can be done
    in as little as minutes allowing even complete hardware failures to often carry
    only the tiniest of system impacts. In fact, keeping backups stored on other standalone
    nodes can allow for essentially instant recovery of lost systems while maintaining
    strong decoupling.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的组织规模较大，或者工作负载需要更快的可用性保护，那么运行多个标准服务器是标准做法。这会在物理硬件设备之间分担负载，并且由于虚拟化，提供了通过快速在其他硬件上重建丢失工作负载来缓解硬件故障的自然方式。如果部署密度过高，备用硬件也是一种选择。借助现代存储、网络、系统管理和备份技术，恢复许多工作负载可能只需几分钟，即使完全硬件故障也通常只会带来微不足道的系统影响。事实上，将备份存储在其他独立节点上可以实现丢失系统的几乎即时恢复，同时保持强大的解耦。
- en: Standalone servers also do not imply that there is no form of unified management.
    Tools like ProxMox or VMware vSphere allow a consolidation of management while
    keeping system hardware independent. Modern tooling has made managing sprawling
    fleets of standalone servers very simple indeed.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 独立的服务器也并不意味着没有统一的管理形式。像ProxMox或VMware vSphere这样的工具允许管理的集中化，同时保持系统硬件的独立性。现代工具使得管理庞大的独立服务器群变得非常简单。
- en: Most every aspect of a stand-alone server can be improved by adding more to
    it and making it more complex, the two things that it always leads on are cost
    and simplicity. No other approach will reliably be able to keep our costs or our
    simplicity as low and in business, these are generally the factors that matter
    most.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 独立服务器的几乎每个方面都可以通过增加更多内容和使其更复杂来改进，它始终引领的两件事是成本和简单性。在商业上，没有其他方法能够可靠地保持我们的成本或我们的简单性如此之低，这通常是最重要的因素。
- en: Many to many servers and storage
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 许多对多服务器和存储
- en: As companies grow there can be an opportunity to consolidate different aspects
    of the architecture in order to save money. Separating networking and storage
    is the most common approach to this. Creating a layer of compute nodes, and a
    layer of storage nodes allows for a lot of flexibility. The primary benefit is
    allowing for the easy movement of resources and better system utilization.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 随着公司成长，可能会有机会整合架构中的不同方面以节省成本。分离网络和存储是最常见的做法。创建一个计算节点层和一个存储节点层可以带来很大的灵活性。其主要好处是能够轻松移动资源，并更好地利用系统。
- en: For example, an organization maybe need fifteen physical compute nodes (traditional
    servers) but only half a dozen storage nodes (SAN or NAS) to support them. Each
    individual system can be easily custom scaled and does not need to match other
    systems in the pool. In this way this approach is not so different from the standalone
    server approach.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个组织可能需要十五个物理计算节点（传统服务器），但只需要六个存储节点（SAN或NAS）来支持它们。每个独立系统可以轻松定制扩展，并且不需要与池中的其他系统匹配。通过这种方式，这种方法与独立服务器方法并没有太大区别。
- en: It should be noted that when doing this, the storage layer is the greater risk,
    compared to the compute layer, for two key reasons. First, it is stateful where
    compute is stateless, which means that here we not only have to protect the availability
    (uptime) of the system, but this is also where we have to protect the data as
    it is stored so we have the risk of data loss as well - there is simply more to
    lose here. Second, storage is more complex than compute and equivalent hardware
    and software at both layers means that the storage layer is just more likely to
    fail due to complexity. This is all risk that also exists in the standalone server,
    but when combined into a single chassis it can be more difficult to understand
    where the risk is occurring, even if we know what the overall resultant risk is.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 应当注意的是，存储层相较于计算层，在做这件事时存在更大的风险，原因有两个。首先，存储是有状态的，而计算是无状态的，这意味着我们不仅需要保护系统的可用性（正常运行时间），还需要保护数据的存储，因此存在数据丢失的风险——这里有更多的东西需要保护。其次，存储比计算更复杂，两个层级上的硬件和软件相同意味着存储层更容易因复杂性而发生故障。这些风险在独立服务器中也存在，但当它们被组合成一个单一机箱时，可能更难理解风险发生的位置，即使我们知道整体的风险结果是什么。
- en: In its simplest incarnation, we would have a single compute server node and
    a single storage node (typically a SAN array) and would connect them directly
    via a straight cable (Ethernet, eSATA, FC, and so on.) This is really more of
    a hypothetical scenario as it is so obviously bloated and illogical without any
    scale, but we can learn from the example to see how we take the single standalone
    server design and, without any benefits of scale, simply double the chassis to
    manage and increase the physical, as well as, logical complexity of the system
    design.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的实现方式中，我们会有一个计算服务器节点和一个存储节点（通常是SAN阵列），并通过一根直连电缆（如以太网、eSATA、FC等）直接连接它们。这实际上是一个假设场景，因为它显然是膨胀且不合逻辑的，缺乏规模化，但我们可以从这个例子中学习，看看如何将单一独立服务器设计，在没有任何规模效益的情况下，简单地将机箱数量翻倍，从而管理并增加系统设计的物理复杂性和逻辑复杂性。
- en: Typically, this type of design is leveraged most to consolidate heavily on storage,
    pushing as much storage into a single node as possible, while having many small
    to medium sized (one to two CPU) servers that allow workloads to move between
    them in order to best balance said workloads. This approach is flexible and generally
    cost-effective, and makes large scalability quite simple.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这种设计的最大优势是能够大规模整合存储，将尽可能多的存储推入单个节点，同时拥有许多小型到中型（一个或两个CPU）的服务器，允许工作负载在这些服务器之间流动，以便最佳地平衡工作负载。这种方法具有灵活性，通常具有成本效益，并且使大规模扩展变得相对简单。
- en: Moving past standalone servers means we start to inject dependencies that need
    to be discussed. At a minimum, when we move to a multi-nodal system, we have the
    complexities of the interconnections (which might be as simple as just a cable,
    or more complex like going through a switching fabric of some sort), any complexities
    that come from configuring the nodes to speak to each other, and the risks of
    the extra components that might fail.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 超越独立服务器的设计意味着我们开始引入需要讨论的依赖关系。至少，当我们转向一个多节点系统时，就会有节点间连接的复杂性（这些连接可能只是简单的电缆，也可能更复杂，比如通过某种交换网络），以及配置节点之间的通信所带来的复杂性，还有额外组件故障的风险。
- en: This kind of design really does nothing to address risk, and actually is far
    riskier than standard standalone servers. This is why it is important to use standalone
    servers as a baseline and discover risk variation from that point. In a *network*
    system design, there is no redundancy, so each workload has a full dependency
    on both its associated compute node and its associated storage node(s). This risk
    may be uniform across a compute node or each workload located there might have
    unique storage configurations so that risks may vary widely between different
    workloads on a single server. This is where risk becomes much more complicated
    to measure because we have to deal with the cumulative risks of the compute node,
    the storage node, the connection between the two, and the configuration! Each
    individual piece is extremely difficult to measure on its own – putting them together,
    we mostly have to look in relative terms only and understand that it is much riskier
    than a standalone server.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计实际上并没有解决风险问题，反而比标准的独立服务器更具风险。这就是为什么将独立服务器作为基准，并从这个基准点出发发现风险变化是很重要的。在一个*网络*系统设计中，没有冗余，因此每个工作负载完全依赖于它所关联的计算节点和存储节点。这个风险可能在计算节点之间是均匀的，或者每个工作负载所在的节点可能有独特的存储配置，从而导致同一服务器上不同工作负载的风险差异很大。在这里，风险变得更加复杂，因为我们必须处理计算节点、存储节点、两者之间的连接以及配置的累计风险！每个单独的部分都非常难以单独衡量——将它们放在一起时，我们大多数情况下只能从相对的角度来看，并理解它比独立服务器更具风险。
- en: Viewing the world as a workload
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将世界视为工作负载
- en: System architecture is a *by the workload* task and there is no specific necessity
    for all workloads in your organization, or even all workloads running on a single
    compute node, to share architectures. Mixing and matching is totally doable and
    somewhat common. Each workload should be being evaluated as to its own needs,
    and then the overall architecture evaluated.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 系统架构是*按工作负载*来设计的，没有必要让组织中的所有工作负载，甚至单一计算节点上运行的所有工作负载，都共享相同的架构。混合搭配完全可行，而且在某些情况下是常见的。每个工作负载应根据其自身需求进行评估，然后评估整体架构。
- en: Often overlooked is the ability to use a complex and less reliable (but potentially
    less expensive at scale) option like network design for workloads that are less
    important, while on the same compute nodes also having local storage that is extremely
    fast and/or reliable for more critical workloads. Mixing and matching can be a
    strong strategy in a large environment where storage consolidation is considered
    necessary without endangering an isolated number of highly critical services in
    order to do so.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 经常被忽视的是，可以为不太重要的工作负载使用复杂且不太可靠（但在大规模情况下可能更便宜）的网络设计选项，同时在相同的计算节点上为更关键的工作负载提供极其快速和/或可靠的本地存储。在需要存储合并的一个大环境中，混合搭配可以是一种强有力的策略，这样做不会危及少数高度关键的服务。
- en: In the same vein, each workload can have its own backup, replication, failover
    and other risk mitigation strategies for deal with disaster. Sharing a compute
    node generally dictates very little as to how reliability and availability from
    workload to workload must be handled. Of course typically all workloads are treated
    the same either out of a desire for standardization and simplicity, but also regularly
    out of a misunderstanding of the range of customization available for each individual
    workload. It is often assumed that choosing a system design is an all or nothing
    endeavour, but this is not the case.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 同理，每个工作负载都可以有自己的备份、复制、故障转移和其他灾难应对的风险缓解策略。共享一个计算节点通常不会对工作负载间如何处理可靠性和可用性提出过多要求。当然，通常所有工作负载都会以相同的方式处理，要么是出于标准化和简化的愿望，也常常是因为误解了每个工作负载可用的定制化范围。人们常常认为选择一个系统设计是一个非此即彼的决定，但事实并非如此。
- en: The main challenges of network system design is that any efficiency gained has
    to offset the additional cost created by needed more overall nodes (separating
    compute and storage means that additional hardware chassis and operating systems
    are necessary for the same tasks) and at any scale additional networking equipment
    is needed to handle the interconnects. Networking equipment can be as simple as
    a single Ethernet switch or as complex as clusters of Fiber Channel or Infiniband
    switches. Switches represent no only additional cost to purchase, but also additional
    points of failure both for hardware and, to a much lesser extent, configuration.
    Often redundant switches are purchased reducing hardware risk but increasing cost
    and configuration complexity. Even in extremely large environments this represents
    additional cost and risk that is very hard to overcome.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 网络系统设计的主要挑战在于，任何获得的效率提升都必须抵消由需要更多节点（分离计算和存储意味着同样的任务需要额外的硬件机箱和操作系统）所带来的额外成本，并且在任何规模下，都会需要更多的网络设备来处理互联连接。网络设备可以是一个简单的以太网交换机，也可以是复杂的光纤通道或
    Infiniband 交换机集群。交换机不仅代表了额外的购买成本，还增加了硬件故障的风险，并且在较小程度上也增加了配置的复杂性。通常会购买冗余交换机来降低硬件风险，但这会增加成本和配置复杂度。即便是在极其庞大的环境中，这也意味着额外的成本和风险，这些往往是难以克服的。
- en: 'The Inverted Pyramid of Doom: Clustered Compute with Risky Storage, aka the
    3-2-1'
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 末日倒金字塔：集群化计算与高风险存储，亦称 3-2-1
- en: Sadly the Inverted Pyramid of Doom (aka 3-2-1 or IPOD) has traditionally, for
    the majority of the 2000s and 2010s, been the most commonly deployed architecture
    in small and medium business and is also the prime example of the absolutely worst
    possible design decision for normal workload needs. It is also the design that
    maximizes profits for vendors and resellers, so it is what everyone wants you
    to buy.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 遗憾的是，倒金字塔（又称 3-2-1 或 IPOD）传统上在 2000 年代和 2010 年代的大多数时间里，一直是中小企业中最常部署的架构，同时也是普通工作负载需求的最糟糕设计决策的典型例子。它也是供应商和转售商最大化利润的设计，因此这是每个人都希望你购买的方案。
- en: 'The IPOD design is differentiated from the network system design above in that
    the compute layer is clustered for high availability, but the storage layer is
    not. As we discussed in the last design storage is both more important to protect
    and more likely to fail. Typically the networking layer (the layer providing connectivity
    between compute and storage) is also clustered for high availability. This nodal
    count by layer creates the naming conventions used: 3-2-1 refers to the design
    having three (or more) compute nodes, connected to two redundant switches, all
    relying on a single storage device which is most typically a SAN.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: IPOD 设计与上述的网络系统设计的区别在于，计算层是集群化的，以实现高可用性，而存储层则不是。如我们在上次的设计中讨论的那样，存储既需要更好地保护，又更容易发生故障。通常，网络层（即提供计算与存储之间连接的层）也会进行集群化以实现高可用性。每一层的节点数量决定了所使用的命名约定：3-2-1
    指的是设计中有三个（或更多）计算节点，连接到两个冗余交换机，所有这些都依赖于一个单一的存储设备，通常是一个 SAN。
- en: When viewed in an architectural drawing, the IPOD is a pyramid with the wide
    portion on top and everything balanced on the point. Hence the term *inverted
    pyramid*, this design is designed to be as costly and risky as possible, hence
    the moniker *of doom*.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构图来看，IPOD 是一个金字塔，宽的部分在上面，一切都平衡在顶点上。因此有了“倒金字塔”这一术语，这种设计的目的就是尽可能地增加成本和风险，因此才有了“末日”的绰号。
- en: Top-down redundancy
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自上而下的冗余
- en: Why is a design so obviously impractical as the IPOD so traditionally popular?
    The answer requires us to understand several factors. First, redundancy, risk,
    and system design are all areas that most businesses, and even most IT departments
    within those businesses, have received no training and are generally completely
    unaware and so represent an easy target for vendors to be manipulative.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么像 IPOD 这样明显不切实际的设计在传统上如此流行？答案需要我们理解几个因素。首先，冗余、风险和系统设计是大多数企业，甚至企业内部的大多数 IT
    部门，缺乏培训且普遍没有意识的领域，因此容易成为供应商进行操控的目标。
- en: 'The real trick comes from two things: linguistics and the simplification enabled
    by top down viewing. The linguistic trick happens because the term *redundancy*
    does not mean what most people believe that it means and this system *has redundancy*,
    but in an all but meaningless way. So, when a customer says, *I need redundancy*
    they actually mean *I need high availability*, but this allows the vendor to state
    that there is redundancy and ignore actual needs. Semantics are super important
    in all business, and IT more than most.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的技巧来自于两点：语言学和自上而下视角带来的简化。语言学的技巧发生在*冗余*这个词的含义上，因为这个词并不像大多数人理解的那样，其系统*确实有冗余*，但几乎是无意义的。所以，当客户说“*我需要冗余*”时，他们实际上是指“*我需要高可用性*”，而这使得供应商可以声称有冗余，忽略实际需求。语义在所有商业领域都非常重要，IT领域尤其如此。
- en: The top-down aspect of the system comes from how we view the architecture. As
    IT professionals, we know that we should view our architecture *from the side*,
    that is seeing the reliability as it stands layer by layer, knowing that compute
    is built on top of the network, and the network on top of the storage. But a vendor
    wanting to steer a customer to believe that there is strong redundancy will demonstrate
    the system *top down* showing a view that only sees the compute layer where there
    is redundancy. The other layers are overly complex and tend to be happily ignored
    by all parties as being *black boxes that do magic*. Ignoring the hard parts and
    just focusing on the trivial, easy part where redundancy is least important makes
    it really easy to mislead a customer.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的自上而下的视角来源于我们如何看待架构。作为IT专业人员，我们知道我们应该*从侧面*看待我们的架构，也就是说，层层分明地看待其可靠性，知道计算建立在网络之上，网络建立在存储之上。但如果供应商想要引导客户相信系统有强大的冗余性，他们会展示一个*自上而下*的视角，只显示有冗余的计算层。其他层次则过于复杂，常常被各方忽视，认为它们是*黑箱，做着魔法般的工作*。忽略困难的部分，仅专注于冗余性不太重要的简单部分，这使得误导客户变得非常容易。
- en: Of course, if we really stop and think about it, what matters is the overall
    reliability of the entire system. Getting distracted by any single layer will
    simply lead us astray. We need to understand all of the layers, and how they interact
    with each other, in order to determine overall reliability, but there is a strong
    emotional drive to see one layer as being extremely reliable (as the compute layer
    here often is) and then feeling that the overall system must therefore be extremely
    reliable. But this is anything but true. The overall reliability of the system
    is driven primarily by the most fragile layer, not the most reliable. The system
    risk is, if you recall from earlier, cumulative. You combine all of the risks
    together because each layer depends one hundred percent on every other layer,
    if any layer fails everything fails. You can demonstrate this easily with a thought
    experiment... if one layer has a 100% chance of failure, and all other layers
    have a 0% chance of failure, the system will still fail 100% of the time. The
    impossibly reliable layers do literally nothing to offset the unreliable layer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果我们真的停下来思考一下，真正重要的是整个系统的总体可靠性。被任何单一层次的事情分散注意力只会让我们误入歧途。我们需要了解所有的层次，以及它们如何相互作用，才能确定整体的可靠性。但人们有强烈的情感驱动，常常认为某一层是极其可靠的（比如这里的计算层通常是），然后觉得整个系统因此也必须是极其可靠的。但事实恰恰相反。系统的整体可靠性主要由最脆弱的层决定，而不是最可靠的层。如你之前所记，系统的风险是累积的。你将所有的风险合并，因为每一层都百分之百依赖于其他所有层，一旦任何一层失败，整个系统都会失败。你可以通过一个思维实验轻松证明这一点……如果某一层的失败概率是100%，而所有其他层的失败概率是0%，那么系统依然会100%失败。那些几乎可靠的层对抵消不可靠层的作用几乎为零。
- en: Redundancy itself is a dangerous word to use. In general English usage, the
    word redundant simply means that you have multiple of something when fewer are
    needed. This can mean that one is a replacement or backup should the other fail,
    but that is not implied and often the term is used to mean something else. In
    RAID, for example, RAID 0 has multiple disks (redundant) but the more redundancy
    the higher the risk, not the lower. RAID 1 is the opposite. Redundancy is polar
    opposite there, even within a single context. This really shows the importance
    of semantics in IT (and business, or really, life in general.) People often use
    redundancy as a proxy work for reliability, but the two mean very different things.
    Use the term you mean and you will get far better information.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 冗余本身是一个危险的词。在一般的英语用法中，冗余意味着你拥有某个事物的多个副本，而实际上只需要更少的副本。这可能意味着当某个副本失败时，另一个副本可以作为替代或备份，但这并不是必然的，且这个词常常被用来表达其他意思。例如，在RAID中，RAID
    0有多个磁盘（冗余），但冗余越多，风险反而越高，而不是越低。RAID 1则恰恰相反。在那里，冗余是它的对立面，甚至在同一个上下文中也是如此。这真正体现了IT中语义学的重要性（实际上在商业中，甚至是生活中也是如此）。人们常常把冗余作为可靠性的替代词，但两者的含义是截然不同的。使用你真正想表达的术语，你将获得更准确的信息。
- en: The biggest problem with the IPOD design is one of practicality. If we were
    to look at it purely from a reliability standpoint we could state that it is safer
    than the network system design because at least some of the layers contain high
    availability measures, even if not all of them do. And this is totally correct,
    but tends to be misleading. Network system design is meant to trade high risk
    for cost savings versus the simple stand alone server design, using the idea of
    *safer than* something that is not even designed to be safe is not exactly wrong,
    but talking about it in that context is done to evoke an emotional response -
    to make the IPOD feel safe, which is not the same as *safer*. If we compare the
    reliability of an IPOD to the stand alone server, it feels quite unsafe and remember,
    we stated at the beginning, the low cost, simple, stand alone server is our baseline
    for comparisons. The problem with the IPOD is that the risk is extremely high,
    approaching the risk of the network system design, while its costs are much higher
    than the network system design and generally much higher than the stand alone
    server design all while having more complexity and effort for the IT team. It
    is the consistent combination of high risk and high cost that makes it problematic
    and generally accepted as the worst design to encounter in the real world.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: IPOD设计中最大的问题是其实用性。如果我们从可靠性的角度来看，它比网络系统设计更安全，因为至少有一些层包含了高可用性措施，即使并非所有层都有。但这确实是正确的，但也容易让人产生误导。网络系统设计的目的是通过牺牲高风险来节省成本，相对于简单的独立服务器设计，使用“比某个根本不打算设计为安全的系统更安全”这一说法，并不完全错，但在这种语境下讲述时，常常会激发一种情感反应——让人觉得IPOD更安全，但这并不等同于*更安全*。如果我们将IPOD的可靠性与独立服务器进行比较，它的安全性会显得非常差。请记住，我们一开始就指出，低成本、简单的独立服务器是我们比较的基准。IPOD的问题在于，它的风险极高，接近网络系统设计的风险，而它的成本又远高于网络系统设计，通常也比独立服务器设计要高，而且给IT团队带来了更多的复杂性和工作量。正是这种高风险和高成本的持续结合，使得IPOD成为一个有问题的设计，并且通常被认为是现实世界中最糟糕的设计。
- en: Outside of production environments, the IPOD is often ideal for large lab environments
    where capacity matters most and reliability does not matter at all. The ability
    to flexibly scale compute with a single consolidated, low cost, highly unreliable
    storage layer can make sense to make large scale labs more affordable.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境以外，IPOD通常非常适合大规模实验室环境，其中容量最为重要，而可靠性则不那么关键。通过灵活扩展计算能力，使用一个统一的、低成本、可靠性较低的存储层，可以使大规模实验室变得更加实惠。
- en: Layered high availability
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分层高可用性
- en: The logical system design derived from what we have already seen is to take
    the separate layers of the network system design, and the high availability clustering
    from the compute layer of the Inverted Pyramid of Doom and apply it to all layers
    giving us a high availability storage layer, a high availability networking layer,
    and a high availability compute layer. In this way we can have large scale compute,
    storage, and networking without any individual layer being a high level of concern.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们已经看到的内容派生出的逻辑系统设计，是将网络系统设计的各个独立层以及“毁灭金字塔”计算层中的高可用性集群应用到所有层，从而得到高可用性的存储层、高可用性的网络层和高可用性的计算层。通过这种方式，我们可以拥有大规模的计算、存储和网络，而不会对任何单独的层次产生高度关注。
- en: where each layer still depends on every other layer, that three highly available
    layers must still be evaluated with the risk of each layer added together. So,
    while we can almost certainly make any individual layer more, or even far more,
    reliable than a single standalone server would be when we accumulate the risk
    of each layer, and then add in the risk of the additional complexities from incorporating
    the layers together, it may or may not remain more reliable than the standalone
    server would be.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层仍然依赖于其他每一层，因此这三层高度可用的层仍然必须被评估，并且每一层的风险需要加在一起。所以，尽管我们几乎可以肯定地让任何单独的一层比单一的独立服务器更可靠，或者甚至远远更可靠，但当我们将每一层的风险累积起来，再加上将这些层结合在一起所带来的额外复杂性风险时，它可能不再比独立服务器更可靠。
- en: Reliability is relative
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠性是相对的
- en: When discussing reliability and these different architectures we have to remember
    to think in terms of apples to apples, not apples to oranges. When we say that
    a single server is a certain level of reliability, and that servers clustered
    with standard high availability technologies have a certain relatively higher
    reliability, we are assuming that all of those servers are roughly identical in
    their individual reliability. In most situations this is true. Whether we are
    talking compute nodes, networking hardware, or storage nodes, for roughly the
    same price range we get similar quality hardware and software with roughly similar
    failure characteristics. So, these different devices *of the same quality* are
    all about the same level of reliability with networking hardware being the most
    reliable (least complex) and storage nodes being the least reliable (because they
    are the most complex.)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论可靠性和这些不同的架构时，我们必须记住要进行同类比较，而不是苹果和橙子的比较。当我们说一个单独的服务器具有一定的可靠性，而使用标准高可用性技术集群的服务器具有相对较高的可靠性时，我们假设这些服务器在个体可靠性上大致相同。在大多数情况下，这种假设是正确的。无论是计算节点、网络硬件，还是存储节点，在大致相同的价格范围内，我们获得的硬件和软件质量相似，并且故障特性也大致相同。因此，这些质量*相同的*不同设备的可靠性大致相同，网络硬件是最可靠的（最简单的），而存储节点则是最不可靠的（因为它们最复杂）。
- en: However we can manipulate this dramatically. A five thousand dollar server will
    generally be much less reliable (and performant) than a five hundred thousand
    dollar server. Yet each is an individual, stand alone server. So clearly we have
    to think in terms both of architectural reliability (the reliability of the system
    design that we make) and in terms of the individual components.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以大幅度地操控这一点。一台五千美元的服务器通常会比一台五十万美元的服务器不那么可靠（和性能差）。然而，每一台都是独立的单体服务器。所以，显然，我们必须同时考虑架构可靠性（我们所设计的系统的可靠性）和个体组件的可靠性。
- en: A common problem found here is that *you get what you pay for* applies not at
    all and you can easily find extremely expensive single-chassis systems for both
    compute and storage nodes that are not highly available at all and may not even
    be as reliable as average devices! As reliability is hard to measure and even
    harder to prove, vendors have little incentive to tell us the truth. Vendors are
    highly incentivized to tell us whatever is likely to make us spend more money
    with them whether it is making us feel that traditional servers are more fragile
    than they really are, or by making wild high availability claims for devices that
    are essentially built from straw (and by pigs.)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这里常见的一个问题是，*你得到的就是你付出的* 这一原则完全不适用，你很容易找到一些极为昂贵的单机系统，不论是计算节点还是存储节点，它们的可用性都不高，甚至可能比普通设备还不可靠！由于可靠性很难衡量，甚至更难证明，供应商几乎没有动力告诉我们真相。供应商有很强的动力告诉我们任何能够让我们花更多钱的事情，无论是让我们觉得传统服务器比实际更脆弱，还是对那些本质上是由稻草（以及猪）构建的设备作出狂野的高可用性声明。
- en: So we must be careful that we consider all of the factors. And we must understand
    that the ability to protect a single chassis (vertical reliability) compared to
    multiple chassis (horizontal reliability) is different. Single chassis reliability
    tends to be incredibly powerful for certain components (such as redundant power
    supplies, high quality components, and mirrored RAID for storage) but tends to
    be complex and problematic for others (CPU, RAM, Motherboards.) And single chassis
    systems, while easier to operate, cannot address some key concerns like physical
    damage (water, fire, forklift) in the same way that multiple chassis can.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们必须小心，考虑所有的因素。我们必须理解，保护单一机箱（垂直可靠性）与保护多个机箱（水平可靠性）是不同的。单一机箱的可靠性对于某些组件（如冗余电源、高质量组件和镜像RAID存储）通常非常强大，但对于其他组件（如CPU、内存、主板）则往往比较复杂和问题多发。而单一机箱系统虽然操作更简单，但无法像多个机箱那样处理一些关键问题，比如物理损坏（水、火、叉车等）。
- en: We must also be keenly aware that marketers and sales people often use confusion
    around reliability as a sales tactic and will push concepts such as *dual controller*
    systems as being essentially impossible to fail but without science or math to
    back it up. Dual controller systems are simply horizontally scaled systems inside
    a single chassis with all of the complexity of the former and the lack of physical
    protection of the later. And any product sold based on being misleading is that
    much more likely to be poorly made as it means that the vendor is unlikely to
    be being held accountable to quality design.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须敏锐地意识到，市场营销人员和销售人员常常利用对可靠性的混淆作为销售策略，并推销像*双控制器*系统这样的概念，声称它几乎不可能出现故障，但却没有科学或数学依据来支持这一说法。双控制器系统实际上只是将系统水平扩展到单一机箱内部，具有前者的所有复杂性，并且缺乏后者的物理保护。任何基于误导性信息进行销售的产品，都更有可能做得很差，因为这意味着供应商不太可能对质量设计承担责任。
- en: It has become known, especially in the early 2010s, that server vendors were
    regularly pushing products branded as high availability or *cannot fail* that
    did not even begin to approach the baseline reliability of traditional servers.
    Since customers could not verify this for themselves, they often just take the
    vendor's word for it and if the businesses loses money, finger pointing is the
    natural recourse.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在2010年代初期，服务器供应商常常推出那些标榜为高可用或*无法失败*的产品，这些产品甚至连传统服务器的基础可靠性都无法接近。由于客户无法自行验证这一点，他们通常只能相信供应商的说法，如果企业因此亏损，指责和推诿是常见的后果。
- en: This approach necessarily is the most expensive design we can reasonably assemble
    because we need multiple devices at each layer, as well as technology to create
    the clustering at each layer. This is best for very large systems where each layer
    is able to scale so large that cost benefits of scale come in at every point.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法必然是我们能够合理组建的最昂贵的设计，因为我们需要在每一层都拥有多个设备，并且还需要技术来创建每一层的集群。这最适合非常大的系统，其中每一层的规模足够大，可以在各个层面实现规模效益。
- en: It is worth noting that essentially all cloud based systems run on this architecture
    due to their enormous scale. Certainly not all as cloud can run using any architecture,
    but this is far and away the most likely to be used in a large, public cloud implementation
    and is most generally what would be found even in a moderate scale private implementation.
    Many clouds do, however, run on stand alone servers even at massive scale.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，几乎所有基于云的系统都运行在这种架构上，这是由于它们的巨大规模。当然，并非所有的云都使用这种架构，因为云可以使用任何架构，但这无疑是大型公共云实施中最常用的架构，并且通常也会出现在中等规模的私有云实施中。然而，许多云确实在独立服务器上运行，即使在巨大的规模下也是如此。
- en: Hyperconvergence
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超融合
- en: The last architectural type that we will look at is hyperconvergence and we
    have now come full circle from increasingly complex designs to one of the least.
    Hyperconvergence as an architecture is anything but new, but for the last few
    decades it has been almost completely ignored before having a sort of renaissance
    in the mid-2010s and is now, along with stand alone servers, the bulwark of system
    architectural design.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要看的是最后一种架构类型——超融合架构，它将复杂设计的趋势推向了一个极简的方向。超融合作为一种架构并不新颖，但在过去几十年里几乎被完全忽视，直到2010年代中期才经历了某种复兴，现在，它与独立服务器一起，成为了系统架构设计的基石。
- en: Hyperconvergence, also called HC or HCI, takes the compute and storage nodes
    of other, more complex architectures, and recombines them back into single servers
    (or you can view it as taking stand alone servers and adding high availability
    through engineering redundancy without adding unnecessary complexity.) Hyperconvergence
    gives us the best of both worlds, simplicity like stand alone servers, but options
    for high availability like Layered High Availability.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 超融合（Hyperconvergence），也叫 HC 或 HCI，将其他更复杂架构中的计算和存储节点重新组合成单一服务器（或者你也可以把它理解为将独立的服务器通过工程冗余添加高可用性，而不增加不必要的复杂性）。超融合为我们提供了两全其美的解决方案，既有独立服务器的简洁性，又有类似分层高可用性的高可用性选项。
- en: Hyperconvergence is both so incredibly simple, but so effective that it can
    be hard to explain. The key strategy is taking the existing power and cost savings
    of the stand alone approach and doing as little as possible while still being
    able to add high availability clustering. By having multiple stand alone nodes
    that are clustered together (are they still stand alone, then?) we make all of
    the pieces highly available, while also reducing how many pieces are needed in
    total.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 超融合既简单又高效，这让它变得很难解释。关键策略是利用独立方法现有的性能和成本节约，同时尽可能少地做事，仍能增加高可用性集群。通过将多个独立节点进行集群（那它们还算独立吗？）我们使所有组件都具备高可用性，同时减少了所需的总组件数。
- en: When done correctly, data can also be guaranteed to be kept locally to a compute
    node, even though storage is replicated between nodes to create storage high availability,
    which not only means that we can get the high performance for our storage like
    we can with stand alone servers, but also that we can avoid a cross-node dependency
    allowing any node to keep working on its own, even if all other nodes and/or the
    network connecting them fails! That means that unlike all other system designs,
    we are only adding more resilience on top of the stand alone server design! That
    is huge. All other designs must put in the bulk of their efforts to overcoming
    their own introduced fragilities and risk failing to adequately do so potentially
    leaving them riskier than they would have been had we done nothing.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当正确实施时，数据也可以保证保留在计算节点本地，即使存储在节点间复制以实现存储高可用性，这不仅意味着我们能够像独立服务器一样获得存储的高性能，还意味着我们可以避免跨节点的依赖，让任何节点即使在所有其他节点和/或连接它们的网络发生故障时，依然能够继续工作！这意味着，与所有其他系统设计不同，我们仅在独立服务器设计的基础上增加了更多的弹性！这非常重要。所有其他设计都必须投入大量精力来克服它们自己引入的脆弱性，且有可能未能充分克服这些问题，导致其风险可能比我们什么都不做时还要高。
- en: Hyperconvergence, therefore, acts as the logical extension of the stand alone
    design and represents probably the most applicable system design for any large
    scale system. It is common to see hyperconvergence as being limited to small systems,
    but it is able to scale to the limits of the clustering technologies - the same
    limits affecting all design options. So all standard designs can go to roughly
    the same size, which is generally far larger than anyone would want to go in practical
    terms within the confines of a single system.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，超融合作为独立设计的逻辑延伸，可能是任何大规模系统设计中最适用的方案。虽然超融合常被认为仅限于小型系统，但它可以扩展到集群技术的极限——这一点适用于所有设计选项。所以所有标准设计的规模大致相同，而这个规模通常远大于实际应用中单一系统的需求。
- en: High availability vs basic clustering
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用性与基础集群
- en: In this section we talk about clustering with the assumption that clustering
    (whether compute, storage, or networking) is done for the purpose of making the
    system highly available, at least within that one layer. High availability clustering
    is not the only kind of clustering, however. In all of these designs, including
    stand alone, we can add generic clustering as a management layer to manage many
    systems together. This can be confusing as the term clustering can be used to
    mean many things.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们假设集群（无论是计算、存储还是网络）是为了使系统在至少这一层面上具有高可用性而进行的。然而，高可用性集群并不是唯一的集群形式。在所有这些设计中，包括独立设计，我们都可以将通用集群作为管理层来管理多个系统。这可能会让人困惑，因为“集群”这个术语可以有多种含义。
- en: Best practices in System Design Architecture
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统设计架构中的最佳实践
- en: The best practice for system design is to keep your architecture as simple as
    possible to meet your needs, but no simpler. Remember that simplicity is a benefit,
    not a caveat. Complexity should be avoided when possible as complexity brings
    cost and risk.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 系统设计的最佳实践是尽可能将架构保持简单，以满足需求，但又不能更简单。记住，简单是一种优势，而非限制。复杂性应尽量避免，因为复杂性会带来成本和风险。
- en: In any assessment, start with the brick. Just one, single, solitary stand alone
    server. Simple and effective. Now evaluate, does this meet your needs? How could
    spending more money better meet your needs?
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何评估中，从砖块开始。就一个，单独的独立服务器。简单而有效。现在评估一下，这是否满足你的需求？如何通过多花钱更好地满足你的需求？
- en: If high availability is needed, then assess hyperconvergence. Nothing can be
    more reliable, architecturally speaking.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要高可用性，请评估超融合架构。从架构角度来说，没人能比它更可靠了。
- en: If you have special cases where you need cost savings at massive scale, other
    designs might be applicable. But remember, no matter how reliable it might feel
    emotionally or how much a sales person may push the solution, hyperconvergence
    is literally impossible to beat for reliability by design. Make sure that any
    design that is not one of these two starting points is being used with a comprehensive
    understanding of all of the risks and costs involved.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有特殊情况需要在大规模下进行成本节约，其他设计可能会适用。但请记住，无论情感上感觉多么可靠，或销售人员如何推销该解决方案，超融合架构在设计上从根本上是无法被超越的可靠性。确保任何不是这两种起点之一的设计，都要在全面了解所有风险和成本的基础上使用。
- en: That is a lot of material that we covered and a lot of turning conventional
    thinking on its ear. It is sad that here *conventional thinking* equates to *blindly
    ignoring needs and risk logic*, but it is what it is. This is a very difficult
    topic because it is so foreign to most people in any technical or business realm
    and such a specialty skill to master. And in many cases, you will get a lot of
    pushback from others who struggle to assess or communicate risks, and fail to
    turn risk information into actionable business decision making.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们覆盖了大量的材料，也颠覆了很多传统思维。令人遗憾的是，*传统思维*在这里等同于*盲目忽视需求和风险逻辑*，但事实就是如此。这是一个非常困难的话题，因为它对大多数人来说既陌生，又是一个非常专业的技能。而且在许多情况下，你会遇到很多来自他人的反对，他们难以评估或沟通风险，也无法将风险信息转化为可执行的商业决策。
- en: At this point you have the tools and knowledge to design systems physically.
    This is a big topic, and it might be worth revisiting from time to time. This
    is very foundational and gives us the starting point to build reliable systems
    farther up the proverbial stack. And now that we know how to approach different
    designs for different purposes we will go on to look at risk itself and learn
    to ask what risk mitigation is right for us.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 此时你已经拥有了物理设计系统所需的工具和知识。这是一个庞大的话题，可能值得时不时地重新审视。这是非常基础的知识，为我们提供了在“象征性堆栈”上构建可靠系统的起点。现在，我们知道如何根据不同的需求设计不同的系统架构，接下来我们将探讨风险本身，学会询问什么样的风险缓解措施最适合我们。
- en: Risk assessment and availability needs
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 风险评估和可用性需求
- en: At the very core of what we do in designing a system architecture is taking
    business needs around performance and availability and applying our understanding
    of risk and, as with everything in business (and therefore IT) assessing against
    costs. In the last section we already talked about risk, a lot. We have to - risk
    and performance pretty much define everything (other than strict capabilities
    and features) for us during our design stages.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们设计系统架构的核心任务中，就是将业务需求（关于性能和可用性）与我们对风险的理解结合，并且像所有商业（因此也是IT）中的事情一样，评估成本。在上一部分中，我们已经讨论了很多关于风险的内容。我们必须这样做——风险和性能几乎定义了我们在设计阶段的所有内容（除了严格的能力和功能）。
- en: 'If we ask our businesses about risk, we almost always receive one of two stock
    answers: *we are not willing to pay anything to mitigate risks* or *we cannot
    afford to go down, it is worth anything to be up one hundred percent of the time*.
    Both answers should be obviously seen as insane and have no reason to ever come
    out of the mouth of any business person or IT professional, and yet they are nearly
    the only answers that you will ever receive providing you with no guidance whatsoever.
    They represent management simply *blowing off* IT and leaving IT to take on all
    decision-making risks without management providing any guidance.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们询问企业关于风险的看法，我们几乎总是会收到两种标准回答之一：*我们不愿意花费任何成本来缓解风险*，或者*我们不能承受停机，必须百分之百的时间保持在线*。这两种回答显然是疯狂的，任何商业人士或IT专业人士都不应该说出这样的答案，然而，这几乎是你得到的唯一答案，完全没有任何指导意义。它们代表着管理层完全*忽视*
    IT，将所有决策风险都留给IT，而管理层却没有提供任何指导。
- en: We have some amount of basic guidance that we can almost always work with. On
    the *low* end of the spectrum the rule of thumb is that if data is valuable enough
    to have stored in the first place, then at a minimum it is worth backing up. This
    is the simplest aspect of data and availability protection, and if your business
    thinks that the data that they store is not worth even backing up, you should
    be asking yourself why you are there yourself. There are extremely special cases
    where storage data is truly ephemeral and does not need a backup, but this situation
    is so unique and rare that it can be safely ignored.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一些基本的指导原则，几乎总是可以应用的。在范围的*低*端，经验法则是，如果数据足够有价值，值得一开始就存储，那么至少它是值得备份的。这是数据和可用性保护中最简单的方面，如果你的公司认为他们存储的数据甚至不值得备份，你应该问问自己，你为什么在这里工作。确实有一些极特殊的情况，存储的数据确实是暂时的，不需要备份，但这种情况是如此独特和罕见，以至于可以安全地忽略不计。
- en: On the other end of the spectrum no system, anywhere, ever is so important that
    it is worth anything to not have downtime. First of all, totally avoiding downtime
    is impossible. No one can do this with any amount of resources. We can make a
    system ridiculously reliable and easily recoverable from nearly endless potential
    scenarios but no government, military, secret cabal, alien species, investment
    bank, or otherwise can possibly meet requirements often demanded by small businesses
    without any discussion whatsoever. The theoretical maximum that can be invested
    into making systems reliable is the entire value of the company in question and
    even if every penny that the largest firms had was invested into reliability and
    nothing else, risk still remains, no matter how small.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在范围的另一端，没有任何一个系统，无论在哪里，都重要到不值得任何形式的停机。首先，完全避免停机是不可能的。没有人能够在任何资源的情况下做到这一点。我们可以使一个系统变得极其可靠，并能够从几乎无尽的潜在场景中轻松恢复，但无论是政府、军队、秘密集团、外星物种、投资银行，还是其他任何机构，都不可能满足小型企业经常要求的那些没有任何讨论的要求。为了使系统可靠所能投入的理论最大值，就是相关公司的全部价值，即使最大的公司将每一分钱都投入到提高可靠性而不做任何其他投资，风险仍然存在，无论多么微小。
- en: Risk and diminishing returns
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 风险与收益递减
- en: Attempting to invest in risk mitigation technology is a tough thing to do because
    as systems become more reliable the cost of *moving the needle* significantly
    towards ever improved reliability becomes more and more costly. For example, getting
    a stand alone server that is well built may give us as much as five nines of availability
    without any special *high availability* features.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试投资于风险缓解技术是一件非常困难的事情，因为随着系统变得越来越可靠，将“针”向更高可靠性移动的成本变得越来越昂贵。例如，获取一台构建良好的独立服务器，可能为我们提供高达五个9的可用性，而无需任何特殊的*高可用性*特性。
- en: We may find that we need much higher availability. Perhaps six or seven nines.
    To get those order of magnitude jumps in reliability will require, almost certainly,
    at least double the investment in hardware as the standalone server. This may
    be well justified by our needs, but the cost per workload just jumped significantly.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能发现我们需要更高的可用性，也许是六个或七个9。为了获得这种数量级的可靠性提升，几乎肯定需要至少双倍于独立服务器的硬件投资。这可能完全符合我们的需求，但每单位工作负载的成本大幅上升。
- en: If we want to move the needle again an order of magnitude beyond that, the price
    jumps yet again. We get less and less protection as we spend more and more money.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要再进一步，提升一个数量级，价格又会再一次跳升。我们投入更多的钱，得到的保护却越来越少。
- en: And so, because our business will rarely be willing or able to clearly define
    for us what our risk aversion level truly is, it often falls to IT and within
    IT to systems administration to carry out this all-important task on behalf of
    management. This will generally take some math, a lot of interviews with many
    different parts of the company, some common sense, and of course, some guesswork.
    Working with risk requires maintaining a logical view while it is tempting to
    become emotional, which is often the mistake made in business. Business owners
    or managers tend to react emotionally either seeing money spent on protection
    as not generating revenue directly and therefore undesirable, or seeing their
    business as not justifying protection and so tending to spend too much to provide
    an impression that the company is seen as valuable because downtime would be such
    a terrible thing.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，由于我们的企业很少愿意或能够清楚地定义我们的风险规避水平，通常需要由IT部门，具体来说是由系统管理团队，代表管理层执行这一极其重要的任务。这通常需要一些数学计算、大量与公司各个部门的访谈、一些常识，当然，还有一些猜测。与风险打交道时，需要保持一种逻辑性的视角，尽管情绪化的反应是非常诱人的，这也是商业中常犯的错误。企业主或经理往往会情绪化地反应，要么认为用于保护的资金没有直接产生收入，因此不值得投入；要么认为他们的业务不值得保护，从而过度支出，试图通过展示公司具有价值来给人留下深刻印象，因为停机对公司来说将是极其可怕的事情。
- en: Of course good management will always been heavily involved in risk assessment
    tasks. This should not fall to IT. While IT has great insight and is a valuable
    contributor to any risk discussion it is the core management, operations, and
    financial departments that truly have the full risk picture necessary to create
    a corporate infrastructure risk strategy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，良好的管理总是会在风险评估任务中发挥重要作用。这不应当由IT部门承担。虽然IT部门具有深刻的洞察力，并且在任何风险讨论中是一个宝贵的贡献者，但真正拥有完整风险图景的，是核心管理、运营和财务部门，他们才是制定公司基础设施风险策略所必需的部门。
- en: When looking at workloads, we must attempt to evaluate what downtime will truly
    cost our business. This is not straightforward in nearly any scenario, but it
    is what we need to understand to have any means of logically discussing risk.
    Most businesses want to simplify downtime cost into the simplest possible terms.
    So dollars per minute or hour is generally how downtime is discussed. For example,
    losing the company's primary line of business application will lose the business
    one thousand dollars per hour of downtime.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估工作负载时，我们必须尝试评估停机会给我们的企业带来什么真正的成本。这在几乎任何情况下都不是一件简单的事情，但这是我们需要理解的内容，以便有任何合理的方式来讨论风险。大多数企业希望将停机成本简化为最简单的术语。因此，每分钟或每小时的美元损失通常是讨论停机时间的方式。例如，失去公司的主要业务应用程序将使企业每小时损失一千美元。
- en: While simple, almost no real world workload actually loses revenue evenly hour
    by hour. In the real world it is most common to see a complex curve. For example,
    in the first minutes, and possibly even hours, we might see almost zero revenue
    loss. But then commonly we see a spike as outages go long enough to first be perceived
    by customers, then to cause customer concern. Lack of confidence and lack of operations
    spikes tend to hit a peak relatively quickly. Then long term revenue loss tends
    to start to kick in after days or weeks as customers leave. But this curve is
    different for every business. Of course, making an entire curve graph of all downtime
    scenarios is difficult and probably impractical, but the business should be able
    to predict significant inflection points along a timeline that represent major
    changes in impact behavior.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管简单，几乎没有现实中的工作负载会在每小时内均匀地损失收入。在现实中，通常会看到一条复杂的曲线。例如，在最初的几分钟，甚至可能是几个小时，我们可能几乎看不到任何收入损失。但随后，我们通常会看到一个峰值，因为停机时间足够长，首先会被客户察觉，接着引起客户的关注。缺乏信心和缺乏操作的峰值往往会相对迅速地达到顶峰。接着，长期的收入损失通常会在几天或几周后开始显现，因为客户流失。但这个曲线对每个企业来说都是不同的。当然，绘制出所有停机场景的完整曲线图是困难的，可能也不实际，但企业应该能够预测沿时间线的关键转折点，这些点代表了影响行为的重大变化。
- en: It is tempting to look at outages as being all or nothing. Basically, ignoring
    workloads and seeing the entire company as completely down, as if the zombie apocalypse
    is happening and all of the staff have been infected. It is a rare workload that
    is going to impact any business in that manner. For example, if a company loses
    an email workload there will likely be an impact, but as email is not real time,
    it might take hours or even days before there is an actual loss of revenue (but
    if email is used to win real time bids, losing even a few minutes might be very
    impactful - it just all depends.) But assuming email could not be restored for
    hours, or days, a normal business would immediately begin mitigating the loss
    of the email workload through other channels. Maybe employees talk to each other
    in person, or use the company's instant messaging products. Perhaps sales teams
    begin to call customers on the phone rather than emailing. Working around a lost
    workload is often far more possible and effective that one realizes until a triage
    process is performed to see what a real world recovery might really look like.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们常常容易将故障看作是全有或全无的情况。基本上，忽视工作负载，将整个公司视为完全瘫痪，就像僵尸末日发生了，所有员工都被感染了一样。能够如此影响企业的工作负载是非常罕见的。例如，如果公司失去了电子邮件工作负载，可能会带来影响，但由于电子邮件不是实时的，可能需要几个小时甚至几天才会实际造成收入损失（但如果电子邮件用于实时竞标，失去几分钟可能会产生非常大的影响——这完全取决于具体情况）。但假设电子邮件无法在几个小时或几天内恢复，正常的企业将立即通过其他渠道开始缓解电子邮件工作负载的损失。也许员工们会面对面交谈，或者使用公司即时通讯工具。也许销售团队开始通过电话联系客户，而不是发送电子邮件。在失去工作负载的情况下找到解决办法，往往比人们想象的要容易和有效，直到通过分诊过程来查看实际的恢复可能是什么样的。
- en: Workload interplay
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作负载相互作用
- en: Something else that we need to understand is how workloads interact with each
    other. As systems administrators we might have excellent insight into technical
    dependencies such as that a key ERP or CRM system depends on another application,
    such as email or financial, to function and if one is down, the other is down,
    too. That is an important aspect of workload dependency, but one that is well
    known and understood just by mentioning it. What is much harder to understand
    is the human workflow interdependence of systems.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们需要理解的内容是工作负载之间是如何相互作用的。作为系统管理员，我们可能对技术依赖关系有深入的了解，比如一个关键的ERP或CRM系统依赖于另一个应用程序（如电子邮件或财务系统）才能正常运行，如果其中一个系统出现故障，另一个也会受到影响。这是工作负载依赖关系的一个重要方面，但只需提到它，大家通常就能理解。更难理解的是系统的人类工作流程相互依赖性。
- en: Some workloads may be exceptionally stand alone. Some may depend significantly
    on others. Others may overlap and provide risk mitigation unofficially.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 有些工作负载可能是完全独立的。有些可能会显著依赖于其他工作负载。其他的工作负载可能会重叠并提供非正式的风险缓解。
- en: Lets look at the third case first. In many an organization today there might
    be traditional telephones, email, a few types of video conferencing solutions,
    and a handful of instant messaging solutions. Even in a tiny organization it is
    easy to casually end up with several overlapping solutions simply because so many
    things come bundled with so much functionality. In a situation like this, losing
    email might matter little for a very long time as internal communications may
    move to instant messaging and customer communications to telephone or video conference.
    Most organizations have the ability to work around a system that is down by using
    other tools at their disposal.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先来看第三种情况。如今许多组织可能拥有传统电话、电子邮件、几种类型的视频会议解决方案，以及一些即时通讯工具。即使在一个小型组织中，也很容易因为许多工具捆绑了大量功能而随意地拥有多个重叠的解决方案。在这种情况下，失去电子邮件可能在很长一段时间内影响不大，因为内部通讯可能会转移到即时通讯工具上，客户通讯可能会通过电话或视频会议进行。大多数组织都有能力通过使用其他可用工具来应对某个系统的故障。
- en: But the converse is also true. Two technically unrelated systems, again lets
    say CRM and email, might not connect together but the human workflow may require
    that both be used at the same time and the loss of either one of them might be
    functionally equivalent to losing them both. So we have to consider all use cases,
    and all mitigation possibilities.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 但反过来也是成立的。两个技术上没有关联的系统，比如CRM和电子邮件，可能无法直接连接，但人类的工作流程可能需要同时使用它们，而失去其中任何一个可能在功能上等同于失去两者。因此，我们必须考虑所有的使用案例以及所有的应对措施。
- en: This interplay knowledge will help us to determine how it makes sense to deploy
    some workloads. For example, if email and instant messaging work to overlap during
    a crisis, it likely makes sense to decouple them as much as possible so that if
    the hardware or software for one was to fail that it would not take down the other.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这种相互作用的知识将帮助我们决定如何合理地部署某些工作负载。例如，如果电子邮件和即时消息在危机期间会相互重叠，那么尽量解耦它们可能更为合适，以便在其中一个的硬件或软件发生故障时，不会波及到另一个系统。
- en: If we have systems, like our email and CRM example, where one is useless without
    the other, then combining the two workloads to share a failure domain might make
    total sense. Meaning, as an example, if we had two independent servers one running
    the CRM and one running email, then each individual server would carry its own
    risk of failing with near certainty that those failures would not happen at an
    overlapping time. Each workload has an equal amount of expected annual downtime
    of X. The total downtime expected for the combined workloads is 2X. Easy math.
    Combine the two workloads onto a single server as each retains an equal amount
    of annual downtime risk, still X, and the combined is still 2X. But the effective
    downtime in the first case is 2X (or 1.99999X as there is some tiny chance of
    the outages overlapping) but in the second case is just 1X. How did we do that?
    Not by reducing any individual risk, but by reducing the effective risk - that
    is the risk impacting the business as a final result. Under the hood, we did reduce
    risk physically as a single server has half the risk of downtime of two equal
    servers simply because there is half as many devices to fail.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一些系统，例如电子邮件和客户关系管理（CRM）系统，两个系统互为依赖，那么将这两种工作负载组合在一起共享失败域可能是完全合理的。举个例子，如果我们有两台独立的服务器，一台运行CRM，另一台运行电子邮件，那么每台服务器都会有自己的故障风险，并且几乎可以肯定，这些故障不会在重叠的时间发生。每个工作负载都有相同的预期年停机时间X。两个工作负载的总停机时间是2X。这个数学很简单。将这两个工作负载合并到同一台服务器上，因为每个工作负载仍然保持相等的年停机风险，即X，那么合并后的总停机时间仍然是2X。但第一个例子的有效停机时间是2X（或者1.99999X，因为停机重叠的几率非常小），而第二个例子的有效停机时间仅为1X。我们是怎么做到的？不是通过减少单一风险，而是通过降低有效风险——即影响到业务最终结果的风险。在背后，我们确实降低了风险，因为一台服务器的停机风险只有两台相等服务器的一半，原因就是故障的设备数减少了一半。
- en: Even a complete company shutdown is not necessary a total loss. Sending staff
    home for a surprise holiday might lower insurance costs and raise morale. Given
    a day or two to spend at home might reinvigorate workers who may be happy to return
    after systems have been restored and work more efficiently or maybe put in a little
    extra time to attempt to recoup lost business. We have to consider mitigation
    strategies when looking at losses from failed workloads. Some businesses may simply
    lose some efficiency, while others may lose customers.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是公司完全停运，也不一定意味着完全的损失。让员工回家享受一个突如其来的假期，可能会降低保险费用并提升士气。给员工一两天的时间待在家里，可能会重新激发他们的活力，他们可能会在系统恢复后高兴地回到工作岗位，更高效地工作，或者为弥补失去的业务投入一些额外时间。我们在评估失败工作负载所带来的损失时，必须考虑缓解策略。一些企业可能只是效率下降，而其他企业则可能会失去客户。
- en: Of course we have to consider the possibility of the opposite. What if you are
    a business that depends heavily on customer perception of high uptime and even
    a tiny outage has a sprawling impact? Maybe your entire business generates only
    one thousand dollars per hour, but loss of customer confidence from even a two
    hour outage (which we might assume could only, at maximum, lose us two thousand
    dollars in this case) resulted in the loss of customers resulting in tens or hundreds
    of thousands of dollars of losses!
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们也必须考虑相反的可能性。如果你是一家严重依赖客户对高可用性认知的企业，即使是一个小小的停机也可能产生广泛的影响呢？也许你的整个业务每小时仅生成一千美元的收入，但即便是两个小时的停机（在这种情况下，我们假设停机最多只会损失两千美元）也可能导致客户信心的丧失，从而引发客户流失，造成几十万甚至几百万美元的损失！
- en: All of these losses are just estimates. Even if an outage actually happens,
    there is no guaranteed way to know what revenue would have been without an outage
    having occurred. So if we cannot know this number for certain after something
    has happened then obviously we cannot know it with any certainty before the event
    that only might happen, has happened. Bottom line... estimating risk is very hard.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些损失都只是估算。即使停机事件真的发生了，也没有可靠的方式来知道如果没有发生停机，收入会是多少。所以如果我们在事件发生后无法确定这个数字，那么显然，在事件只可能发生、尚未发生之前，我们也无法确定这一数字。结论是……估算风险是非常困难的。
- en: 'In a large organization, consider playing *what if* games on a weekend with
    some staff from different departments. Run through scenarios of *X or Y has failed*
    and attempt to mitigate in a nearly real-world simulation on a small scale. Can
    you keep working with one tool or another? Which departments become dysfunctional,
    which keep humming along, how do your customers see the situation? This kind of
    *game* is best played with a combination of strong planners who are thinking about
    risk strategies and writing procedures as well as with a group of perceivers (for
    example: triage experts) who do not plan, but work tactically on the ground figuring
    out how to keep working with the tools at their disposal.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个大型组织中，可以考虑在周末与来自不同部门的员工进行 *假设* 游戏。模拟一些 *X 或 Y 失败了* 的场景，并尝试在小范围内进行几乎真实世界的模拟来减轻问题。你能用某个工具继续工作吗？哪些部门会变得功能失效，哪些仍能正常运行，你的客户如何看待这种情况？这种类型的
    *游戏* 最适合由强大的规划者与感知者（例如：急救专家）结合进行。规划者思考风险策略并编写程序，而感知者则不做计划，而是在现场进行战术工作，找出如何利用现有工具继续工作。
- en: Defining high availability
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义高可用性
- en: One of my favorite quotes in all of IT comes from John Nicholson who said *High
    availability isn't something that you buy, it is something that you do*. It is
    so tempting to see high availability as an intrinsic need in IT, and then to see
    it as so complicated that we cannot know how to approach it, and so fall prey
    to vendors who slap the unverified name *high availability* onto products, or
    even just slip it into a product name, and act as if buying a product can deliver
    high availability when logically, this is impossible. Imagine buying a high availability
    airplane, as an example. While you can make one airplane must more reliable than
    another, almost all of your overall reliability and safety comes from the pilot,
    not the plane. The same is true in IT. The best made product does little if operated
    poorly. A million-dollar cluster without backups is likely not as safe as a desktop
    with good backups!
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 IT 领域最喜欢的一句名言来自约翰·尼科尔森，他说过 *高可用性不是你能购买的东西，而是你需要做的事情*。我们常常会把高可用性看作是 IT 中的一个内在需求，认为它复杂到无法理解，因此会被那些将未经验证的
    *高可用性* 标签贴在产品上的厂商所欺骗，甚至仅仅把它作为产品名称的一部分，假装购买某个产品就能实现高可用性，然而从逻辑上讲，这几乎是不可能的。比如，想象一下购买一架高可用性的飞机。虽然你可以使一架飞机比另一架飞机更可靠，但你整体的可靠性和安全性主要来自飞行员，而非飞机本身。在
    IT 领域也是如此。即便是做工最好的产品，如果操作不当，其效果也微乎其微。一个没有备份的百万美元集群，可能比一个备份完善的台式机还不安全！
- en: So first we need to establish a baseline for measurement. In our last section
    we said that stand alone server infrastructure serves as our baseline. This baseline
    has to represent what we will call *standard availability*. We now have two ways
    that we would hope that we can look at this availability. One is in absolute terms
    by giving a number such as a *nines* number and through industry evidence, it
    appears that well maintained, well-made stand alone servers can approach five
    nines of availability which roughly means six minutes, or less, of unplanned downtime
    per year (planned downtime for maintenance can represent a potential problem,
    but is not itself included in a reliability figure such as this.)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，首先我们需要建立一个衡量的基准。在上一节中，我们提到独立服务器基础设施作为我们的基准。这一基准必须代表我们所说的 *标准可用性*。我们现在有两种方法来衡量这种可用性。一种是通过给出一个
    *九* 数字，来以绝对的方式表示可用性，通过行业证据，维护良好、做工精良的独立服务器可以达到五个九的可用性，这大致意味着每年有六分钟或更少的计划外停机时间（计划内的维护停机时间可能是潜在问题，但并不包括在这样的可靠性数字中）。
- en: Now keep in mind, when we are talking about a server or a system design architecture,
    we are not including the final workload, only the platform providing an underlying
    system onto which a hypervisor will be installed. So essentially hardware availability.
    Any software running on top may have its own reliability concerns and no amount
    of platform stability will fix instability from bad code in the final workload,
    for example.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在请记住，当我们谈论服务器或系统设计架构时，我们并不包括最终的工作负载，只是指提供底层系统的平台，平台上将安装虚拟机监控程序。所以，基本上是指硬件可用性。在其上运行的任何软件可能有自己的可靠性问题，平台的稳定性无法解决来自最终工作负载中糟糕代码带来的不稳定问题。
- en: The other, and generally more useful, way to look at reliability of system architecture
    is not in unmeasurable absolute terms, but in relative terms comparing different
    designs to one another. No one really knows what system reliability really is.
    It is not a big secret that server vendors are keeping from us, they simply do
    not know. Every little system configuration difference produces very different
    reliability numbers and, like we said about planes, the users operating the systems
    create the largest impact in terms of reliability. A company with a pristine datacenter
    and continuous onsite support that responds to alerts immediately and spare parts
    on hand or nearby might be able to squeeze very different reliability numbers
    out of the same server stuck in a closet without air conditioning, lots of dust,
    and generally ignored. There are simply too many factors involved. And even if
    we could somehow account for all of the potential variation, in order to get meaningful
    statistics on systems so complex with failure rates so low, we would need to operate
    thousands or tens of thousands of servers for more than a decade to collect useful
    numbers and then all of the data would be outdated by more than a decade. So,
    for all intents and purposes, it cannot be measured.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 另外一种，通常也更有用的方式来看待系统架构的可靠性，不是以无法衡量的绝对标准，而是通过相对方式将不同设计进行比较。没有人真正知道系统的可靠性是什么。服务器供应商并没有在对我们保密，他们只是不了解。每一个小小的系统配置差异都会产生非常不同的可靠性数据，就像我们对飞机的说法一样，操作系统的用户对可靠性产生了最大的影响。一个拥有完美数据中心、持续现场支持、能及时响应警报并且有备件随时待命的公司，可能会从同一台困在没有空调、充满灰尘并且通常被忽视的机房里的服务器中挤出完全不同的可靠性数据。影响因素实在是太多了。即使我们能够
    somehow 考虑所有潜在的变化，为了获得关于如此复杂且故障率如此低的系统的有意义统计数据，我们也需要运营数千台甚至数万台服务器超过十年，才能收集到有用的数据，而这些数据过十年后都会过时。所以，实际上，这些都无法被衡量。
- en: So, our most important tool is not talking in terms of *nines*, that is a great
    marketing tool and something that managers steeped in big heavy processes like
    Six Sigma like to repeat, but it means nothing in this context, but rather looking
    at orders of magnitude of systems deviating from our baseline. A system that is
    significantly more available than our baseline can be classified as *high availability*
    and a system that is significantly less available than our baseline can be classified
    as *low availability* and any system that is roughly the same as baseline remains
    *standard availability*. Beyond these general terms it becomes all but impossible
    to discuss.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们最重要的工具不是以*九个*为单位谈论，这是一个伟大的营销工具，也是一些深陷于复杂流程（如六西格玛）的管理者喜欢重复的内容，但在这个语境下并没有实际意义。相反，我们应该着眼于系统与基准的偏差数量级。比基准可用性高得多的系统可以被归类为*高可用性*，而比基准可用性低得多的系统可以被归类为*低可用性*，任何与基准大致相同的系统则保持*标准可用性*。超出这些一般术语之后，讨论几乎变得不可能。
- en: A system design like hyperconvergence would be generally classified as *high
    availability* as it is the most reliable design approach. And an IPOD would generally
    be classified as *low availability* as it is closer to the least reliable design
    approach, which is the network system design. Layered clustering is generally
    considered high availability, but not *as high* as hyperconvergence. Of course,
    in this case we are only considering the availability of the system design and
    ignoring individual components. If we use extremely highly available individual
    components at every layer of an IPOD, we can theoretically get it back up to standard
    availability, but likely at great cost.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 类似超融合的系统设计通常被归类为*高可用性*，因为它是最可靠的设计方法。而IPOD通常被归类为*低可用性*，因为它更接近于最不可靠的设计方法——网络系统设计。分层集群通常被认为是高可用性，但不*如超融合*那样高。当然，在这种情况下，我们仅仅考虑系统设计的可用性，而忽略单个组件。如果我们在IPOD的每一层都使用极高可用性的单个组件，理论上可以将其恢复到标准可用性，但可能需要付出极大的成本。
- en: It is far more valuable to think of reliability in relative terms, rather than
    absolute ones. It is almost trivial to look at a standalone server, an IPOD, and
    hyperconvergence and see how there is a clear *high*, *medium*, and *low* availability
    based on nothing but common sense and the location of risk, risk mitigation, and
    risk accumulation in the design. It requires no special training or math to see
    how dramatically each is separated from the next and how improving the overall
    quality of components moves the absolute reliability number, but the relative
    does not change. And at the end of the day, this is all that we can know.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 从相对角度考虑可靠性要比从绝对角度考虑更有价值。单纯通过常识和风险的定位、风险缓解、风险积累等设计要素，几乎可以轻松看出一个独立服务器、一个IPOD以及超融合系统在可用性上的明显*高*、*中*、*低*差异。这不需要特别的培训或数学能力，就能明了每个系统之间的区别以及如何提升组件的整体质量会移动绝对可靠性数值，而相对可靠性却不变。最终，我们所能知道的就是这些。
- en: By knowing what downtime impact will look like financially for our business,
    even if it is only a very rough estimate, we have something to work with when
    attempting to decide on how to invest in risk mitigation. We should never invest
    more in risk mitigation than what calculated risk shows as our potential losses.
    This sounds obvious but is a common stumbling point for assessment in many firms.
    For example, if a possible outage might cost us one thousand dollars and protecting
    effectively against that outage would cost two thousand dollars, we should not
    at all entertain paying to mitigate that risk.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 通过了解停机对我们业务的财务影响，即使只是一个非常粗略的估算，我们在决定如何投资风险缓解时就有了参考依据。我们永远不应在风险缓解上投入超过计算风险所显示的潜在损失。这听起来很明显，但在许多公司中这是一个常见的评估难点。例如，如果一次可能的停机会导致我们损失一千美元，而有效地保护这一停机的费用是两千美元，我们就绝不应考虑花钱来缓解这个风险。
- en: We should think of risk mitigation as a form of outage itself, for mathematical
    reasons. This makes calculations easier to understand. With good risk mitigation
    we would incur a minimal financial penalty now (say spending one thousand dollars)
    to protect against a large potential outage (that might cost us one hundred thousand
    dollars.) The upfront cost is guaranteed, the future risk is only a possibility.
    So any risk mitigation must therefore be much smaller than the potential damage
    that it is meant to protect against.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该把风险缓解看作是一种停机本身，从数学角度来看，这是为了让计算变得更容易理解。通过良好的风险缓解，我们现在会承受最低的财务损失（例如花费一千美元）来防止一次可能导致十万美元损失的大规模停机。前期成本是确定的，而未来风险仅仅是一个可能性。因此，任何风险缓解的投入都应该远低于其所要保护的潜在损害。
- en: 'An analogy of my own that I have been using for years to describe paying more
    for risk mitigation than the potential damage of the outage itself is: *That''s
    like shooting yourself in the face today to avoid maybe getting a headache next
    year*.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我用来形容为了缓解风险而支付的费用超出了潜在损害的类比已经用了多年：*这就像今天为了避免明年可能会得头痛而朝自己脸上开枪*。
- en: 'When comparing standard availability systems and high availability systems
    we might be talking about a difference of only several minutes per year, on average,
    of downtime. High availability, therefore, has to justify its cost and complexity
    very quickly. A massive public website where just a minute or two of being unavailable
    could cost millions in purchases or worse, erode customer confidence, therefore
    could easily justify a large expenditure in high availability systems even if
    the time saved seems trivial. But an internal system servicing employees where
    customer confidence is not a factor, and downtime does not lead users to turn
    to competitors (for example: email system, financial, CRM, and others) the lose
    of even several minutes a day, let alone a year, is likely to have no real financial
    impact whatsoever and investing heavily to protect those systems would be wasteful.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较标准可用性系统和高可用性系统时，我们可能只是在讨论每年平均几分钟的停机时间差异。因此，高可用性必须迅速证明其成本和复杂性的合理性。对于一个巨大的公共网站，如果仅仅一两分钟的不可用就可能导致数百万的购买损失，或者更糟糕的是，损害客户信任，那么即使节省的时间看起来微不足道，仍然可以轻松证明在高可用性系统上的大量支出是合理的。但对于一个服务员工的内部系统而言，客户信任并非关键因素，且停机不会导致用户转向竞争对手（例如：电子邮件系统、财务系统、CRM系统等），即便是每天失去几分钟，甚至一年，也可能对财务没有实际影响，重金投入来保护这些系统反而是浪费。
- en: So, how do we apply all of this to best practices? The hard answer is that risk
    assessments and resulting system design are very hard things to do. Determining
    risk is a long process involving a lot of math, logic, and to some degree, guessing.
    It requires that we understand our businesses and our technology stacks deeply.
    It demands that we engage the business at all levels, and from all departments,
    and consolidate information that is generally siloed. It forces us to evaluate
    other risk assessments against logic and expected emotional reactions.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何将这一切应用到最佳实践中呢？困难的答案是，风险评估和随之而来的系统设计是非常艰难的工作。确定风险是一个漫长的过程，涉及大量的数学、逻辑，并在某种程度上需要猜测。这要求我们深入了解我们的业务和技术栈。它要求我们在各个层面和各部门与业务进行互动，整合通常是孤立的信息。它迫使我们将其他风险评估与逻辑和预期的情感反应进行对比。
- en: Rules of thumb tell us that the majority of systems that we deploy should be
    standalone servers, and nearly all remaining systems should be hyperconverged.
    These two standard patterns represent the near totality of what proper design
    will look like in the real world. All other designs are realistically relegated
    to extremely niche use cases with the IPOD being the ultimate *anti-pattern* of
    what not to do except for the most extreme of special cases.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 经验法则告诉我们，我们部署的大多数系统应该是独立服务器，几乎所有剩下的系统应该是超融合的。这两种标准模式代表了现实世界中正确设计的近乎全部内容。所有其他设计实际上都被归类为极为小众的用例，其中IPOD是最典型的*反模式*，除了极端特殊情况外，绝对不要去做。
- en: We have covered a lot of material in this chapter. But now we have an idea of
    how we make risk determinations, how we design our architectures based on that
    assessment. We understand how and why we use different kinds of virtualization,
    and why we always virtualize. And we know how to evaluate the use of cloud and
    locality for our deployments. Now to put all of this together! We use all of these
    tools in deciding the deployment of every workload! So many options, but that
    is what makes our careers challenging and fulfilling (and what makes us worth
    our salaries.)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们覆盖了很多内容。但现在我们已经知道如何做出风险判断，如何基于这种评估设计架构。我们理解了如何以及为何使用不同种类的虚拟化，为什么我们总是进行虚拟化。而且我们知道如何评估云和局部性在部署中的使用。现在，将这一切结合起来！我们在决定每个工作负载的部署时都会使用这些工具！有这么多选择，但这正是让我们的职业充满挑战和成就感（也是让我们值回薪水的原因）。
- en: Summary
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In summary, system architecture is complex and requires us to really dig into
    business needs, how operations works, talk to key roles throughout the organization
    and elicit input, and take a broad view of technological building blocks to construct
    solutions that deliver the performance and reliability that our workloads need
    at the minimum cost.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，系统架构是复杂的，要求我们真正深入了解业务需求、运营方式，与组织中的关键角色沟通并获取意见，并广泛地考虑技术构建块，构建能够以最低成本提供所需性能和可靠性的解决方案。
- en: We looked at fundamental components with virtual machines and containers and
    should now be able to defend our use of them and choose properly between them,
    as well as be able to use traditional containers without becoming confused with
    more recent application containers. And we learned about locality. You should
    be able to navigate the complicated linguistic minefield that is managers attempting
    to talk about the placement and ownership of server resources, analyze costs and
    risks and find the right option for your organization. Colocation, cloud, traditional
    virtualization, on premises are all options that you understand.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了虚拟机和容器的基本组件，现在应该能够为它们的使用辩护，并在它们之间做出正确选择，还能使用传统容器而不被较新的应用容器搞混。而且我们学到了局部性。你应该能够穿越复杂的语言迷宫，理解经理们讨论服务器资源的部署和所有权时的难点，分析成本和风险，并为组织找到合适的选择。共址、云、传统虚拟化、本地部署，所有这些选择你都能理解。
- en: And finally, the big piece, system design and architecture. Taking the physical
    and logical components of our system and building a full functional platform that
    empowers our workloads rather than crippling them. This has been a long chapter
    and touches on a lot of topics that are very rarely taught individual, let alone
    together. These are some really hard topics, and it is probably worth covering
    a lot of this material again before moving on.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来谈谈系统设计和架构。将我们系统的物理和逻辑组件整合，构建一个完整的功能平台，能够提升我们的工作负载，而不是削弱它们。这是一个很长的章节，涉及了许多非常少被单独教授的话题，更不用说将它们结合起来了。这些都是非常难的话题，可能在继续之前值得再次深入探讨这些内容。
- en: For many of us in systems administration we might use the material in this chapter
    almost never. For others it might be nearly everyday skills. These topics are
    often ones that allow you to completely elevate your career by demonstrating a
    concrete ability to take seemingly mundane technical minutia and applying background
    system design decisions to key organizational needs. Of all of our topics in this
    book, this one is probably the one that should empower you more than any other
    to stand out among your peers and cross organizational boundaries.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多从事系统管理的我们来说，可能几乎永远不会使用本章节中的内容。而对于其他人来说，这可能是几乎每天都需要的技能。这些话题通常能让你通过展示能够将看似平凡的技术细节与系统设计决策结合，解决关键的组织需求，彻底提升你的职业生涯。在本书的所有话题中，这个话题可能是最能让你脱颖而出，跨越组织边界的一个。
- en: I hope that with the information presented here that you can filter through
    sales and marketing misinformation, apply solid logic and reasoning, and build
    on concepts that will remain timeless. Taking the time to really understand failure
    domains, additive risk, false redundancy, and more will make you better at nearly
    every aspect of your information technology journey whether your goals are purely
    technical, or you dream of sitting in the board room chairs.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望通过这里提供的信息，您能够过滤掉销售和市场营销的虚假信息，运用严密的逻辑和推理，并在永恒的概念上进行构建。花时间真正理解故障域、附加风险、虚假冗余等，将使你在信息技术旅程的几乎每个方面变得更加出色，无论你的目标是纯粹的技术性，还是梦想着坐上董事会的椅子。
- en: In our next chapter, we are going to return to the seemingly more pedestrian
    topic of system patching, and move from the high level system strategies to in
    the trenches security and stability warfare.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一个章节中，我们将回到看似更为平凡的话题——系统补丁管理，并从高层次的系统策略转向实际操作中的安全性和稳定性战争。
