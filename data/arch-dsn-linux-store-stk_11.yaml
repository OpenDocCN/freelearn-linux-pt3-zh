- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Tuning the I/O Stack
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调优 I/O 堆栈
- en: Well, here we are at the end of our journey. Just because you are reading the
    introduction of the final chapter does not mean that you’ve read through the entire
    book, but I’ll take my chances. If you’ve indeed followed us along, then I hope
    your journey was worth it and has left you yearning for more.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们已经到了旅程的终点。仅仅因为你正在阅读最后一章的介绍，并不意味着你已经读完了整本书，但我还是要冒险这么说。如果你真的跟随我们走到这里，那么我希望这段旅程对你来说是值得的，并且让你渴望了解更多。
- en: Getting back to brass tacks, the previous two chapters centered on the performance
    analysis of the I/O stack. [*Chapter 9*](B19430_09.xhtml#_idTextAnchor160) focused
    on the most common disk metrics and the tools that can help us to identify performance
    bottlenecks in physical disks. In any performance analysis, the physical disks
    come under far more scrutiny than any other layer, which can sometimes be misleading.
    Therefore, in [*Chapter 10*](B19430_10.xhtml#_idTextAnchor184), we saw how we
    can investigate the higher layers in the I/O stack, such as filesystems and the
    block layer.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 回到正题，前两章主要集中在 I/O 堆栈的性能分析。[*第9章*](B19430_09.xhtml#_idTextAnchor160) 重点介绍了最常见的磁盘指标以及帮助我们识别物理磁盘性能瓶颈的工具。在任何性能分析中，物理磁盘往往比任何其他层次受到更多的关注，这有时可能会产生误导。因此，在[*第10章*](B19430_10.xhtml#_idTextAnchor184)中，我们探讨了如何检查
    I/O 堆栈中的更高层次，如文件系统和块层。
- en: This brings us to the next logical step in our quest. Once we’ve identified
    the elements plaguing our environment, what steps can we take to mitigate those
    limitations? It is important to have specific goals for the desired tuning outcomes
    because, at the end of the day, performance tuning is a trade-off between choices.
    For instance, tuning the system for low latency may reduce its overall throughput.
    A performance baseline should first be determined, and any tweaks or adjustments
    should be carried out in small sets. This chapter will deal with the different
    tweaks that can be applied to improve I/O performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这将引导我们进入下一个逻辑步骤：一旦我们识别出环境中的瓶颈，接下来可以采取哪些步骤来缓解这些限制？设定具体的目标对于调优结果至关重要，因为最终，性能调优是不同选择之间的权衡。例如，调优系统以降低延迟可能会减少其整体吞吐量。首先应确定一个性能基线，任何调整或改动都应分小范围进行。本章将探讨可以应用的不同调整方法，以改善
    I/O 性能。
- en: 'Here’s an outline of what follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是大纲：
- en: How memory usage affects I/O
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存使用如何影响 I/O
- en: Tuning the memory subsystem
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调优内存子系统
- en: Tuning the filesystem
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调优文件系统
- en: Choosing the right scheduler
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择合适的调度器
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The material presented in this chapter builds on the concepts discussed in preceding
    chapters. If you’ve followed along and have become familiar with the functions
    of each layer in the disk I/O hierarchy, you’ll find this chapter much easier
    to follow. If you have a prior understanding of memory management in Linux, that
    will be a huge plus.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中展示的内容是基于前面章节讨论的概念。如果你已经跟随阅读并熟悉了磁盘 I/O 层次中每一层的功能，那么你会发现本章内容更容易理解。如果你对 Linux
    中的内存管理有先前了解，那将是一个巨大的优势。
- en: The commands and examples presented in this chapter are distribution-agnostic
    and can be run on any Linux operating system, such as Debian, Ubuntu, Red Hat,
    or Fedora. There are quite a few references to the kernel source code. If you
    want to download the kernel source, you can download it from [https://www.kernel.org](https://www.kernel.org).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中展示的命令和示例与发行版无关，可以在任何 Linux 操作系统上运行，如 Debian、Ubuntu、Red Hat 或 Fedora。文中提到了一些内核源代码的参考。如果你想下载内核源代码，可以从[https://www.kernel.org](https://www.kernel.org)下载。
- en: How memory usage affects I/O
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存使用如何影响 I/O
- en: 'As we’ve seen, VFS serves as an entry point for our I/O requests and includes
    different types of caches, the most important of which is the page cache. The
    purpose of page cache is to improve I/O performance and minimize the expense of
    I/O as generated by swapping and file system operations, thus avoiding unnecessary
    trips to the underlying physical disks. Although we haven’t explored it in these
    pages, it is important to have an idea about how the kernel goes about managing
    its memory management subsystem. The memory management subsystem is also referred
    to as the **virtual memory manager** (**VMM**). Some of the responsibilities of
    the virtual memory manager include the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，VFS 作为我们 I/O 请求的入口点，并包含不同类型的缓存，其中最重要的是页面缓存。页面缓存的目的是提高 I/O 性能，并最小化由于交换和文件系统操作产生的
    I/O 成本，从而避免不必要的访问底层物理磁盘。虽然我们在这些页面中没有详细探讨，但了解内核如何管理其内存管理子系统是很重要的。内存管理子系统也被称为**虚拟内存管理器**（**VMM**）。虚拟内存管理器的一些职责包括：
- en: Managing the allocation of physical memory for all the user space and kernel
    space applications
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理所有用户空间和内核空间应用程序的物理内存分配
- en: Implementation of virtual memory and demand paging
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟内存和需求分页的实现
- en: The mapping of files into processes address space
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文件映射到进程的地址空间
- en: Freeing up memory in case of shortage, either by pruning or swapping caches
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在内存不足的情况下，释放内存，要么通过修剪缓存，要么通过换出缓存
- en: 'As it’s often said, *the best I/O is the one that is avoided*. The kernel follows
    this approach and makes generous allocation of the free memory, filling it up
    with the different types of caches. The greater the amount of free memory available,
    the more effective the caching mechanism. This all works well for general use
    cases, where applications perform small-scale requests and there is a relative
    amount of page caches available:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如人们常说的，*最好的 I/O 就是避免 I/O*。内核遵循这一方法，充分分配空闲内存，用于填充不同类型的缓存。空闲内存越多，缓存机制越有效。这一方法在一般使用场景中表现良好，尤其是在应用程序执行小规模请求且有一定数量的页面缓存可用的情况下：
- en: '![Figure 11.1 – A page cache can speed up I/O performance](img/B19430_11_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – 页面缓存可以加速 I/O 性能](img/B19430_11_01.jpg)'
- en: Figure 11.1 – A page cache can speed up I/O performance
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – 页面缓存可以加速 I/O 性能
- en: Conversely, if memory is scarcely available, not only will the caches be pruned
    regularly but the data might also get swapped out to disk, which will ultimately
    hurt performance. The kernel works under the **temporal locality principle**,
    meaning that the recently accessed blocks of data are more likely to be accessed
    again. This is generally good for most cases. It could take a few milliseconds
    to read data from a random part of the disk, whereas accessing that same data
    from memory if it is cached only takes a few nanoseconds. Therefore, any request
    that can be readily served from the page cache minimizes the cost of an I/O operation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果内存资源稀缺，不仅缓存会定期被修剪，数据可能还会被换出到磁盘，这最终会影响性能。内核遵循**时间局部性原理**，意味着最近访问的数据块更可能会再次被访问。这对大多数情况来说通常是有利的。从磁盘的随机位置读取数据可能需要几毫秒，而如果该数据已经缓存到内存中，访问它只需几纳秒。因此，任何能够直接从页面缓存中提供的数据请求都能最小化
    I/O 操作的成本。
- en: Tuning the memory subsystem
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调优内存子系统
- en: 'It’s a bit strange that how Linux deals with memory can have a major say in
    disk performance memory. As already explained, the default behavior of the kernel
    works well in most cases. However, as they say, an excess of everything is bad.
    Frequent caching can result in a few problematic scenarios:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有点奇怪的是，Linux 如何处理内存实际上可能会对磁盘性能内存产生重大影响。正如之前所解释的，内核的默认行为在大多数情况下效果良好。然而，正如人们所说，凡事过犹不及。频繁缓存可能会导致一些问题场景：
- en: When the kernel has accumulated a large amount of data in the page cache and
    eventually starts to flush that data onto disk, the disk will remain busy for
    quite some time because of the excessive write operations. This can adversely
    affect the overall I/O performance and increase disk response times.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当内核在页面缓存中积累了大量数据，并最终开始将数据刷新到磁盘时，由于频繁的写入操作，磁盘将会长时间处于忙碌状态。这可能会对整体 I/O 性能产生不利影响，并增加磁盘响应时间。
- en: The kernel does not have a sense of the criticality of the data in the page
    cache. Hence, it does not distinguish between *important* and *unimportant* I/O.
    The kernel picks whichever block of data it deems appropriate and schedules it
    for a write or read operation. For instance, if an application performs both background
    and foreground I/O operations, then usually, the priority of the foreground operations
    should be higher. However, I/O belonging to background tasks can overwhelm foreground
    tasks.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核并不理解页面缓存中数据的重要性。因此，它不会区分*重要*和*不重要*的 I/O。内核会选择它认为合适的数据块，并安排进行读写操作。例如，如果一个应用程序同时执行后台和前台
    I/O 操作，通常前台操作的优先级应该更高。然而，属于后台任务的 I/O 可能会压倒前台任务。
- en: The cache provided by the kernel usually enables applications to obtain better
    performance when reading and writing data, but the algorithms used by the page
    cache are not designed for a particular application; they are designed to be general-purpose.
    In most cases, this default behavior would work just fine, but in some cases,
    this can backfire. For some self-caching applications, such as database management
    systems, this approach might not offer the best results. Applications such as
    databases have a better understanding of the way data is organized internally.
    Hence, these systems prefer to have their own caching mechanism to improve read
    and write performance.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 内核提供的缓存通常能够使应用程序在读取和写入数据时获得更好的性能，但页面缓存所使用的算法并不是为特定应用程序设计的；它们是为了通用用途而设计的。在大多数情况下，这种默认行为效果很好，但在某些情况下，这可能适得其反。对于一些自缓存应用程序，例如数据库管理系统，这种方法可能不会提供最佳的结果。像数据库这样的应用程序更好地理解数据在内部的组织方式。因此，这些系统更倾向于使用自己的缓存机制，以提高读写性能。
- en: Using direct I/O
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用直接 I/O
- en: If data is cached directly at the application level, then moving data from disk
    to the page cache and back to the application’s cache will constitute a significant
    overhead, resulting in more CPU and memory usage. In such scenarios, it might
    be desirable to bypass the kernel’s page cache altogether and leave the responsibility
    of caching to the application. This is known as **direct I/O**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据直接在应用程序级别进行缓存，那么将数据从磁盘移动到页面缓存，再返回到应用程序缓存将构成显著的开销，导致更多的 CPU 和内存使用。在这种情况下，可能希望完全绕过内核的页面缓存，将缓存的责任交给应用程序。这就是**直接
    I/O**。
- en: 'Using direct I/O, all the file reads and writes go directly from the application
    to the storage device, bypassing the kernel’s page cache. The `O_DIRECT` flag
    on a system call, such as `open ()`. The `O_DIRECT` flag is only a status flag
    (represented by DIR), which is passed by the application while opening or creating
    a file so that it can go around the page cache of the kernel:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用直接 I/O 时，所有文件的读写操作直接从应用程序传输到存储设备，绕过了内核的页面缓存。`O_DIRECT` 标志用于系统调用，例如`open()`。`O_DIRECT`
    标志仅仅是一个状态标志（表示为 DIR），它由应用程序在打开或创建文件时传递，以便绕过内核的页面缓存：
- en: '![Figure 11.2 – The different ways of performing I/O](img/B19430_11_02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – 执行 I/O 的不同方式](img/B19430_11_02.jpg)'
- en: Figure 11.2 – The different ways of performing I/O
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 执行 I/O 的不同方式
- en: It doesn’t make sense to use direct I/O for regular applications, as it can
    cause performance deterioration. However, for self-caching applications, it can
    offer significant gains. The recommended method is to check the status of direct
    I/O via an application. However, if you want to check via the command line, use
    the `lsof` command to check the flags through which a file is opened.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于常规应用程序来说，直接 I/O 并不合理，因为它可能导致性能下降。然而，对于自缓存应用程序，它可以提供显著的性能提升。推荐的方法是通过应用程序检查直接
    I/O 的状态。然而，如果你想通过命令行进行检查，可以使用`lsof`命令查看文件是通过哪些标志打开的。
- en: '![Figure 11.3 – Checking direct I/O](img/B19430_11_03.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 – 检查直接 I/O](img/B19430_11_03.jpg)'
- en: Figure 11.3 – Checking direct I/O
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 检查直接 I/O
- en: For files opened by the application through the `O_DIRECT` flag, the **FILE-FLAG**
    column of the output will include the **DIR** flag.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于通过`O_DIRECT`标志由应用程序打开的文件，输出的**FILE-FLAG**列将包括**DIR**标志。
- en: The performance gains from direct I/O come from avoiding the CPU cost of copying
    data from disk into the page cache, and from steering clear of the double buffering,
    once in the application and once in the filesystem.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 直接 I/O 带来的性能提升源于避免了将数据从磁盘复制到页面缓存的 CPU 开销，并避免了双重缓存，一次是在应用程序中，一次是在文件系统中。
- en: Controlling the write-back frequency
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制写回频率
- en: As already explained, caching has its advantages, as it accelerates many accesses
    to files. Once most of the free memory has been occupied by the cache, the kernel
    has to make a decision on how to free memory in order to entertain incoming I/O
    operations. Using the **Least Recently Used** (**LRU**) approach, the kernel does
    two things – it evicts old data from the page cache and even offloads some of
    it to the swap area, in order to make room for incoming requests.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，缓存有其优势，因为它加速了对文件的访问。一旦大多数空闲内存被缓存占用，内核就必须做出决定，如何释放内存以处理即将到来的 I/O 操作。使用**最近最少使用**（**LRU**）方法，内核做了两件事——它从页面缓存中驱逐旧数据，甚至将部分数据卸载到交换区，以腾出空间处理新的请求。
- en: 'Again, it all comes down to the specifics. The default approach is good enough,
    and that is exactly how the kernel should go about making room for incoming data.
    However, consider the following scenarios:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 归根结底，一切都取决于具体情况。默认的处理方法已经足够好，这正是内核处理如何为即将到来的数据腾出空间的方式。然而，考虑到以下几种场景：
- en: What if the data currently in the cache won’t be accessed again in the future?
    This is true for most backup operations. A backup operation will read and write
    a lot of data from the disk, which will be cached by the kernel. However, it is
    unlikely that this data, which is present in the cache, will be accessed in the
    near future. However, the kernel will keep this data in the cache and might evict
    the older pages, which had a greater probability of being accessed again.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果当前缓存中的数据将来不会再次被访问怎么办？对于大多数备份操作来说，这种情况是成立的。备份操作会从磁盘读取和写入大量数据，这些数据将被内核缓存。然而，缓存中的这些数据在不久的将来不太可能被再次访问。不过，内核仍然会将这些数据保留在缓存中，并可能驱逐更旧的页面，这些页面更有可能再次被访问。
- en: Swapping data to disk will generate a lot of disk I/O, which won’t be good for
    performance.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据交换到磁盘会产生大量的磁盘 I/O，这对性能是不利的。
- en: When a large amount of data has been cached, a system crash can result in a
    major loss of data. This can be a significant concern if data is of a sensitive
    nature.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当大量数据被缓存时，系统崩溃可能会导致大量数据丢失。如果数据具有敏感性，这可能是一个重要问题。
- en: 'It’s not possible to disable the page cache. Even if there was, it’s not something
    that should be done. There are, however, a number of parameters that can be tweaked
    to control its behavior. As shown here, there are several parameters that can
    be controlled through the `sysctl` interface:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 无法禁用页面缓存。即使有这种选项，也不建议这样做。不过，确实有一些参数可以调整，以控制其行为。如这里所示，有几个参数可以通过`sysctl`接口进行控制：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let us look at them in detail:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这些情况：
- en: '`vm.dirty_background_ratio`: The write-back flusher threads initiate the flushing
    of dirty pages to disk when the percentage of dirty pages in the cache surpasses
    a certain threshold. Prior to this threshold, no pages are written to the disk.
    Once flushing begins, it occurs in the background without causing any disturbance
    to the foreground processes.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vm.dirty_background_ratio`：当缓存中脏页的百分比超过某个阈值时，写回刷新线程会启动，将脏页刷新到磁盘。在此阈值之前，页面不会被写入磁盘。一旦刷新开始，它会在后台进行，不会打扰前台进程。'
- en: '`vm.dirty_ratio`: This refers to the threshold of system memory utilization,
    beyond which the writing process gets blocked and dirty pages are written out
    to the disk.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vm.dirty_ratio`：这是指系统内存利用率的阈值，超过该阈值后，写入过程会被阻塞，脏页将被写入磁盘。'
- en: For large memory systems, hundreds of GB of data can be flushed from the page
    cache to disk, which will cause noticeable delays and adversely affect not only
    the disk’s performance but also overall system performance. In such cases, lowering
    these values might be helpful, as data will be flushed to disk on a regular basis,
    avoiding the write storm.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大内存系统，大量的 GB 数据可能会从页面缓存刷新到磁盘，这将导致显著的延迟，不仅会影响磁盘的性能，还会影响整体系统的性能。在这种情况下，降低这些值可能会有所帮助，因为数据将定期刷新到磁盘，避免了写入风暴。
- en: 'You can check the current values of these parameters using `sysctl` – for instance,
    if the values are as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`sysctl`检查这些参数的当前值——例如，如果这些值如下：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Think of `vm.dirty_ratio` as the upper limit. Using these previously mentioned
    values means that when the percentage of dirty pages in the cache reaches 10%,
    the background threads are triggered to write them to the disk. However, when
    the total number of dirty pages in the cache exceeds 20%, all writes are blocked
    until a portion of the dirty pages are written to the disk. These two parameters
    have the following two counterparts:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `vm.dirty_ratio` 看作上限。使用前面提到的这些值意味着，当缓存中脏页的百分比达到 10% 时，后台线程会被触发，将它们写入磁盘。然而，当缓存中脏页的总数超过
    20% 时，所有写入都会被阻塞，直到部分脏页被写入磁盘。这两个参数有以下两个对应参数：
- en: '`vm.dirty_background_bytes`: This denotes the amount of dirty memory, expressed
    in bytes, that triggers the background flusher threads to initiate writing back
    to the disk. This is the counterpart of `vm.dirty_background_ratio`, and only
    one of them can be configured. The value can be defined either as a percentage
    or a precise number of bytes.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vm.dirty_background_bytes`：这是触发后台刷写线程启动磁盘写入的脏内存数量，以字节为单位。它是 `vm.dirty_background_ratio`
    的对应参数，也只能配置其中一个。该值可以以百分比或确切的字节数定义。'
- en: '`vm.dirty_bytes`: This is the amount of dirty memory, expressed in bytes, that
    results in the writing process getting blocked and writing out the dirty pages
    to the disk. This controls the same tunable as `vm.dirty_ratio`, and only one
    of them can be set.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vm.dirty_bytes`：这是脏内存的数量，以字节为单位，当该值达到一定程度时，写入过程会被阻塞，直到脏页被写入磁盘。它控制与 `vm.dirty_ratio`
    相同的可调参数，二者只能设置一个。'
- en: '`vm.dirty_expire_centisecs`: This indicates how long something can be in the
    cache before it needs to be written. This tunable specifies the age at which the
    dirty data is deemed suitable for write-back by the flusher threads. The time
    duration is measured in hundredths of a second.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vm.dirty_expire_centisecs`：这表示某个数据在缓存中可以存在多久，直到需要写入。这个可调参数指定了脏数据被认为适合由刷写线程回写的年龄。时间单位是百分之一秒。'
- en: To summarize, the default behavior of the kernel’s page cache works well most
    of the time, and usually, it won’t require any tweaking. However, for intelligent
    applications such as large-scale databases, the frequent caching of data can become
    a hurdle. Fortunately, there are a few workarounds available. Such applications
    can be configured to use direct I/O, which will bypass the page cache. The kernel
    also offers several parameters that can be used to tweak the behavior of the page
    cache. However, it is important to note that changing these values can result
    in increased I/O traffic. Therefore, workload-specific testing should be conducted
    prior to making changes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，内核的页面缓存默认行为在大多数情况下表现良好，通常不需要做任何调整。然而，对于像大规模数据库这样的智能应用程序，频繁缓存数据可能会成为一个障碍。幸运的是，存在一些解决方法。此类应用程序可以配置为使用直接
    I/O，这样可以绕过页面缓存。内核还提供了几个参数，可以用来调整页面缓存的行为。然而，值得注意的是，改变这些值可能会导致 I/O 流量增加。因此，在进行更改之前，应进行特定工作负载的测试。
- en: Tuning the filesystem
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调优文件系统
- en: As we focus on tuning the different components that can impact I/O performance,
    we’ll try to steer our conversation away from the hardware side of things. Given
    the advancements in hardware, upgrading the memory, compute, network, and storage
    apparatus is bound to add at least some level of performance gains. However, most
    of the time, the magnitude of those gains will be limited. You need a well-designed
    and configured software stack to take advantage of that hardware. As we’re not
    focusing on a particular type of application, we’ll try to present some general
    tweaks that can be used to fine-tune your I/O. Again, note that the parameters
    that will be presented here or were discussed earlier require thorough testing
    and may not offer the same results in different environments.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们专注于调优可能影响 I/O 性能的不同组件时，我们会尽量避免讨论硬件方面的内容。鉴于硬件的进步，升级内存、计算、网络和存储设备肯定会带来一定的性能提升。然而，大多数时候，这些提升的幅度是有限的。你需要一个设计良好且配置得当的软件堆栈来充分利用这些硬件。由于我们并不专注于某种特定类型的应用程序，因此我们会尝试提供一些通用的调整方法，用于微调
    I/O。再次强调，下面提到的参数或之前讨论的参数需要进行充分的测试，并且在不同的环境中可能不会得到相同的结果。
- en: Coming back to the topic of our discussion, filesystems are responsible for
    organizing data on disk and are the point of contact for an application to perform
    I/O. This makes them an ideal candidate for the tuning and troubleshooting process.
    Some applications explicitly mention the filesystem that should be used for optimal
    performance. As Linux supports different flavors of filesystems that use different
    techniques to store user data, some mount options might not be common among filesystems.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们讨论的主题，文件系统负责在磁盘上组织数据，并且是应用程序执行 I/O 操作的接触点。这使得它们成为调优和故障排除过程的理想候选者。有些应用程序明确指出应使用哪种文件系统来获得最佳性能。由于
    Linux 支持多种不同类型的文件系统，这些文件系统使用不同的技术存储用户数据，因此一些挂载选项可能在文件系统之间并不通用。
- en: Block size
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 块大小
- en: Filesystems address physical storage in terms of blocks. A **block** is a group
    of physical sectors and is the fundamental unit of I/O for a filesystem. Each
    file in the filesystem will occupy at least one block, even if the file contains
    nothing. By default, a block size of 4 KB is used for most filesystems. If the
    application mostly creates a large number of small-sized files in the filesystem,
    typically of a few bytes or less than a couple of KB, then it is best to use smaller
    block sizes than the default value of 4 KB.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统以块为单位处理物理存储。**块**是物理扇区的组合，是文件系统的基本 I/O 单元。文件系统中的每个文件至少占用一个块，即使文件为空。默认情况下，大多数文件系统使用
    4 KB 的块大小。如果应用程序主要在文件系统中创建大量小文件，通常是几字节或不到几 KB，那么最好使用比默认的 4 KB 小的块大小。
- en: Filesystems perform better if applications use the same read and write size
    as the block size, or use a size that is a multiple of the block size. The block
    size for a filesystem can only be specified during its creation and cannot be
    changed afterward. Therefore, the block size needs to be decided before creating
    the filesystem.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用程序使用与块大小相同的读写大小，或使用块大小的倍数，文件系统的性能会更好。文件系统的块大小只能在创建时指定，之后无法更改。因此，在创建文件系统之前需要确定块大小。
- en: Filesystem I/O alignment
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件系统 I/O 对齐
- en: The concept of I/O alignment is generally overlooked, but this can have a huge
    impact on the filesystem performance. This is especially true for the complex
    enterprise storage systems of today, which consist of flash drives that have different
    page sizes and some form of RAID configuration on top of them.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: I/O 对齐的概念通常被忽视，但它对文件系统的性能有巨大的影响。尤其对于今天复杂的企业存储系统来说更为明显，这些系统由具有不同页面大小的闪存驱动器组成，并且上面可能有某种形式的
    RAID 配置。
- en: 'The I/O alignment for filesystems is concerned with how data is distributed
    and organized across the filesystem. That’s one side of the coin. If the underlying
    physical storage consists of a striped RAID configuration, the data should be
    aligned with the underlying storage geometry for optimal performance. For instance,
    for a RAID device with a 64 K per-disk stripe size and 10 data-bearing disks,
    the filesystem should be created as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统的 I/O 对齐涉及数据在文件系统中的分布和组织方式。这是一个方面。如果底层物理存储由条带化 RAID 配置组成，为了获得最佳性能，数据应该与底层存储几何结构对齐。例如，对于一个每磁盘
    64K 条带大小和 10 个数据承载磁盘的 RAID 设备，应该如下创建文件系统：
- en: 'For XFS, it should be the following:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 XFS，应如下所示：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'XFS provides two sets of tunables in this regard. Depending on the specification
    units that you’ve set, you can use these:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: XFS 提供了两组可调参数。在你设置的规格单位的基础上，可以使用以下内容：
- en: '`Sunit`: A stripe unit, in 512-byte blocks'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Sunit`：条带单元，以 512 字节为单位的块'
- en: '`swidth`: A stripe width, in 512-byte blocks'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`swidth`：条带宽度，以 512 字节为单位的块'
- en: 'Alternatively, you can use these:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以使用以下内容：
- en: '`su`: A per-disk stripe unit, in K if suffixed with *k*'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`su`：每磁盘条带单元，如果带有 *k* 后缀，则以 KB 为单位'
- en: '`sw`: A stripe width, by the number of data disks'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sw`：条带宽度，按数据磁盘数量计算'
- en: 'For Ext4, the command would look as follows:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Ext4，命令应如下所示：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Ext4 also provides a couple of tunables, which can be used as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Ext4 还提供了几个可调参数，可按以下方式使用：
- en: '`stride`: The number of filesystem blocks on each data-bearing disk in that
    stripe'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`：每个数据承载磁盘上每个条带中的文件系统块数'
- en: '`stripe-width`: The total stripe-width in filesystem blocks, equal to (stride)
    x (the number of data-bearing disks)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stripe-width`：文件系统块中的总条带宽度，等于（步幅）x（数据承载磁盘数量）'
- en: LVM I/O alignment
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LVM I/O 对齐
- en: Every abstraction layer created on top of a RAID device must be aligned to a
    multiple of `Stripe Width`, plus any required initial alignment offset. This ensures
    that a read or write request of a single block at the filesystem will not span
    the RAID stripe boundaries and cause multiple stripes to be read and written at
    the disk level, adversely affecting performance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAID设备上创建的每一层抽象都必须对齐到`Stripe Width`的倍数，并加上任何所需的初始对齐偏移量。这可以确保文件系统中单个块的读写请求不会跨越RAID条带边界，从而避免在磁盘级别读取和写入多个条带，进而影响性能。
- en: 'The first physical extent allocated within the physical volume should be aligned
    to a multiple of the RAID `Stripe Width`. If the physical volume is created directly
    on a raw disk, then it should also be offset by any required initial alignment
    offset. To check where the physical extents start, use the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理卷内分配的第一个物理区段应该对齐到RAID `Stripe Width`的倍数。如果物理卷是直接在原始磁盘上创建的，则它也应当根据需要偏移任何初始对齐偏移量。要检查物理区段的起始位置，可以使用以下命令：
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, `pe_start` refers to the first physical extent.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`pe_start`指的是第一个物理区段。
- en: The logical volumes are always allocated a contiguous range of physical extents
    when possible. If a contiguous range doesn’t exist, non-contiguous ranges might
    be allocated. Since a non-contiguous range of extents can impact performance,
    there is an option (`--contiguous`) while creating a logical volume to prevent
    the non-contiguous allocation of extents.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑卷在可能的情况下总是分配一个连续的物理区段范围。如果不存在连续范围，可能会分配非连续范围。由于非连续的区段范围可能会影响性能，因此在创建逻辑卷时有一个选项（`--contiguous`），可以防止分配非连续的区段。
- en: Journaling
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志记录
- en: As explained in [*Chapter 3*](B19430_03.xhtml#_idTextAnchor053), the concept
    of journaling guarantees data consistency and integrity if I/O operations on a
    filesystem fail due to external events. Any changes that need to be performed
    on the filesystem are first written to a journal. Once data has been written to
    a journal, it is then written to the appropriate location on the disk. If there
    is a system crash, the filesystem replays the journal to see if any operation
    was left in an incomplete state. This reduces the likelihood that the filesystem
    will become corrupted if there are any hardware failures.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第3章*](B19430_03.xhtml#_idTextAnchor053)所述，日志记录的概念确保了在由于外部事件导致文件系统的I/O操作失败时，数据的一致性和完整性。任何需要对文件系统进行的更改都会先写入日志。一旦数据被写入日志，它将被写入磁盘上的相应位置。如果发生系统崩溃，文件系统会回放日志，检查是否有任何操作处于不完整状态。这减少了文件系统在硬件故障时损坏的可能性。
- en: Apparently, the journaling approach adds extra overhead and can potentially
    affect filesystem performance. However, given the sequential nature of journal
    writes, the filesystem performance is not affected. So, it is recommended to keep
    the filesystem journal enabled to ensure data integrity.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，日志记录方法会增加额外的开销，可能会影响文件系统性能。然而，考虑到日志写入的顺序性，文件系统性能不会受到影响。因此，建议保持启用文件系统日志以确保数据完整性。
- en: It is, however, recommended to change the mode of the filesystem journal to
    suit your needs. Most filesystems don’t have multiple journaling modes but Ext4
    offers a great deal of flexibility in this regard. The Ext4 offers three journaling
    modes. Among them, the **write-back mode** offers considerably better performance
    than the ordered and data mode. The write-back mode only journals the metadata
    and does not follow any order when writing the data and metadata to disk. The
    **ordered mode** on the other hand follows a strict order and first writes the
    actual data before the metadata. The **data mode** offers the lowest performance,
    as it has to write both the data and metadata to a journal, resulting in twice
    the number of operations.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，建议根据需要更改文件系统日志的模式。大多数文件系统没有多种日志记录模式，但Ext4在这方面提供了很大的灵活性。Ext4提供了三种日志记录模式。其中，**写回模式**的性能明显优于有序模式和数据模式。写回模式仅记录元数据，并且在将数据和元数据写入磁盘时不遵循任何顺序。而**有序模式**则遵循严格的顺序，首先写入实际数据，然后再写入元数据。**数据模式**提供最低的性能，因为它必须将数据和元数据都写入日志，导致操作次数翻倍。
- en: Another thing that can be done to improve journaling is to use an external journal.
    The default location for a filesystem journal is on the same block device as the
    data. If the I/O workload is metadata-intensive and the synchronous metadata writes
    to the journal must complete successfully before any associated data writes can
    start, this can result in I/O contention and may impact performance. In such cases,
    it can be a good idea to use an external device for filesystem journaling. The
    journal size is typically very small and requires very little storage space. The
    external journal should ideally be placed on fast physical media with a battery-backed
    write-back cache.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可以用来改进日志记录的做法是使用外部日志。文件系统日志的默认位置是与数据存储在同一块设备上。如果 I/O 工作负载是以元数据为主，而且在开始任何关联数据写入之前，必须确保同步的元数据写入日志成功完成，这可能导致
    I/O 争用并影响性能。在这种情况下，使用外部设备进行文件系统日志记录可能是一个不错的选择。日志的大小通常非常小，所需存储空间也很少。外部日志最好放置在具有电池备份写回缓存的快速物理介质上。
- en: Barriers
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 屏障
- en: As mentioned earlier, most filesystems make use of journaling to keep track
    of changes that have not yet been written to disk. A **write barrier** is a kernel
    mechanism that guarantees the proper ordering and accurate writing of filesystem
    metadata onto persistent storage, even if storage devices with unstable write
    caches lose power. Write barriers enforce proper on-disk ordering of journal commits
    by forcing the storage device to flush its cache at certain intervals. This makes
    volatile write caches safe to use, but it can incur some performance deficit.
    If the storage device cache is battery-backed, disabling filesystem barriers may
    offer some performance improvement.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，大多数文件系统使用日志记录来跟踪尚未写入磁盘的更改。**写屏障**是一种内核机制，确保文件系统元数据在持久存储上按正确的顺序和准确地写入，即使存储设备的写缓存因失电而丢失。写屏障通过强制存储设备在某些间隔刷新其缓存来强制保证日志提交的磁盘顺序。这使得不稳定的写缓存可以安全使用，但可能会带来一些性能损失。如果存储设备的缓存有电池备份，禁用文件系统屏障可能会提高一些性能。
- en: Timestamps
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间戳
- en: The kernel records information about when files were created (`ctime`) and last
    modified (`mtime`) ,as well as when they were last accessed (`atime`). If an application
    frequently modifies a bunch of files, then their corresponding timestamps will
    need to be updated every time. Performing these modifications also requires I/O
    operations, and when there are too many of them, there is a cost associated with
    them.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 内核记录关于文件创建时间（`ctime`）、最后修改时间（`mtime`）以及最后访问时间（`atime`）的信息。如果一个应用程序频繁修改大量文件，那么每次都会需要更新它们相应的时间戳。执行这些修改还需要进行
    I/O 操作，而当这些操作过多时，会带来一定的成本。
- en: To mitigate this, there is a special mount option for filesystems called `noatime`.
    When a filesystem is mounted with the `noatime` option, reading from the filesystem
    will not update the file’s `atime` information. The `noatime` setting is significant,
    as it removes the requirement for a system to perform writes to the filesystem
    for files that are only read. This can lead to noticeable performance improvements,
    since write operations can be expensive.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这一问题，文件系统有一个特殊的挂载选项叫做 `noatime`。当文件系统以 `noatime` 选项挂载时，从文件系统读取数据将不会更新文件的
    `atime` 信息。`noatime` 设置很重要，因为它消除了系统为仅读取的文件执行写操作的要求。这可以显著提高性能，因为写操作可能是非常昂贵的。
- en: Read-ahead
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预读
- en: The read-ahead functionality in filesystems can enhance file access performance
    by proactively fetching data that is expected to be required soon and storing
    it in the page cache, which allows for faster access compared to retrieving the
    data from the disk. A higher read-ahead value indicates that the system will prefetch
    data further ahead of the current read position. This is especially true for sequential
    workloads.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统中的预读功能可以通过主动获取预计很快需要的数据并将其存储在页缓存中来提高文件访问性能，这比从磁盘中读取数据更快。较高的预读值表示系统将在当前读取位置之前更远的地方预取数据。这对于顺序工作负载尤其有效。
- en: Discarding unused blocks
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 丢弃未使用的块
- en: As we explained in [*Chapter 8*](B19430_08.xhtml#_idTextAnchor134), in SSDs,
    a write operation can be done at the page level, but the erase operation always
    affects entire blocks. As a result, writing data to SSDs is very fast as long
    as empty pages can be used. However, once previously written pages need to be
    overwritten, the writes slow down considerably, impacting performance. The `trim`
    command tells the SSD to discard the blocks that are no longer needed and can
    be deleted. The filesystem is the only component in the I/O stack that knows the
    parts of the SSD that should be trimmed. Most filesystems offer mount parameters
    that implement this feature.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第8章*](B19430_08.xhtml#_idTextAnchor134)中解释的那样，在SSD中，写操作可以在页面级别进行，但擦除操作总是影响整个块。因此，只要能使用空闲页面，写入SSD的数据非常快。然而，一旦需要覆盖先前写入的页面，写入速度会显著变慢，影响性能。`trim`命令告诉SSD丢弃那些不再需要并且可以删除的块。文件系统是I/O堆栈中唯一知道哪些SSD部分应该被修剪的组件。大多数文件系统提供了实现这一功能的挂载参数。
- en: To summarize, filesystems, to a certain extent, map logical addresses to physical
    addresses. When an application writes data, the filesystem decides how to distribute
    writes properly in order to make the best use of the underlying physical storage.
    This makes filesystems a very important layer when it comes to performance tuning.
    Most of the changes in filesystems cannot be done on the fly; they’re either performed
    during filesystem creation or require unmounting and remounting the filesystem.
    So, the decision regarding the choice of filesystem parameters should be made
    in advance, as changing things afterward can be a disruptive activity.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，文件系统在一定程度上将逻辑地址映射到物理地址。当应用程序写入数据时，文件系统决定如何适当地分配写入操作，以便更好地利用底层的物理存储。这使得文件系统在性能调优方面非常重要。大多数文件系统的更改无法即时完成；它们要么在文件系统创建时执行，要么需要卸载并重新挂载文件系统。因此，关于选择文件系统参数的决定应提前做出，因为事后更改可能会带来干扰。
- en: Choosing the right scheduler
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的调度程序
- en: 'The sole purpose of I/O schedulers is to optimize disk access requests. There
    are some common techniques used by schedulers, such as merging I/O requests that
    are adjacent on disk. The idea is to avoid frequent trips to the physical storage.
    Aggregating requests that are situated in close proximity on the disk reduces
    the frequency of the drive’s seeking operations, thereby enhancing the overall
    response time of disk operations. I/O schedulers aim to optimize throughput by
    rearranging access requests into sequential order. However, this strategy may
    cause some I/O requests to wait for an extended time, resulting in latency problems
    in certain situations. I/O schedulers strive to achieve a balance between maximizing
    throughput and distributing I/O requests equitably among all processes. As with
    all other things, Linux has a variety of I/O schedulers available. Each has its
    own set of strengths:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: I/O调度程序的唯一目的是优化磁盘访问请求。调度程序使用一些常见技术，例如合并磁盘上相邻的I/O请求。其目的是避免频繁访问物理存储。将磁盘上相近的请求聚集在一起，可以减少硬盘寻道操作的频率，从而提高磁盘操作的总体响应时间。I/O调度程序旨在通过将访问请求重新排列成顺序来优化吞吐量。然而，这种策略可能导致某些I/O请求等待较长时间，从而在某些情况下造成延迟问题。I/O调度程序努力在最大化吞吐量和在所有进程之间公平分配I/O请求之间找到平衡。像其他所有事物一样，Linux提供了多种I/O调度程序，每个调度程序都有其自身的优点：
- en: '| **Use case** | **Recommended** **I/O scheduler** |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| **使用场景** | **推荐的** **I/O调度程序** |'
- en: '| Desktop GUI, interactive applications, and soft real-time applications, such
    as audio and video players | **Budget Fair Queuing** (**BFQ**), as it guarantees
    good system responsiveness and a low latency for time-sensitive applications |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 桌面GUI、交互式应用程序和软实时应用程序，如音频和视频播放器 | **预算公平队列** (**BFQ**)，因为它保证了良好的系统响应性和低延迟，适用于时间敏感型应用程序
    |'
- en: '| Traditional mechanical drives | BFQ or **Multiqueue** (**MQ**)-Deadline –
    both are considered suitable for slower drives. Kyber/none are biased in favor
    of faster disks. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 传统机械硬盘 | BFQ或**多队列** (**MQ**)-Deadline – 这两者都被认为适用于较慢的硬盘。Kyber/none偏向于支持更快的磁盘。
    |'
- en: '| High-performing SSDs and NVMe drives as local storage | Preferable none,
    but Kyber might also be a good alternative in some cases |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 高性能SSD和NVMe驱动器作为本地存储 | 最佳选择是没有，但在某些情况下Kyber也可能是一个不错的替代方案 |'
- en: '| Enterprise storage arrays | None, as most storage arrays have built-in logic
    to schedule I/Os more efficiently |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 企业存储阵列 | 无，因为大多数存储阵列都具有内置逻辑，可以更高效地调度I/O |'
- en: '| Virtualized environments | MQ-Deadline is a good option. If the hypervisor
    layer does its own I/O scheduling, then using the none scheduler might be beneficial,
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 虚拟化环境 | MQ-Deadline 是一个不错的选择。如果虚拟机管理程序（hypervisor）层执行自身的 I/O 调度，那么使用 none
    调度器可能会带来好处，|'
- en: Table 11.1 – Some use cases for I/O schedulers
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11.1 – I/O 调度器的一些使用案例
- en: The good thing is that an I/O scheduler can be changed on the fly. It is also
    possible to use a different I/O scheduler for every storage device on the system.
    A good starting point to select or fine-tune an I/O scheduler is to determine
    the system’s purpose or role. It is generally accepted that there is no single
    I/O scheduler that can meet all of a system’s diverse I/O demands.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 好的一点是，I/O 调度器可以在运行时动态更换。还可以为系统上的每个存储设备使用不同的 I/O 调度器。选择或微调 I/O 调度器的一个良好起点是确定系统的用途或角色。普遍认为，没有单一的
    I/O 调度器能够满足系统所有多样化的 I/O 需求。
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: After having spent two chapters trying to diagnose and analyze the performance
    of different layers in the I/O stack, this chapter focused on the performance-tuning
    aspect of the I/O stack. Throughout this book, we’ve familiarized ourselves with
    the multi-level hierarchy of the I/O stack and built an understanding of the components
    that can impact the overall I/O performance.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在花费了两章时间尝试诊断和分析 I/O 堆栈不同层次的性能后，本章重点关注 I/O 堆栈的性能调优方面。在本书中，我们已经熟悉了 I/O 堆栈的多层次层级，并建立了对可能影响整体
    I/O 性能的各个组件的理解。
- en: We started this chapter by briefly going through the functions of the memory
    subsystem and how it can impact the I/O performance of a system. As all write
    operations are, by default, first performed in the page cache, the way the page
    cache is configured to behave can have a major say in an application’s I/O performance.
    We also explained the concept of direct I/O and defined some of the different
    parameters that can be used to tweak the write-back cache.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们简要介绍了内存子系统的功能及其如何影响系统的 I/O 性能。由于所有写操作默认首先在页缓存中执行，因此页缓存的配置行为在很大程度上会影响应用程序的
    I/O 性能。我们还解释了直接 I/O 的概念，并定义了一些可以用于调整写回缓存的不同参数。
- en: We also looked at the different tuning options when it comes to filesystems.
    Filesystems offer different mount options that can be changed to reduce some I/O
    overhead. Additionally, the filesystem block size, its geometry, and I/O alignment
    in terms of the underlying RAID configuration can also impact performance. Finally,
    we explained some use cases of the different scheduling flavors in Linux.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了文件系统的不同调优选项。文件系统提供了不同的挂载选项，这些选项可以改变以减少一些 I/O 开销。此外，文件系统的块大小、其几何结构和基于底层
    RAID 配置的 I/O 对齐方式也可能影响性能。最后，我们解释了 Linux 中不同调度策略的使用案例。
- en: I guess that’s a wrap! I sincerely hope that this book took you on an enlightening
    exploration of the intricate layers comprising the Linux kernel’s storage stack.
    Starting with an introduction to VFS in [*Chapter 1*](B19430_01.xhtml#_idTextAnchor015),
    we tried to navigate the complex terrain of storage architecture. Each chapter
    delves deeper into the intricacies of the Linux storage stack, exploring topics
    such as VFS data structures, filesystems, the role of the block layer, multi-queue
    and device-mapper frameworks, I/O scheduling, the SCSI subsystem, physical storage
    hardware, and its performance tuning and analysis. Our goal was to prioritize
    the conceptual side of things and examine the flow of disk I/O activity, which
    is why we’ve not dived too much into general storage administration tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我想这就结束了！我真诚地希望本书带领你深入探索了构成 Linux 内核存储堆栈的复杂层次。从 [*第 1 章*](B19430_01.xhtml#_idTextAnchor015)
    的 VFS 介绍开始，我们试图探索存储架构的复杂领域。每一章都深入探讨了 Linux 存储堆栈的细节，涉及了 VFS 数据结构、文件系统、块层的作用、多队列和设备映射框架、I/O
    调度、SCSI 子系统、物理存储硬件及其性能调优与分析等主题。我们的目标是优先考虑概念方面的内容，分析磁盘 I/O 活动的流向，这也是我们没有过多深入一般存储管理任务的原因。
- en: As we bring our adventure to a close, I hope you’ve gained a comprehensive understanding
    of the Linux storage stack, its major components, and their interactions, and
    now possess the necessary knowledge and skills to make informed decisions, analyze,
    troubleshoot, and optimize storage performance in Linux environments.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们结束这次探索时，我希望你已经全面理解了 Linux 存储堆栈、其主要组件及其相互作用，并且现在具备了做出明智决策、分析、故障排除和优化 Linux
    环境中存储性能的必要知识和技能。
