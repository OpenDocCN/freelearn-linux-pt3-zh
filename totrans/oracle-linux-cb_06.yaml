- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Eliminating All the SPOFs! An Exercise in Redundancy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s crucial to have redundancy in your architecture to ensure smooth operations.
    Eliminating **Single Points of Failure** (**SPOFs**) is a common way to achieve
    this. Implementing **High-Availability** (**HA**) technology is one way to eliminate
    SPOFs. HA technology is crucial because it helps businesses maintain operational
    continuity, enhance performance, improve reliability, facilitate **Disaster Recovery**
    (**DR**), build customer trust, and comply with regulations. With HA technology,
    minimizing downtime and ensuring continuous service availability is possible,
    contributing to overall success and competitiveness in today’s technology-driven
    world.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: What about DR? We are not covering the failure of data centers in this chapter.
    That being said, the approach to implementing DR is very different than HA. When
    running in the cloud, look for services such as Oracle Full Stack Disaster Recovery
    Service that automate the DR process for the entire tech stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recipes in this chapter will help you eliminate SPOFs in your environment.
    We’ll start with a general overview of what HA is, as well as availability, and
    then get into several examples of how you can add redundancy to your application.
    The four most common technologies that are used to help eliminated SPOFs in applications
    are HAProxy, Corosync, Pacemaker, and GlusterFS. Each of these provides a specific
    set of features to help make an application highly available:'
  prefs: []
  type: TYPE_NORMAL
- en: '**HAProxy**: This is a load balancer, and allows you to balance web workloads
    between servers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Corosync**: This is a communication system that enables communication to
    help implement HA within applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pacemaker**: Pacemaker is an open source resource manager that is used to
    build small and large clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GlusterFS**: This is a scalable network filesystem that allows multiple nodes
    to read and write data to the same storage at the same time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following recipes will help you implement these common HA technologies.
    This includes everything from load-balancing applications and clustering application
    systems to clustered storage and redundant networking:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting 99.999% availability and beyond
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing a website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making HAProxy highly available with Keepalived
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HA clustering for all with Corosync and Pacemaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing a filesystem across multiple machines – cluster or distribute?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating, configuring, and monitoring Ethernet traffic over bond
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For most of these recipes, you will need a pair of Oracle Linux 8 systems. As
    with most of these recipes, a VM on your desktop using a desktop virtualization
    product such as Oracle VirtualBox is recommended. A small VM is fine, with two
    cores, 2 GB RAM, and a few free gigabytes of disk space. You will also need some
    additional disks assigned to the VM, ideally at least five equally sized disks.
    Before you start, patch your system to the latest packages available. This only
    takes a few minutes and can save a ton of time when troubleshooting issues that
    are caused by a bug.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the recipes in this book have their related configuration files available
    on GitHub at [https://github.com/PacktPublishing/Oracle-Linux-Cookbook](https://github.com/PacktPublishing/Oracle-Linux-Cookbook).
  prefs: []
  type: TYPE_NORMAL
- en: Getting 99.999% availability and beyond
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe will discuss the differences between DR and HA and how to architect
    HA solutions. Before we get into that, let’s refine the definition of a few key
    terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High availability**, or **HA**: This means protecting from the failure of
    a single component. Think of this as protecting against the failure of a system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disaster recovery**, or **DR**: This is the failure of the data center or
    cloud region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability nines**: When referring to *nines of availability*, it is a
    way to quantify the uptime or reliability of a system by specifying the number
    of nines in the uptime percentage. Each *nine* represents a decimal place in the
    uptime percentage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s a breakdown of the most commonly used *nines* and their corresponding
    uptime percentages, assuming 24x7x365 operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Nines** | **Downtime** **per Year** | **Downtime** **per Month** |'
  prefs: []
  type: TYPE_TB
- en: '| 99 | 3d 14h 56m 18s | 7h 14m 41s |'
  prefs: []
  type: TYPE_TB
- en: '| 99.9 | 8h 41m 38s | 43m 28s |'
  prefs: []
  type: TYPE_TB
- en: '| 99.99 | 52m 10s | 4m 21s |'
  prefs: []
  type: TYPE_TB
- en: '| 99.999 | 5m 13s | 26s |'
  prefs: []
  type: TYPE_TB
- en: '| 99.9999 | 31s | 2.6s |'
  prefs: []
  type: TYPE_TB
- en: Table 6.1 – Nines downtime
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding table, each additional nine in the uptime percentage signifies
    a higher level of availability and a reduced tolerance for downtime. Achieving
    higher numbers of nines typically requires implementing redundant systems, failover
    mechanisms, and rigorous maintenance practices to minimize downtime and ensure
    continuous operation. In addition, when setting up a **Service-Level Agreement**
    (**SLA**), you can also define the uptime during business hours and exclude scheduled
    maintenance. As an example, using a working schedule of Monday through Friday
    with 12 working hours a day, and 10 holidays off per year, the matrix would look
    very different!
  prefs: []
  type: TYPE_NORMAL
- en: '| **Nines** | **Downtime** **per Year** | **Downtime** **per Month** |'
  prefs: []
  type: TYPE_TB
- en: '| 99 | 1d 7h 2m 58s | 2h 35m 14s |'
  prefs: []
  type: TYPE_TB
- en: '| 99.9 | 3h 6m 18s | 15m 31s |'
  prefs: []
  type: TYPE_TB
- en: '| 99.99 | 18m 38s | 1m 33s |'
  prefs: []
  type: TYPE_TB
- en: '| 99.999 | 1m 52s | 9s |'
  prefs: []
  type: TYPE_TB
- en: '| 99.9999 | 11s | 1s |'
  prefs: []
  type: TYPE_TB
- en: Table 6.2 – Business hours downtime
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When setting SLAs with the business, carefully understand the differences between
    including maintenance windows and operational hours within the SLA.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When designing HA systems, there are several considerations that need to be
    taken into account to ensure the system is resilient and can handle failures.
    Here are some key considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Redundancy**: Having redundancy in HA systems is essential. It requires replicating
    components or whole systems to eliminate potential SPOFs. Redundancy can be implemented
    at different levels, such as hardware, software, and network infrastructure. To
    minimize the impact of localized failures, it’s crucial to distribute redundant
    components across different physical locations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failover and load balancing**: It is important for HA systems to be equipped
    with failover mechanisms that enable automatic switching to a backup system whenever
    a failure occurs. One way to achieve this is through replicating data and services
    across multiple servers, coupled with the use of load-balancing techniques that
    ensure the even distribution of workload. With load balancing, traffic can be
    easily redirected to available servers in the event of a server failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: When designing HA systems, it is important to ensure that
    they can handle increased workloads and scale effortlessly. This can be achieved
    through horizontal scaling, which entails adding more servers to distribute the
    load, or vertical scaling, which involves adding resources to existing servers.
    Additionally, the system should be capable of dynamically adjusting resource allocation
    based on demand to prevent overloading.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data replication and backup**: Maintaining data integrity and availability
    is crucial for HA systems. To ensure that data can still be accessed in case of
    a system failure, it is essential to replicate data across multiple storage systems
    or databases. Additionally, performing regular backups is vital to safeguard against
    potential data loss or corruption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: Systems used for highly available architectures should
    have fault tolerance, which means they must be able to function even if specific
    components or subsystems malfunction. Achieving this requires creating a system
    that can manage errors with ease, recover automatically, and ensure continuity
    of service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disaster recovery**: Having a DR plan is crucial for HA systems to effectively
    deal with catastrophic events such as natural disasters or widespread outages.
    This plan entails generating off-site backups, setting up secondary data centers,
    and relying on cloud-based services to guarantee business continuity, even amid
    extreme situations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Documentation and testing**: It is crucial to document the system architecture,
    configurations, and procedures to effectively troubleshoot and maintain the HA
    system. Regular testing, such as failover tests, load testing, and DR drills,
    plays a significant role in identifying potential issues and ensuring the system
    operates as intended in various scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost and complexity**: Designing, implementing, and maintaining HA systems
    can be both complex and costly. It is important to carefully consider the available
    budget, as well as the expertise and resources required to effectively manage
    and monitor the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By addressing these considerations, you can design a robust and resilient HA
    system that ensures HA, fault tolerance, and continuity of critical services.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a rule, you should pick the right technology for the right subsystem and
    application.
  prefs: []
  type: TYPE_NORMAL
- en: When aiming to achieve HA for a web application, the first step is to place
    a load balancer in front of the web servers. This enables scaling of the application
    while also offering some fault tolerance for these systems. However, attention
    should also be given to the data tier, which can be addressed by clustering the
    database or building a cluster capable of running the database, depending on the
    limitations of the database technology.
  prefs: []
  type: TYPE_NORMAL
- en: If you are utilizing a technology such as Oracle Database, you have the option
    to establish a database-specific cluster known as **Oracle Real Application Clusters**
    (**Oracle RAC**). This cluster allows for both scalability and availability. With
    RAC, the database remains accessible for queries as long as one node is online.
    While other databases may utilize their own exclusive clustering technology (such
    as MySQL Cluster), you may opt to utilize generic cluster technologies such as
    Pacemaker for cluster management and Corosync for inter-cluster communications.
    This approach presents the advantage of enabling almost any technology to be made
    highly available in a Linux environment.
  prefs: []
  type: TYPE_NORMAL
- en: You can achieve HA in storage by implementing filesystems across the entire
    cluster. Gluster allows you to mount a filesystem across multiple servers, while
    at the same time replicating the storage across servers. This provides both scalability
    and reliability at the filesystem level.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the network is a common point of failure, and using network bonding
    technologies can enable both HA as well as some scaling abilities. This works
    by combining at least two network ports into a single virtual port.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The best HA architectures mix these approaches to cover the entire technology
    stack.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing a website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, most applications are web-based, whether it’s a traditional web interface
    or a RESTful API. This first tier is typically set up for HA using a load balancer.
    A load balancer is a system that distributes incoming network traffic or workload
    across multiple servers or resources. Its main goal is to optimize resource utilization,
    improve performance, and ensure the reliability and availability of applications
    or services. When multiple servers are involved in serving a particular application
    or service, a load balancer acts as an intermediary between the client and the
    server pool. It receives incoming requests from clients and intelligently distributes
    them across the available servers based on various algorithms, such as round-robin,
    least connections, or weighted distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The load balancer is responsible for ensuring the servers’ optimal health and
    performance by redirecting traffic from overloaded or problematic servers. This
    distribution of workloads helps to prevent any one server from getting overwhelmed,
    thus enhancing response time and overall system capacity and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: While a load balancer can help distribute the workloads, the actual server load
    is based on other factors, so do not expect all servers to have the same utilization
    of CPU, RAM, networking, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancers not only distribute traffic but also offer advanced features,
    such as SSL termination, session persistence, caching, and content routing. They
    are extensively used in web applications, cloud-based services, and other environments
    that demand HA and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular load balancers is HAProxy.
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy is a great open source load-balancer option. Standing for **High Availability
    Proxy**, **HAProxy** is widely used due to its excellent performance and ability
    to improve the availability and scalability of applications. Operating at the
    application layer (Layer 7) of the OSI model, this software is able to make routing
    decisions based on specific application-level information, such as HTTP headers
    and cookies. Compared to traditional network-level (Layer 4) load balancers, HAProxy
    allows for more advanced load-balancing and traffic-routing capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key features and capabilities of HAProxy include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load balancing**: With HAProxy, incoming traffic can be evenly distributed
    across multiple servers using various algorithms, such as round-robin, least connections,
    and source IP, among others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High availability**: One of the beneficial features of HAProxy is its ability
    to support active-passive failover setups. In the event the active server becomes
    unavailable, a standby server will take over. Additionally, it also has the capability
    to monitor the health of servers and make automatic adjustments to the load-balancing
    pool by adding or removing servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proxying**: One of HAProxy’s primary functions is to act as a reverse proxy,
    which involves receiving client requests and directing them to the correct backend
    servers. Additionally, it can function as a forward proxy by intercepting client
    requests and directing them to external servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SSL/TLS termination**: With HAProxy, SSL/TLS encryption and decryption can
    be efficiently managed, taking the load off of the backend servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Session persistence**: HAProxy is capable of preserving session affinity
    by routing follow-up requests from a client to the same backend server, thus guaranteeing
    the proper operation of session-based applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Health checks and monitoring**: To guarantee the availability and optimal
    performance of backend servers, HAProxy conducts routine health checks. It has
    the ability to identify failed servers and promptly exclude them from the load-balancing
    pool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logging and statistics**: With HAProxy, administrators can effectively monitor
    and analyze traffic patterns, performance metrics, and error conditions. Its detailed
    logging and statistics feature makes this possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HAProxy can be deployed on various operating systems and is often used in high-traffic
    web environments, cloud infrastructure, and containerized deployments. Its versatility
    and extensive feature set make it a powerful tool for managing and optimizing
    application traffic and open source-based load balancers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this recipe, we will put HAProxy on one system (as the load balancer) and
    then two identical web servers to balance traffic to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – HAProxy example diagram](img/B18349_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – HAProxy example diagram
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started, we first need three servers. For this exercise, we will call
    them `lb1`, `web1`, and `web2`. They are identical systems, each with 8 GB RAM,
    4 vCPUs, and 100 GB of drive space. The filesystems have 50 GB in `/`, 5 GB in
    `/home`, and 8 GB in swap. The remaining disk space is unallocated. You will also
    need the IP address for each host. In this example, the following IP addresses
    were used:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Host** | **IP** |'
  prefs: []
  type: TYPE_TB
- en: '| **Web1** | `192.168.56.200` |'
  prefs: []
  type: TYPE_TB
- en: '| **Web2** | `192.168.56.201` |'
  prefs: []
  type: TYPE_TB
- en: '| **Lb1** | `192.168.56.202` |'
  prefs: []
  type: TYPE_TB
- en: Table 6.3 – HAProxy IP addresses
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the server is built, patch it to the current software with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once the software is patched, reboot the systems.
  prefs: []
  type: TYPE_NORMAL
- en: Web servers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For both web servers, we will install Apache, using the following commands
    as the root user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We next need to enable port `80` to pass through the firewall, with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We then need to start the server and make it start on boot. This is done with
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We now need to put in a basic home page for this server.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on your environment, you may need to edit the Apache config file
    in `/etc/httpd/conf/httpd.conf` to specify your `servername`. In the config file,
    it will be a single entry on a single line, like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ServerName server.m57.local:80**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This needs to go into `/var/www/html/index.html`. The following is an example
    file. Adjust the text as needed so each server is unique. This way, you can see
    what server is being hit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, point your browser to the system to test it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Simple website](img/B18349_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Simple website
  prefs: []
  type: TYPE_NORMAL
- en: Next, repeat the process on the other web server. Once that system is up, we
    will set up the load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are using `httpd` (TLS/SSL) on your server, don’t forget to enable `https`
    in your local firewalls as well.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the single load-balancer system, we will need to install HAProxy. This
    is done using the `dnf` command as root:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We next need to open up port `80` in the firewall with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will need to edit the config file. The config file is located in `/etc/haproxy/haproxy.cfg`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The config file has two main sections, `global` and `defaults`. There can only
    be a single `global` section in the config file. This is where TLS/SSL config
    data, the logging configuration, and the user and group settings go for the user
    running `haproxy`. By default, `haproxy` runs as the user haproxy with the group
    `haproxy`. For most use cases, the `global` section should not need to be changed.
    An example is seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – HAProxy global settings](img/B18349_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – HAProxy global settings
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section, called the `defaults` section, is where you will make most
    of your edits. This is built using three subsections: `frontend`, `backend`, and
    `listen`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `frontend` section listens on all IP addresses and ports that are defined.
    This is what users will connect to. A HAProxy server can have multiple `frontend`
    sections, though each one needs a unique name and IP/port combination.
  prefs: []
  type: TYPE_NORMAL
- en: The `backend` section defines the servers being load balanced to, defining the
    load-balancing method as well as the servers and ports where traffic is being
    sent to. A HAProxy server can have multiple `backend` sections, though each one
    needs a unique name.
  prefs: []
  type: TYPE_NORMAL
- en: The `listen` section is used to define how you can monitor the load balancer,
    with the port, URI, and authentication information needed to monitor HAProxy.
    Normally, you will only have one `listen` section.
  prefs: []
  type: TYPE_NORMAL
- en: For the same frontend, we will be listening on port `80` of the `lb1` system,
    called `www_app`, and will define this IP/port combination to use the `www_servers`
    backend.
  prefs: []
  type: TYPE_NORMAL
- en: frontend
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `frontend` section of the HAProxy configuration file offers various options
    to manage incoming traffic behavior. These options can be used to control the
    traffic on a frontend. Here are some commonly used frontend options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bind`: Defines the IP address and port on which the frontend will listen for
    incoming traffic. For example, `bind *:80` listens on all IP addresses on port
    `80`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode`: Specifies the mode of the frontend, such as `http`, `tcp`, or `ssl`.
    For HTTP traffic, use `mode http`. If you want to load balance a generic TCP port,
    use `mode tcp`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`option`: Enables or disables specific options for the frontend. Some commonly
    used options are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`option httplog`: Enables HTTP request/response logging'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`option dontlognull`: Prevents logging of requests with missing or empty user-agent
    strings'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`option forwardfor`: Adds the client’s IP address to the HTTP request headers
    when using HTTP proxy mode'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`option http-server-close`: Forces the server connection to close after processing
    a request, rather than using keep-alive'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout`: Configures various timeouts for the frontend:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout client`: Sets the maximum allowed time for the client to establish
    a connection and send data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout server`: Sets the maximum allowed time for the server to respond to
    a request'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout connect`: Sets the maximum time to wait for a connection to the backend
    server'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`acl`: Defines rules for matching specific conditions. ACLs are used in conjunction
    with backend configurations to control traffic routing based on various criteria.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_backend`: Specifies which backend to use for handling traffic that matches
    specific ACL conditions. It allows you to direct traffic to different backend
    servers based on certain conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_backend`: Defines the default backend to use if no ACL conditions
    match the incoming traffic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`redirect`: Performs a URL redirection for specific conditions. For example,
    you can use the [https://example.com](https://example.com) redirect location to
    redirect HTTP traffic to HTTPS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http-request` and `http-response`: These are used to add custom HTTP request/response
    headers or to perform specific actions based on HTTP request/response data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`capture`: Captures parts of the request or response headers and saves them
    into variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the sample frontend, we will define the frontend as `www_app` binding to
    all IPs on the load-balancer system on port `80`. This looks like the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Example frontend](img/B18349_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Example frontend
  prefs: []
  type: TYPE_NORMAL
- en: backend
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When using HAProxy, the backend options play a crucial role in configuring
    the behavior of backend servers and the routing of traffic toward them. These
    options are specifically designated within the `backend` section of the HAProxy
    configuration file. Here are some frequently utilized backend options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mode`: Specifies the mode of the backend, such as `http`, `tcp`, or `ssl`.
    For HTTP traffic, use `http` mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`balance`: Defines the load-balancing algorithm to distribute traffic across
    backend servers. Common options include the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`balance roundrobin`: Requests are distributed in a round-robin fashion to
    each server in sequence'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`balance leastconn`: Traffic is sent to the server with the lowest number of
    active connections'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`balance source`: Based on a hash of the client’s IP address, traffic is directed
    to a specific server consistently'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`server`: Defines the backend servers and their addresses, ports, and optional
    parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout`: Configures various timeouts for the backend:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout server`: Sets the maximum allowed time for the server to respond to
    a request'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout tunnel`: Configures the maximum time allowed to establish a tunnel
    (used in TCP mode)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http-request` and `http-response`: Similar to frontend options, these are
    used to add custom HTTP request/response headers or perform specific actions based
    on HTTP request/response data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cookie`: Configures sticky session persistence using cookies. It allows the
    backend server to be selected based on a specific cookie value from the client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`check`: Enables health checks for backend servers to determine their availability.
    If a server fails the health check, HAProxy will stop sending traffic to it until
    it recovers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`option`: Enables or disables specific options for the backend. Some commonly
    used options include the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`option httpchk`: Enables HTTP health checks instead of TCP health checks'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`option redispatch`: Allows HAProxy to reselect a server if the connection
    to the selected server fails'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`errorfile`: Specifies a file to use as a custom error page for backend server
    errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the sample backend, it is defined as `www_servers` and will use `roundrobin`
    load balancing against the `web1` and `web2` servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – HAProxy sample backend](img/B18349_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – HAProxy sample backend
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is highly recommended to always use the `check` option for your servers.
    If you do not run the checks, the system will still send traffic to the server!
  prefs: []
  type: TYPE_NORMAL
- en: listen
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In HAProxy, the `listen` section is used to define a frontend and backend configuration
    together in one block, making it a convenient way to combine both. The `listen`
    section allows you to define options specific to the listening socket and how
    the incoming traffic is handled. The following are some commonly used options
    in the `listen` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bind`: Defines the IP address and port on which HAProxy will listen for incoming
    traffic. For example, `bind *:80` listens on all IP addresses on port `80`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats`: Enables the HAProxy statistics page for monitoring and managing HAProxy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats enable`: Enables statistics monitoring for HAProxy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats uri`: Specifies the URI path for accessing the statistics page. For
    example, `stats uri /haproxy_stats` sets the statistics page to be accessible
    at `http://your-haproxy-ip/haproxy_stats`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats realm`: Sets the realm (authentication realm) for HTTP basic authentication
    when accessing the statistics page. This adds a layer of security to prevent unauthorized
    access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats auth`: Configures the username and password for HTTP basic authentication
    when accessing the statistics page. The format is `stats` `auth username:password`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats hide-version`: Hides the HAProxy version number from the statistics
    page to enhance security.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats show-node`: Displays the server node names on the statistics page. This
    is useful when using dynamic server templates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats refresh`: Sets the interval (in milliseconds) for automatic refresh
    of the statistics page. For example, `stats refresh 10s` refreshes the page every
    10 seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats admin`: Specifies the IP address and port for allowing administrative
    access to HAProxy statistics. It allows remote management of HAProxy using the
    statistics page. For example, stats admin if `localhost` permits access only from
    the local machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats maxconn`: Limits the number of connections allowed to the statistics
    page. It helps to prevent overload and potential denial-of-service attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`errorfile`: Specifies a file to use as a custom error page for frontend errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the sample `listen` section, we will define it as metrics, allowing admin
    access from `192.168.56.1`. The user will use the username as `admin` and the
    password `passw0rd` to log in. This is seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – HAProxy listen sample](img/B18349_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – HAProxy listen sample
  prefs: []
  type: TYPE_NORMAL
- en: Since the status page is running on port `8080`, don’t forget to add the port
    to the firewall and reload the firewall. This can be done with the following command;
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have our two web servers, and the load balancer configured, we
    need to start the load balancer. This is done using `systemctl`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following to start HAProxy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following to check the status:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you edit the config file, do not forget to reload HAProxy with the following
    command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, point your browser to the load balancer IP. You will get the web server
    page. This is seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Working HAProxy](img/B18349_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Working HAProxy
  prefs: []
  type: TYPE_NORMAL
- en: Since the rule is `roundrobin`, and we configured the timeout at one minute,
    wait a minute and then reload the page. You will see a new server.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Working load balancing](img/B18349_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Working load balancing
  prefs: []
  type: TYPE_NORMAL
- en: 'As an admin, you will also want to check on the health of your resources. Point
    your browser to the stats URL, and enter the username and password configured.
    This will show the stats page. In the case of this example, the URL is [http://lb1.m57.local:8080/stats](http://lb1.m57.local:8080/stats).
    You will see a sample in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 6.9 – HAProxy status page](img/B18349_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – HAProxy status page
  prefs: []
  type: TYPE_NORMAL
- en: On the sample page, you will see that `web1` is offline. You can also see how
    much traffic each frontend and backend rule has processed, and to what servers.
  prefs: []
  type: TYPE_NORMAL
- en: Making HAProxy highly available with Keepalived
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous recipe, we used HAProxy to give our web servers some redundancy.
    The challenge with that solution is we now have a failure point in the load balancer
    itself. When architecting for HA, you need to cover all points of failure to make
    sure there is redundancy and that there is no SPOF. In this recipe, we will use
    Keepalived to add some HA to our configuration. Keepalived is a software application
    that is open source and designed for Linux-based systems. Its main function is
    to manage network load balancing and failover, ensuring the HA of web services.
    Keepalived is often used alongside HAProxy. The software primarily uses the **Virtual
    Router Redundancy Protocol** (**VRRP**) to achieve fault tolerance and evenly
    distribute the load. Keepalived uses the following features to provide its redundancy:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High availability**: With Keepalived, you can establish a cluster of backup
    servers that utilize a shared **Virtual IP** (**VIP**) address. This setup ensures
    that even if the primary server experiences a failure, a secondary server will
    automatically take over and handle incoming traffic, resulting in minimal downtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VRRP**: VRRP is a commonly used protocol that enables automatic router failover
    in IP networks. Keepalived utilizes VRRP to keep a VIP address operational, which
    can be assigned to any node within the cluster as needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Health checking**: The monitoring system of Keepalived regularly checks the
    health of active servers. If a server becomes unresponsive, Keepalived will remove
    it from the pool and redirect traffic to the healthy servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Notification mechanisms**: With Keepalived, it’s possible to set up notifications
    for failover events or when certain thresholds are exceeded. These notifications
    are useful for keeping an eye on the overall health of the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a cluster, Keepalived designates one node as the master and the others as
    backups. The master node handles incoming traffic and responds to ARP requests
    for the VIP address, while the backups act as standby routers and monitor the
    master’s status. The nodes communicate using the VRRP protocol, with the master
    periodically sending VRRP advertisements to show its functioning. Often, a VIP
    is used to allow a single IP address for end user access. This normal operation
    is seen in the following figure, where we have Keepalived managing two HAProxy
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Keepalived normal operations](img/B18349_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Keepalived normal operations
  prefs: []
  type: TYPE_NORMAL
- en: If the backups stop receiving advertisements or detect any issues with the master,
    one backup node will take over as the new master. Keepalived checks the servers’
    health using mechanisms such as ICMP (ping) checks or Layer 4 checks (e.g., checking
    whether a specific port is open) and removes failed servers from the pool. The
    VRRP priority is adjusted to ensure a healthy backup server takes over. This is
    seen in the following figure, where the VIP that users connect to has migrated
    over to the second node, and that node is now using HAProxy to manage the workload.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Keepalived failed node](img/B18349_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Keepalived failed node
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe expands upon the previous one, adding a second load-balancer system
    and a VIP. You will need to build the second load balancer and acquire an additional
    IP address for the VIP. Your IPs should be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Host** | **IP** |'
  prefs: []
  type: TYPE_TB
- en: '| **web1** | `192.168.56.200` |'
  prefs: []
  type: TYPE_TB
- en: '| **web2** | `192.168.56.201` |'
  prefs: []
  type: TYPE_TB
- en: '| **lb1** | `192.168.56.202` |'
  prefs: []
  type: TYPE_TB
- en: '| **lb2** | `192.168.56.203` |'
  prefs: []
  type: TYPE_TB
- en: '| **vip** | `192.168.56.204` |'
  prefs: []
  type: TYPE_TB
- en: Table 6.4 – Keepalived IP addresses
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you start configuring Keepalived, you need to configure HAProxy on the
    second server. You can easily install HAProxy, open up the firewall ports, and
    copy over the config file from the existing system.
  prefs: []
  type: TYPE_NORMAL
- en: You can test this by simply pointing your browser to the second load balancer
    and seeing the app server.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will start to configure Keepalived.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each load balancer, you will need to install Keepalived as the root user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will need to edit the Keepalived config file. This is found in `/etc/keepalived/keepalived.conf`.
    There are two major sections to edit, `global_defs` and `vrpp_instance`.
  prefs: []
  type: TYPE_NORMAL
- en: '`global_defs` is the global definition used by Keepalived. These settings are
    used by all `vrrp_instance` types configured in the system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several parameters that you will need to update:'
  prefs: []
  type: TYPE_NORMAL
- en: '`notification_email`: This is a list of email addresses that will be emailed
    when there is an event'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`notification_email_from_user`: This is the sending email address'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`smtp_server`: This is the SMTP relay server'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`router_id`: This is a unique name for this Keepalived cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the sample, this section looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Keepalived globals](img/B18349_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Keepalived globals
  prefs: []
  type: TYPE_NORMAL
- en: The next section is `vrrp_instance`. You can have multiple `vrrp_instance` types
    in the cluster, each supporting different VIPs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For `vrrp_instance`, you need to give each one a unique name. Additionally,
    there are several parameters that will need to be updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '`state`: The state of the instance, usually master for the primary node and
    backup for the secondary node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`interface`: The Ethernet interface used for this host in the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virtual_router_id`: A unique number for this instance. No other instances
    should use the same ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`authentication`: This section defines how members are authenticated:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`auth_type`: Normally sent to PASS, to allow nodes to authenticate as members
    of this instance. There is a second support type called `auth_pass`: The password
    for the instance.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virtual_ipaddress`: A list of VIPs managed by this instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our example, the section will look as follows;
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Keepalived vrrp_instance](img/B18349_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Keepalived vrrp_instance
  prefs: []
  type: TYPE_NORMAL
- en: Once the config file is built, copy it over to the second load balancer. Do
    not forget to change the state to `BACKUP` on the second system, and also update
    the interface if it is different on that system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, start Keepalived on both nodes with the following command as the root
    user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You can now point your browser to the VIP! This is seen in the following example.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Keepalived VIP in use](img/B18349_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Keepalived VIP in use
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the status by looking at the journal entries for the daemon,
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This will show all the activity of the daemon. You should see `Sending gratuitous
    ARP` messages, as this is the system checking the health. You will also see messages
    about the state, such as `Entering MASTER STATE` or `Entering BACKUP STATE`, as
    the system switches between `MASTER` and `BACKUP`.
  prefs: []
  type: TYPE_NORMAL
- en: HA clustering for all with Corosync and Pacemaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipes, we addressed HA by distributing traffic between two
    active application servers. However, this method is only effective for stateless
    applications where the server or browser doesn’t contain specific user or session
    data. For applications that are not stateless or run on a complex server, a different
    approach to HA is necessary. The solution is to start and stop the application
    components on different servers, using the combination of Pacemaker and Corosync.
    These two open source software projects work together to provide HA clustering
    for Linux-based systems. They coordinate and manage multiple nodes in a cluster,
    ensuring that critical services remain available even during hardware or software
    failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Corosync serves as the communication layer for the HA cluster stack, allowing
    for dependable communication between nodes. It utilizes a membership and quorum
    system to monitor the cluster’s active nodes and guarantee that only one node
    operates as the primary (or master) at a given time. The messaging layer is essential
    for sharing data regarding the cluster’s state, node status, and resource conditions.
    Corosync plays a vital role in the cluster’s functionality, providing key features
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster communication**: Corosync enables nodes to exchange messages reliably
    and efficiently, allowing them to coordinate and synchronize their actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Membership and quorum**: Corosync is a tool that keeps track of active nodes
    in a cluster and uses a quorum algorithm to ensure that there are enough nodes
    available to make decisions. This helps avoid split-brain scenarios and makes
    sure that only one node is active. It’s crucial to avoid split-brain clusters
    because they can cause data inconsistencies, corruption, and service disruptions.
    A split-brain scenario occurs when nodes in a cluster lose communication with
    each other. As a result, each node thinks it’s the only active one in the cluster.
    This can happen because of network issues, communication failures, or misconfigurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When there is a split-brain scenario, several nodes within the cluster may begin
    running services or using shared resources on their own, thinking that they are
    the only active node. This can cause conflicts and data inconsistencies since
    each node operates independently without any coordination. When possible, use
    an odd number of nodes in a cluster, or enable some protection using quorum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pacemaker is a cluster resource manager that utilizes Corosync’s messaging
    and membership features to manage cluster resources and handle resource failover.
    It determines which node in the cluster should run specific services (resources)
    based on established policies and constraints. Pacemaker brings the following
    features to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource management**: With Pacemaker, administrators can set up resources
    that require strong availability, such as IP addresses, services, databases, and
    applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource monitoring**: Pacemaker continuously monitors the status of resources
    and nodes to detect failures or changes in the cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource failover**: If a node fails or there are resource problems, Pacemaker
    will begin a failover process, transferring resources to functioning nodes to
    guarantee uninterrupted availability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource constraints**: Administrators can set constraints and rules for
    resource placement and failover, defining which nodes are preferred or prohibited
    for specific resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Colocation and order constraints**: Pacemaker allows defining relationships
    between resources, specifying which resources must run together on the same node
    or in a specific order'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster management**: Pacemaker provides various command-line utilities and
    graphical interfaces (such as Hawk) for managing and configuring the cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, you will need two VMs, each with at least two vCPUs, 8 GB
    of RAM, and 50 GB of disk space. You should have Oracle Linux 8 installed, and
    also a third IP address for a floating VIP to be managed by the cluster. Both
    of the web servers will be patched to the latest software. For this example, the
    following IPs will be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Host** | **IP** |'
  prefs: []
  type: TYPE_TB
- en: '| **Web1** | `192.168.56.200` |'
  prefs: []
  type: TYPE_TB
- en: '| **Web2** | `192.168.56.201` |'
  prefs: []
  type: TYPE_TB
- en: '| **vip** | `192.168.56.204` |'
  prefs: []
  type: TYPE_TB
- en: Table 6.5 – HA cluster IPs
  prefs: []
  type: TYPE_NORMAL
- en: Before we start with the cluster, you will also need to set up an `httpd` (Apache
    2.4) server on each host. This is similar to other hosts set up in other recipes.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, on both servers, as root, install the Apache web server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We do need to enable the status page for Apache. This is one way the resource
    will be checked. To do this, copy the following lines into `/etc/httpd/conf.d/status.conf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We also need a simple web page. For testing purposes, put the following into
    `/var/www/html/index.html` on both servers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When setting up an application such as a web server, putting your content directory
    (such as `/var/www/html`) on a Gluster filesystem makes it easier to manage updating
    your content. This also works for other data that the application uses, such as
    temporary state data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, on both servers, add port `80` to the local firewall with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, for testing purposes, manually start the server on both nodes. Do not
    enable the service to automatically start. This will be done later when Pacemaker
    is configured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You should now see a basic page on both servers, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – httpd server test](img/B18349_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – httpd server test
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also test the `server-status` page using the `wget` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Sample output of a success is seen in the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Successful server-status](img/B18349_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Successful server-status
  prefs: []
  type: TYPE_NORMAL
- en: You are now ready to install and configure Pacemaker and Corosync.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have installed the Apache `httpd` server that we will cluster,
    let’s start by installing the software. First, we need to enable the `addons`
    repo. This is done with the following commands on both servers as root.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, on both nodes, enable the repo with the following commands as root:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you will install the software:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the installation of these packages is complete, a new user named `hacluster`
    will be added to your system. Please note that remote login will be disabled for
    this user after installation. To carry out tasks such as synchronizing the configuration
    or starting services on other nodes, it is necessary to set the same password
    for the `hacluster` user on both nodes. We can use the `passwd` command to set
    the password:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to enable the `pcs` service and start it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to open up the firewall for the cluster port. This is done with
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now we’re done with both nodes for a bit. The next few commands can be done
    on either of the nodes, but note, you still should be root.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to add both nodes to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If your nodes are not resolvable in DNS or the `/etc/hosts` file, you can optionally
    add `addr=$IPADDR` for each host after the hostname. But it’s highly recommended
    to make sure all hosts are resolvable. This option, if used, would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pcs host auth web1 addr=192.168.56.200 node2 addr=192.168.56.201 -****u hacluster**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will create the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can start the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify that the cluster is running, we can check with the `pcs` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'A healthy new cluster should return a similar output as the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Cluster status](img/B18349_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Cluster status
  prefs: []
  type: TYPE_NORMAL
- en: 'In a cluster consisting of only two nodes, the quorum operates differently
    than in clusters with more nodes. In such a cluster, the quorum value is set to
    `1` to ensure that the primary node is always considered in quorum. If both nodes
    go offline due to a network outage, they compete to fence each other, and the
    first to succeed wins the quorum. To increase the chances of a preferred node
    winning the quorum, the fencing agent can be configured to give it priority. This
    is done with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step is to disable **STONITH**, which stands for **Shoot The Other
    Node In The Head**. This is an advanced fencing tool that requires configuration
    specific to your environment. If you want to experiment with this technology then
    check out the official Oracle docs here – [https://docs.oracle.com/en/operating-systems/oracle-linux/8/availability/availability-ConfiguringFencingstonith.html#ol-pacemaker-stonith](https://docs.oracle.com/en/operating-systems/oracle-linux/8/availability/availability-ConfiguringFencingstonith.html#ol-pacemaker-stonith):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: To set up a cluster, we’ll need to create resources. A resource agent name has
    two or three fields separated by a colon. The first field is the resource class
    that indicates the standard followed by the resource agent and helps Pacemaker
    locate the script. For example, the IPaddr2 resource agent follows the **Open
    Cluster Framework** (**OCF**) standard. The second field varies based on the standard
    used, and OCF resources use it for the OCF namespace. The third field denotes
    the name of the resource agent.
  prefs: []
  type: TYPE_NORMAL
- en: Meta-attributes and instance attributes are available for resources. Meta-attributes
    are not resource-type dependent, while instance attributes are specific to each
    resource agent.
  prefs: []
  type: TYPE_NORMAL
- en: In a cluster, resource operations refer to the actions that can be taken on
    a specific resource, such as starting, stopping, or monitoring it. These operations
    are identified by the `op` keyword. To ensure the resource remains healthy, we
    will add a monitor operation with a 15-second interval. The criteria for determining
    whether the resource is healthy depends on the resource agent being used. This
    is also why we enabled the `server-status` page on the `httpd` server, as the
    `httpd` agent uses that page to help determine the health of the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s add the VIP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, we will add the `httpd` server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have two resources, we also will need to tie them together as a
    group. With most applications, multiple resources need to be on the same physical
    server at the same time. This could be IP addresses, Tomcat servers, `httpd` servers,
    and so on. We are going to call this group `WebApp` and add both the VIP and `https`
    servers to it. Each resource is added individually, so two commands will need
    to be run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will use the `pcs status` command to check the configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – pcs status](img/B18349_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – pcs status
  prefs: []
  type: TYPE_NORMAL
- en: We can now see the cluster, with its resource group, `WebApp`, with both the
    server VIPs.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that everything is configured, let’s start up the resources and manage
    them. We can first start the entire cluster. This will online all nodes in the
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also set up the cluster to start on boot with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: You will need to run both of these commands as root on both nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Next up, let’s look at a few useful commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes a resource gets broken; maybe it was a bad config file, or maybe
    you started a resource outside of the cluster control, confusing the cluster.
    Once you fix the issues, you will likely need to refresh the resource. This will
    tell the cluster to forget about the failure and restart the service clear of
    any errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'You also can check the details of a resource, using the `config` option. This
    is helpful if you forget how the resource was configured. An example is seen in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Resource configuration](img/B18349_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – Resource configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s move `WebApp` to server `web2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run `move`, you can also monitor the move by checking the constraints.
    This is a little cleaner than using the `pcs` `status` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Cluster constraints](img/B18349_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Cluster constraints
  prefs: []
  type: TYPE_NORMAL
- en: The power of the Pacemaker/Corosync technology is its flexibility. You can cluster
    just about anything with it, making it a powerful tool for the sysadmin.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing a filesystem across multiple machines – cluster or distribute?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you start using technologies such as load balancers and clustering software,
    you often end up in a situation where you need the same files on multiple servers.
    While you could simply copy the files, what if you could mount the files on each
    of the servers, sharing the filesystem across the systems without the SPOF that
    an NFS server introduces? One of the easiest ways to do this is to use Gluster.
  prefs: []
  type: TYPE_NORMAL
- en: '**Gluster**, also known as **GlusterFS**, is an open source distributed filesystem
    that provides scalable and flexible storage for large volumes of data. Initially
    developed by Gluster Inc., it is now maintained by the open source community.
    Gluster uses a distributed architecture to create a single and unified filesystem
    that can span across multiple servers and storage devices. This approach allows
    you to aggregate the storage capacity of multiple servers and present it as a
    single, well-structured filesystem to users and applications. It has a wide range
    of applications, such as data storage, backup, and content delivery.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Key features and concepts of Gluster include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: Adding more storage servers to the cluster allows Gluster
    to easily accommodate growing data storage needs while scaling horizontally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Redundancy**: Gluster ensures data availability by replicating data across
    multiple nodes for redundancy and fault tolerance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: Gluster supports various storage options, including local
    disks, NAS, and cloud storage. It can be customized to fit specific use cases
    and technologies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filesystem abstraction**: It provides users and applications with a standard
    filesystem interface, making integration into existing systems relatively easy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data distribution**: Data is distributed across the cluster in a way that
    improves both performance and reliability. Data can be distributed evenly or based
    on specific criteria.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic healing**: Gluster has a self-healing feature that automatically
    detects and repairs data inconsistencies or corrupted files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gluster is often used in environments where large-scale, distributed storage
    is required, such as web servers, cloud computing, big data analytics, and media
    streaming services. It provides a cost-effective and flexible solution for managing
    data across a network of servers and storage devices.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, you will need two Oracle Linux 8 systems, each with access
    to YUM repos. For this exercise, we will call them `gluster1` and `gluster2`.
    They are identical systems, each with 8 GB RAM, 4 vCPUs, and 100 GB of drive space.
    The filesystems have 50 GB in `/`, 5 GB in `/home`, and 8 GB in swap. The remaining
    disk space is unallocated. Additionally, for this example, each node will have
    a 100 GB LUN used for storing Gluster data.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Having at least three nodes in a cluster is highly recommended to avoid split-brain
    clusters. Although a two-node cluster is possible, it poses a risk of corrupt
    data if the system ever splits its brain. Split-brain clusters are undesirable
    in distributed computing environments because they can result in data inconsistency,
    corruption, and operational issues. Split brain occurs when the nodes in a cluster
    lose connectivity or communication with each other, leading to the cluster’s division
    into multiple isolated nodes. Each node thinks it is the active or primary cluster,
    resulting in the potential for conflicts and data discrepancies.
  prefs: []
  type: TYPE_NORMAL
- en: 'On each server, you will need to perform the following prep work:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an XFS filesystem on the 100 GB LUN. This space will be used to store
    the Gluster data, known as **bricks**. In the context of Gluster, a *brick* refers
    to a basic storage unit within the storage cluster. A cluster is made up of multiple
    bricks, which are essentially directories on storage servers or devices where
    data is stored. Each brick represents a portion of the overall storage capacity
    of the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since we will be using Gluster to manage the storage, we will not be using
    LVM on the filesystem. On these systems, `/dev/sdb` is the 100 GB LUN. The following
    commands are used to create and mount the filesystem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 6.21 – Bricks mounted](img/B18349_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.21 – Bricks mounted
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to make sure that all the nodes are in the `/etc/hosts` file.
    In this example, `gluster1`, `gluster2`, and `gluster3` are in the file, using
    both the short name and the **Fully Qualified Doman Name** (**FQDN**). This is
    seen in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: dnf -y install oracle-gluster-release-el8
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: dnf -y config-manager --enable ol8_gluster_appstream ol8_baseos_latest ol8_appstream
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: dnf -y module enable glusterfs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: dnf -y install @glusterfs/server
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'systemctl status glusterd command. Verify that the service is active and running,
    as seen in the following example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 6.22 – Gluster daemon is running](img/B18349_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.22 – Gluster daemon is running
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s configure the firewall to allow the `glusterfs` port with the following
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Additionally, to improve security, let’s create a self-signed key, to encrypt
    the communication between the nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we are using a self-signed certificate. In a secure production
    environment, you will want to consider using a commercially signed certificate.
  prefs: []
  type: TYPE_NORMAL
- en: We will use these files later to encrypt the communication.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that all the prep work has been completed on each of the nodes, we will
    create the trusted storage pools and encrypt the communications. This will have
    everything ready to create volumes, where data is stored and shared.
  prefs: []
  type: TYPE_NORMAL
- en: 'A trusted storage pool in Gluster pertains to a setup where a cluster of Gluster
    servers, referred to as storage nodes or peers, have established trust among themselves
    to work together within a storage cluster. This trust is established through a
    trusted storage pool configuration that typically involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Authentication**: Various methods, such as SSH keys, certificates, or shared
    secrets, can be used to authenticate nodes in the trusted storage pool. This ensures
    that only authorized servers are part of the storage cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authorization**: After nodes are authenticated, they authorize each other
    to access and manipulate specific data within the Gluster storage cluster. The
    authorization settings determine which nodes have read and write access to particular
    volumes or bricks within the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Communication**: Members of the trusted storage pool communicate over a secure
    network to replicate data, synchronize metadata, and perform other cluster-related
    operations, ensuring that the storage cluster functions cohesively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data integrity**: Trusted storage pools ensure data integrity and redundancy
    via distributed replication across multiple nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: It is possible to add more storage nodes to the trusted pool,
    which enhances storage capacity and performance. The trusted nature of the pool
    makes it easy for new nodes to join the cluster and contribute to its resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Gluster, a trusted storage pool is a crucial element as it lays the foundation
    for the fault-tolerant and distributed nature of the filesystem. It guarantees
    that all nodes within the cluster can work seamlessly and securely in collaboration
    with each other. The following steps will walk you through how to create a GlusterFS
    on two hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the pool, we need to probe the other nodes in the cluster. In this
    example, we will probe from `gluster1` to `gluster2` using the `gluster peer probe`
    `gluster2` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[root@gluster2 etc]#  gluster pool list'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: UUID                                 Hostname        State
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b13801f3-dcbd-487b-b3f3-2e95afa8b632                         gluster1        Connected
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'gluster1 and localhost connected as this was run on gluster2. If you run the
    same command from gluster1, you will see gluster2 as the remote host:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have the cluster, let’s create a replicated volume. This volume
    will re-write the bricks across the cluster, enabling protection against failed
    storage or a failed node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following command will create the volume:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'df command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 6.23 – data1 mounted on /mnt](img/B18349_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.23 – data1 mounted on /mnt
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to mount this on other nodes, you will need to repeat the command
    on each node, updating the node name as needed. The following example shows mounting
    on `gluster1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to enable the encryption. This is done by touching the secure-access
    file on each node using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: gluster volume set data1 client.ssl on
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'glusterd:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Gluster communication is now encrypted for this volume.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is more you can do with Gluster. First, volumes have multiple options
    when you create them, each offering options for replication and distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed**: When using distributed volumes, files are randomly distributed
    across the bricks in the volume. This type of volume is useful when the need is
    to scale storage, and redundancy is not necessary or is already provided by other
    hardware or software layers. However, it is important to note that disk or server
    failures can result in significant data loss, as the data is spread randomly across
    the bricks in the volume. An example command to build a distributed volume is
    the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Replicated**: Files are copied across bricks for HA in replicated volumes.
    An example command to build a replicated volume is the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Distributed replicated**: Distributed files across replicated bricks in the
    volume for improved read performance, HA, and reliability. When creating a distributed
    replicated volume, the number of nodes should be a multiple of the number of bricks.
    An example command to build a distributed replicated volume is the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Dispersed**: This volume type utilizes erasure codes to efficiently protect
    against disk or server failures. It works by striping the encoded data of files
    across multiple bricks in the volume while adding redundancy to ensure reliability.
    Dispersed volumes allow for customizable reliability levels with minimal space
    waste. A dispersed volume must have at least three bricks. An example command
    to build a dispersed volume is the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Distributed dispersed**: Distributes data across dispersed bricks, providing
    the same benefits of distributed replicated volumes but using dispersed storage.
    A dispersed volume must have at least six bricks. An example command to build
    a distributed dispersed volume is the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When adding bricks to any volume, you can put more than one brick on a Gluster
    node. Simply define the additional brick in the command. In this example, a distributed
    dispersed volume is created, by putting two bricks on each node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Volumes can be stopped with the `gluster stop volume volumename` command. An
    example to stop the `data1` volume is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also add bricks to a volume to grow it. This can be done after a new
    node is added to the cluster. In the following example, `gluster3` was added to
    the cluster with the `gluster node probe gluster3` command first. Then, `data1`
    was grown with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When adding bricks to a volume, make sure you add the required number of bricks.
    Volume types such as distributed replicated volumes will need more than a single
    brick added.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also check the status of all volumes with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'An example is seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.24 – volume status](img/B18349_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.24 – volume status
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the summary for a single volume by adding the volume name to the
    command. You can also see more details by adding the `detail` option. These can
    be combined, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Volume details](img/B18349_06_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.25 – Volume details
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to see performance information about a volume, the `top` option
    can be used. This will show what bricks are being used for read/write activity
    as well as I/O throughput to each brick. The basic command is `gluster volume
    top volume_name option`, with `volume_name` being the name of the volume and the
    options being as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`read`: This shows the highest read calls for each brick, as well as the counts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`write`: This shows the highest write calls for each brick, as well as the
    counts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`open`: This shows what bricks have open file descriptors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`opendir`: This shows what bricks have open calls on each directory, as well
    as the counts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`read-perf`: This shows read-performance throughput by brick. Run using the
    options `bs` (for block size) `1024` and `count 1024`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`write-perf`: This shows read-performance throughput by brick. Run using the
    options `bs` (for block size) `1024` and `count 1024`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Several examples are seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.26 – volume top examples](img/B18349_06_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.26 – volume top examples
  prefs: []
  type: TYPE_NORMAL
- en: Volumes can also be deleted. This is done with the `gluster volume delete volume_name`
    command, where `volume_name` is the volume being deleted. As a note, when deleting
    volumes, don’t forget to use the `rm` command to delete the bricks from storage.
  prefs: []
  type: TYPE_NORMAL
- en: Generating, configuring, and monitoring Ethernet traffic over bond
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When using bare-metal servers as dedicated hosts or Linux systems that host
    virtual machines using the KVM hypervisor, the network can be a weak point. Fortunately,
    this issue can be resolved by implementing Ethernet bonding, also known as network
    bonding, or **Network Interface Card** (**NIC**) bonding. It is a technology in
    Linux that allows you to combine multiple NICs into a single logical interface.
    This logical interface, known as a bond or bonded interface, provides increased
    network bandwidth, fault tolerance, and load balancing. These are summarized as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load balancing**: Bonding distributes network traffic across multiple NICs,
    increasing bandwidth. Various algorithms, such as round-robin, active-backup,
    and XOR, can be used depending on specific requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: In the event of an NIC or network link failure, Ethernet
    bonding can automatically switch traffic to another active NIC. This provides
    redundancy and fault tolerance, ensuring network connectivity remains available
    even if one NIC becomes unavailable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Link aggregation**: Bonding can be used to create **link aggregation groups**
    (**LAGs**) or NIC teams, which enhance bandwidth and redundancy in HA setups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will configure bonding, and then show some common tools that
    will allow you to both monitor and generate Ethernet traffic over the bond.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there are a few technologies you need to be familiar with.
  prefs: []
  type: TYPE_NORMAL
- en: MAC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **Media Access Control** (**MAC**) address is a hardware identifier assigned
    to network interfaces such as Ethernet cards and Wi-Fi adapters for communication
    on a local network. It is hardcoded into the network hardware during manufacturing
    and is used at the data link layer (Layer 2) of the OSI model. One of the most
    important features of MAC addresses is that they must be unique. Each MAC address
    is meant to be globally unique, and manufacturers bear the responsibility of ensuring
    that no two network interfaces have the same MAC address, though this can be a
    challenging task to accomplish in practice, especially in virtualized environments.
    This can be an issue with networking, as duplicate MAC addresses on any network
    will cause issues. Additionally, many of the bonding modes rely on MAC addresses
    to load balance traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Bonding modes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Bonding modes refer to the various strategies or algorithms used to determine
    how network traffic is distributed across the physical network interfaces that
    have been aggregated into a bonded interface using the Linux bonding driver. These
    modes control the load-balancing and failover behavior of the bonded interface.
    The choice of bonding mode depends on your specific network requirements and goals.
    Here are some common Linux bonding modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`balance-rr`: In this mode, outgoing network traffic is distributed evenly
    across the available network interfaces in a round-robin fashion. It’s a simple
    load-balancing mode that provides improved outbound traffic performance but does
    not consider the state of the interfaces, which can lead to uneven inbound traffic
    distribution. Occasionally, this mode does not work well with some switching systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`active-backup`: A commonly used mode, which is often referred to as failover
    mode, this mode has a primary interface, while the others remain on standby. If
    the primary interface fails, the next available interface is automatically activated
    to ensure continuity. This mode provides redundancy and is one of the easiest
    modes to get working in any environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`balance-xor`: This mode utilizes a straightforward XOR operation to maintain
    a balance between the transmission and reception of data. The process involves
    distributing traffic based on the MAC addresses of the source and destination.
    This guarantees that packets between the same endpoints will always take the same
    path. The primary purpose of this mode is to ensure fault tolerance. Occasionally,
    this mode does not work well with some switching systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`balance-tlb`: When operating in this mode, the outgoing traffic is distributed
    among all available interfaces based on their current load. However, incoming
    traffic is not actively balanced, and it is only received by the active interface.
    This mode is particularly useful when the switch does not support **Link Aggregation
    Control Protocol** (**LACP**). Occasionally, this mode does not work well with
    some switching systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`balance-alb`: This mode actively balances both incoming and outgoing traffic
    by considering the availability and load of each interface. Occasionally, this
    mode does not work well with some switching systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LACP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All of the preceding modes can operate without any changes to the switches that
    the server is connected to. However, there is another mode that is more commonly
    used, called LACP. LACP is the most complex mode used to aggregate multiple network
    connections, usually Ethernet, into a single high-bandwidth link. This process
    is commonly known as link aggregation, NIC teaming, or bonding. LACP is defined
    by the IEEE 802.3ad standard and is frequently used in enterprise and data center
    environments to enhance network performance, redundancy, and fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, to utilize LACP, switches must be configured to use it. As an administrator,
    it is essential to communicate your configuration requirements to ensure that
    the switch is configured in a compatible mode. The configuration must match on
    both ends for LACP to work correctly. Most enterprise-grade network switches and
    server NICs provide LACP support. Key characteristics and features of LACP include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Aggregated links**: LACP enables the aggregation of multiple physical network
    links into a single logical link, which appears as a single interface to network
    devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increased bandwidth**: Aggregating multiple links with LACP can boost network
    bandwidth for bandwidth-intensive applications and server-to-switch connections.
    However, each MAC-to-MAC connection is usually limited to the speed of a single
    member of the aggregated link. If you have a host with two 1 Gb/s ports in the
    link, you will likely be unable to get more than 1 Gb/s of communication between
    the host and a client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load balancing**: LACP can distribute network traffic across aggregated links
    using various load-balancing algorithms, preventing network congestion on a single
    link while optimizing network utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: In addition to providing increased bandwidth, LACP also
    offers redundancy and fault-tolerance capabilities. If one physical link fails,
    LACP can automatically redirect traffic to the remaining active links, minimizing
    downtime and ensuring network availability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic protocol**: LACP is a dynamic protocol that dynamically negotiates
    and establishes link aggregations between network devices using LACP frames.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modes**: LACP supports two modes of operation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Active mode**: In this mode, the device actively sends LACP frames to negotiate
    and establish link aggregations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Passive mode**: In passive mode, the device listens for LACP frames but does
    not actively send them. It relies on the other end configured in active mode to
    initiate the aggregation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LACP is commonly used in scenarios where HA and network performance are critical,
    such as server-to-switch connections, inter-switch links, and connections to **Storage
    Area Networks** (**SANs**). It allows organizations to make efficient use of available
    network resources and improve network reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, you will need three Oracle Linux 8 systems, each with access
    to yum repos. For this exercise, we will call them `networking`, `client1`, and
    `client2`. They are mostly identical systems, each with 8 GB RAM, 4 vCPUs, and
    100 GB of drive space. The difference is that networking should have two network
    interfaces on the same network. The filesystems have 50 GB in `/`, 5 GB in `/home`,
    and 8 GB in swap. The remaining disk space is unallocated.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While this can be done from the GUI, this recipe will cover doing this from
    the command line. As a note, when working on the main network connection to the
    server, you will want to be on the system console. Doing this via a remote connection
    such as SSH can leave you in a situation where you lose access to the server.
    Following are the steps to configure a redundant connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing you will need to do is create the bond. In this example, we
    will create it using the `balance-alb` mode to best balance both incoming and
    outgoing traffic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will configure the bond to use DHCP. On production servers, you would
    normally use a manually configured IP address:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are adding a port into a bond that is already in use, you should delete
    that port now. In this case, `enp0s3` was in use, so it was deleted with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '**nmcli connection** **del enp0s3**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will start the connection and check the status:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 6.27 – nmcli device output with a working bind](img/B18349_06_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.27 – nmcli device output with a working bind
  prefs: []
  type: TYPE_NORMAL
- en: You can see `bond0` as the device, with members `enp0s3` and `enp0s8`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you look at the IP address, you will see that that is now on the `bond0`
    device. This can be checked with the `ifconfig bond0` command, with the output
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.28 – Output from ifconfig bond0](img/B18349_06_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.28 – Output from ifconfig bond0
  prefs: []
  type: TYPE_NORMAL
- en: You can continue to use the system normally now, but with `bond0` being the
    network device.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have a working bond, let’s look at the traffic going in and out.
    To do this, we need to install the `iptraf-ng` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'This tool allows you to monitor Ethernet traffic on a server. For this example,
    we will run the `iptraf-ng` command. This will launch the program, and you will
    be on the main screen, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.29 – iptraf main menu](img/B18349_06_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.29 – iptraf main menu
  prefs: []
  type: TYPE_NORMAL
- en: 'From here, we will look at the **General interface statistics** by hitting
    the *S* key. This will then show a real-time flow of traffic on each interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.30 – General interface statistics](img/B18349_06_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.30 – General interface statistics
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that both in and out traffic is balanced between the two physical
    interfaces. This is because the bond was built using the `balance-alb` mode. To
    generate this traffic, a simple flood ping was used from the `client2` system.
    This was done with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Be careful using the `-f` option, as this will flood the network with traffic,
    and it is generally not acceptable to do so on production networks without coordinating
    with the network and security teams. It can cause performance issues for systems
    using the network.
  prefs: []
  type: TYPE_NORMAL
- en: There is a better way to really stress the network, though. That is to use a
    tool that will generate the maximum levels of packets the interface will support.
  prefs: []
  type: TYPE_NORMAL
- en: iperf – network stress tool
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will use a tool called `iperf`. It will generate the maximum amount of traffic
    that the interface can support. To install `iperf`, run the following command
    on all systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We also will need to open up a TCP port for the system to use and reload the
    firewall. Each instance of the server can only handle a single client connection.
    We will add multiple ports to enable running multiple servers, each with a different
    port. This is done with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '`iperf` works using a client-server model. In order to use it, we first need
    to start an `iperf` server on the networking system. With this example, we will
    set up the server to listen on port `8001` using TCP. This is started with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'We will repeat the same command in a different window, running on port `8002`
    instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Now, with two servers running, we can run a test from `client1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test is run with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The test will kick off and for a few seconds run the maximum traffic possible,
    as seen in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.31 – iperf client output](img/B18349_06_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.31 – iperf client output
  prefs: []
  type: TYPE_NORMAL
- en: 'While the test runs, we can monitor the performance from the `iptraf` command,
    and we will see most of the traffic is hitting a single interface. This is seen
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.32 – Single-client traffic](img/B18349_06_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.32 – Single-client traffic
  prefs: []
  type: TYPE_NORMAL
- en: This traffic is all on the `enp0s8` interface. This is because the load-balancing
    algorithm uses the MAC address of the client. This limits the traffic to a single
    interface in the bond.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up, can run a test from `client1` and `client2` simultaneously. The difference
    is that `client2` will use port `8002`. The results are seen in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.33 – Test with two different clients](img/B18349_06_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.33 – Test with two different clients
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that both interfaces are being used for heavy network loads.
    This is happening because each client is using a different MAC address. If a third
    client were used, we would then see it competing for traffic for one of the interfaces,
    potentially causing issues.
  prefs: []
  type: TYPE_NORMAL
- en: This is an important factor to consider when bonding in environments where heavy
    network performance is required from multiple clients. When using bonds, and you
    have random performance latency issues, monitor the ports. You might face some
    contention. One way to address this is to add additional ports to the bond. You
    can also upgrade the interfaces to units that can support more bandwidth.
  prefs: []
  type: TYPE_NORMAL
