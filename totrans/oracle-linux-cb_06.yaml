- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Eliminating All the SPOFs! An Exercise in Redundancy
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消除所有 SPOFs！冗余练习
- en: It’s crucial to have redundancy in your architecture to ensure smooth operations.
    Eliminating **Single Points of Failure** (**SPOFs**) is a common way to achieve
    this. Implementing **High-Availability** (**HA**) technology is one way to eliminate
    SPOFs. HA technology is crucial because it helps businesses maintain operational
    continuity, enhance performance, improve reliability, facilitate **Disaster Recovery**
    (**DR**), build customer trust, and comply with regulations. With HA technology,
    minimizing downtime and ensuring continuous service availability is possible,
    contributing to overall success and competitiveness in today’s technology-driven
    world.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在架构中确保冗余是至关重要的，以确保平稳的操作。消除**单点故障**（**SPOFs**）是实现这一目标的常见方法。实施**高可用性**（**HA**）技术是消除
    SPOFs 的一种方式。HA 技术至关重要，因为它帮助企业保持运营连续性，提升性能，提高可靠性，促进**灾难恢复**（**DR**），建立客户信任，并遵守法规。通过
    HA 技术，可以最小化停机时间并确保持续的服务可用性，从而为今天技术驱动的世界中的整体成功和竞争力做出贡献。
- en: Note
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 注释
- en: What about DR? We are not covering the failure of data centers in this chapter.
    That being said, the approach to implementing DR is very different than HA. When
    running in the cloud, look for services such as Oracle Full Stack Disaster Recovery
    Service that automate the DR process for the entire tech stack.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 那么灾难恢复呢？本章不涵盖数据中心故障的问题。话虽如此，灾难恢复的实施方法与 HA 非常不同。在云中运行时，可以寻找像 Oracle 全栈灾难恢复服务这样的服务，它们为整个技术堆栈自动化灾难恢复过程。
- en: 'The recipes in this chapter will help you eliminate SPOFs in your environment.
    We’ll start with a general overview of what HA is, as well as availability, and
    then get into several examples of how you can add redundancy to your application.
    The four most common technologies that are used to help eliminated SPOFs in applications
    are HAProxy, Corosync, Pacemaker, and GlusterFS. Each of these provides a specific
    set of features to help make an application highly available:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的操作指南将帮助你消除环境中的 SPOFs。我们将首先概述 HA 和可用性，然后介绍如何为你的应用程序增加冗余的几个示例。帮助消除应用程序中的 SPOFs
    的四种最常见技术是 HAProxy、Corosync、Pacemaker 和 GlusterFS。它们每个都提供了一套特定的功能来帮助使应用程序高度可用：
- en: '**HAProxy**: This is a load balancer, and allows you to balance web workloads
    between servers'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HAProxy**：这是一个负载均衡器，允许你在服务器之间平衡 Web 工作负载'
- en: '**Corosync**: This is a communication system that enables communication to
    help implement HA within applications'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Corosync**：这是一个通信系统，支持应用程序内部实现 HA 的通信。'
- en: '**Pacemaker**: Pacemaker is an open source resource manager that is used to
    build small and large clusters'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pacemaker**：Pacemaker 是一个开源资源管理器，用于构建小型和大型集群。'
- en: '**GlusterFS**: This is a scalable network filesystem that allows multiple nodes
    to read and write data to the same storage at the same time'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GlusterFS**：这是一种可扩展的网络文件系统，允许多个节点同时读取和写入同一存储中的数据。'
- en: 'The following recipes will help you implement these common HA technologies.
    This includes everything from load-balancing applications and clustering application
    systems to clustered storage and redundant networking:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的操作指南将帮助你实现这些常见的 HA 技术。这包括从负载均衡应用程序和集群应用系统到集群存储和冗余网络的一切内容：
- en: Getting 99.999% availability and beyond
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得 99.999% 的可用性及更高
- en: Load balancing a website
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网站的负载均衡
- en: Making HAProxy highly available with Keepalived
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keepalived 使 HAProxy 高可用
- en: HA clustering for all with Corosync and Pacemaker
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Corosync 和 Pacemaker 实现 HA 集群
- en: Sharing a filesystem across multiple machines – cluster or distribute?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个机器间共享文件系统——集群还是分布式？
- en: Generating, configuring, and monitoring Ethernet traffic over bond
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成、配置并监控通过 bond 的以太网流量
- en: Technical requirements
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For most of these recipes, you will need a pair of Oracle Linux 8 systems. As
    with most of these recipes, a VM on your desktop using a desktop virtualization
    product such as Oracle VirtualBox is recommended. A small VM is fine, with two
    cores, 2 GB RAM, and a few free gigabytes of disk space. You will also need some
    additional disks assigned to the VM, ideally at least five equally sized disks.
    Before you start, patch your system to the latest packages available. This only
    takes a few minutes and can save a ton of time when troubleshooting issues that
    are caused by a bug.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Many of the recipes in this book have their related configuration files available
    on GitHub at [https://github.com/PacktPublishing/Oracle-Linux-Cookbook](https://github.com/PacktPublishing/Oracle-Linux-Cookbook).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Getting 99.999% availability and beyond
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe will discuss the differences between DR and HA and how to architect
    HA solutions. Before we get into that, let’s refine the definition of a few key
    terms:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '**High availability**, or **HA**: This means protecting from the failure of
    a single component. Think of this as protecting against the failure of a system.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disaster recovery**, or **DR**: This is the failure of the data center or
    cloud region.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability nines**: When referring to *nines of availability*, it is a
    way to quantify the uptime or reliability of a system by specifying the number
    of nines in the uptime percentage. Each *nine* represents a decimal place in the
    uptime percentage.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s a breakdown of the most commonly used *nines* and their corresponding
    uptime percentages, assuming 24x7x365 operations:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '| **Nines** | **Downtime** **per Year** | **Downtime** **per Month** |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| 99 | 3d 14h 56m 18s | 7h 14m 41s |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| 99.9 | 8h 41m 38s | 43m 28s |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| 99.99 | 52m 10s | 4m 21s |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| 99.999 | 5m 13s | 26s |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| 99.9999 | 31s | 2.6s |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: Table 6.1 – Nines downtime
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding table, each additional nine in the uptime percentage signifies
    a higher level of availability and a reduced tolerance for downtime. Achieving
    higher numbers of nines typically requires implementing redundant systems, failover
    mechanisms, and rigorous maintenance practices to minimize downtime and ensure
    continuous operation. In addition, when setting up a **Service-Level Agreement**
    (**SLA**), you can also define the uptime during business hours and exclude scheduled
    maintenance. As an example, using a working schedule of Monday through Friday
    with 12 working hours a day, and 10 holidays off per year, the matrix would look
    very different!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '| **Nines** | **Downtime** **per Year** | **Downtime** **per Month** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| 99 | 1d 7h 2m 58s | 2h 35m 14s |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| 99.9 | 3h 6m 18s | 15m 31s |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| 99.99 | 18m 38s | 1m 33s |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| 99.999 | 1m 52s | 9s |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| 99.9999 | 11s | 1s |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: Table 6.2 – Business hours downtime
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: When setting SLAs with the business, carefully understand the differences between
    including maintenance windows and operational hours within the SLA.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When designing HA systems, there are several considerations that need to be
    taken into account to ensure the system is resilient and can handle failures.
    Here are some key considerations:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计HA系统时，需要考虑几个方面，以确保系统具有弹性并能够应对故障。以下是一些关键考虑因素：
- en: '**Redundancy**: Having redundancy in HA systems is essential. It requires replicating
    components or whole systems to eliminate potential SPOFs. Redundancy can be implemented
    at different levels, such as hardware, software, and network infrastructure. To
    minimize the impact of localized failures, it’s crucial to distribute redundant
    components across different physical locations.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冗余**：在HA系统中实现冗余是必不可少的。这需要复制组件或整个系统，以消除潜在的单点故障（SPOF）。冗余可以在不同的层面实现，如硬件、软件和网络基础设施。为了最小化局部故障的影响，必须将冗余组件分布在不同的物理位置。'
- en: '**Failover and load balancing**: It is important for HA systems to be equipped
    with failover mechanisms that enable automatic switching to a backup system whenever
    a failure occurs. One way to achieve this is through replicating data and services
    across multiple servers, coupled with the use of load-balancing techniques that
    ensure the even distribution of workload. With load balancing, traffic can be
    easily redirected to available servers in the event of a server failure.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障转移和负载均衡**：HA系统必须配备故障转移机制，使其在发生故障时能够自动切换到备份系统。实现这一点的一种方式是通过将数据和服务复制到多台服务器，并结合使用负载均衡技术，确保负载的均匀分配。通过负载均衡，流量可以在服务器故障时轻松地重定向到可用的服务器。'
- en: '**Scalability**: When designing HA systems, it is important to ensure that
    they can handle increased workloads and scale effortlessly. This can be achieved
    through horizontal scaling, which entails adding more servers to distribute the
    load, or vertical scaling, which involves adding resources to existing servers.
    Additionally, the system should be capable of dynamically adjusting resource allocation
    based on demand to prevent overloading.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：在设计高可用性（HA）系统时，确保系统能够处理增加的工作负载并能够轻松扩展非常重要。这可以通过水平扩展来实现，即添加更多的服务器以分配负载，或者通过垂直扩展来实现，即向现有服务器添加资源。此外，系统应能够根据需求动态调整资源分配，以防止过载。'
- en: '**Data replication and backup**: Maintaining data integrity and availability
    is crucial for HA systems. To ensure that data can still be accessed in case of
    a system failure, it is essential to replicate data across multiple storage systems
    or databases. Additionally, performing regular backups is vital to safeguard against
    potential data loss or corruption.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据复制和备份**：维护数据的完整性和可用性对HA系统至关重要。为了确保在系统故障时仍能访问数据，必须将数据复制到多个存储系统或数据库中。此外，定期备份对于防范潜在的数据丢失或损坏至关重要。'
- en: '**Fault tolerance**: Systems used for highly available architectures should
    have fault tolerance, which means they must be able to function even if specific
    components or subsystems malfunction. Achieving this requires creating a system
    that can manage errors with ease, recover automatically, and ensure continuity
    of service.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：用于高可用架构的系统应具备容错能力，这意味着即使特定组件或子系统发生故障，系统仍然能够正常运行。实现这一点需要构建一个能够轻松处理错误、自动恢复并确保服务连续性的系统。'
- en: '**Disaster recovery**: Having a DR plan is crucial for HA systems to effectively
    deal with catastrophic events such as natural disasters or widespread outages.
    This plan entails generating off-site backups, setting up secondary data centers,
    and relying on cloud-based services to guarantee business continuity, even amid
    extreme situations.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灾难恢复**：对于HA系统而言，拥有灾难恢复（DR）计划对于有效应对自然灾害或大规模停机等灾难性事件至关重要。该计划包括生成异地备份、设置备用数据中心以及依赖基于云的服务，确保即使在极端情况下也能保证业务连续性。'
- en: '**Documentation and testing**: It is crucial to document the system architecture,
    configurations, and procedures to effectively troubleshoot and maintain the HA
    system. Regular testing, such as failover tests, load testing, and DR drills,
    plays a significant role in identifying potential issues and ensuring the system
    operates as intended in various scenarios.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档和测试**：记录系统架构、配置和操作流程对于有效地排查故障和维护HA系统至关重要。定期测试，如故障转移测试、负载测试和灾难恢复演练，对于识别潜在问题并确保系统在各种场景中按预期运行起着重要作用。'
- en: '**Cost and complexity**: Designing, implementing, and maintaining HA systems
    can be both complex and costly. It is important to carefully consider the available
    budget, as well as the expertise and resources required to effectively manage
    and monitor the system.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本和复杂性**：设计、实施和维护高可用性系统可能既复杂又昂贵。重要的是仔细考虑可用预算，以及有效管理和监控系统所需的专业知识和资源。'
- en: By addressing these considerations, you can design a robust and resilient HA
    system that ensures HA, fault tolerance, and continuity of critical services.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解决这些考虑因素，您可以设计一个稳健且具有韧性的高可用性系统，确保高可用性、容错性和关键服务的连续性。
- en: How to do it…
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: As a rule, you should pick the right technology for the right subsystem and
    application.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，您应选择适合的技术来应对合适的子系统和应用程序。
- en: When aiming to achieve HA for a web application, the first step is to place
    a load balancer in front of the web servers. This enables scaling of the application
    while also offering some fault tolerance for these systems. However, attention
    should also be given to the data tier, which can be addressed by clustering the
    database or building a cluster capable of running the database, depending on the
    limitations of the database technology.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在旨在实现高可用性（HA）的Web应用程序中，第一步是将负载均衡器放置在Web服务器前面。这使得应用程序可以扩展，同时为这些系统提供一些容错能力。然而，还应关注数据层，可以通过对数据库进行集群化或根据数据库技术的限制构建一个能够运行数据库的集群来解决这一问题。
- en: If you are utilizing a technology such as Oracle Database, you have the option
    to establish a database-specific cluster known as **Oracle Real Application Clusters**
    (**Oracle RAC**). This cluster allows for both scalability and availability. With
    RAC, the database remains accessible for queries as long as one node is online.
    While other databases may utilize their own exclusive clustering technology (such
    as MySQL Cluster), you may opt to utilize generic cluster technologies such as
    Pacemaker for cluster management and Corosync for inter-cluster communications.
    This approach presents the advantage of enabling almost any technology to be made
    highly available in a Linux environment.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是如Oracle数据库这样的技术，您可以选择建立一个特定于数据库的集群，称为**Oracle Real Application Clusters**（**Oracle
    RAC**）。这个集群同时支持可扩展性和可用性。使用RAC时，只要一个节点在线，数据库就可以继续提供查询服务。虽然其他数据库可能会利用其独有的集群技术（例如MySQL
    Cluster），您也可以选择使用通用的集群技术，如Pacemaker进行集群管理，使用Corosync进行集群间的通信。这种方法的优点是可以在Linux环境下使几乎任何技术实现高可用性。
- en: You can achieve HA in storage by implementing filesystems across the entire
    cluster. Gluster allows you to mount a filesystem across multiple servers, while
    at the same time replicating the storage across servers. This provides both scalability
    and reliability at the filesystem level.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在整个集群上实现文件系统来实现存储的高可用性。Gluster允许您在多个服务器之间挂载文件系统，同时在服务器之间复制存储。这在文件系统级别提供了可扩展性和可靠性。
- en: Finally, the network is a common point of failure, and using network bonding
    technologies can enable both HA as well as some scaling abilities. This works
    by combining at least two network ports into a single virtual port.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，网络是常见的故障点，使用网络绑定技术可以实现高可用性以及一定的扩展能力。这通过将至少两个网络端口合并为一个虚拟端口来工作。
- en: Note
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The best HA architectures mix these approaches to cover the entire technology
    stack.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的高可用性架构将这些方法混合使用，覆盖整个技术栈。
- en: Load balancing a website
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网站负载均衡
- en: Nowadays, most applications are web-based, whether it’s a traditional web interface
    or a RESTful API. This first tier is typically set up for HA using a load balancer.
    A load balancer is a system that distributes incoming network traffic or workload
    across multiple servers or resources. Its main goal is to optimize resource utilization,
    improve performance, and ensure the reliability and availability of applications
    or services. When multiple servers are involved in serving a particular application
    or service, a load balancer acts as an intermediary between the client and the
    server pool. It receives incoming requests from clients and intelligently distributes
    them across the available servers based on various algorithms, such as round-robin,
    least connections, or weighted distribution.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The load balancer is responsible for ensuring the servers’ optimal health and
    performance by redirecting traffic from overloaded or problematic servers. This
    distribution of workloads helps to prevent any one server from getting overwhelmed,
    thus enhancing response time and overall system capacity and scalability.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: While a load balancer can help distribute the workloads, the actual server load
    is based on other factors, so do not expect all servers to have the same utilization
    of CPU, RAM, networking, and so on.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Load balancers not only distribute traffic but also offer advanced features,
    such as SSL termination, session persistence, caching, and content routing. They
    are extensively used in web applications, cloud-based services, and other environments
    that demand HA and scalability.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular load balancers is HAProxy.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy is a great open source load-balancer option. Standing for **High Availability
    Proxy**, **HAProxy** is widely used due to its excellent performance and ability
    to improve the availability and scalability of applications. Operating at the
    application layer (Layer 7) of the OSI model, this software is able to make routing
    decisions based on specific application-level information, such as HTTP headers
    and cookies. Compared to traditional network-level (Layer 4) load balancers, HAProxy
    allows for more advanced load-balancing and traffic-routing capabilities.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key features and capabilities of HAProxy include the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '**Load balancing**: With HAProxy, incoming traffic can be evenly distributed
    across multiple servers using various algorithms, such as round-robin, least connections,
    and source IP, among others.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High availability**: One of the beneficial features of HAProxy is its ability
    to support active-passive failover setups. In the event the active server becomes
    unavailable, a standby server will take over. Additionally, it also has the capability
    to monitor the health of servers and make automatic adjustments to the load-balancing
    pool by adding or removing servers.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proxying**: One of HAProxy’s primary functions is to act as a reverse proxy,
    which involves receiving client requests and directing them to the correct backend
    servers. Additionally, it can function as a forward proxy by intercepting client
    requests and directing them to external servers.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SSL/TLS termination**: With HAProxy, SSL/TLS encryption and decryption can
    be efficiently managed, taking the load off of the backend servers.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Session persistence**: HAProxy is capable of preserving session affinity
    by routing follow-up requests from a client to the same backend server, thus guaranteeing
    the proper operation of session-based applications.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Health checks and monitoring**: To guarantee the availability and optimal
    performance of backend servers, HAProxy conducts routine health checks. It has
    the ability to identify failed servers and promptly exclude them from the load-balancing
    pool.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logging and statistics**: With HAProxy, administrators can effectively monitor
    and analyze traffic patterns, performance metrics, and error conditions. Its detailed
    logging and statistics feature makes this possible.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HAProxy can be deployed on various operating systems and is often used in high-traffic
    web environments, cloud infrastructure, and containerized deployments. Its versatility
    and extensive feature set make it a powerful tool for managing and optimizing
    application traffic and open source-based load balancers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'For this recipe, we will put HAProxy on one system (as the load balancer) and
    then two identical web servers to balance traffic to:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – HAProxy example diagram](img/B18349_06_01.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – HAProxy example diagram
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started, we first need three servers. For this exercise, we will call
    them `lb1`, `web1`, and `web2`. They are identical systems, each with 8 GB RAM,
    4 vCPUs, and 100 GB of drive space. The filesystems have 50 GB in `/`, 5 GB in
    `/home`, and 8 GB in swap. The remaining disk space is unallocated. You will also
    need the IP address for each host. In this example, the following IP addresses
    were used:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '| **Host** | **IP** |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '| **Web1** | `192.168.56.200` |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '| **Web2** | `192.168.56.201` |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: '| **Lb1** | `192.168.56.202` |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: Table 6.3 – HAProxy IP addresses
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the server is built, patch it to the current software with the following
    command:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once the software is patched, reboot the systems.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Web servers
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For both web servers, we will install Apache, using the following commands
    as the root user:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We next need to enable port `80` to pass through the firewall, with the following
    command:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We then need to start the server and make it start on boot. This is done with
    the following command:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We now need to put in a basic home page for this server.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on your environment, you may need to edit the Apache config file
    in `/etc/httpd/conf/httpd.conf` to specify your `servername`. In the config file,
    it will be a single entry on a single line, like the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '**ServerName server.m57.local:80**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'This needs to go into `/var/www/html/index.html`. The following is an example
    file. Adjust the text as needed so each server is unique. This way, you can see
    what server is being hit:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, point your browser to the system to test it:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Simple website](img/B18349_06_02.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Simple website
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Next, repeat the process on the other web server. Once that system is up, we
    will set up the load balancer.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: If you are using `httpd` (TLS/SSL) on your server, don’t forget to enable `https`
    in your local firewalls as well.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Load balancer
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the single load-balancer system, we will need to install HAProxy. This
    is done using the `dnf` command as root:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We next need to open up port `80` in the firewall with the following command:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, we will need to edit the config file. The config file is located in `/etc/haproxy/haproxy.cfg`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'The config file has two main sections, `global` and `defaults`. There can only
    be a single `global` section in the config file. This is where TLS/SSL config
    data, the logging configuration, and the user and group settings go for the user
    running `haproxy`. By default, `haproxy` runs as the user haproxy with the group
    `haproxy`. For most use cases, the `global` section should not need to be changed.
    An example is seen in the following figure:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – HAProxy global settings](img/B18349_06_03.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – HAProxy global settings
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section, called the `defaults` section, is where you will make most
    of your edits. This is built using three subsections: `frontend`, `backend`, and
    `listen`.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The `frontend` section listens on all IP addresses and ports that are defined.
    This is what users will connect to. A HAProxy server can have multiple `frontend`
    sections, though each one needs a unique name and IP/port combination.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: The `backend` section defines the servers being load balanced to, defining the
    load-balancing method as well as the servers and ports where traffic is being
    sent to. A HAProxy server can have multiple `backend` sections, though each one
    needs a unique name.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: The `listen` section is used to define how you can monitor the load balancer,
    with the port, URI, and authentication information needed to monitor HAProxy.
    Normally, you will only have one `listen` section.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: For the same frontend, we will be listening on port `80` of the `lb1` system,
    called `www_app`, and will define this IP/port combination to use the `www_servers`
    backend.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: frontend
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `frontend` section of the HAProxy configuration file offers various options
    to manage incoming traffic behavior. These options can be used to control the
    traffic on a frontend. Here are some commonly used frontend options:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '`bind`: Defines the IP address and port on which the frontend will listen for
    incoming traffic. For example, `bind *:80` listens on all IP addresses on port
    `80`.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode`: Specifies the mode of the frontend, such as `http`, `tcp`, or `ssl`.
    For HTTP traffic, use `mode http`. If you want to load balance a generic TCP port,
    use `mode tcp`.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode`: 指定前端的模式，例如 `http`、`tcp` 或 `ssl`。对于 HTTP 流量，使用 `mode http`。如果要负载均衡一个通用的
    TCP 端口，请使用 `mode tcp`。'
- en: '`option`: Enables or disables specific options for the frontend. Some commonly
    used options are as follows:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option`: 启用或禁用前端的特定选项。一些常用选项如下：'
- en: '`option httplog`: Enables HTTP request/response logging'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option httplog`: 启用 HTTP 请求/响应日志记录'
- en: '`option dontlognull`: Prevents logging of requests with missing or empty user-agent
    strings'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option dontlognull`: 防止记录缺失或空白用户代理字符串的请求'
- en: '`option forwardfor`: Adds the client’s IP address to the HTTP request headers
    when using HTTP proxy mode'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option forwardfor`: 在使用 HTTP 代理模式时，将客户端的 IP 地址添加到 HTTP 请求头中'
- en: '`option http-server-close`: Forces the server connection to close after processing
    a request, rather than using keep-alive'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option http-server-close`: 强制服务器在处理请求后关闭连接，而不是使用持久连接（keep-alive）'
- en: '`timeout`: Configures various timeouts for the frontend:'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout`: 配置前端的各种超时时间：'
- en: '`timeout client`: Sets the maximum allowed time for the client to establish
    a connection and send data'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout client`: 设置客户端建立连接并发送数据的最大时间限制'
- en: '`timeout server`: Sets the maximum allowed time for the server to respond to
    a request'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout server`: 设置服务器响应请求的最大时间限制'
- en: '`timeout connect`: Sets the maximum time to wait for a connection to the backend
    server'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout connect`: 设置等待连接后端服务器的最大时间'
- en: '`acl`: Defines rules for matching specific conditions. ACLs are used in conjunction
    with backend configurations to control traffic routing based on various criteria.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acl`: 定义匹配特定条件的规则。ACL 与后端配置配合使用，根据不同标准控制流量路由。'
- en: '`use_backend`: Specifies which backend to use for handling traffic that matches
    specific ACL conditions. It allows you to direct traffic to different backend
    servers based on certain conditions.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_backend`: 指定用于处理符合特定 ACL 条件的流量的后端。它允许你根据特定条件将流量引导到不同的后端服务器。'
- en: '`default_backend`: Defines the default backend to use if no ACL conditions
    match the incoming traffic.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`default_backend`: 定义当没有 ACL 条件匹配传入流量时使用的默认后端。'
- en: '`redirect`: Performs a URL redirection for specific conditions. For example,
    you can use the [https://example.com](https://example.com) redirect location to
    redirect HTTP traffic to HTTPS.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`redirect`: 在特定条件下执行 URL 重定向。例如，可以使用[https://example.com](https://example.com)
    重定向地址将 HTTP 流量重定向到 HTTPS。'
- en: '`http-request` and `http-response`: These are used to add custom HTTP request/response
    headers or to perform specific actions based on HTTP request/response data.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`http-request` 和 `http-response`: 用于添加自定义 HTTP 请求/响应头，或根据 HTTP 请求/响应数据执行特定操作。'
- en: '`capture`: Captures parts of the request or response headers and saves them
    into variables.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`capture`: 捕获请求或响应头的部分内容并将其保存到变量中。'
- en: 'For the sample frontend, we will define the frontend as `www_app` binding to
    all IPs on the load-balancer system on port `80`. This looks like the following
    figure:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于示例前端，我们将前端定义为 `www_app`，绑定到负载均衡系统上所有 IP 的 `80` 端口。如下图所示：
- en: '![Figure 6.4 – Example frontend](img/B18349_06_04.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 示例前端](img/B18349_06_04.jpg)'
- en: Figure 6.4 – Example frontend
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 示例前端
- en: backend
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: backend
- en: 'When using HAProxy, the backend options play a crucial role in configuring
    the behavior of backend servers and the routing of traffic toward them. These
    options are specifically designated within the `backend` section of the HAProxy
    configuration file. Here are some frequently utilized backend options:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 HAProxy 时，后端选项在配置后端服务器行为和流量路由方面起着至关重要的作用。这些选项专门在 HAProxy 配置文件的 `backend`
    部分中指定。以下是一些常用的后端选项：
- en: '`mode`: Specifies the mode of the backend, such as `http`, `tcp`, or `ssl`.
    For HTTP traffic, use `http` mode.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode`: 指定后端的模式，例如 `http`、`tcp` 或 `ssl`。对于 HTTP 流量，请使用 `http` 模式。'
- en: '`balance`: Defines the load-balancing algorithm to distribute traffic across
    backend servers. Common options include the following:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`balance`: 定义负载均衡算法，用于将流量分配到后端服务器。常见的选项包括：'
- en: '`balance roundrobin`: Requests are distributed in a round-robin fashion to
    each server in sequence'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`balance roundrobin`: 按照轮询方式依次将请求分配给每个服务器'
- en: '`balance leastconn`: Traffic is sent to the server with the lowest number of
    active connections'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`balance leastconn`: 将流量发送到活动连接数最少的服务器'
- en: '`balance source`: Based on a hash of the client’s IP address, traffic is directed
    to a specific server consistently'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`server`: Defines the backend servers and their addresses, ports, and optional
    parameters.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout`: Configures various timeouts for the backend:'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout server`: Sets the maximum allowed time for the server to respond to
    a request'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout tunnel`: Configures the maximum time allowed to establish a tunnel
    (used in TCP mode)'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http-request` and `http-response`: Similar to frontend options, these are
    used to add custom HTTP request/response headers or perform specific actions based
    on HTTP request/response data.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cookie`: Configures sticky session persistence using cookies. It allows the
    backend server to be selected based on a specific cookie value from the client.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`check`: Enables health checks for backend servers to determine their availability.
    If a server fails the health check, HAProxy will stop sending traffic to it until
    it recovers.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`option`: Enables or disables specific options for the backend. Some commonly
    used options include the following:'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`option httpchk`: Enables HTTP health checks instead of TCP health checks'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`option redispatch`: Allows HAProxy to reselect a server if the connection
    to the selected server fails'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`errorfile`: Specifies a file to use as a custom error page for backend server
    errors.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the sample backend, it is defined as `www_servers` and will use `roundrobin`
    load balancing against the `web1` and `web2` servers:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – HAProxy sample backend](img/B18349_06_05.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – HAProxy sample backend
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: It is highly recommended to always use the `check` option for your servers.
    If you do not run the checks, the system will still send traffic to the server!
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: listen
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In HAProxy, the `listen` section is used to define a frontend and backend configuration
    together in one block, making it a convenient way to combine both. The `listen`
    section allows you to define options specific to the listening socket and how
    the incoming traffic is handled. The following are some commonly used options
    in the `listen` section:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '`bind`: Defines the IP address and port on which HAProxy will listen for incoming
    traffic. For example, `bind *:80` listens on all IP addresses on port `80`.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats`: Enables the HAProxy statistics page for monitoring and managing HAProxy.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats enable`: Enables statistics monitoring for HAProxy.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats uri`: Specifies the URI path for accessing the statistics page. For
    example, `stats uri /haproxy_stats` sets the statistics page to be accessible
    at `http://your-haproxy-ip/haproxy_stats`.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats realm`: Sets the realm (authentication realm) for HTTP basic authentication
    when accessing the statistics page. This adds a layer of security to prevent unauthorized
    access.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats auth`: Configures the username and password for HTTP basic authentication
    when accessing the statistics page. The format is `stats` `auth username:password`.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats hide-version`: Hides the HAProxy version number from the statistics
    page to enhance security.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats show-node`: Displays the server node names on the statistics page. This
    is useful when using dynamic server templates.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats refresh`: Sets the interval (in milliseconds) for automatic refresh
    of the statistics page. For example, `stats refresh 10s` refreshes the page every
    10 seconds.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats admin`: Specifies the IP address and port for allowing administrative
    access to HAProxy statistics. It allows remote management of HAProxy using the
    statistics page. For example, stats admin if `localhost` permits access only from
    the local machine.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stats maxconn`: Limits the number of connections allowed to the statistics
    page. It helps to prevent overload and potential denial-of-service attacks.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`errorfile`: Specifies a file to use as a custom error page for frontend errors.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the sample `listen` section, we will define it as metrics, allowing admin
    access from `192.168.56.1`. The user will use the username as `admin` and the
    password `passw0rd` to log in. This is seen in the following figure:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – HAProxy listen sample](img/B18349_06_06.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – HAProxy listen sample
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Since the status page is running on port `8080`, don’t forget to add the port
    to the firewall and reload the firewall. This can be done with the following command;
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How it works…
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have our two web servers, and the load balancer configured, we
    need to start the load balancer. This is done using `systemctl`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following to start HAProxy:'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Use the following to check the status:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you edit the config file, do not forget to reload HAProxy with the following
    command:'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, point your browser to the load balancer IP. You will get the web server
    page. This is seen in the following figure:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Working HAProxy](img/B18349_06_07.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Working HAProxy
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Since the rule is `roundrobin`, and we configured the timeout at one minute,
    wait a minute and then reload the page. You will see a new server.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Working load balancing](img/B18349_06_08.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Working load balancing
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'As an admin, you will also want to check on the health of your resources. Point
    your browser to the stats URL, and enter the username and password configured.
    This will show the stats page. In the case of this example, the URL is [http://lb1.m57.local:8080/stats](http://lb1.m57.local:8080/stats).
    You will see a sample in the following figure:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 6.9 – HAProxy status page](img/B18349_06_09.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – HAProxy status page
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: On the sample page, you will see that `web1` is offline. You can also see how
    much traffic each frontend and backend rule has processed, and to what servers.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Making HAProxy highly available with Keepalived
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous recipe, we used HAProxy to give our web servers some redundancy.
    The challenge with that solution is we now have a failure point in the load balancer
    itself. When architecting for HA, you need to cover all points of failure to make
    sure there is redundancy and that there is no SPOF. In this recipe, we will use
    Keepalived to add some HA to our configuration. Keepalived is a software application
    that is open source and designed for Linux-based systems. Its main function is
    to manage network load balancing and failover, ensuring the HA of web services.
    Keepalived is often used alongside HAProxy. The software primarily uses the **Virtual
    Router Redundancy Protocol** (**VRRP**) to achieve fault tolerance and evenly
    distribute the load. Keepalived uses the following features to provide its redundancy:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '**High availability**: With Keepalived, you can establish a cluster of backup
    servers that utilize a shared **Virtual IP** (**VIP**) address. This setup ensures
    that even if the primary server experiences a failure, a secondary server will
    automatically take over and handle incoming traffic, resulting in minimal downtime.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VRRP**: VRRP is a commonly used protocol that enables automatic router failover
    in IP networks. Keepalived utilizes VRRP to keep a VIP address operational, which
    can be assigned to any node within the cluster as needed.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Health checking**: The monitoring system of Keepalived regularly checks the
    health of active servers. If a server becomes unresponsive, Keepalived will remove
    it from the pool and redirect traffic to the healthy servers.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Notification mechanisms**: With Keepalived, it’s possible to set up notifications
    for failover events or when certain thresholds are exceeded. These notifications
    are useful for keeping an eye on the overall health of the cluster.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a cluster, Keepalived designates one node as the master and the others as
    backups. The master node handles incoming traffic and responds to ARP requests
    for the VIP address, while the backups act as standby routers and monitor the
    master’s status. The nodes communicate using the VRRP protocol, with the master
    periodically sending VRRP advertisements to show its functioning. Often, a VIP
    is used to allow a single IP address for end user access. This normal operation
    is seen in the following figure, where we have Keepalived managing two HAProxy
    systems.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Keepalived normal operations](img/B18349_06_10.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Keepalived normal operations
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: If the backups stop receiving advertisements or detect any issues with the master,
    one backup node will take over as the new master. Keepalived checks the servers’
    health using mechanisms such as ICMP (ping) checks or Layer 4 checks (e.g., checking
    whether a specific port is open) and removes failed servers from the pool. The
    VRRP priority is adjusted to ensure a healthy backup server takes over. This is
    seen in the following figure, where the VIP that users connect to has migrated
    over to the second node, and that node is now using HAProxy to manage the workload.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Keepalived failed node](img/B18349_06_11.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Keepalived failed node
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe expands upon the previous one, adding a second load-balancer system
    and a VIP. You will need to build the second load balancer and acquire an additional
    IP address for the VIP. Your IPs should be similar to the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '| **Host** | **IP** |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| **web1** | `192.168.56.200` |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| **web2** | `192.168.56.201` |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| **lb1** | `192.168.56.202` |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| **lb2** | `192.168.56.203` |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| **vip** | `192.168.56.204` |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: Table 6.4 – Keepalived IP addresses
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you start configuring Keepalived, you need to configure HAProxy on the
    second server. You can easily install HAProxy, open up the firewall ports, and
    copy over the config file from the existing system.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: You can test this by simply pointing your browser to the second load balancer
    and seeing the app server.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will start to configure Keepalived.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'For each load balancer, you will need to install Keepalived as the root user:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Next, we will need to edit the Keepalived config file. This is found in `/etc/keepalived/keepalived.conf`.
    There are two major sections to edit, `global_defs` and `vrpp_instance`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '`global_defs` is the global definition used by Keepalived. These settings are
    used by all `vrrp_instance` types configured in the system.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several parameters that you will need to update:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '`notification_email`: This is a list of email addresses that will be emailed
    when there is an event'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`notification_email_from_user`: This is the sending email address'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`smtp_server`: This is the SMTP relay server'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`router_id`: This is a unique name for this Keepalived cluster'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the sample, this section looks like the following:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Keepalived globals](img/B18349_06_12.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Keepalived globals
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: The next section is `vrrp_instance`. You can have multiple `vrrp_instance` types
    in the cluster, each supporting different VIPs.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'For `vrrp_instance`, you need to give each one a unique name. Additionally,
    there are several parameters that will need to be updated:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '`state`: The state of the instance, usually master for the primary node and
    backup for the secondary node.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`interface`: The Ethernet interface used for this host in the cluster.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virtual_router_id`: A unique number for this instance. No other instances
    should use the same ID.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`authentication`: This section defines how members are authenticated:'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`auth_type`: Normally sent to PASS, to allow nodes to authenticate as members
    of this instance. There is a second support type called `auth_pass`: The password
    for the instance.'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virtual_ipaddress`: A list of VIPs managed by this instance.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our example, the section will look as follows;
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Keepalived vrrp_instance](img/B18349_06_13.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Keepalived vrrp_instance
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Once the config file is built, copy it over to the second load balancer. Do
    not forget to change the state to `BACKUP` on the second system, and also update
    the interface if it is different on that system.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, start Keepalived on both nodes with the following command as the root
    user:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can now point your browser to the VIP! This is seen in the following example.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Keepalived VIP in use](img/B18349_06_14.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Keepalived VIP in use
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the status by looking at the journal entries for the daemon,
    using the following command:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This will show all the activity of the daemon. You should see `Sending gratuitous
    ARP` messages, as this is the system checking the health. You will also see messages
    about the state, such as `Entering MASTER STATE` or `Entering BACKUP STATE`, as
    the system switches between `MASTER` and `BACKUP`.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: HA clustering for all with Corosync and Pacemaker
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipes, we addressed HA by distributing traffic between two
    active application servers. However, this method is only effective for stateless
    applications where the server or browser doesn’t contain specific user or session
    data. For applications that are not stateless or run on a complex server, a different
    approach to HA is necessary. The solution is to start and stop the application
    components on different servers, using the combination of Pacemaker and Corosync.
    These two open source software projects work together to provide HA clustering
    for Linux-based systems. They coordinate and manage multiple nodes in a cluster,
    ensuring that critical services remain available even during hardware or software
    failures.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Corosync serves as the communication layer for the HA cluster stack, allowing
    for dependable communication between nodes. It utilizes a membership and quorum
    system to monitor the cluster’s active nodes and guarantee that only one node
    operates as the primary (or master) at a given time. The messaging layer is essential
    for sharing data regarding the cluster’s state, node status, and resource conditions.
    Corosync plays a vital role in the cluster’s functionality, providing key features
    such as the following:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster communication**: Corosync enables nodes to exchange messages reliably
    and efficiently, allowing them to coordinate and synchronize their actions.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Membership and quorum**: Corosync is a tool that keeps track of active nodes
    in a cluster and uses a quorum algorithm to ensure that there are enough nodes
    available to make decisions. This helps avoid split-brain scenarios and makes
    sure that only one node is active. It’s crucial to avoid split-brain clusters
    because they can cause data inconsistencies, corruption, and service disruptions.
    A split-brain scenario occurs when nodes in a cluster lose communication with
    each other. As a result, each node thinks it’s the only active one in the cluster.
    This can happen because of network issues, communication failures, or misconfigurations.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: When there is a split-brain scenario, several nodes within the cluster may begin
    running services or using shared resources on their own, thinking that they are
    the only active node. This can cause conflicts and data inconsistencies since
    each node operates independently without any coordination. When possible, use
    an odd number of nodes in a cluster, or enable some protection using quorum.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'Pacemaker is a cluster resource manager that utilizes Corosync’s messaging
    and membership features to manage cluster resources and handle resource failover.
    It determines which node in the cluster should run specific services (resources)
    based on established policies and constraints. Pacemaker brings the following
    features to the cluster:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource management**: With Pacemaker, administrators can set up resources
    that require strong availability, such as IP addresses, services, databases, and
    applications'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource monitoring**: Pacemaker continuously monitors the status of resources
    and nodes to detect failures or changes in the cluster'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource failover**: If a node fails or there are resource problems, Pacemaker
    will begin a failover process, transferring resources to functioning nodes to
    guarantee uninterrupted availability'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource constraints**: Administrators can set constraints and rules for
    resource placement and failover, defining which nodes are preferred or prohibited
    for specific resources'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Colocation and order constraints**: Pacemaker allows defining relationships
    between resources, specifying which resources must run together on the same node
    or in a specific order'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster management**: Pacemaker provides various command-line utilities and
    graphical interfaces (such as Hawk) for managing and configuring the cluster'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, you will need two VMs, each with at least two vCPUs, 8 GB
    of RAM, and 50 GB of disk space. You should have Oracle Linux 8 installed, and
    also a third IP address for a floating VIP to be managed by the cluster. Both
    of the web servers will be patched to the latest software. For this example, the
    following IPs will be used:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '| **Host** | **IP** |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| **Web1** | `192.168.56.200` |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| **Web2** | `192.168.56.201` |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| **vip** | `192.168.56.204` |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: Table 6.5 – HA cluster IPs
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Before we start with the cluster, you will also need to set up an `httpd` (Apache
    2.4) server on each host. This is similar to other hosts set up in other recipes.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'First, on both servers, as root, install the Apache web server:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We do need to enable the status page for Apache. This is one way the resource
    will be checked. To do this, copy the following lines into `/etc/httpd/conf.d/status.conf`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We also need a simple web page. For testing purposes, put the following into
    `/var/www/html/index.html` on both servers.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: When setting up an application such as a web server, putting your content directory
    (such as `/var/www/html`) on a Gluster filesystem makes it easier to manage updating
    your content. This also works for other data that the application uses, such as
    temporary state data.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, on both servers, add port `80` to the local firewall with the following
    command:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, for testing purposes, manually start the server on both nodes. Do not
    enable the service to automatically start. This will be done later when Pacemaker
    is configured:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You should now see a basic page on both servers, as shown in the following
    screenshot:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – httpd server test](img/B18349_06_15.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – httpd server test
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also test the `server-status` page using the `wget` command:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Sample output of a success is seen in the following screenshot.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Successful server-status](img/B18349_06_16.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Successful server-status
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: You are now ready to install and configure Pacemaker and Corosync.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have installed the Apache `httpd` server that we will cluster,
    let’s start by installing the software. First, we need to enable the `addons`
    repo. This is done with the following commands on both servers as root.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'First, on both nodes, enable the repo with the following commands as root:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, you will install the software:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once the installation of these packages is complete, a new user named `hacluster`
    will be added to your system. Please note that remote login will be disabled for
    this user after installation. To carry out tasks such as synchronizing the configuration
    or starting services on other nodes, it is necessary to set the same password
    for the `hacluster` user on both nodes. We can use the `passwd` command to set
    the password:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we need to enable the `pcs` service and start it:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we need to open up the firewall for the cluster port. This is done with
    the following command:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now we’re done with both nodes for a bit. The next few commands can be done
    on either of the nodes, but note, you still should be root.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to add both nodes to the cluster:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'If your nodes are not resolvable in DNS or the `/etc/hosts` file, you can optionally
    add `addr=$IPADDR` for each host after the hostname. But it’s highly recommended
    to make sure all hosts are resolvable. This option, if used, would look as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '**pcs host auth web1 addr=192.168.56.200 node2 addr=192.168.56.201 -****u hacluster**'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will create the cluster:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, we can start the cluster:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To verify that the cluster is running, we can check with the `pcs` command:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'A healthy new cluster should return a similar output as the following example:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Cluster status](img/B18349_06_17.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Cluster status
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'In a cluster consisting of only two nodes, the quorum operates differently
    than in clusters with more nodes. In such a cluster, the quorum value is set to
    `1` to ensure that the primary node is always considered in quorum. If both nodes
    go offline due to a network outage, they compete to fence each other, and the
    first to succeed wins the quorum. To increase the chances of a preferred node
    winning the quorum, the fencing agent can be configured to give it priority. This
    is done with the following command:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The last step is to disable **STONITH**, which stands for **Shoot The Other
    Node In The Head**. This is an advanced fencing tool that requires configuration
    specific to your environment. If you want to experiment with this technology then
    check out the official Oracle docs here – [https://docs.oracle.com/en/operating-systems/oracle-linux/8/availability/availability-ConfiguringFencingstonith.html#ol-pacemaker-stonith](https://docs.oracle.com/en/operating-systems/oracle-linux/8/availability/availability-ConfiguringFencingstonith.html#ol-pacemaker-stonith):'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: To set up a cluster, we’ll need to create resources. A resource agent name has
    two or three fields separated by a colon. The first field is the resource class
    that indicates the standard followed by the resource agent and helps Pacemaker
    locate the script. For example, the IPaddr2 resource agent follows the **Open
    Cluster Framework** (**OCF**) standard. The second field varies based on the standard
    used, and OCF resources use it for the OCF namespace. The third field denotes
    the name of the resource agent.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Meta-attributes and instance attributes are available for resources. Meta-attributes
    are not resource-type dependent, while instance attributes are specific to each
    resource agent.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: In a cluster, resource operations refer to the actions that can be taken on
    a specific resource, such as starting, stopping, or monitoring it. These operations
    are identified by the `op` keyword. To ensure the resource remains healthy, we
    will add a monitor operation with a 15-second interval. The criteria for determining
    whether the resource is healthy depends on the resource agent being used. This
    is also why we enabled the `server-status` page on the `httpd` server, as the
    `httpd` agent uses that page to help determine the health of the system.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s add the VIP address:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next up, we will add the `httpd` server:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now that we have two resources, we also will need to tie them together as a
    group. With most applications, multiple resources need to be on the same physical
    server at the same time. This could be IP addresses, Tomcat servers, `httpd` servers,
    and so on. We are going to call this group `WebApp` and add both the VIP and `https`
    servers to it. Each resource is added individually, so two commands will need
    to be run:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we will use the `pcs status` command to check the configuration:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – pcs status](img/B18349_06_18.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – pcs status
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: We can now see the cluster, with its resource group, `WebApp`, with both the
    server VIPs.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that everything is configured, let’s start up the resources and manage
    them. We can first start the entire cluster. This will online all nodes in the
    cluster:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can also set up the cluster to start on boot with the following commands:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: You will need to run both of these commands as root on both nodes.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Next up, let’s look at a few useful commands.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes a resource gets broken; maybe it was a bad config file, or maybe
    you started a resource outside of the cluster control, confusing the cluster.
    Once you fix the issues, you will likely need to refresh the resource. This will
    tell the cluster to forget about the failure and restart the service clear of
    any errors:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You also can check the details of a resource, using the `config` option. This
    is helpful if you forget how the resource was configured. An example is seen in
    the following figure:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Resource configuration](img/B18349_06_19.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – Resource configuration
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s move `WebApp` to server `web2`:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'When you run `move`, you can also monitor the move by checking the constraints.
    This is a little cleaner than using the `pcs` `status` command:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Cluster constraints](img/B18349_06_20.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Cluster constraints
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: The power of the Pacemaker/Corosync technology is its flexibility. You can cluster
    just about anything with it, making it a powerful tool for the sysadmin.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Sharing a filesystem across multiple machines – cluster or distribute?
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you start using technologies such as load balancers and clustering software,
    you often end up in a situation where you need the same files on multiple servers.
    While you could simply copy the files, what if you could mount the files on each
    of the servers, sharing the filesystem across the systems without the SPOF that
    an NFS server introduces? One of the easiest ways to do this is to use Gluster.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '**Gluster**, also known as **GlusterFS**, is an open source distributed filesystem
    that provides scalable and flexible storage for large volumes of data. Initially
    developed by Gluster Inc., it is now maintained by the open source community.
    Gluster uses a distributed architecture to create a single and unified filesystem
    that can span across multiple servers and storage devices. This approach allows
    you to aggregate the storage capacity of multiple servers and present it as a
    single, well-structured filesystem to users and applications. It has a wide range
    of applications, such as data storage, backup, and content delivery.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'Key features and concepts of Gluster include the following:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: Adding more storage servers to the cluster allows Gluster
    to easily accommodate growing data storage needs while scaling horizontally.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Redundancy**: Gluster ensures data availability by replicating data across
    multiple nodes for redundancy and fault tolerance.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: Gluster supports various storage options, including local
    disks, NAS, and cloud storage. It can be customized to fit specific use cases
    and technologies.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filesystem abstraction**: It provides users and applications with a standard
    filesystem interface, making integration into existing systems relatively easy.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data distribution**: Data is distributed across the cluster in a way that
    improves both performance and reliability. Data can be distributed evenly or based
    on specific criteria.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic healing**: Gluster has a self-healing feature that automatically
    detects and repairs data inconsistencies or corrupted files.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gluster is often used in environments where large-scale, distributed storage
    is required, such as web servers, cloud computing, big data analytics, and media
    streaming services. It provides a cost-effective and flexible solution for managing
    data across a network of servers and storage devices.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, you will need two Oracle Linux 8 systems, each with access
    to YUM repos. For this exercise, we will call them `gluster1` and `gluster2`.
    They are identical systems, each with 8 GB RAM, 4 vCPUs, and 100 GB of drive space.
    The filesystems have 50 GB in `/`, 5 GB in `/home`, and 8 GB in swap. The remaining
    disk space is unallocated. Additionally, for this example, each node will have
    a 100 GB LUN used for storing Gluster data.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Having at least three nodes in a cluster is highly recommended to avoid split-brain
    clusters. Although a two-node cluster is possible, it poses a risk of corrupt
    data if the system ever splits its brain. Split-brain clusters are undesirable
    in distributed computing environments because they can result in data inconsistency,
    corruption, and operational issues. Split brain occurs when the nodes in a cluster
    lose connectivity or communication with each other, leading to the cluster’s division
    into multiple isolated nodes. Each node thinks it is the active or primary cluster,
    resulting in the potential for conflicts and data discrepancies.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'On each server, you will need to perform the following prep work:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Create an XFS filesystem on the 100 GB LUN. This space will be used to store
    the Gluster data, known as **bricks**. In the context of Gluster, a *brick* refers
    to a basic storage unit within the storage cluster. A cluster is made up of multiple
    bricks, which are essentially directories on storage servers or devices where
    data is stored. Each brick represents a portion of the overall storage capacity
    of the cluster.
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since we will be using Gluster to manage the storage, we will not be using
    LVM on the filesystem. On these systems, `/dev/sdb` is the 100 GB LUN. The following
    commands are used to create and mount the filesystem:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![Figure 6.21 – Bricks mounted](img/B18349_06_21.jpg)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
- en: Figure 6.21 – Bricks mounted
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to make sure that all the nodes are in the `/etc/hosts` file.
    In this example, `gluster1`, `gluster2`, and `gluster3` are in the file, using
    both the short name and the **Fully Qualified Doman Name** (**FQDN**). This is
    seen in the following code snippet:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: dnf -y install oracle-gluster-release-el8
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: dnf -y config-manager --enable ol8_gluster_appstream ol8_baseos_latest ol8_appstream
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: dnf -y module enable glusterfs
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: dnf -y install @glusterfs/server
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'systemctl status glusterd command. Verify that the service is active and running,
    as seen in the following example:'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![Figure 6.22 – Gluster daemon is running](img/B18349_06_22.jpg)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
- en: Figure 6.22 – Gluster daemon is running
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s configure the firewall to allow the `glusterfs` port with the following
    commands:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Additionally, to improve security, let’s create a self-signed key, to encrypt
    the communication between the nodes:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we are using a self-signed certificate. In a secure production
    environment, you will want to consider using a commercially signed certificate.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: We will use these files later to encrypt the communication.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that all the prep work has been completed on each of the nodes, we will
    create the trusted storage pools and encrypt the communications. This will have
    everything ready to create volumes, where data is stored and shared.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'A trusted storage pool in Gluster pertains to a setup where a cluster of Gluster
    servers, referred to as storage nodes or peers, have established trust among themselves
    to work together within a storage cluster. This trust is established through a
    trusted storage pool configuration that typically involves the following steps:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '**Authentication**: Various methods, such as SSH keys, certificates, or shared
    secrets, can be used to authenticate nodes in the trusted storage pool. This ensures
    that only authorized servers are part of the storage cluster.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authorization**: After nodes are authenticated, they authorize each other
    to access and manipulate specific data within the Gluster storage cluster. The
    authorization settings determine which nodes have read and write access to particular
    volumes or bricks within the cluster.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Communication**: Members of the trusted storage pool communicate over a secure
    network to replicate data, synchronize metadata, and perform other cluster-related
    operations, ensuring that the storage cluster functions cohesively.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data integrity**: Trusted storage pools ensure data integrity and redundancy
    via distributed replication across multiple nodes.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: It is possible to add more storage nodes to the trusted pool,
    which enhances storage capacity and performance. The trusted nature of the pool
    makes it easy for new nodes to join the cluster and contribute to its resources.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Gluster, a trusted storage pool is a crucial element as it lays the foundation
    for the fault-tolerant and distributed nature of the filesystem. It guarantees
    that all nodes within the cluster can work seamlessly and securely in collaboration
    with each other. The following steps will walk you through how to create a GlusterFS
    on two hosts.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the pool, we need to probe the other nodes in the cluster. In this
    example, we will probe from `gluster1` to `gluster2` using the `gluster peer probe`
    `gluster2` command:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[root@gluster2 etc]#  gluster pool list'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: UUID                                 Hostname        State
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b13801f3-dcbd-487b-b3f3-2e95afa8b632                         gluster1        Connected
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'gluster1 and localhost connected as this was run on gluster2. If you run the
    same command from gluster1, you will see gluster2 as the remote host:'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Now that we have the cluster, let’s create a replicated volume. This volume
    will re-write the bricks across the cluster, enabling protection against failed
    storage or a failed node.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following command will create the volume:'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'df command:'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![Figure 6.23 – data1 mounted on /mnt](img/B18349_06_23.jpg)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
- en: Figure 6.23 – data1 mounted on /mnt
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to mount this on other nodes, you will need to repeat the command
    on each node, updating the node name as needed. The following example shows mounting
    on `gluster1`:'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, we need to enable the encryption. This is done by touching the secure-access
    file on each node using the following command:'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: gluster volume set data1 client.ssl on
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'glusterd:'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Gluster communication is now encrypted for this volume.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-443
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is more you can do with Gluster. First, volumes have multiple options
    when you create them, each offering options for replication and distribution:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed**: When using distributed volumes, files are randomly distributed
    across the bricks in the volume. This type of volume is useful when the need is
    to scale storage, and redundancy is not necessary or is already provided by other
    hardware or software layers. However, it is important to note that disk or server
    failures can result in significant data loss, as the data is spread randomly across
    the bricks in the volume. An example command to build a distributed volume is
    the following:'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '**Replicated**: Files are copied across bricks for HA in replicated volumes.
    An example command to build a replicated volume is the following:'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '**Distributed replicated**: Distributed files across replicated bricks in the
    volume for improved read performance, HA, and reliability. When creating a distributed
    replicated volume, the number of nodes should be a multiple of the number of bricks.
    An example command to build a distributed replicated volume is the following:'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '**Dispersed**: This volume type utilizes erasure codes to efficiently protect
    against disk or server failures. It works by striping the encoded data of files
    across multiple bricks in the volume while adding redundancy to ensure reliability.
    Dispersed volumes allow for customizable reliability levels with minimal space
    waste. A dispersed volume must have at least three bricks. An example command
    to build a dispersed volume is the following:'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '**Distributed dispersed**: Distributes data across dispersed bricks, providing
    the same benefits of distributed replicated volumes but using dispersed storage.
    A dispersed volume must have at least six bricks. An example command to build
    a distributed dispersed volume is the following:'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'When adding bricks to any volume, you can put more than one brick on a Gluster
    node. Simply define the additional brick in the command. In this example, a distributed
    dispersed volume is created, by putting two bricks on each node:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Volumes can be stopped with the `gluster stop volume volumename` command. An
    example to stop the `data1` volume is the following:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'You can also add bricks to a volume to grow it. This can be done after a new
    node is added to the cluster. In the following example, `gluster3` was added to
    the cluster with the `gluster node probe gluster3` command first. Then, `data1`
    was grown with the following command:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Note
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: When adding bricks to a volume, make sure you add the required number of bricks.
    Volume types such as distributed replicated volumes will need more than a single
    brick added.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also check the status of all volumes with the following command:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'An example is seen in the following screenshot:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.24 – volume status](img/B18349_06_24.jpg)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
- en: Figure 6.24 – volume status
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the summary for a single volume by adding the volume name to the
    command. You can also see more details by adding the `detail` option. These can
    be combined, as seen in the following screenshot:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Volume details](img/B18349_06_25.jpg)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
- en: Figure 6.25 – Volume details
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to see performance information about a volume, the `top` option
    can be used. This will show what bricks are being used for read/write activity
    as well as I/O throughput to each brick. The basic command is `gluster volume
    top volume_name option`, with `volume_name` being the name of the volume and the
    options being as follows:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '`read`: This shows the highest read calls for each brick, as well as the counts.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`write`: This shows the highest write calls for each brick, as well as the
    counts.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`open`: This shows what bricks have open file descriptors.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`opendir`: This shows what bricks have open calls on each directory, as well
    as the counts.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`read-perf`: This shows read-performance throughput by brick. Run using the
    options `bs` (for block size) `1024` and `count 1024`.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`write-perf`: This shows read-performance throughput by brick. Run using the
    options `bs` (for block size) `1024` and `count 1024`.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Several examples are seen in the following figure:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.26 – volume top examples](img/B18349_06_26.jpg)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
- en: Figure 6.26 – volume top examples
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: Volumes can also be deleted. This is done with the `gluster volume delete volume_name`
    command, where `volume_name` is the volume being deleted. As a note, when deleting
    volumes, don’t forget to use the `rm` command to delete the bricks from storage.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: Generating, configuring, and monitoring Ethernet traffic over bond
  id: totrans-482
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When using bare-metal servers as dedicated hosts or Linux systems that host
    virtual machines using the KVM hypervisor, the network can be a weak point. Fortunately,
    this issue can be resolved by implementing Ethernet bonding, also known as network
    bonding, or **Network Interface Card** (**NIC**) bonding. It is a technology in
    Linux that allows you to combine multiple NICs into a single logical interface.
    This logical interface, known as a bond or bonded interface, provides increased
    network bandwidth, fault tolerance, and load balancing. These are summarized as
    follows:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '**Load balancing**: Bonding distributes network traffic across multiple NICs,
    increasing bandwidth. Various algorithms, such as round-robin, active-backup,
    and XOR, can be used depending on specific requirements.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: In the event of an NIC or network link failure, Ethernet
    bonding can automatically switch traffic to another active NIC. This provides
    redundancy and fault tolerance, ensuring network connectivity remains available
    even if one NIC becomes unavailable.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Link aggregation**: Bonding can be used to create **link aggregation groups**
    (**LAGs**) or NIC teams, which enhance bandwidth and redundancy in HA setups.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will configure bonding, and then show some common tools that
    will allow you to both monitor and generate Ethernet traffic over the bond.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there are a few technologies you need to be familiar with.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: MAC
  id: totrans-489
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **Media Access Control** (**MAC**) address is a hardware identifier assigned
    to network interfaces such as Ethernet cards and Wi-Fi adapters for communication
    on a local network. It is hardcoded into the network hardware during manufacturing
    and is used at the data link layer (Layer 2) of the OSI model. One of the most
    important features of MAC addresses is that they must be unique. Each MAC address
    is meant to be globally unique, and manufacturers bear the responsibility of ensuring
    that no two network interfaces have the same MAC address, though this can be a
    challenging task to accomplish in practice, especially in virtualized environments.
    This can be an issue with networking, as duplicate MAC addresses on any network
    will cause issues. Additionally, many of the bonding modes rely on MAC addresses
    to load balance traffic.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: Bonding modes
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Bonding modes refer to the various strategies or algorithms used to determine
    how network traffic is distributed across the physical network interfaces that
    have been aggregated into a bonded interface using the Linux bonding driver. These
    modes control the load-balancing and failover behavior of the bonded interface.
    The choice of bonding mode depends on your specific network requirements and goals.
    Here are some common Linux bonding modes:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '`balance-rr`: In this mode, outgoing network traffic is distributed evenly
    across the available network interfaces in a round-robin fashion. It’s a simple
    load-balancing mode that provides improved outbound traffic performance but does
    not consider the state of the interfaces, which can lead to uneven inbound traffic
    distribution. Occasionally, this mode does not work well with some switching systems.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`active-backup`: A commonly used mode, which is often referred to as failover
    mode, this mode has a primary interface, while the others remain on standby. If
    the primary interface fails, the next available interface is automatically activated
    to ensure continuity. This mode provides redundancy and is one of the easiest
    modes to get working in any environment.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`balance-xor`: This mode utilizes a straightforward XOR operation to maintain
    a balance between the transmission and reception of data. The process involves
    distributing traffic based on the MAC addresses of the source and destination.
    This guarantees that packets between the same endpoints will always take the same
    path. The primary purpose of this mode is to ensure fault tolerance. Occasionally,
    this mode does not work well with some switching systems.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`balance-tlb`: When operating in this mode, the outgoing traffic is distributed
    among all available interfaces based on their current load. However, incoming
    traffic is not actively balanced, and it is only received by the active interface.
    This mode is particularly useful when the switch does not support **Link Aggregation
    Control Protocol** (**LACP**). Occasionally, this mode does not work well with
    some switching systems.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`balance-alb`: This mode actively balances both incoming and outgoing traffic
    by considering the availability and load of each interface. Occasionally, this
    mode does not work well with some switching systems.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LACP
  id: totrans-498
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All of the preceding modes can operate without any changes to the switches that
    the server is connected to. However, there is another mode that is more commonly
    used, called LACP. LACP is the most complex mode used to aggregate multiple network
    connections, usually Ethernet, into a single high-bandwidth link. This process
    is commonly known as link aggregation, NIC teaming, or bonding. LACP is defined
    by the IEEE 802.3ad standard and is frequently used in enterprise and data center
    environments to enhance network performance, redundancy, and fault tolerance.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: 'However, to utilize LACP, switches must be configured to use it. As an administrator,
    it is essential to communicate your configuration requirements to ensure that
    the switch is configured in a compatible mode. The configuration must match on
    both ends for LACP to work correctly. Most enterprise-grade network switches and
    server NICs provide LACP support. Key characteristics and features of LACP include
    the following:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: '**Aggregated links**: LACP enables the aggregation of multiple physical network
    links into a single logical link, which appears as a single interface to network
    devices.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increased bandwidth**: Aggregating multiple links with LACP can boost network
    bandwidth for bandwidth-intensive applications and server-to-switch connections.
    However, each MAC-to-MAC connection is usually limited to the speed of a single
    member of the aggregated link. If you have a host with two 1 Gb/s ports in the
    link, you will likely be unable to get more than 1 Gb/s of communication between
    the host and a client.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load balancing**: LACP can distribute network traffic across aggregated links
    using various load-balancing algorithms, preventing network congestion on a single
    link while optimizing network utilization.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: In addition to providing increased bandwidth, LACP also
    offers redundancy and fault-tolerance capabilities. If one physical link fails,
    LACP can automatically redirect traffic to the remaining active links, minimizing
    downtime and ensuring network availability.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic protocol**: LACP is a dynamic protocol that dynamically negotiates
    and establishes link aggregations between network devices using LACP frames.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modes**: LACP supports two modes of operation:'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Active mode**: In this mode, the device actively sends LACP frames to negotiate
    and establish link aggregations.'
  id: totrans-507
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Passive mode**: In passive mode, the device listens for LACP frames but does
    not actively send them. It relies on the other end configured in active mode to
    initiate the aggregation.'
  id: totrans-508
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LACP is commonly used in scenarios where HA and network performance are critical,
    such as server-to-switch connections, inter-switch links, and connections to **Storage
    Area Networks** (**SANs**). It allows organizations to make efficient use of available
    network resources and improve network reliability.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-510
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, you will need three Oracle Linux 8 systems, each with access
    to yum repos. For this exercise, we will call them `networking`, `client1`, and
    `client2`. They are mostly identical systems, each with 8 GB RAM, 4 vCPUs, and
    100 GB of drive space. The difference is that networking should have two network
    interfaces on the same network. The filesystems have 50 GB in `/`, 5 GB in `/home`,
    and 8 GB in swap. The remaining disk space is unallocated.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-512
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While this can be done from the GUI, this recipe will cover doing this from
    the command line. As a note, when working on the main network connection to the
    server, you will want to be on the system console. Doing this via a remote connection
    such as SSH can leave you in a situation where you lose access to the server.
    Following are the steps to configure a redundant connection.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing you will need to do is create the bond. In this example, we
    will create it using the `balance-alb` mode to best balance both incoming and
    outgoing traffic:'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Next, we will configure the bond to use DHCP. On production servers, you would
    normally use a manually configured IP address:'
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Note
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are adding a port into a bond that is already in use, you should delete
    that port now. In this case, `enp0s3` was in use, so it was deleted with the following
    command:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: '**nmcli connection** **del enp0s3**'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now, we will start the connection and check the status:'
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '![Figure 6.27 – nmcli device output with a working bind](img/B18349_06_27.jpg)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
- en: Figure 6.27 – nmcli device output with a working bind
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: You can see `bond0` as the device, with members `enp0s3` and `enp0s8`.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: 'When you look at the IP address, you will see that that is now on the `bond0`
    device. This can be checked with the `ifconfig bond0` command, with the output
    as follows:'
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.28 – Output from ifconfig bond0](img/B18349_06_28.jpg)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
- en: Figure 6.28 – Output from ifconfig bond0
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: You can continue to use the system normally now, but with `bond0` being the
    network device.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-531
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have a working bond, let’s look at the traffic going in and out.
    To do this, we need to install the `iptraf-ng` command:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'This tool allows you to monitor Ethernet traffic on a server. For this example,
    we will run the `iptraf-ng` command. This will launch the program, and you will
    be on the main screen, as seen in the following screenshot:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.29 – iptraf main menu](img/B18349_06_29.jpg)'
  id: totrans-535
  prefs: []
  type: TYPE_IMG
- en: Figure 6.29 – iptraf main menu
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: 'From here, we will look at the **General interface statistics** by hitting
    the *S* key. This will then show a real-time flow of traffic on each interface:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.30 – General interface statistics](img/B18349_06_30.jpg)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
- en: Figure 6.30 – General interface statistics
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that both in and out traffic is balanced between the two physical
    interfaces. This is because the bond was built using the `balance-alb` mode. To
    generate this traffic, a simple flood ping was used from the `client2` system.
    This was done with the following command:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Note
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: Be careful using the `-f` option, as this will flood the network with traffic,
    and it is generally not acceptable to do so on production networks without coordinating
    with the network and security teams. It can cause performance issues for systems
    using the network.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: There is a better way to really stress the network, though. That is to use a
    tool that will generate the maximum levels of packets the interface will support.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: iperf – network stress tool
  id: totrans-545
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will use a tool called `iperf`. It will generate the maximum amount of traffic
    that the interface can support. To install `iperf`, run the following command
    on all systems:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We also will need to open up a TCP port for the system to use and reload the
    firewall. Each instance of the server can only handle a single client connection.
    We will add multiple ports to enable running multiple servers, each with a different
    port. This is done with the following commands:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '`iperf` works using a client-server model. In order to use it, we first need
    to start an `iperf` server on the networking system. With this example, we will
    set up the server to listen on port `8001` using TCP. This is started with the
    following command:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We will repeat the same command in a different window, running on port `8002`
    instead:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Now, with two servers running, we can run a test from `client1`.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: 'The test is run with the following command:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The test will kick off and for a few seconds run the maximum traffic possible,
    as seen in the following output:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.31 – iperf client output](img/B18349_06_31.jpg)'
  id: totrans-558
  prefs: []
  type: TYPE_IMG
- en: Figure 6.31 – iperf client output
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: 'While the test runs, we can monitor the performance from the `iptraf` command,
    and we will see most of the traffic is hitting a single interface. This is seen
    in the following screenshot:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.32 – Single-client traffic](img/B18349_06_32.jpg)'
  id: totrans-561
  prefs: []
  type: TYPE_IMG
- en: Figure 6.32 – Single-client traffic
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: This traffic is all on the `enp0s8` interface. This is because the load-balancing
    algorithm uses the MAC address of the client. This limits the traffic to a single
    interface in the bond.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up, can run a test from `client1` and `client2` simultaneously. The difference
    is that `client2` will use port `8002`. The results are seen in the following
    screenshot:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.33 – Test with two different clients](img/B18349_06_33.jpg)'
  id: totrans-565
  prefs: []
  type: TYPE_IMG
- en: Figure 6.33 – Test with two different clients
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that both interfaces are being used for heavy network loads.
    This is happening because each client is using a different MAC address. If a third
    client were used, we would then see it competing for traffic for one of the interfaces,
    potentially causing issues.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: This is an important factor to consider when bonding in environments where heavy
    network performance is required from multiple clients. When using bonds, and you
    have random performance latency issues, monitor the ports. You might face some
    contention. One way to address this is to add additional ports to the bond. You
    can also upgrade the interfaces to units that can support more bandwidth.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
