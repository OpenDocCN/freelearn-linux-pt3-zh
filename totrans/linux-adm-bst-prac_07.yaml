- en: '*Chapter 9*: Backup and Disaster Recovery Approaches'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I have said it already earlier in this book and it bears repeating no matter
    how many times it takes: nothing is as important in what we do as system administrators
    as maintaining good backups. This is our utmost priority. It is so important that
    many organizations maintain an independent system administration team that handles
    nothing but backups to make sure that it maintains constant attention.'
  prefs: []
  type: TYPE_NORMAL
- en: Backups are not glamorous, and they are rarely exciting. This does not just
    make them a challenge for us in the technical world to want to spend time thinking
    about them when we could be implementing new automation or something else admittedly
    more exciting, but it also means that management often does not prioritize budgets
    or prioritize around backups. This creates a potential danger for system administrators
    that our careers can be slowed if we focus on critical functions like backups
    instead of doing flashy, high-profile projects to keep the interest of management
    while at the same time often being at very high risk of being punished if the
    backups that they do not prioritize do not work flawlessly when needed.
  prefs: []
  type: TYPE_NORMAL
- en: Backups are moving more and more to the forefront of our consciousness, however,
    as they have moved from primarily protecting against catastrophic hardware failure
    to being on the front lines of security concern. Backups have evolved in recent
    years and now present opportunities to shine politically within our organizations
    and opportunities for renewed technical interest. The boring backup and restore
    landscape of the 1990s and 2000s is, quite literally, a thing of the past and
    today we have so many approaches, options, and products that we really must approach
    backups as a broader concern than we have in the past.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we are going to cover a lot of different aspects of backups.
    We will start with a broad overview of how we take backups and what components
    exist within backup systems. Then we will look at similar technologies that are
    often used in conjunction with and might be sometimes confused with backups. Then
    we will cover in depth what we have mentioned throughout this book, the idea of
    a modern *DevOps centric backup approach* and talk extensively about that. Then
    we will explore agents and the issues with crash consistency of our data in backups.
    Finally, we tackle triage and what we can do to make restores better once a disaster
    has happened.
  prefs: []
  type: TYPE_NORMAL
- en: Without further ado, let us begin what is assuredly our single most important
    chapter in this book. Of any book, I would assume.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we are going to learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Agents and crash consistency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backup strategies and mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snapshots, archives, backups, and disaster recovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backups in a DevOps world
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triage concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agents and crash consistency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next session we are going to look at mechanisms for taking backups. Before
    we do that, I want to look at why backups are so hard to do well in the first
    place. In order to do that I will work with two examples. One is a text document
    that we create using our favorite text editor. I will assume that you are a normal
    person and love the vi editor as much as I do. And we will compare that to another
    common use case, the data file of an enterprise database system.
  prefs: []
  type: TYPE_NORMAL
- en: When we talk about backups, we are talking about taking data that is stored
    on a physical medium and replicating that data somewhere that is separate from
    the original system in such a way that it is able to survive in many cases when
    the original system has failed. That is a very high-level view of the goal of
    backups. It serves our purposes. Therefore, in order to perform a backup we must
    be able to take the data that the system has, read it, move it, and write it.
  prefs: []
  type: TYPE_NORMAL
- en: Of these steps it may seem like moving data to another location or writing the
    data somewhere would be the biggest challenges. This is incorrect. Being able
    to read the data is actually where the real problems tend to exist. Primarily
    because what data we need to read is not always clear.
  prefs: []
  type: TYPE_NORMAL
- en: How do computers store data on disk in the first place? Not every computer system
    works the same way, but there are certain basics that exist for which we know
    no alternatives. While working with data, computers hold the data in their random-access
    memory. While there it is completely volatile, but it is very fast. When the computer
    is done with a file it takes the data that is in its memory and then writes it
    onto the storage device, presumable a hard disk or SSD.
  prefs: []
  type: TYPE_NORMAL
- en: If we attempt to take a backup of a system while data is in the computer's memory
    and not yet written to disk our backup would contain none of the data in question.
    This may sound silly to say, but it is necessary to remember. It is common for
    people both technical and management to assume that data is always stored somewhere,
    even when it has only gotten into memory and not had time to be stored anywhere
    yet.
  prefs: []
  type: TYPE_NORMAL
- en: Once data is written to the disk, and then a backup is taken, the data from
    the disk should be copied to the backup location. All is well with the world.
  prefs: []
  type: TYPE_NORMAL
- en: This is all well and good, but computers deal with situations far more complicated
    than just receiving files, and saving them to disk. In the real world, computers
    nearly always are reading existing files off of their storage mechanism, holding
    it in memory while manipulating the data, and then saving it again back to disk
    with the new changes incorporated into the data set. This is where things start
    to become tricky.
  prefs: []
  type: TYPE_NORMAL
- en: We should start using our examples now. First the case with a text file editing
    in `vi` (if you insist, you can edit in Atom or something else.) We will also
    assume that we are not creating a new file, but rather editing a configuration
    file that already exists on the disk. A great example is the `hosts` file (found
    at `/etc/hosts`.)
  prefs: []
  type: TYPE_NORMAL
- en: If we take a backup while the file is being edited, we would get a copy of the
    old data before any edits because any edits that are happening exists only in
    memory, not on disk. The fact that the file is open at the time does not mean
    that data is being written to disk. The old data is still on the disk while the
    new data exists in memory.
  prefs: []
  type: TYPE_NORMAL
- en: When the file is saved, the data is written to the disk. Once written, we can
    make our copy. Of course, there can be a short moment while the data is being
    written where what is on disk is neither the original version nor the new version
    but something in between as the data is not finished being written.
  prefs: []
  type: TYPE_NORMAL
- en: We can attempt to lock the files, but even with a file being locked we have
    a complicated situation. The backup mechanism, whatever it is, has to decide if
    it is going to skip over a locked file, wait for the lock to be released, or ignore
    the lock and take the backup anyway. No situation is very good. We either fail
    to backup all of the files in any particular backup process, we risk a long wait
    for an event that might *never* happen, or we risk getting a file that is out
    of date or worse, corrupt. There is no completely simple answer.
  prefs: []
  type: TYPE_NORMAL
- en: Locking mechanisms in Linux
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When processes are using files on Linux, and more or less on any operating system,
    there are three essential strategies that we normally see in action. The first,
    and most obvious, is to do nothing. That is correct. In many cases there is no
    locking performed whatsoever. The contents of a file are read and the file itself
    is not marked as being open. It is just read. This approach is great in that any
    other process can continue to use the file in question in whatever manner they
    desire, but it also gives us no information as to whether or not the file is about
    to be updated in some way.
  prefs: []
  type: TYPE_NORMAL
- en: The primary alternative mechanism on Linux is called an advisory lock. With
    an advisory lock, the operating system marks when a file has been opened by a
    process. The locks are so called advisory because the operating system advises
    other processes that the file is in use. Another process can opt to ignore the
    lock and continue to read, or even to overwrite, the file that is locked. This
    is handy in that we can lock a file and not worry that we are completely blocking
    others from accessing it. The risk is that our lock is not honored, and race conditions
    are encountered where data is changed out of order or data that is believed to
    be saved gets overwritten. Ignoring an advisory lock to peek at a file and check
    out some old data is pretty safe. Ignoring an advisory lock to overwrite the file
    with changes without the original process knowing that it has happened is dangerous.
    Processes that fully support and honor an advisory lock are called cooperative
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: The third option is a mandatory lock. As the name implies, processes have no
    option but to honor a mandatory lock. Mandatory locks are managed by the Linux
    kernel and only exist if a filesystem is specifically mounted with this form of
    locking enabled. Mandatory locks on Linux, however, suffer from implementation
    problems that make them unreliable and subject to some uncommon race conditions
    which effectively defeat the purpose of the lock. Because of this mandatory locking
    is almost always ignored on Linux systems.
  prefs: []
  type: TYPE_NORMAL
- en: Locking is conceptually hard. It carries risk and system overhead. The best
    way to respond to a locked file is not always clear and may vary by many different
    use cases of which the operating system is not aware. Because the operating system
    does not know how or why a file is being accessed in the first place, it has little
    means of usefully enforcing limits on secondary access. There is nearly always
    a reason that we might want to read a locked file and sometimes a good reason
    to even write to a locked file. Trusting that we will only run processes that
    will behave as needed for our use cases is our best course of action in nearly
    all cases. When more complex access is required, such as is the case with most
    database files, it is necessary to move to single process access with that process
    handling additional data access from higher in the application stack where more
    knowledge of the intended use of the file is know.
  prefs: []
  type: TYPE_NORMAL
- en: Other than locking, the major concept that we have to understand is quiescence.
    Quiescence simply refers to the storage reaching a state of dormancy, a lack of
    fluctuation. When we say that we have achieved quiescence in our storage we mean
    that all of the data that is currently *in flight* whether actively being used
    by an application or just being held in one of any number of different cache layers
    has been written to disk. In some ways, we can think of locks as being a mechanism
    meant to warn us (or a process) as to a system not being currently quiesced.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there is no universal mechanism to enforce or even ensure quiescence
    on Linux. There is none on Windows either, dangerously contrary to many claims
    that VSS (the Volume Shadow Service) does this. VSS is the standard LVM on Windows
    and as such is often used in many storage operations. It is commonly said that
    VSS guarantees that all files are quiesced, but this is false. VSS has special
    hooks into common applications, primarily from Microsoft, such as SQL Server,
    Exchange, and Active Directory so that they are able to communicate effective
    with the storage layer as to the state of their quiescence. This is an amazing
    feature and incredibly handy to be integrated with the operating system automatically.
    It does not address third party applications which rarely have VSS integration
    leaving them dangerously unquiesced while many Windows administrators believe
    that VSS magically manages data flushing from all application layers that it does
    not, and cannot.
  prefs: []
  type: TYPE_NORMAL
- en: At the operating system level, where we typically have to work as system administrators,
    our primary tools for quiescence are snapshots from a logical volume manager (such
    as those provided by LVM or ZFS) or to freeze the filesystem itself as is possible
    with XFS. The higher up the stack (higher means closer to the application itself)
    we freeze the system, the less likely that there will be some artefact still in
    flight and not written to disk. At the end of the day, however, unless an application
    itself guarantees that it has flushed all data down to disk and left nothing in
    a cache or in process of some sort, the best that we can hope for is a best effort
    situation. This is true regardless of the operating system. It is a universal
    challenge. Computers are simply really complex things.
  prefs: []
  type: TYPE_NORMAL
- en: In the majority of cases, nearly all cases in fact, one or more of these quiescence
    methods will work just fine to get data safely to disk. In fact, most backups
    and most businesses rarely even rely on this much protection and typically rely
    on best effort file locks and multiple backups to provide working copies of a
    sufficient number of files on any given system. Most workloads write to the disk
    infrequently making relatively few into high-risk scenarios. But we cannot overlook
    at all of these methods leave us with no ability to truly ensure that a system
    is completely quiesced and we are always taking some degree of chance that critical
    data will be in flight in some form. If we are talking about a web server that
    only serves out static websites the risk might be so low as to simply ignore it.
    If we are talking about a critical enterprise database used for financial and
    accounting data the risk might be enormous and one that could never be undertaken
    no matter what. So, if we want to take backups at a system level, we need to consider
    our workloads, their quiescence, and what risk that creates for us.
  prefs: []
  type: TYPE_NORMAL
- en: When we take a backup using one of these mechanisms that does not coordinate
    end to end with the application workloads to ensure that the data on physical
    media is complete and quiet, we refer to our backup as being *crash consistent*.
    This is an important term and one that is used often. Crash consistency does not
    mean that the system has actually crashed, but rather it refers to the state that
    storage is in after the abrupt crash or loss of power in a computer system.
  prefs: []
  type: TYPE_NORMAL
- en: Crash consistency is such a critical idea that we need to really understand
    what we mean when we say it. For some it is said as a scary warning of impending
    doom. Others use it to imply a reliable system state. So, what does it really
    mean?
  prefs: []
  type: TYPE_NORMAL
- en: When any computer crashes completely, meaning that there is no ability for recovery
    or protection after the event has occurred such as when there is catastrophic
    hardware failure or power is suddenly unavailable to the system, there is absolutely
    no mechanism to ensure that the data that is on the disk is accurate or complete.
    As any computer user is aware, sudden loss of power to a computer is generally
    not a problem, except that any unsaved changes to a file or our latest progress
    in a video game or whatever will be lost. This feels obvious and we do not normally
    think of it as being a significant problem of system design. Once in a while,
    we will find that a program or data set has become corrupted. We all fear this
    when a system returns from a crash, but rarely is it actually a problem - at least
    as an end user.
  prefs: []
  type: TYPE_NORMAL
- en: This state of a system after an actual crash is essentially identical to the
    state of a system where a backup is taken without any quiescence. Any data that
    is currently being modified will be lost. Once in a while data that we thought
    was already saved to disk will be corrupted. By and large, everything will be
    there. This is why we call backups or any copy in this state crash consistent,
    meaning in the same consistency as a catastrophic crash on a physical machine.
  prefs: []
  type: TYPE_NORMAL
- en: When talking about desktops it is easier to illustrate the point. The same risks
    exist on servers, but to a much more serious degree. With a desktop we tend to
    have a single user who has a very good idea of all data that is likely to be currently
    in flight and storage is pretty straightforward. In a server environment not only
    do we likely have lots of storage complexity with different possible and possibly
    risky mechanisms at every turn, but we likely have larger and more numerous cache
    situations throughout the stack and one or more multi-user workloads sitting on
    top that will rarely be interacting with users meaningfully at the time of a crash.
  prefs: []
  type: TYPE_NORMAL
- en: Consider an example web server, a common example, with data stored in a database.
    A user has some interaction with the web page maybe entering contact details for
    sales or uploading a form or processing some banking transaction or placing an
    order. The user believes that the transaction completes as they receive feedback
    saying that their submission is complete. The user will likely not refresh the
    page or return as to them, the transaction is completely. But is it?
  prefs: []
  type: TYPE_NORMAL
- en: The web server may have queued the transaction in memory to be handled a few
    seconds later, or left part of the data in a cache. Or it may have written the
    transaction to a database, and that database might still be holding it in a cache.
    Maybe the database has written the data to its file, but the filesystem has the
    data in a cache. Maybe the filesystem has committed the data, but the logical
    volume manager has the data in a cache. If you have hardware RAID, there is likely
    a cache there and before data gets written out to physical disks it will sit in
    that cache. The physical disks themselves can have caches which, one would hope,
    would be disabled in this situation but may not be and represent yet another place
    where data might be cached before the drive head actually puts the data physically
    onto the disk.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, lots of mechanisms probably exist to protect against many of these
    failures. Disk caches should be disabled, RAID caches are often protected by being
    non-volatile RAM or have a battery to allow them to flush to physical disk even
    after the system experiences a power loss, databases often log transactions before
    doing their final write so have the ability to roll back if they cannot roll forward
    and applications generally do not report that all is well to the end user until
    at least some degree of certainty exists that the transaction has been recorded.
    All of those are situations that we hope exist, however, and nothing guarantees
    them except for a combination of application and system design end to end. The
    best designed application can still be fooled by a hypervisor that presents volatile
    RAM as if it were a hard drive and reports quiescence where there is no storage
    written at all. So, trust is required.
  prefs: []
  type: TYPE_NORMAL
- en: Dropping data in flight is only one concern. The other is corruption. Many layers
    along this path have a potential to become corrupted by an incomplete write operation.
    An individual file is often where we see corruption occur, this is the most common
    place. Twenty five years ago, we saw filesystem corruption regularly as well.
    Today this is not a common worry, but it does exist as a risk, still. Different
    filesystems carry different levels of risk for this type of scenario. In theory
    a logical volume management layer could hold some corruption risk. Certainly a
    storage layer of RAID or RAIN can become corrupted and sometimes, such as parity
    RAID, have some potential for total data loss during a corruption event.
  prefs: []
  type: TYPE_NORMAL
- en: All of these steps are unlikely, but all are possible. Missing or corrupt data
    will generally not happen during a normal crash. How likely a disaster is to happen
    depends on many factors. We should neither look at crash consistency level solutions
    as useless or total loss waiting to happen, but neither should we pretend that
    they do not ignore basic risks of not having any system in place to ensure consistency.
    In all cases we are simply rolling the dice and hoping for the best. I have seen
    a great many small businesses lose data with what they had thought were completely
    reliable backups because they were depending on crash consistency and a vendor
    had pretended that data corruption was not a concern. Because crash consistent
    mechanisms are cheap and simple, they get a lot of attention from vendors.
  prefs: []
  type: TYPE_NORMAL
- en: The alternative approach is called **application consistency**. We call it this
    because it refers to the potential state of the system when the workload application(s)
    are able to confirm that they have flushed all of their data to disk and have
    nothing currently in-flight. Applications may do this simply by confirming that
    they are quiesced currently, or they may have mechanisms to force such a state
    to occur. In either case, they avoid the problems of crash consistency by applying
    intelligence from the very top of the application stack and verifying that data
    is ready to be backed up.
  prefs: []
  type: TYPE_NORMAL
- en: This approach requires the application support providing this quiescence and
    if we want to coordinate this action with a backup mechanism, of any sort, we
    have to have some way for the backup mechanism to call the application to request
    quiescence, or for the application to call the backup mechanism. In the Windows
    world there is a standard interface in VSS as we discussed earlier than any application
    can support if they chose to do so. Linux lacks a similar standard at this time.
  prefs: []
  type: TYPE_NORMAL
- en: Because all of these mechanisms are complex and non-standard it presents major
    challenges when working with anything outside of a limited scope of popular services
    that are well known and supported. In reality, Windows is also similarly limited
    but the use of a smaller set of very standard services is more common there.
  prefs: []
  type: TYPE_NORMAL
- en: Many applications choose to tackle this problem by incorporating their own backup
    services rather than attempting to depend on some other mechanism. Some may build
    an extremely robust backup mechanism that directly supports many different options
    such as built-in schedules, multiple backup levels, and storing directly to many
    different types of media. This is not very common but certainly exists.
  prefs: []
  type: TYPE_NORMAL
- en: The most common option used by everything from the simplest of applications
    to enterprise databases is to simple write a backup file from the application
    directly to the local filesystem. The is extremely common and very effective.
    This produces a simple file or set of files that can be designated as a backup
    and kept from being used other than for the purposes of another backup mechanism
    using these files to then copy or send to another location.
  prefs: []
  type: TYPE_NORMAL
- en: A really common example of this is MySQL and MariaDB databases. Databases are
    our hardest type of application to safely backup and so nearly any database system
    will incorporate some means of safely protecting the data without having to resort
    to a drastic, brute-force step such as shutting down the databases, coping files,
    and starting it back up when completed.
  prefs: []
  type: TYPE_NORMAL
- en: MySQL example with mysqldump utility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Almost certainly the most well-known application level backup tool in the Linux
    ecosystem is `mysqldump` that comes with MySQL and MariaDB. This simple command
    line tool connects to a running MySQL or MariaDB database server, locks it and
    quiesces all of its data in RAM (it has no need to flush to disk), and then saves
    a copy of this data to disk. It is as simple to use as any utility could be while
    also being outrageously power. It has almost no impact on a running system and
    because it does not need to do any complicated flush to disk before taking a fully
    application-consistent copy to disk it does not need to stop read operations to
    the database and only needs to halt write applications for the briefest of moments.
  prefs: []
  type: TYPE_NORMAL
- en: 'I will show here the simple, single command to take an easy backup of every
    database managed by a single MySQL or MariaDB instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This example is about as basic as you can get, and yet it is so effective. You
    can run it anytime and because it takes the current time during the process it
    will not overwrite another backup file, even one takes just a minute earlier.
    You can make it write anywhere on disk that you like. You can use standard utilities
    to compress the resulting backup file while it is being written or at a later
    time very easily. Keep in mind this is a backup file, a file for the purpose of
    making a backup, but not an actual backup yet at this point.
  prefs: []
  type: TYPE_NORMAL
- en: Once this file is stored to disk, it is safe and any backup tool can now take
    this file without worry than an application is using it or modifying it and place
    it wherever it makes sense. Total flexibility and simple application-consistency.
  prefs: []
  type: TYPE_NORMAL
- en: At this point I think we understand consistency, how locks and quiescence play
    a role, and all of the problems that we may face while trying to make a backup
    of even a single file or application (remember that not all backups involve data
    that is actually in file format on storage anywhere), let alone trying to back
    up entire filesystems or systems. Appreciating the challenge is key to understanding
    why we are concerned about different aspects of different types of backup tools
    which we will look at next together.
  prefs: []
  type: TYPE_NORMAL
- en: Backup strategies & mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Backups are actually far more complex animals than most people imagine. So often
    when dealing with backups we are simply told to *take a backup* as if this is
    a straightforward activity with few variables. In the real world we do have some
    stock approaches that meet the majority of needs, if only minimally. There are
    cases, however, where to do effective backups requires a lot more thought and
    deep understanding of our workloads and infrastructure to be able to get correct.
  prefs: []
  type: TYPE_NORMAL
- en: In the good old days, you know like the 1980s and 1990s, backups were almost
    always the same. They involved a simplistic agent of some sort, like the standard
    Linux `tar` command, that would run on a schedule (that we probably had to set
    manually with something like `cron`) that would take all of the files in a directory
    or, more likely, the entire system and package them up as a single file and place
    that single, large file onto a tape device. That tape device would then require
    a human to remove the tape and transport somewhere for safe keeping.
  prefs: []
  type: TYPE_NORMAL
- en: Over the years new technologies would come out and backups slowly became more
    robust and more complex to discuss. Tape became less popular and other backup
    targets ranging from swappable hard disks to constantly online storage emerged.
    With backup media evolving the mechanisms that took backups had an opportunity
    to move from discrete *one tape per backup* schedules to more flexible or complex
    designs. With all of this came complex backup applications and the backup world
    went from simple and basic to quite complex.
  prefs: []
  type: TYPE_NORMAL
- en: Because there are so many variables today, simply taking a backup is no longer
    a straightforward concept. People all have different views and ideas of what a
    backup entails based on their desires and experience. It is so varied that we
    even risk tunnel vision and thinking that all people see and experience backups
    similarly when, in reality, there are so many different ways being used in the
    real world to handle backup needs.
  prefs: []
  type: TYPE_NORMAL
- en: Types of backups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we will discuss the actual backup mechanisms that may be used. These
    generally fall into two categories: agent based and agentless (I really hate both
    of these terms) but people normally think of them as three categories with a third
    being ad-hoc scripts. All of these ideas around their identities are totally wrong,
    of course, as are so many things that people tend to say. The terms have made
    inroads and you will need to use them regardless of their inaccuracies, however.'
  prefs: []
  type: TYPE_NORMAL
- en: System level backups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: System level backups are the so-called agent-based backups. These get this name
    because typically a software agent that runs on top of the operating system is
    installed and is visible. It can be seen in an installed software list somewhere
    on the system. The agent then runs and grabs files from *inside* of the operating
    system context and send them off somewhere else to be stored. There is often additional
    processing steps such as packaging, compression, deduplication, and encryption,
    but all of those are technically optional.
  prefs: []
  type: TYPE_NORMAL
- en: Everything is an agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we say that we have an agent-based backup solution it brings to mind software
    that must be purchased, downloaded, and installed onto our computers. Certainly,
    this is a very common thing that people will do.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of an agent is, however, far broader. Backup agents may be complex
    backup utilities that are built into the operating system rather than obtained
    separately. In the Linux world this is just as likely to be the case as not because
    so many powerful backup options are included in the repository ecosystems of the
    typical business-class distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Agents get even broader still. Classic utilities like `tar` and `rsync` are,
    or at least can be, backup agents. They are installed software components that
    can be used to carry out a backup from inside of the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: We can keep going. If you write your own script to move blocks or files around
    that, too, is an agent. The idea of an agent is incredibly broad and, logically,
    some component is always needed to perform a backup job and that component is
    always an agent.
  prefs: []
  type: TYPE_NORMAL
- en: By working from inside of the operating system, an agent located here has the
    ability to utilize obvious access methods to the data that we want to back up.
    It can access the block devices directly either by talking at a block level with
    a tool like `dd` or by using a slightly high-level logical volume tool like LVM
    which can provide robust block level handling; or it can use the filesystem layer
    to request files one by one; or it can use APIs to talk to running applications
    that can provide some type of higher level managed data access. In one or more
    of these ways an agent can access the data necessary for a backup.
  prefs: []
  type: TYPE_NORMAL
- en: All of this is to say that agents have the task of talking to the operating
    system in an attempt to trigger some level of quiescence from the storage subsystem
    to increase the chances of getting a consistent backup. Some agents talk extensively
    to many applications and some talk to none.
  prefs: []
  type: TYPE_NORMAL
- en: Agents do all of the work and offer us many ways to talk to our data. But they
    have the limitation that they can only access what the operating system can access
    and they can only be used when the operating system is up and running. This last
    point means that any resources that the operating system itself is using will
    be in use at the same time that the agent needs to access them. This makes for
    a real challenge in system-wide consistency. However, we rarely are truly concerned
    with consistency on a full system scale. Of course, we would prefer it, but the
    majority of system files are of little consequence and easily replaced if lost
    or corrupted. Typically, only critical application data or maybe installed applications
    themselves are of major concern and the rest is just useful in case of a need
    to restore quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, but not universally, with system-level backups it is necessary when
    performing a restore operation to install an empty operating system and then install
    the restore agent and allow the agent to run against the stored backup and put
    files back into place inside the operating system. This often means that system-level
    backups are excellent at taking backups of individual files and being able to
    restore individual files but find it more difficult to deal with having to restore
    entire lost systems.
  prefs: []
  type: TYPE_NORMAL
- en: There was a time when essentially all backups worked this way and the general
    assumption was that nearly all restore operations were to retrieve single lost
    files rather than to retrieve entire failed systems.
  prefs: []
  type: TYPE_NORMAL
- en: People just do not lose files like they used to
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In writing this chapter I was struck by a massive, fundamental change in the
    computing experience for most people over the last thirty years. Through all of
    my younger years in the industry and even before simply as a computer user, the
    big concern that we always had was losing, deleting, or having an individual file
    become damaged in some way. We really all pictures the concept of backups as just
    being lots of individual files being copied somewhere, and we envisioned any restoration
    to be a process of copying those files back once we had a place to put them and
    more often than not, needing to do so when there had been no failure whatsoever.
  prefs: []
  type: TYPE_NORMAL
- en: This last point is what is most interesting to me. For decades it was assumed
    that people were going to accidentally overwrite or just delete critical files
    that they needed with great regularity, and it really happened. Needed to track
    down and restore an individual file that an end user deleted was a completely
    common task that was done so often that most large companies had teams dedicated
    to doing nothing else.
  prefs: []
  type: TYPE_NORMAL
- en: Computing has changed such that today, this is rarely the case and the idea
    that an important file would be lost on its own does not seem impossible, but
    certainly seems unlikely. I assume users have become better educated and far more
    diligent with how files are managed, and of course a huge percentage of work that
    used to be done as individual files has moved to some sort of database of data
    stored behind an application front end that protects end users from themselves
    (this could range from Google Docs to Flickr, but even end users working with
    their own file management has decreased significantly.) Most operating systems,
    and even some applications themselves, now implement *trash* features that hold
    deleted files until they are explicitly disposed of to give end users lots of
    time to change their minds or find what was deleted accidentally. And finally
    there are features like self-service file restores included in operating systems
    or from simple add-on applications that allow files to be brought back from other
    media without needing to engage traditional backups or backup teams as the data
    is still stored somewhere locally (not to mention online backup systems that can
    be used by end users without intervention.)
  prefs: []
  type: TYPE_NORMAL
- en: It is simply interesting to note that something that had become such a common
    problem that it drove most of an industry not that many years ago is effectively
    gone today. The days of computers being seen as file management and manipulation
    devices are gone and have given way to being online data consumption devices where
    most users are not even file-aware any longer. Many younger users can even be
    confused at the concept of file and storage management today.
  prefs: []
  type: TYPE_NORMAL
- en: Platform level backups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The alternative approach is to take a backup from the platform level, that is
    from the layer underneath the operating system. In practical terms this means
    the virtualization layer which would entail typically the hypervisor, but in some
    cases can be limited to external (to the operating system) storage virtualization,
    this would be especially common if using a SAN device, for example.
  prefs: []
  type: TYPE_NORMAL
- en: This approach became all the rage with the advent of virtualization. New backup
    vendors entered the market with new technology aimed at making extra-operating
    system backups easier and faster. Backups at this level, of course, do not have
    access to things like the filesystem or application APIs to communicate with those
    components or to have knowledge of the data layout. So backups taken at this level
    must do so by talking directly to the block devices or to hypervisor level storage
    abstraction layers, which are essentially block devices themselves, leaving us
    generally blind to what the data is that we are actually getting. At this layer
    we might know, but only maybe, about the physical separation of devices presented
    to the operating system. But we have no visibility into filesystems or deeper.
    So all of the concerns that we have with system-level backups and consistency
    are potentially magnified many times over. Not only do we have no way to know
    if an individual application workload has quiesced, but we cannot even tell inform
    the system-managed filesystem, file locks, or logical volume manager that we are
    attempting to read the block devices. So if we take a backup at this level alone,
    we are totally blind and simply reading blocks off of a virtual disk with no reason
    to believe that it is in any sort of a consistent state and nothing tells the
    operating system that anything is happening at all. If we do this alone, the operating
    system literally has no way to know that it is happening.
  prefs: []
  type: TYPE_NORMAL
- en: It does not take very much thinking about how this mechanism has to work to
    realize that platform-level backups without an agent are all but impossible. We
    could shut down a virtual machine and take a backup of its storage layer while
    the machine is powered down, of course. The shutdown process of a total operating
    system is effectively the best way to ensure full quiescence and consistency.
    How do we shut down a virtual machine from the platform level without just pulling
    the virtual plug on it and putting ourselves into a state of crash consistency?
    Surprisingly, with an agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the big secret of platform-level (aka *agentless*) backups: they use
    an agent! They have to. In the real world taking a backup directly from any lower
    level service would be completely unreliable. But I hear you murmuring *I''ve
    done this, everyone has, and we don''t install any agents*.'
  prefs: []
  type: TYPE_NORMAL
- en: What makes the backup agents in virtualization environments hard to identify
    is because we install them automatically and universally so that we do not think
    about it. In some cases, these agents are built right into the operating system.
    They have different names depending on the platform that you are using. The most
    famous is VMware Tools as VMware uses this and the installation process is well
    known. KVM, the primary hypervisor in the Linux world, has the guest agent built
    not just into most Linux-based operating systems, but actually baked right into
    the Linux kernel itself! So you never actually need to see the agent, even though
    it is almost always there. Many Linux distributions do the same with Hyper-V's
    equivalent agent called Linux Integration Services, which are not built into the
    kernel but are often included in the operating system automatically.
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, the agents from the hypervisor vendors that I have used as examples
    are not strictly backup agents, they are general purpose agents used to coordinate
    activity between the hypervisor and the operating system, but in reality, that
    is simply a more advanced agent function and nothing more. They are agents in
    every possible sense of the word, they are software that has to be installed into
    the operating system to work, and just like more traditional backup agents they
    are sometimes included with the operating system and are sometimes third-party
    add-on packages.
  prefs: []
  type: TYPE_NORMAL
- en: These agents are generally part of a set of high-performance drivers needed
    to make full virtual machines work efficiently and so are rarely left missing.
    They provide a critical channel that allows the underlying platform layer to communicate
    up to the operating system layer or even higher. This communications channel is
    what is used to issue graceful shutdown commands to a virtual machine as well
    as to inform it that it needs to quiesce a filesystem or take a snapshot of a
    volume. In theory this agent could even hook into a specific application, although
    this is mostly only theoretical in the Linux world.
  prefs: []
  type: TYPE_NORMAL
- en: There are important advantages to taking platform level backups. If we ponder
    the workings of these backup systems, we easily see that the expectation is going
    to be a block-level image taken of an entire file system container or block device.
    The real advantage to this is in its completeness. If we take an image level backup,
    rather than a file backup, we have an opportunity to have the entire device, not
    just portions of it, that will presumably allow us to restore a system completely
    rather than having to rebuild and then restore partially.
  prefs: []
  type: TYPE_NORMAL
- en: Disk images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we talk about file level backups the essential mechanism, we are discussing
    is ultimately a file copy of some sort. File X exists on the filesystem that we
    want to protect and in order to protect the data that is in that file we copy
    that file to another location so that the data can exist in more than one place
    at the same time. Easy.
  prefs: []
  type: TYPE_NORMAL
- en: With full block device copies at the platform level, we refer to the resulting
    copy as an image or a disk image. In theory we could be copying from one physical
    block device to another or, at the very least, from a virtual block device to
    a physical one. In nearly all real-world cases, especially those involving backups,
    what we actually do is copy the block device to a file. That file is called a
    disk image and is sometimes referred to oddly as an ISO file.
  prefs: []
  type: TYPE_NORMAL
- en: We call this an image because it is essentially a picture of the entire disk
    as it was in any given moment. We also use the term snapshot to refer to this
    same operation. In regular English a snapshot is an image. The words are nearly
    interchangeable. The same is true in computing. Sometimes one is used to imply
    something different from the other, but there is no accepted definition that makes
    them not completely overlap.
  prefs: []
  type: TYPE_NORMAL
- en: In every day usage, images are used to refer to images stored as regular files,
    such as ISO files and contain a complete copy of the original filesystem. Snapshots
    are used to refer to a nearly identical scenario but where a logical volume manager
    creates a partial image file that is in some way linked to the original file and
    may contain only the differences between the two. But that file is often used
    to make a standalone image file, which is indistinguishable from an image file
    otherwise, but is still called a snapshot. Which leads to the inappropriate situation
    where two identical files can have different designations based solely on untraceable
    and unknowable histories. Obviously, there are common misconceptions in what these
    files are that lead to these different uses of names for the same thing. Nevertheless,
    images and snapshots are overlapping concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Because a disk image contains the entire contents of a block device it can be
    used to directly restore the entire block device, even onto hardware that has
    no knowledge of what that block device should contain. If you backup a Linux server
    and restore it, the entire system is restored, even if it was encrypted. An image
    will not bypass the encryption, but the encryption will do nothing to alter the
    imaging process.
  prefs: []
  type: TYPE_NORMAL
- en: Most virtualization platforms, even when using external storage devices like
    a SAN device, will storage the disk(s) belonging to a virtual machine as one or
    more disk image files (often in an advanced format rather than a raw ISO file.)
    This demonstrates that the image file is a full block device (virtually) and so
    can be used anywhere that any other block device is used. This is very important
    when we want to talk about restoring our data. If we have a complete image file,
    no restoration is needed if it exists in a place where we can access it. We can
    mount it directly under the right circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: Image backups have some big advantages because they can be taken *closer to
    the hardware* when it comes to performance. There are also big disadvantages,
    so it is not all roses.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced snapshot technologies that allow the system to essentially freeze a
    moment in time and work with it while the system is still under active use is
    a big deal for allowing major backups of active systems to have effectively. Taking
    full system images is by far the easiest process for handling backups because
    we do not have to think about what we are backing up, we just grab everything.
    Not only can we grab an entire operating system, but we can grab all of every
    operating system that exists on a single hypervisor. We can even take backups
    of virtual machines that are powered down as well as those that are actively running.
    System level backups have to happen per system, can only be done when the system
    is running, and only in very limited situations can they effectively take a complete
    backup and even more rarely can they find a way to do it that provides for a complete
    block level recreation of the system that can be restored in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: Platform level backups have been all the rage for over a decade now for good
    reasons. They are efficient, they protect against failing to select the right
    data to be backed up, they can easily be handled by a different team than the
    standard system administrators if necessary, and they can be done at large scale
    with essentially no deployments needing to be done to work (other than the deployments
    of agents that are needed otherwise, and the deployment of the backup software
    to the hypervisor layer.) Platform level backups fit perfectly into how most organizations
    want to be able to work, and they are exceptionally easy for support vendors to
    provide blindly as a service without needed to do any due diligence.
  prefs: []
  type: TYPE_NORMAL
- en: Platform level backups come with plenty of caveats too, however, and we need
    to be aware of why we might not want to be focused on them most of the time. Being
    blind they generally require the most storage capacity to hold the backups as
    they tend to contain a lot of data that is unnecessary such as system files. This
    bloat also means that moving the resulting images around whether for archival
    purposes or to get the file(s) where they need to be for a restore will potentially
    take longer than would otherwise be required. Agents still need to be deployed.
    Mistakes are easily made because the system is almost universally misunderstood
    and, like so many things that are too complicated for the average user to grasp,
    they are seen as a panacea rather than just another tool to consider.
  prefs: []
  type: TYPE_NORMAL
- en: Quiescence is a real struggle for platform level backup. Even when all agents
    are properly in place, the nature of those agents is that they expose fewer options
    and have fewer hooks into the application layer meaning that there is a higher
    risk that backups taken from this layer will only be crash consistent. Operating
    system files will be protected because there is nearly always proper communications
    made to the operating system itself via the agent, but almost no application workload
    will be protected by that.
  prefs: []
  type: TYPE_NORMAL
- en: Platform level backup is flashy and cool. It is an easy way for vendors to capitalize
    on backup fears while doing minimal work; it is an easy path to big margins with
    any actual risks being pushed off to the customers who rarely do their homework.
    It makes customers feel safe because the risks are too complex for even more IT
    practitioners to grok, and it lets them tell themselves that they are protected.
    Ignorance makes people sleep well at night.
  prefs: []
  type: TYPE_NORMAL
- en: Platform level backup is a power tool and a modern marvel, but it is still just
    one mechanism, just one approach and certainly not one that fits the bill every
    time.
  prefs: []
  type: TYPE_NORMAL
- en: That was a lot of information about two very simple backup mechanisms. We needed
    that solid understanding so that we can move on to talking about many concepts
    that exist in and around the backup and recovery world.
  prefs: []
  type: TYPE_NORMAL
- en: Snapshots, archives, backups, and disaster recovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many technologies that are either confused with backups, or may be
    a component of a backup. In this section I want to break down these basics and
    make sure that we understand what they are, why they matter, how they are used,
    and when we should leverage them ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: Snapshots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our last section we talked about taking block device images, how images and
    snapshots are truly the same thing, and what people tend to mean when using the
    term snapshot instead of the term images. Now we are going to really delve into
    snapshots.
  prefs: []
  type: TYPE_NORMAL
- en: Snapshots, as people tend to use the term, are amazing tools for doing some
    extraordinary things with storage. Snapshots are typically used to grab a momentary
    image of the state of a block storage device. The term snapshot is very descriptive
    in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'For most snapshot systems, and as it is intended by most people using the term,
    the snapshot that is taken is kept on local storage along with the original data
    and the two are intrinsically linked. The assumption is that the snapshot contains
    only changes, or the differential, between it and the original data. There are
    multiple ways to achieve this, but essentially the end results are the same: a
    file much smaller than the original data that is quick to create but is able to
    perfectly recreate the state, or *snapshot*, of the block device at the time that
    the snapshot was taken.'
  prefs: []
  type: TYPE_NORMAL
- en: The obvious risk to this process is that the snapshot, presumably, contains
    *only* the differences between the time that it was taken and the original data.
    So, there is no protection here against system failure. If the original file becomes
    damaged or lost, the snapshot is useless. A snapshot might be an effective protection
    mechanism against accidentally deleting a file, malware, or ransomware at the
    system level because it allows a system to revert to its pre-compromised state
    easily and quickly. This is an important value and early snapshot mechanisms were
    often used expressly as a means of protecting against file deletion and overwrite
    mistakes. But since they are tightly coupled to the original data, the most important
    protections afforded us by true backups are completely missing here.
  prefs: []
  type: TYPE_NORMAL
- en: This risk has led to the mantra of *Snapshots are not backups!* You hear this
    everywhere. And it is true, they are not, on their own. Snapshots are a really
    critical component of backups, though. Overstating the mantra has caused many
    people to incorrectly believe that backups created using a snapshot as a source
    are not real backups.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Snapshots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A snapshot is a generic idea that can be executed in many ways. Two mechanisms
    popular for snapshots are copy on write and redirect on write. Nearly any production
    system that you encounter will utilize one of these two mechanisms. Understanding
    these two give a good insight into the thinking and design of snapshots and explain
    why they can be so powerful.
  prefs: []
  type: TYPE_NORMAL
- en: First, copy on write (sometimes called COW Snapshots.) When a copy on write
    snapshot is initiated, a block device is essentially frozen in time. Mostly this
    is theoretical because nothing actually happens, and until there is an attempt
    to make a change to the storage, nothing will happen.
  prefs: []
  type: TYPE_NORMAL
- en: When someone attempts to write new data to the block device, at that time the
    storage system takes any block that is about to be written and copies it to a
    new location and then overwrites the original block with the new data. There is
    a performance hit while all this copying is going on, of course, and as changes
    start to add up the size of the copied data will grow. So while the snapshot is
    literally of zero size when we first initiate it, it keeps growing as long as
    we keep writing to the block device.
  prefs: []
  type: TYPE_NORMAL
- en: Copy on write snapshots are popular because they have so little penalty to being
    destroyed. To delete the snapshot all that has to be done is the extra data be
    deleted. The working block storage is untouched in all of this. Even a process
    monitoring the original block device would never know that there was a snapshot
    somewhere because everything related to the snapshot is external to the original
    block device. So there is basically no penalty to cleaning up the snapshot when
    it is no longer needed.
  prefs: []
  type: TYPE_NORMAL
- en: The alternative approach, redirect on write, works a bit differently by manipulating
    points. These points point to all of the block locations of a storage device.
    When a block is changed by writing new data to it, the system does not modify
    the original block at all, but rather writes the new data in a new location and
    simply points that blocks pointer at the new location instead. In this way there
    is essentially no impact to any given write operation. The impact is not zero,
    but it is extremely low, especially compared to copy on write which requires moving
    existing data around and rewriting it during normal operations.
  prefs: []
  type: TYPE_NORMAL
- en: With redirect on write we get some great features like the ability to maintain
    essentially indefinite versions of our storage device. In fact there are storage
    devices that simply use redirect on write for all operations and do not think
    of changes as snapshots but treat the entire storage system as an eternal snapshotting
    mechanism so that any portion of the storage can be rolled back to literally any
    point in time.
  prefs: []
  type: TYPE_NORMAL
- en: The caveat of redirect on write, because it sounds pretty perfect at first glance,
    is that you still have a growing amount of data over time and if you keep interim
    versions of data, rather than just a single point in time, the degree of growth
    can end up rather staggering. If you then need to clean that up the process of
    cleaning up the system of pointers can get somewhat complex and has a performance
    penalty.
  prefs: []
  type: TYPE_NORMAL
- en: Copy on write tends to be the best choice when we are talking about short term
    snapshots that are being taken, for example, just before a major system update
    is performed and the need to roll back to the time just before the update is critical,
    or a snapshot is taken in order to send data externally in a backup operation.
    At the end of either of these tasks, the snapshot would be destroyed and forgotten
    about. Copy on write is all about creating and destroying the entire snapshot
    quickly, not keeping it around, because the penalties of copy on write will exist
    only for a short time and the destruction process is painless.
  prefs: []
  type: TYPE_NORMAL
- en: Redirect on write is really powerful when we intend to keep the snapshot around
    for a long time and when we want multiple snapshots that build off of one another.
    Because there is almost no penalty during use and only a real penalty during the
    destruction of the snapshot(s) it plays the opposite role of copy on write.
  prefs: []
  type: TYPE_NORMAL
- en: 'So: redirect on write is probably the best choice for using snapshot as ongoing
    data protection and copy on write is typically what is used under the hood for
    things like backup software to originate a dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Even if we do not have any special backup software, we can use a snapshot taken
    by our storage system and use it as the building block of a meaningful, manual
    backup. This is far simpler than it sounds. In the simplest of examples, assuming
    that we have something like a mounted tape drive on the same system to write to,
    we simply use the storage systems ability to mount the snapshot as an immutable
    version of the block device and copy that to the tape.
  prefs: []
  type: TYPE_NORMAL
- en: When we do this, the source snapshot file might be absolutely tiny, or theoretically
    even zero bytes, but what is sent to the tape (or any other storage media) is
    a complete copy of the entire block storage device in the state that it was at
    the time that the snapshot was taken. This is the miracle of snapshot technology.
    The automatic and generally completely transparent recreation or rehydration of
    the original block device state while using minimal, often trivial, additional
    storage space.
  prefs: []
  type: TYPE_NORMAL
- en: When we do this, we have the semantic challenge of what do we call the copy
    that is no longer on the original media. It is an image, of course, but typically
    we still refer to it as a snapshot as it is identical to the snapshot in every
    way and represents a snapshot of the block device at a point in time. So, when
    we do this, the statement that snapshots are not backups becomes false because
    we can have a true backup that is also a snapshot. The correct statement would
    be that a snapshot is not typically a backup. Snapshots do often get used as backups,
    however, and even more often as the basis of backups.
  prefs: []
  type: TYPE_NORMAL
- en: Because snapshots provide the ability to freeze the block device in a moment
    (or more than one moment) in time and then provide a way to utilize that frozen
    moment without having to interrupt the block device for future options it is perfect
    as a means of creating a source from which to take a backup while allowing the
    running system to continue on its merry way. It must be noted, though, that traditional
    snapshots all happen on top of the same underlying physical block device. So,
    while we do not need to halt the storage device in order to perform snapshot operations,
    we do use IOPS (input/output operations per second) from the original device.
    Our operations are not zero overhead, magically, but they are much lower overhead
    than other backup mechanisms will tend to be.
  prefs: []
  type: TYPE_NORMAL
- en: Because of all of this, snapshots are a popular tool to use under the hood and
    behind the scenes to make many modern backups possible. Essentially every hypervisor
    or storage device level backup (platform backup) is powered by snapshots. In many
    cases, even system level (so called agent based backups inside of the operating
    system) use snapshots today, across all operating systems, not only Linux-based
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: Archives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Closely related to, but importantly different from, backups are archives. These
    two concepts are very often confused for each other, even in very enterprise circles.
    In theory, just by saying the names and asking someone, anyone, to describe what
    they think of a backup and an archive is enough to get someone to self-describe
    why they are not the same thing. But taking *I can define it* and moving that
    to *I can clearly articulate and internalize that the two are different* is not
    always automatic.
  prefs: []
  type: TYPE_NORMAL
- en: A backup, which we will define more in a moment, is a copy at a minimum. That
    much should be clear. Our English use of the term denotes this. We refer to things
    as our *backup copy*. If you articulate it well, everyone always agrees that if
    it is not a copy, then it is not a backup.
  prefs: []
  type: TYPE_NORMAL
- en: An archive is different. Archiving something does not suggest that there is
    not a copy, we sometimes even say things like *archival copy*, but nothing in
    the definition of archive is it suggested that a copy is required. An archive
    refers to long term storage, often assumed to be either lower cost, harder to
    access, or otherwise *archival*. Long term storage, but not necessarily close
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Archival storage could simply be a second hard drive where data does not change.
    Maybe it is offline tape. Maybe it is cold cloud storage. Archival storage does
    not mean, necessarily, that the performance or accessibility of the storage is
    less than regular storage would be, but it is very common for it to be.
  prefs: []
  type: TYPE_NORMAL
- en: Most organizations use archival storage as a lower cost means of maintaining
    emergency or occasional access to data that is no longer needed on a regular basis.
    It is easy to talk about potential scenarios for this. Old data could be last
    year's financial records, copies of old receipts, video footage from material
    already processed, old invoices, old meeting minutes, blueprints from completed
    projects, old project artefacts, you get the picture. Businesses produce enormous
    amounts of data that they never anticipate needing to use again, but cannot necessarily
    delete entirely.
  prefs: []
  type: TYPE_NORMAL
- en: The assumption is that an archive, if it contains data of any importance, also
    needs to be backed up just like any other storage. The rule of thumb is that anything
    worth storing is worth backing up, if you feel that the cost or effort of taking
    a backup is too much then you should carefully reevaluate the desire to continue
    to store the data at all. Why continue to pay for its storage?
  prefs: []
  type: TYPE_NORMAL
- en: In a properly planned storage infrastructure this is exactly what happens and
    archives are a powerful mechanism for data retention and protection when properly
    combined with a backup. As the term archive means that the data is not changing
    it means that there should never be a lock on the data and backups are as simple
    as can be. None of the complexities that face the backup process exist when dealing
    with an archive. The need to take backups often also does not exist; a single
    backup might be all that is needed if the archive is static. Or if the archive
    is only rarely changed, maybe a monthly or annual backup might be adequate.
  prefs: []
  type: TYPE_NORMAL
- en: Converting backups to archives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A not uncommon thing to have happen is for a backup to become an archive. This
    happens far more frequently than you might imagine and happens for reasons that
    most of us can identify with. I have seen it happen in the largest of organizations
    with no policy to avoid it.
  prefs: []
  type: TYPE_NORMAL
- en: Under normal circumstances, all data that you want to store is live and in its
    proper location for use and available normally. A backup is taken of this data
    and, in case of disaster, the system can be restored. until there is a disaster
    all of the data exists both in the original location as well as in one (or more)
    backup location(s). This redundancy of locations is what makes the copies into
    backups. Technically, if the original storage location fails, the primary backup
    (which might be the only backup) stops being a backup and turns into the temporarily
    primary source location of the data. We never speak of it in this way as we all
    know what we mean by *restoring from backup*, but for that period of time when
    the original storage is gone, the first backup is now the master of that data
    and not a backup any more. It may have backups of itself, and generally we would,
    but you can only have a backup when there is data that is not a backup for the
    backup to be a backup of! Hard to explain, but a critical concept.
  prefs: []
  type: TYPE_NORMAL
- en: The reason that this matters is because once you take a backup of data, it can
    be very easy to feel that our backup protects us and that we can then delete the
    original source data. It feels like we can, because there is a backup already.
    The problem here is that if we delete the original data, the backup (or at least
    the first backup) stops being a backup and instead turns into an archive! And
    in most cases, if the assumed master source location is no longer there, there
    may be no mechanism to take any further backups of this archived data. There may
    not even be any means of locating it.
  prefs: []
  type: TYPE_NORMAL
- en: I have seen this myself in the real world. A daily backup task would run and
    end users just assumed that they did not need to store any data that they needed,
    even though it was a legal requirement to keep it and to keep it backed up, on
    the primary systems and so would delete the data immediately upon creation assuming
    that their data was protected by the backup system. There were some critical flaws
    in this plan, however.
  prefs: []
  type: TYPE_NORMAL
- en: First, by deleting the original, any backups that existed became the source
    location rather than a backup and were simply a tape-based archive. Given that
    there was a legal requirement for the data to be retained and backed up, this
    violated the legal retention requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the backup mechanism was staggered where some backups are kept for years,
    some for months, and some for weeks. So if the data happen to exist during a run
    that was kept for years there was a good chance that the one, singular tape that
    contained the files in question might be readable to able to retrieve the data,
    but if the data existed only during a backup run for a weekly job, even the archival
    version would be deleted in just a few weeks when that tape was overwritten.
  prefs: []
  type: TYPE_NORMAL
- en: Third, and unrelated to the archival situation, often the data was deleted before
    the daily backup job ran at all meaning that the data was deleted with no backup
    or archive ever existing and so was lost instantly. The result was that no backups
    ever existed, and once in a while a file would get lucky and be retained for a
    few years without backup, but most files were either never archived at all or
    were archived only briefly and deleted in a few weeks when tapes were reused.
  prefs: []
  type: TYPE_NORMAL
- en: Given that the requirement was seven years of retention, plus a backup of that
    retention, it is easy to see how far off the mark the process was simply because
    the end users thought that they could intentionally delete the original files
    because they thought a magic backup process was somehow protecting them. The backup
    team thought that all data was being kept live for seven years and that if any
    file was lost or corrupt that the end users would alert the backup team to restore
    it almost immediately. That the end users would themselves intentionally destroy
    the data and never alert the backup team that the data had been destroyed was
    never considered in the workflow, because why would it be?
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to see how we can, through actions of the end users in many cases,
    accidentally convert our backup location into an archival location and potentially
    lose data because we do not understand the complexities of the backup processes.
    If data is not going to be retained permanently in a primary location, then very
    complex processes are often needed to ensure a safe workflow for deletion.
  prefs: []
  type: TYPE_NORMAL
- en: Archives are a powerful mechanism to lower cost and keep our mainline storage
    lean. Maybe we use archives only to reduce cost. In many cases, by keeping excess
    data away from our top tier of storage, it will allow us to invest in faster,
    but smaller systems for the data that we use every day.
  prefs: []
  type: TYPE_NORMAL
- en: Archives can apply to portions of a file as well, of course. Sometimes even
    databases use a mix of storage locations to allow them to be able to store tables
    or portions of tables that are accessed regularly on the fastest storage, while
    rarely touched tables or parts of tables can be kept on slower storage that costs
    less. This could be automatic inside of a database engine, or an application might
    use multiple database systems and move data between them at the application layer,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: Archives are a useful tool, but in no way a substitute for backups.
  prefs: []
  type: TYPE_NORMAL
- en: Backups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is hard to believe that we have gone this far without actually digging into
    exactly what we mean when we say *backup*. As with many things, it is hard to
    sometimes jump directly into a definition as there is just so much that we have
    to consider.
  prefs: []
  type: TYPE_NORMAL
- en: At the most basic, a backup is a copy of an original set of data. We talked
    about this above. If the data does not become redundant, then it cannot be a backup.
    This is the most obvious piece of the puzzle.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the data must be stored twice. Meaning the hardware must exist more than
    once.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases we can do a comparison with paper or some other physical form
    of data storage. If we have a piece of paper with an important code on it, we
    feel a sense of urgency to copy that data somewhere - to make a backup. We know
    how easy it is to smudge, burn, lose, or send a piece of paper through the wash.
  prefs: []
  type: TYPE_NORMAL
- en: Probably the most confusing piece of a backup requirement is that it be strongly
    decoupled from the original material. Being strongly decoupled is a term that
    involves some amount of opinion that makes determining appropriate levels of decoupling
    a little bit hard. What is decoupled enough for one organization may also not
    be enough for another.
  prefs: []
  type: TYPE_NORMAL
- en: When we use the term tightly coupled, we are referring to situations where the
    original data, and the copy of the data, have a connection between them. At the
    most tightly coupled is something like the snapshot concept where anything that
    happens to the original file will ruin the snapshot a well, in all cases. That
    is fully coupled. Slightly less coupled would be something like storing the copy
    on the same physical device. The farther separated the copy becomes from the original
    data, the less coupled it is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coupling can involve more than physical connections. It can also include concepts
    such as being stored on systems that share credentials like usernames and passwords.
    Coupling can be complex and there is no single, clear-cut way to describe it.
    When we talk about keeping backups heavily decoupled, though, we generally means
    a few things such as: completely disparate hardware and media, physically separate
    location, and disconnected authentication.'
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind every additional step of decoupling is to keep any event that
    might happen to the original data to have little to no chance of also happening
    to the backup data. This could be an accidental file deletion, a failing hardware
    storage device, a ransomware attack, flood, or fire. There are so many ways to
    lose our original data, or to lose access to it. We have to consider these possibilities
    as broadly as possible and assess how much decoupling, and what type, make sense.
    No two systems are ever truly decoupled completely, but we can decouple to a practical
    level quite easily.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is easy and very few organizations make the mistake of attempting
    to put a backup file onto the same media as the original data. It does happen,
    however. Using the same drive happens very rarely, but attempting to use a second
    drive inside of the same device is sadly somewhat common. Of course, as you can
    imagine, many events that would cause data to be lost on one drive in a computer
    can cause data on another drive to be lost. A fire inside the chassis, extreme
    shock, flooding, loss of power, theft, data corruption caused by many types of
    events, or malicious attack are all likely to destroy a backup simultaneously
    with the original data. This defeats the purpose of the backup almost entirely.
  prefs: []
  type: TYPE_NORMAL
- en: To compare to the physical world, we can compare to paper. You have a piece
    of paper on which you are storing critical data. You keep that piece of paper
    in a filing cabinet. You are worried about protecting that paper from something
    bad happening. You can use a single piece of paper and write the information on
    it twice. You can have a second piece of paper in the same folder in the same
    filing cabinet with the data duplicated. You can put a copy on a separate piece
    of paper in a second filing cabinet sitting next to the first one. Or you can
    put a copy of the paper in a separate filing cabinet in a different building.
  prefs: []
  type: TYPE_NORMAL
- en: In these examples, it is easy to see the progression from tightly coupled to
    highly decoupled. When sharing a single piece of paper, or two papers in the same
    filing cabinet, the risk is obvious. Almost anything that would hurt the first
    copy would damage the second. Having paper in a second filing cabinet at least
    gives a modicum of protection that there might be two different keys for the two
    cabinets, a cabinet falling into water *might* not affect the other, file damage
    to one *might* not affect the other, and so forth, but it does not take much to
    see that their risks are still coupled, just not as closely.
  prefs: []
  type: TYPE_NORMAL
- en: By putting a filing cabinet into a completely different building, possibly on
    a different floor or even in a different town, then having a flood, fire, or theft
    that is able to destroy both the original and the backup at the same time is extremely
    unlikely. There is also the chance of a volcanic event, nuclear war, or meteor
    strike that could still destroy both copies even if located miles apart. This
    is why we say strong decoupled, but never totally decoupled.
  prefs: []
  type: TYPE_NORMAL
- en: With our backups we have to consider just how decoupled our data is, and how
    much it costs to decouple it further. We also have to consider the efficacy of
    decoupling and how that impacts our business. It is a complex question. For most
    businesses, though, we will want our backups to have physical distance, a variety
    of media, a separation of authentication, and multiple copies. What our backups
    contain and how they will be used play into this heavily.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no simple best practice here, even common rules of thumb rarely apply
    outside of the extreme basics. Best practices dictate:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Best Practice: If data is worth storing, it is worth backing up.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Best Practice: A backup needs to be highly decoupled from the original data.'
  prefs: []
  type: TYPE_NORMAL
- en: The range of possibilities with backups are just so broad that we really must
    evaluate the needs, across the board, for every business and, in many cases, individual
    workloads. In some cases, such as backups of entire operating systems, our primary
    concern may be around rapid recovery in case of hardware failure and the actual
    contents of the backup may be trivial. In another case our backup may contain
    large amounts of highly proprietary data and keeping that backup safe and secure,
    ensuring that it cannot fall into the wrong hands, takes precedence.
  prefs: []
  type: TYPE_NORMAL
- en: Why tape still matters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Inevitably when talking about backups, the discussion of tape media surfaces.
    Generally, you get one of two responses: tape is amazing and I always use it or
    tape is dead. Wildly disparate opinions.'
  prefs: []
  type: TYPE_NORMAL
- en: Once upon a time, tape was essentially the only backup media option. Solid state
    drives did not exist yet and hard drives were outrageously expensive per megabyte
    and had a terrible shelf life and shock capabilities. Tape was the only affordable
    and durable media option, and even it was not very good at the time.
  prefs: []
  type: TYPE_NORMAL
- en: In the decades since, everything has changed, as it often does. Hard drives,
    solid state drives, and services that host these for you have all become very
    standard. The biggest factors driving us to tape are gone, we have many options
    that are all viable. This has led many people to focus on these newer options
    and not keep up with the advancements in tape, but just like all of the other
    technologies, tape has advanced too and quite significantly so.
  prefs: []
  type: TYPE_NORMAL
- en: Tape is a media almost purpose built for backups. It can move linear data at
    an extremely high rate, is very low cost per megabyte stored, and has incredible
    shelf durability and shock handling. Tape naturally, unless you leave the tape
    in the drive, becomes decoupled from the original system as simply as by just
    ejecting the tape from the drive. You can even have a remote tape system with
    automated tape ejection to make both technical and physical decoupling a completely
    automated process!
  prefs: []
  type: TYPE_NORMAL
- en: Tape carries the benefit of allowing each tape to be at least partially decoupled
    from each other. Tapes can be stored in different locations and multiple tapes
    can be used for multiple copies of data. In some cases, groups of tapes might
    be stored together in a single box, effectively *re-coupling* the tapes to some
    degree. If tapes are stored apart from each other, though, dramatic decoupling
    is possible not just between the source data and the backup, but within the backup(s)
    itself!
  prefs: []
  type: TYPE_NORMAL
- en: Tape is not perfect. It requires physically mounting the tape before being able
    to restore and it is terrible at locating single files buried deep within a backup.
    Tape shines at backing up or restoring large quantities of continuous data, but
    once you start to search for specific data the performance declines quickly. Some
    companies address the human component of tape management by employing robots and
    tape libraries, but this effectively puts the tapes back *online* and potentially
    re-couples them to the original data, at least partially, taking away one of their
    layers of protection. Straight tape, without a robot or library, has the benefit
    of needing to hack a human, on top of hacking computers, to be able to destroy
    the original data and the backup at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Tape is useful enough that even some online cloud backup providers use tape
    as a component of their storage solutions. Tape has an important place not just
    in the modern world, but in the foreseeable one.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, backups may not be a single solution for a workload. A single workload
    or system might need multiple backups taken of it. One backup that is kept locally
    and moderately coupled that can be used for rapid restores in the event of an
    accident or hard drive failure. Remote backup to tape or immutable cloud storage
    for long term, highly decoupled data retention in the event of ransomware or total
    site loss.
  prefs: []
  type: TYPE_NORMAL
- en: Decoupling is so critical to a backup being functional or useful that we must
    include it in our definition of backups, even if we cannot absolutely clearly
    define what constitutes a significant enough level of decoupling. This is because
    what is adequate for one organization or situation may not be for another. For
    me, this means that our definition of decoupling has to be subjective. Stakeholders
    need to define what a backup is to protect against and define the necessary decoupling
    from there.
  prefs: []
  type: TYPE_NORMAL
- en: It should not be glossed over that modern ransomware has become a driving force
    in organizations beginning to analyze their traditional levels of backup coupling
    because suddenly the reach and threat of backups having any real level of coupling
    is dramatic. Ransomware techniques, at the time of writing, aggressively include
    strategies to ransom backups themselves whenever possible and techniques to hide
    their activities to thwart the ability of backups to protect against system encryptions.
    Backups remain the best defense against such threats, but ensuring extreme levels
    of decoupling, often requiring techniques like immutable storage and physically
    taking backups offline so that no computer can reach them without human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: If you cannot recover, it was not a backup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I have heard this said so many times and I still love it. Simply, if your backup
    does not work when there is a disaster, was it really a backup at all? To a small
    degree this is overstating the case. A backup can fail and that failure can coincide
    with the moment when an original workload has failed. But this should be statistically
    so unlikely and if it were to happen there should be a coincidence that is outrageously
    obvious.
  prefs: []
  type: TYPE_NORMAL
- en: What many want to express is a need for testing backups. Backups are complex
    and knowing the speed, process, and effectiveness of using the available mechanisms
    is a very critical component to any backup process. If you have never tested your
    systems, you have to assume that they will not work. And even if you have tested
    them, it is best to have tested them recently. Some backup systems even do automatic
    restoration tests to demonstrate the efficacy of the backup every, single time.
  prefs: []
  type: TYPE_NORMAL
- en: Backup tests can be misleading. Like many other data protection mechanisms like
    RAID, redundancy power supplies, or failover SAN controllers the way that we tend
    to test backups in a predictable, pristine environment rarely reflects how a restore
    would be required under emergency conditions. It is common for backup tests to
    be performed when systems are idle or slower than usual and to reflect best case
    scenarios resulting in nearly always passing tests even on systems that would
    almost always fail in a real-world scenario. Consider adverse conditions such
    as actively failing, rather than having completely failed, systems, heavy load
    during operations, or an inconsistent data state (crash consistency) that will
    not be reflective during a test scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Backups may seem like a simple thing and every backup vendor is going to present
    their product as eliminating the need for you to understand your data. Vendors
    hope for your blind trust that they will do the impossible and offer you a chance
    to open your wallet and simply hope for the best. As system administrators it
    is our task to understand our data, know how our backup mechanisms work, determine
    the consistency and coupling needs of our organization and workloads, and to assemble
    a backup solution that meets or beats those needs.
  prefs: []
  type: TYPE_NORMAL
- en: Disaster recovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ultimately the purpose of any backup mechanism is to enable disaster recovery.
    Disaster recovery is something that we all hope that we will never have to do,
    and yet is the most important moment in most of our careers. Disaster scenarios
    are when we earn our keep more than at any other time. Your ability to perform
    calmly and coolly when a disaster is striking, to be ready with the knowledge
    of how to get your workloads back online, and being able to adjust to whatever
    twist or surprise is thrown at you is key. Performance during a disaster can mean
    salary differences of hundreds of percents.
  prefs: []
  type: TYPE_NORMAL
- en: As we have dug through the many types and approaches to backups, it should be
    natural that there are now many ways to recover as well. When planning for disaster
    recovery we really must take all of this into consideration as our planning process
    will very so greatly.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use image-based backup methods, then the assumption is that we will approach
    restore processes by restoring an entire block device image (or images) all at
    once. This has some significant advantages during disaster recovery because we
    only have a single step: restore the entire system. In some cases, our restore
    mechanisms will automatically restore all systems at once, not just a single one!
    This is a very alluring prospect. The caveats to this method are that restores
    are typically slower and generally only crash consistent. Doing high speed restores
    with the utmost of reliability is the most challenging for this method.'
  prefs: []
  type: TYPE_NORMAL
- en: Using file-based backups we need to, in almost all cases, first restore a blank
    operating system, either a template or a vanilla build, and then restores individual
    files back to it in the place where they originally went. This method is theoretically
    faster than the image-base complete restore, but in practice rarely is because
    the time to build the base operating system from traditional methods. While theoretically
    you could build the base system from modern methods, this generally does not happen
    because if you were to do so you would naturally move on to DevOps style backups.
    However, these start to overlap conceptually here and you can use a file-based
    backup mechanism in conjunction with traditional file restores. The change would,
    of course, be that the file backups would be isolated to the meaningful system
    data rather than a blind backup of anything and everything.
  prefs: []
  type: TYPE_NORMAL
- en: And lastly, a full DevOps style restore. This is more complex as there is no
    clear single definition. The assumption here is that rebuilds of the base operating
    system will be nearly instant because of any number of automated build mechanisms.
    And then that the data restore will be heavily minimized so that it, too, can
    happen at great speed.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, all of this will be going on in conjunction with triage operations
    that we will discuss shortly. Planning and timing these operations so that they
    are well known, that processes are tested, and restore times are predictable under
    different scenarios provides invaluable information that will be needed during
    a disaster scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Disaster recovery planning should be more than just individual workloads being
    tested in isolation, and it should be more than testing only data loss. Different
    organizations have different risks and testing multiple types of scenarios is
    important. We think of data restores when we think of disaster recovery, but that
    might not be what we are facing. If we have workloads running in multiple locations,
    we might be looking at how to work from a slower connection to a less than ideal
    data center location. A disaster recovery test might involve testing the ability
    to spin up workloads on backup systems, to failover clusters, or to run from a
    secondary data center.
  prefs: []
  type: TYPE_NORMAL
- en: The best practice in disaster recovery is to always test your scenarios and
    to test them regularly. Never blindly trust that our backups, our planning, our
    networking or data centers will work. We need to test not only the technology
    behind our plans, but the procedures of those plans as well.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have discussed mechanisms and terms in and around backups, it is
    time to really look at how the modern world of DevOps can redefine how our backups
    can work.
  prefs: []
  type: TYPE_NORMAL
- en: Backups in a DevOps world
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In earlier sections of this book, we have talked about modern concepts impacting
    the world of system administration such as DevOps and infrastructure as code.
    You may be wondering if these modern concepts have a potential impact on the worlds
    of backups and disaster recovery. Good question! And if the section title has
    not given away the answer, I will clue you in now: yes, yes they do!'
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally we think of restoring data as either the very old fashioned way
    of just restoring individual files, or the more modern (think last two decades)
    way of restoring entire systems including the operating system and all of the
    files that go with it. We are so accustomed to thinking of restoring systems in
    this way that it is often very hard to think about the problem in any other context.
  prefs: []
  type: TYPE_NORMAL
- en: In the ultra-modern DevOps style world where systems are built via automation
    and defined in code or configuration files we have to start to think about nearly
    everything in new contexts. When systems can be automatically built easily and
    rapidly through standard, non-disaster processes, the need to restore those systems
    rather than starting fresh completely vanishes. Imagine if instead of fixing a
    car after an accident if for less money, and less time, and more reliably you
    could have a brand new, but identical, copy delivered to your door - you would
    never waste time trying to restore something broken when a pristine, perfect copy
    can be had.
  prefs: []
  type: TYPE_NORMAL
- en: Version control systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When talking about backups, especially as they relate to DevOps, we should also
    talk about version (or revision) control systems. Version control systems, like
    GIT, Mercurial, or Subversion, are not themselves backup systems, but act as mechanisms
    to sync some of the most important data on a system to another location, where
    backups will often occur.
  prefs: []
  type: TYPE_NORMAL
- en: When talking about version control in the context of backups, it can be a bit
    confusion as in some cases we might be looking at our operating system as being
    a master location and a version control system can be used to replicate configuration
    files to another location and from there, they can be simply backed up using any
    number of normal mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: Because version control systems do not only store current data but also historical
    versions and changes to files, they become essentially immutable and therefore
    useful in a backup style situation since the backup mechanism does not need to
    deal with versions over time. The version control system holds all versions and
    changes to the files over their history. So a backup of the entire version control
    system automatically includes all of the changes to the files. Because of this,
    end users accidentally deleting files, bad changes to files, ransomware attacks
    on data all become moot as the ability to roll back to just before a bad change
    was made is built in.
  prefs: []
  type: TYPE_NORMAL
- en: Version control systems typically can be used to replicate files (and their
    version histories) not only between one end point and a version control server,
    but also to many additional end points. For example, a system administration workstation
    or jump box (which we will describe in the following chapter) might contain a
    full copy of the configuration files for every Linux system in a company separate
    from any server or backup system. Even if every official copy of the data was
    lost due to enormous catastrophe, a single workstation may be able to recreate
    the entire set of documentation.
  prefs: []
  type: TYPE_NORMAL
- en: These types of systems are already in wide use for non-backup related reasons
    and are considered absolute *must have* tools in the DevOps world. We may already
    be using them in places for system administration tasks. Even in systems that
    do not have a DevOps process, these tools can potential be used as if they were
    as far as data protection is concerned.
  prefs: []
  type: TYPE_NORMAL
- en: Even the most traditional (meaning as far from DevOps as you can get) Linux
    system can benefit from the user of version control for its configuration files.
    Organizations do not even need to stand up their own infrastructures for version
    control if it does not make sense for them to do so. Vendors such as Gitlab and
    Microsoft via GitHub provide enterprise hosted version control systems for free
    with extensive features and access control systems.
  prefs: []
  type: TYPE_NORMAL
- en: A small company with legacy style systems that wanted to embrace version control
    protection could, in a contrived example, simply got under the `/etc` file system
    and add it to a remote GIT repository and do a push. Voila, all of the data is
    protected, that quickly and easily. Set a `cron` job to run hours to push any
    additional changes and you have automated a robust backup system with essentially
    zero effort and definitely zero cost.
  prefs: []
  type: TYPE_NORMAL
- en: Version control systems are one of those poorly kept secrets of the development
    world that have leaked into and become embraced by many other professions. Any
    system that works heavily with text files should be jumping onto the version control
    bandwagon as quickly as possible. Version control is one of the simplest ways
    to take data protection to the next level. This approach carries far more advantages
    than that simple example might imply, however, restoring a system carries a non-trivial
    amount of risk that there is corruption or worse, an infection or root kit, that
    gets restored. A clean start removes those risks giving you the peace of mind
    of starting fresh. Likely the fastest path to a working system, performed in the
    most repeatable and predictable way, with the lowest risk. This approach is also
    the easiest to test. You naturally test at least part of this approach every time
    you build a new server whether for production, testing, staging, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: When we build our base operating systems, install applications, and deploy configurations
    via DevOps-style automation we leave ourselves with only our system specific data
    that needs to be restored from backup systems, and even that only part of the
    time. If you consider a typical multi-tiered application, data that is unique
    and cannot be pulled from the initial build process is generally limited to database
    files or limited file storage. In an application running on multiple operating
    system instances, we often expect this data to only exist on one layer of those
    nodes, generally the database. If that node is replicated, we generally only need
    to take a backup of the data from a single node in the cluster (because we often
    have all of the data replicated between all nodes.) Through this process we eliminate
    many points of backup, many types of backups, and identify only the actual data
    that may or may not be at risk.
  prefs: []
  type: TYPE_NORMAL
- en: Every environment varies and in one we may find that data that requires specific
    protection to account for only one percent of all data, in another it might be
    ninety nine. We cannot say with any certainty what you will be expected to find
    in the real world as every organization and workload is so different. Universally
    the ability to limit the scale of backups, and therefore restores, brings advantages.
    The smaller the backup means the faster and less impactful the backup will be.
    The smaller the backup size the lower the cost to store it. Smaller means less
    to verify against corruption. The same smaller size that reduces backup time also
    reduces restore time in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: In our multi-tiered example, we may need to rebuild three nodes to get our application
    back up and running. The top-level load balancing and proxy layer will be assumed
    to have no unique data and can be *restored* via the already tested build process
    that built it initially. The application layer should likewise have no unique
    data and be able to be automatically restored using the already tested build process.
  prefs: []
  type: TYPE_NORMAL
- en: Our last standard layer, the database, should again, be built with all configuration
    and applications being deployed completely using the build process. At this point,
    all of the layers of our application, all of the configuration for it, have all
    been restored and our backup and restore mechanisms have not even come into play.
    The only piece missing at this stage is the restoration of the latest data in
    the database. The database is in place, but not the data that goes inside of it.
    It is now that our restore process kicks off and puts that data back where it
    belongs. This might be a simple file copy from a remote location to the database
    location with a restart of the database after the files are in place. Or maybe
    a database-specific restore operation has to happen on the data to ingest it again.
    In any case, the restore is of a minimal amount of data, of very limited types.
  prefs: []
  type: TYPE_NORMAL
- en: This approach changes everything that we traditionally think about backups.
    It changes how quickly and how impactful backup operations are, it changes the
    tools that we need to use and potentially eliminates them altogether, it makes
    restores fast and even, potentially, fully automated!
  prefs: []
  type: TYPE_NORMAL
- en: If we go back to our example case and assume we are using the popular MySQL
    application as our database platform, and that all of our necessary data is stored
    in this one spot which is a reasonable assumption for many common workload designs,
    our need to use special backup tools likely does not exist. Nor do we need to
    rely on complicated or risky mechanisms like snapshots. We can use built-in tools
    to the database platform to reliably get a complete dataset from the workload
    layer (we hesitate to say application layer as that is a software engineering
    term that would be different here from the systems term) where we know that the
    workload, with all of the necessary intelligence to do so, was able to stabilize
    and lock the data during the backup operation so that our backup is safely *application*
    *consistent*.
  prefs: []
  type: TYPE_NORMAL
- en: When we step back and start ignoring convention, and we stop focusing on simply
    *buying a solution* but instead put on our IT hats and determine how best to protect
    our environment we can often find ways to both protect our organizations and to
    save money all at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: IT provides solutions, vendors sell components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Broader than backups, and even broader than systems administration, the concept
    of where solutions come from is fundamentally something that we need to understand
    to do our jobs well at any level in IT. At its core, it is ITs job, and no one
    else's, to produce solutions for our business. In many cases we will need to turn
    to vendors to supply one or more components of any solution. IT is hardly going
    to fabricate their own CPUs and assemble parts to make servers or write their
    own operating systems and so vendors (which could, in this context, include those
    who do not get paid but provide things like open-source software including your
    Linux based operating system itself) are a necessary part of the solution process.
  prefs: []
  type: TYPE_NORMAL
- en: Vendors are not solution providers (although a great many will call themselves
    that as a marketing name, of course) but rather sales organizations. Their role
    is to provide access to tools that will hopefully benefit us in the pursuit of
    a solution. It is IT that determines which tools are right to use, selects them,
    and uses them to assemble the final solution to meet the needs of the business.
    IT, which is a department that provides solutions, is something we do, not something
    we buy. We cannot simply depend on a vendor to sell us a product that will protect
    us; it is not that simple. We have to understand the business need, the workload,
    the backup products, the approach and put all of this information together to
    make a cohesive solution that works for us. Every business is unique, every solution
    should be as well.
  prefs: []
  type: TYPE_NORMAL
- en: No vendor-supplied tool can take into account all of our unique backup needs
    today, let alone be adaptable for any changes in the future. If you can imagine
    it, many companies are so addicted to the vendor buying process that they attempt
    to adapt their workloads and internal processes (and needs) to fit the needs of
    a product that the vendor wants to sell! This would be like relocating to an undesirable
    house so that your boat salesman can excuse having pushed to sell you a boat when
    a car would have gotten you to work easily from your existing house.
  prefs: []
  type: TYPE_NORMAL
- en: DevOps and similar infrastructure have really exposed the extent to which traditional
    *just buy what a salesman wants you to buy* processes have required businesses
    to adapt to the purchase, rather than choosing to buy what makes sense for the
    business. In previous technology generations the options were so much less broad,
    and the differences so much smaller that it was easy to hide or dismiss the inefficiencies.
    Today that is not possible.
  prefs: []
  type: TYPE_NORMAL
- en: Enhanced backup and recovery processes turn out to be one of the best reasons
    to consider investing in DevOps and infrastructure as code engineering efforts.
    Typically, we can find many reasons to make DevOps attractive, but better backups
    is easily the best benefit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand how backups can be taken with a truly modern infrastructure,
    we are on to our last topic here: triage. Time to move on from taking backups,
    to using them!'
  prefs: []
  type: TYPE_NORMAL
- en: Triage concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Planning is important and prepares you for many eventualities. When disaster
    finally strikes, though, most planning is going to go straight out the window.
    All of your assurances that your backups are good are not going to make you relax,
    end users are going to be panicking, management is going to forget that you have
    to fix things and pull you into meetings, stress is high, and nothing is quite
    as expected from the planning process.
  prefs: []
  type: TYPE_NORMAL
- en: Triage is hard because every workload, time of day, current situation has so
    many dynamic elements. We have to be ready to adjust to anything, and we have
    to get our systems back online as quickly as possible.
  prefs: []
  type: TYPE_NORMAL
- en: At the moment of a disaster is when things matter most and this is where system
    administrators really prove their mettle. Being prepared for a disaster is relatively
    easy, but staying cool and logical, evaluating the situation in real time, and
    managing the people around you all become unpredictable and very emotional challenges.
  prefs: []
  type: TYPE_NORMAL
- en: There is no simple guide to triage and not everyone is going to be good at it.
    The more we are prepared ahead of time, the more we understand the entire environment,
    and the better we know the business environment that we are working in the more
    adaptable we are going to be.
  prefs: []
  type: TYPE_NORMAL
- en: Triage is a skill best handled by a perceiver, in the Myers-Briggs chart. As
    such this is where administration, over engineering, really shines to its most
    extreme.
  prefs: []
  type: TYPE_NORMAL
- en: When analyzing an outage our first step is to evaluate the situation and determine
    the extent of the disaster. Are we only missing services? Is it all services?
    Is there data loss? What is the current, ongoing, and future expected impact from
    the outage. If we do not have the data at hand, keep people busy gathering that
    data for you.
  prefs: []
  type: TYPE_NORMAL
- en: Triage is especially needed to assist the business in understanding what it
    can do during the time of an outage. Many businesses panic or have no plan (or
    if they have a plan fail to action it properly.) But business behavior needs to
    be coordinated with technology recovery efforts. As we begin to approach our recovery
    we need to be working to also keep the business working as best as possible. Of
    course, we hope that corporate management will step in and guide operations, based
    on ITs assessments of what impact is and what recovery is likely to look like,
    to make them as efficient as possible. Often IT needs to be ready to provide guidance
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: In any triage operation with have to determine the criticality to our workloads
    as well as the potential for restoration. The most critical workload will still
    take a backseat if it will take weeks to restore when many minimal services can
    be up and running in short order. We need to consider loss of productivity, loss
    of customer confidence, failure to meet contracts, and similar concerns when deciding
    how to approach our disaster recovery.
  prefs: []
  type: TYPE_NORMAL
- en: Recovery planning, even in the moment, needs to be coordinated with the business.
    What can the business do to assist in recovery, what can technology do to enable
    that? With good cooperation different businesses may find many different paths
    at their disposal to make the damage of an outage minimized.
  prefs: []
  type: TYPE_NORMAL
- en: Businesses can take actions as simple as sending staff home to relax to get
    people out of the way and allow IT more freedom to undertake restoration. A staff
    with a surprise vacation day or two might be refreshed and excited to return to
    the office and attempt to at least partially make up for lost time. Instead of
    making the team frustrated that they cannot be productive, why not reward them
    for their hard work?
  prefs: []
  type: TYPE_NORMAL
- en: Shifting communications to systems that are not down is important as well. Can
    staff move to phones if email is down? Email if phones are down? Instant messaging?
    Voice chat through some other platform? Maybe this is a good time to visit customers
    in person!
  prefs: []
  type: TYPE_NORMAL
- en: There is no way to list the countless ways that businesses can leverage an outage.
    What we need is creativity and the freedom to work with the business to help them
    see how they can keep working as best as they can, and allow them to direct us
    in how we can recover in the best way for them.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking about triage operations makes it evidence the importance of concepts
    that we have already discussed such as self-recovering systems, minimized restore
    sizes, and careful planning. It also highlights how important it is to have operations
    and other departments engaged, involved, informed of available plans, and ready
    to assist and coordinate when things fall apart. Good planning makes triaging
    better, but you cannot plan your triage operations. There are too many variables
    to think that we can truly plan for every contingency.
  prefs: []
  type: TYPE_NORMAL
- en: I wish that triage and disaster recovery oversight was something we could teach
    concretely. It is a scary situation and all we can do it make sure that the right
    people with triage and perception mindsets are empowered and at the ready when
    the time comes, have good backups, good restore processes, and as much planning
    as makes sense while having an organization ready to work together to minimize
    impact as a coherent team.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nothing matters like backups. I feel like that is at least the fifth time that
    I have written that in this book, and it is certainly not enough. Today backups
    are more important than they have ever been. We face more disaster scenarios and
    more advanced data loss situations than ever before in our industry. Backups have
    always been and will likely always be our strongest defense against complete failure.
  prefs: []
  type: TYPE_NORMAL
- en: Backups have been changing, quite a lot, in the last several years. The assumptions
    as to how we would approach backups even ten years ago do not readily apply today,
    and yet many organizations still use legacy applications, legacy designs, and
    need to still use legacy backups. So, our job is a complex one and our desire
    for modern backups may be needed to drive towards more modern application designs
    so that we can protect them in a better way.
  prefs: []
  type: TYPE_NORMAL
- en: But now we understand the mechanisms underlying different approaches to backups,
    why we want to consider backing up in different ways, and how we can advance our
    backup practices into the future. Backups are probably the best place for you
    to set yourself apart; nothing matters more and rarely is anything as forgotten
    as much as backups.
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter we are going to look at how users exist and interact in
    Linux systems and how we can approach authentication, remote access, and security.
  prefs: []
  type: TYPE_NORMAL
