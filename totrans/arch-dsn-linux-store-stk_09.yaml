- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Analyzing Physical Storage Performance
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析物理存储性能
- en: '*"*When you have eliminated the impossible, whatever remains, however improbable,
    must be the truth." — Sir Arthur Conan Doyle'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*“当你排除了不可能的事情后，剩下的，无论多么不可能，必须就是事实。”— 亚瑟·柯南·道尔*'
- en: Now that we’re done with understanding the nitty gritty of the storage landscape
    in Linux, we can put that understanding to practical use. I always like to compare
    the I/O stack with the OSI model in networking, where each layer has a dedicated
    function and uses a different data unit for communication. Over the course of
    the *first eight chapters*, we’ve increased our understanding of the layered hierarchy
    of the storage stack and its conceptual model. If you are still following along,
    you may have gained some understanding of how even the most basic requests from
    an application have to navigate through numerous layers before being processed
    by the underlying disks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了 Linux 存储环境的细节，可以将这些理解应用于实际操作。我总是喜欢将 I/O 堆栈与网络中的 OSI 模型进行比较，每一层都有其专门的功能，并使用不同的数据单元进行通信。在*前八章*中，我们已经加深了对存储堆栈分层结构及其概念模型的理解。如果你还在继续跟随，你可能已经理解了即使是最基本的应用请求也需要经过多个层级才能被底层磁盘处理。
- en: Being the good folks that we are, when we work with someone, we can be too willing
    to be captious and tend to enjoy nitpicking. This leads us to the next phase in
    our journey – how do we gauge and measure the performance of our storage? There
    is always going to be a significant performance gap between the compute and storage
    resources, as a disk is orders of magnitude slower than a processor and memory.
    This makes performance analysis a very broad and complex domain. How do you determine,
    how much is too much, and how slow is too slow? A set of values might be perfectly
    suitable for an environment, while the same set would ring alarm bells elsewhere.
    Depending upon workloads, these variables differ in every environment.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 作为好人，我们在与他人合作时，可能过于挑剔，喜欢吹毛求疵。这将引领我们进入下一个阶段——我们如何评估和衡量存储性能？计算资源与存储资源之间总会有显著的性能差距，因为磁盘的速度比处理器和内存慢几个数量级。这使得性能分析成为一个非常广泛且复杂的领域。你如何判断多少才算过多，多少才算过慢？一组数值可能对某个环境非常合适，而在另一个环境中却会引发警报。根据工作负载，这些变量在每个环境中都会有所不同。
- en: There are a lot of tools and tracing mechanisms available in Linux that can
    assist in identifying potential bottlenecks in overall system performance. We’re
    going to keep our focus on the storage subsystem in particular and use these tools
    to get a sense of what is happening behind the scenes. Some of the tools are available
    by default in most Linux distributions, which serve as a good starting point.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Linux 中，有许多工具和跟踪机制可以帮助识别系统性能的潜在瓶颈。我们将重点关注存储子系统，利用这些工具来了解幕后发生的情况。一些工具在大多数 Linux
    发行版中默认提供，可以作为一个很好的起点。
- en: 'Here’s a summary of what we’ll cover in the chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将在本章中讨论的内容概述：
- en: How do we gauge performance?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何评估性能？
- en: Understanding storage topology
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解存储拓扑
- en: Analyzing physical storage
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析物理存储
- en: Using disk I/O analysis tools
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用磁盘 I/O 分析工具
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter is a lot more hands-on and requires prior experience with the
    Linux command line. Most of the readers might already be aware of some of the
    tools and technologies discussed in this chapter. Having basic system administration
    skills will be helpful, as these tools deal with resource monitoring and analysis.
    It would be best to have the required privileges (root or sudo) to run these tools.
    Depending upon the Linux distribution of your choice, you’ll need to install the
    relevant packages. To install `iostat` and `iotop` on Debian/Ubuntu, use the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章更注重实践操作，要求读者有一定的 Linux 命令行经验。大多数读者可能已经熟悉本章讨论的一些工具和技术。具备基本的系统管理技能会有所帮助，因为这些工具涉及资源监控和分析。最好拥有所需的权限（root
    或 sudo）来运行这些工具。根据你选择的 Linux 发行版，你需要安装相关的包。在 Debian/Ubuntu 上安装 `iostat` 和 `iotop`，使用以下命令：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To install `iostat` and `iotop` on Fedora/Red Hat, use the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Fedora/Red Hat 上安装 `iostat` 和 `iotop`，使用以下命令：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To install Performance Co-Pilot, you can refer to the installation instructions
    in their official documentation at the following link: [https://pcp.readthedocs.io/en/latest/HowTos/installation/index.html](https://pcp.readthedocs.io/en/latest/HowTos/installation/index.html).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Performance Co-Pilot，你可以参考他们官方文档中的安装说明，网址如下：[https://pcp.readthedocs.io/en/latest/HowTos/installation/index.html](https://pcp.readthedocs.io/en/latest/HowTos/installation/index.html)。
- en: The usage of these commands is the same on all Linux distributions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令在所有Linux发行版中的使用方法相同。
- en: How do we gauge performance?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们如何评估性能？
- en: There are different lenses through which we can assess the performance of a
    system. A common approach is to equate the overall system performance with the
    speed of the processor. If go back to simpler times when single-processor systems
    were the order of the day and compare them with modern multi-socket, multi-core
    systems, we’ll see that the processor performance has increased by, to put it
    simply, an epic proportion. If we compare the improvement factor for processor
    performance with that of a disk, the processor is a runaway winner.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过不同的视角来评估系统的性能。一种常见的方法是将整体系统性能与处理器的速度等同。如果我们回到单处理器系统普遍存在的简单时代，并与现代的多插槽、多核心系统进行比较，我们会发现处理器的性能已经以一种简单来说是史诗般的速度增长。如果我们将处理器性能的提升与磁盘性能的提升做比较，处理器无疑是遥不可及的赢家。
- en: The response times for storage devices are usually measured in milliseconds.
    For processors and memory, that value is in nanoseconds. This results in a state
    of incongruity between the application requirements and what the underlying storage
    can actually deliver. The performance of the storage subsystem has not progressed
    at the same rate. Therefore, the argument about equating system performance with
    processor performance has faded away. Just like a chain is only as strong as its
    weakest link, the overall system performance is also dependent on its slowest
    component.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 存储设备的响应时间通常以毫秒为单位进行测量。对于处理器和内存，这个值通常是纳秒级别的。这导致了应用程序需求与底层存储实际能够提供的性能之间的矛盾。存储子系统的性能进展速度并没有跟上处理器的脚步。因此，将系统性能等同于处理器性能的观点逐渐消失。就像链条的强度取决于最弱的一环，整个系统的性能也依赖于最慢的组件。
- en: 'Most tools and utilities tend to focus solely on disk performance and do not
    give much insight into the performance of the higher layers. As we’ve discovered
    on this journey, there’s a whole plethora of operations happening behind the scenes
    when an application sends an I/O request to the storage device. Keeping this in
    mind, we will divide our performance analysis into the following two parts:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数工具和实用程序通常只关注磁盘性能，并未深入分析更高层次的性能。正如我们在这个过程中发现的那样，当应用程序向存储设备发送I/O请求时，幕后发生了大量的操作。考虑到这一点，我们将把性能分析分为以下两部分：
- en: An analysis of physical storage
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理存储的分析
- en: An analysis of higher layers in the I/O stack such as filesystems and block
    layer
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对I/O栈中更高层次的分析，如文件系统和块层
- en: For both cases, we’re going to explain the relevant metrics and how they can
    affect performance. An analysis of filesystems and the block layer will be covered
    in [*Chapter 10*](B19430_10.xhtml#_idTextAnchor184). We’ll also see how we can
    check these metrics through the available tools in Linux distributions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们将解释相关的度量标准以及它们如何影响性能。文件系统和块层的分析将在[*第10章*](B19430_10.xhtml#_idTextAnchor184)中进行讨论。我们还将看到如何通过Linux发行版中的可用工具检查这些度量标准。
- en: Understanding storage topology
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解存储拓扑
- en: Most enterprise environments usually contain a mix of the following types of
    storage.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数企业环境通常包含以下几种类型的存储。
- en: '**Direct Attached Storage (DAS)**: This is the most common type of storage
    and is attached directly to a system, such as the hard drive in your laptop. Since
    data center environments need to have a certain level of redundancy at every layer,
    the directly attached storage in enterprise servers consists of several disks
    that are grouped in a RAID configuration to improve performance and data protection.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接附加存储（DAS）**：这是最常见的存储类型，直接连接到系统，例如你笔记本电脑中的硬盘。由于数据中心环境需要在每一层都有一定级别的冗余，企业服务器中的直接附加存储由多个磁盘组成，这些磁盘被组合成RAID配置，以提高性能和数据保护。'
- en: '**Fibre Channel storage area network**: This is a block-level storage protocol
    that makes use of fibre channel technology and allows servers to access storage
    devices. It offers extremely high performance and low response times compared
    to traditional DAS and is used to run mission-critical applications. It is also
    far more expensive than other options, as it requires specialized hardware, such
    as fibre channel adapters, fibre channel switches, and storage arrays.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**光纤通道存储区域网络**：这是一种块级存储协议，利用光纤通道技术，使服务器能够访问存储设备。与传统的DAS相比，它提供了极高的性能和低延迟，并用于运行关键任务应用。由于需要专用硬件，如光纤通道适配器、光纤通道交换机和存储阵列，因此其成本远高于其他选项。'
- en: '**iSCSI SAN**: This is also a block storage protocol that can use the existing
    network infrastructure and allow hosts to access storage devices. iSCSI SANs utilize
    the TCP/IP network as a means to transport SCSI packets between the source and
    target block storage. As it doesn’t make use of a dedicated network such as FC
    SAN, it has a lower performance than FC SAN. However, it is far easier and inexpensive
    to implement iSCSI SAN, as it doesn’t require specialized adapters or switches.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**iSCSI SAN**：这也是一种块级存储协议，可以利用现有的网络基础设施，使主机能够访问存储设备。iSCSI SAN通过TCP/IP网络传输SCSI数据包，实现源端与目标块存储之间的通信。由于它不依赖于专用网络（如FC
    SAN），因此性能低于FC SAN。然而，由于不需要专用适配器或交换机，iSCSI SAN的实施要容易且成本较低。'
- en: '**Network-Attached Storage (NAS)**: NAS is a file-level storage protocol. Like
    iSCSI SANs, NAS arrays also rely on the existing network infrastructure and do
    not require any additional hardware. However, since the storage is accessed through
    file-level mechanisms, the performance is on the lower side. Nevertheless, NAS
    arrays are the most inexpensive of the lot and are usually used to store long-term
    backups.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络附加存储（NAS）**：NAS是一种文件级存储协议。像iSCSI SAN一样，NAS阵列也依赖现有的网络基础设施，不需要额外的硬件。然而，由于存储是通过文件级机制访问的，其性能较低。尽管如此，NAS阵列是最具成本效益的选择，通常用于存储长期备份。'
- en: 'A simplified comparison of these technologies is shown in *Figure 9**.1*. To
    focus solely on the differences involved in accessing each type of storage, the
    additional details in the higher layers have been left out:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术的简化比较见*图 9.1*。为了专注于访问每种存储类型的差异，已省略了高层中的附加细节：
- en: '![Figure 9.1 – The different storage topologies](img/B19430_09_01.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – 不同的存储拓扑结构](img/B19430_09_01.jpg)'
- en: Figure 9.1 – The different storage topologies
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 不同的存储拓扑结构
- en: We’re not going to include fibre switches or any SAN arrays in our discussion.
    However, keep in mind that there are a lot of components involved in accessing
    the different types of storage technologies. Every layer warrants careful examination,
    and as such, you should always have a topology map in mind when diagnosing storage
    environments.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在讨论中不会包括光纤交换机或任何SAN阵列。然而，请记住，访问不同类型存储技术涉及许多组件。每一层都需要仔细检查，因此在诊断存储环境时，您应该始终心中有一张拓扑图。
- en: Analyzing physical storage
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析物理存储
- en: Performance defines how well a disk drive functions when accessing, retrieving,
    or saving data. There are quite a few yardsticks that can help to define the performance
    of the disk subsystem. For those of you who have worked with storage vendors while
    evaluating and purchasing high-end storage arrays, IOPS will be a very familiar
    term. Vendors like to throw this acronym around a lot and cite a storage system’s
    IOPS as one of its main selling points.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 性能定义了磁盘驱动器在访问、检索或保存数据时的表现。有很多指标可以帮助定义磁盘子系统的性能。对于那些在评估和购买高端存储阵列时与存储供应商合作过的人来说，IOPS（每秒输入输出操作数）是一个非常熟悉的术语。供应商通常会频繁提到这个缩写，并将存储系统的IOPS作为其主要卖点之一。
- en: '**Input Output Operations per Second** (**IOPS**) might very well be an entirely
    useless figure, unless it is coupled with other capabilities of a storage system,
    such as the response time, the read and write ratio, throughput, and block size.
    The IOPS figure is usually referred to as *hero numbers*, and it rarely provides
    any insight into the capabilities of the system unless it is coupled with other
    metrics. When you purchase a vehicle, you need to know the intricate details,
    such as its acceleration, fuel economy, and how well it will handle bends and
    corners. You rarely think about its top speed. Similarly, you need to know all
    the capabilities of the storage system.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**每秒输入输出操作数**（**IOPS**）可能是一个完全没有用的数字，除非它与存储系统的其他能力结合使用，比如响应时间、读写比、吞吐量和块大小。IOPS
    通常被称为*英雄数字*，除非与其他指标结合，否则它很少能提供系统能力的任何洞见。当你购买一辆汽车时，你需要了解诸如加速、燃油经济性以及它如何应对弯道等详细信息。你很少会考虑它的最高速度。同样，你也需要了解存储系统的所有能力。'
- en: Keeping our focus on the physical disk, we’ll first define the **time-based
    performance metrics**, since they are the ones that explain how and where time
    is spent. Any time you hear the word **latency** or **delay** while analyzing
    performance, that usually is an indication of *lost time*. It is the time that
    could have been spent while working on something, but instead, it was spent waiting
    for something to happen.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 聚焦于物理磁盘，我们首先定义**基于时间的性能指标**，因为它们是解释时间如何以及在哪里花费的指标。每当你在分析性能时听到**延迟**或**延时**这两个词时，通常是*失去的时间*的一个标志。这是本可以用于处理某些事情的时间，但却花费在等待某些事情发生上。
- en: Understanding disk service time
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解磁盘服务时间
- en: 'Let us first develop an understanding of the time-dependent metrics that we
    need to look for when analyzing physical disks. Once we’ve gained a conceptual
    understanding, we’ll use specific tools to look for potential bottlenecks. The
    following figure represents the most common *time-centered* metrics to gauge disk
    performance:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们了解在分析物理磁盘时需要关注的时间相关指标。一旦我们对这些指标有了概念性的理解，我们将使用具体工具来寻找潜在的瓶颈。下图展示了用于衡量磁盘性能的最常见的*时间中心*指标：
- en: '![Figure 9.2 – Disk service times](img/B19430_09_02.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 磁盘服务时间](img/B19430_09_02.jpg)'
- en: Figure 9.2 – Disk service times
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 磁盘服务时间
- en: It’s important to state here that the aforementioned metrics do not account
    for the time spent going through the kernel’s I/O hierarchy, such as filesystems,
    the block layer, and scheduling. We’re going to take a look at them separately.
    For now, we’re only going to focus on the physical layer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，上述指标并没有考虑到通过内核I/O层级（如文件系统、块层和调度）所花费的时间。我们将分别探讨这些内容。目前，我们只关注物理层。
- en: 'The terms used in *Figure 9**.2* are explained here:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.2* 中使用的术语在这里解释：'
- en: '**I/O wait**: An I/O request can either wait in a queue or be actively served.
    An I/O request is inserted into the disk’s queue before being dispatched for servicing.
    The amount of time spent waiting in the queue is quantified as I/O wait.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**I/O 等待**：I/O 请求可以在队列中等待，也可以被积极处理。I/O 请求在被调度处理之前会先被插入到磁盘的队列中。在队列中等待的时间量被量化为
    I/O 等待时间。'
- en: '**I/O service time**: The I/O service time amounts to the time during which
    the disk controller actively serviced the I/O request. In other words, it is the
    amount of time an I/O request was not waiting in a queue. The servicing time includes
    the following:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**I/O 服务时间**：I/O 服务时间是指磁盘控制器在积极处理 I/O 请求期间的时间。换句话说，它是 I/O 请求未在队列中等待的时间。服务时间包括以下内容：'
- en: The *disk seek time* is the time taken to move the disk read-write head, with
    radial movement, to the specified track.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*磁盘寻道时间*是指将磁盘读写头通过径向运动移动到指定磁道所花费的时间。'
- en: Once the read-write head is placed on the correct track, the platter surface
    rotates to move the exact sector (from where data is to be read from or written
    to) and line it up with the read-write head. The amount of time spent here is
    known as *rotational latency*.
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦读写头放置在正确的磁道上，盘片表面将旋转，将准确的扇区（从中读取或写入数据的区域）对齐到读写头的位置。在此处花费的时间被称为*旋转延迟*。
- en: Once the read-write head is positioned over the correct sector, the actual I/O
    operation is performed. This amounts to the *transfer time*. Transfer time is
    the time taken to transfer data to/from the disk from/to the host system.
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦读写磁头定位到正确的扇区，就会执行实际的I/O操作。这就是*传输时间*。传输时间是指将数据从磁盘传输到主机系统或从主机系统传输到磁盘所需的时间。
- en: '**Response time**: The response time or latency is the aggregate of service
    and wait times and can be thought of as the *round trip time* of an I/O request.
    It is expressed in milliseconds and is the most consequential term when working
    with storage devices, as it denotes the entire time from the issuance of an I/O
    request to its actual completion, as depicted in *Figure 9**.3*:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**响应时间**：响应时间或延迟是服务时间和等待时间的总和，可以看作是I/O请求的*往返时间*。它以毫秒为单位，是与存储设备工作时最重要的术语，因为它表示从发出I/O请求到实际完成所需的全部时间，如*图9.3*所示：'
- en: '![Figure 9.3 – Disk latency](img/B19430_09_03.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 磁盘延迟](img/B19430_09_03.jpg)'
- en: Figure 9.3 – Disk latency
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 磁盘延迟
- en: 'As shown in *Figure 9**.4*, storage vendors usually mention the following seek
    time specifications:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图9.4*所示，存储供应商通常会提到以下寻道时间规格：
- en: '**Full stroke**: This represents the time taken by the read write head to move
    from the innermost to the outermost track on the disk'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全程寻道**：这是读写磁头从磁盘最内圈移动到最外圈轨道所需的时间。'
- en: '**Average**: This is the average time taken by the read write head to move
    from one random track to another'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均寻道时间**：这是读写磁头从一个随机轨道移动到另一个轨道所需的平均时间。'
- en: '**Track to track**: This is the time taken by the read write head to move between
    two adjacent tracks'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轨道到轨道**：这是读写磁头在两个相邻轨道之间移动所需的时间。'
- en: 'The disk seek time specifications are shown in *Figure 9**.4*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘寻道时间规格见*图9.4*：
- en: '![Figure 9.4 – The disk seek time specifications](img/B19430_09_04.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – 磁盘寻道时间规格](img/B19430_09_04.jpg)'
- en: Figure 9.4 – The disk seek time specifications
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 磁盘寻道时间规格
- en: 'The transfer rate can be broken down into internal and external transfer rates:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 传输速率可以分为内部和外部传输速率：
- en: '**Internal transfer rate**: This is the speed at which data is transferred
    from the disk’s platter surface to its internal cache or buffer.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内部传输速率**：这是数据从磁盘的盘片表面传输到其内部缓存或缓冲区的速度。'
- en: '**External transfer rate**: Once the data has been fetched in the buffer, it
    is then transferred to the host bus adapter controller via the disk’s supported
    interface or protocol. As highlighted in *Figure 9**.5*, the speed at which data
    is transferred from the buffer to the host bus adapter determines the external
    transfer rate:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部传输速率**：一旦数据被提取到缓冲区，它将通过磁盘支持的接口或协议传输到主机总线适配器控制器。如*图9.5*所示，从缓冲区到主机总线适配器的数据传输速度决定了外部传输速率：'
- en: '![Figure 9.5 – The disk transfer rates](img/B19430_09_05.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – 磁盘传输速率](img/B19430_09_05.jpg)'
- en: Figure 9.5 – The disk transfer rates
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – 磁盘传输速率
- en: As we explained in [*Chapter 8*](B19430_08.xhtml#_idTextAnchor134), unlike mechanical
    drives, SSDs do not use any mechanical components. Therefore, concepts such as
    rotational latency and seek time do not apply to them. The *response time* encapsulates
    all the time-related aspects, and it is the term that is most frequently used
    when checking for performance-related issues.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第8章*](B19430_08.xhtml#_idTextAnchor134)中解释的那样，与机械硬盘不同，SSD 不使用任何机械组件。因此，诸如旋转延迟和寻道时间等概念不适用于它们。*响应时间*涵盖了所有与时间相关的方面，它是检查性能相关问题时最常用的术语。
- en: Disk access patterns
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 磁盘访问模式
- en: 'The mechanical drives are most affected by the I/O access patterns. The I/O
    pattern generated by an application can be a combination of sequential and random
    operations:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 机械硬盘最受I/O访问模式的影响。应用程序生成的I/O模式可以是顺序操作和随机操作的组合：
- en: '**Sequential I/O**: Sequential I/O operations refer to I/O requests that read
    from or write data to consecutive or contiguous disk locations. For mechanical
    drives, this results in a major performance boost, as this requires a very small
    movement from the read write head. This reduces the disk seek time.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序I/O**：顺序I/O操作指的是从连续或相邻磁盘位置读取或写入数据的I/O请求。对于机械硬盘，这会显著提高性能，因为这只需要读写磁头移动极小的距离，从而减少了磁盘寻道时间。'
- en: '**Random I/O**: Random I/O requests are performed on non-contiguous locations
    on the disk, and as you can guess, this results in longer disk seek times, which
    has a negative impact on disk performance.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机I/O**：随机I/O请求会在磁盘的非连续位置上执行，正如你所猜测的那样，这会导致更长的磁盘寻道时间，从而对磁盘性能产生负面影响。'
- en: Again, the random I/O operations impact the rotating mechanical drives and do
    not affect the SSDs as such. Although, since reading adjacent bytes on a disk
    requires a much smaller effort from the controller, sequential operations on SSDs
    are faster than random operations. However, this difference is much smaller compared
    to mechanical drives.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，随机I/O操作影响旋转机械硬盘，但不会像机械硬盘那样影响SSD。尽管如此，由于在磁盘上读取相邻字节对控制器的要求要小得多，因此在SSD上进行顺序操作比随机操作更快。然而，与机械硬盘相比，这种差异要小得多。
- en: Determining reads/writes ratio and I/O size
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定读取/写入比例和I/O大小
- en: IOPS alone do not paint the full picture of the disk’s performance and should
    always be taken with a grain of salt. It is important to look at the size of I/O
    requests and the ratio of read and write operations. For instance, complex storage
    systems are designed for specific read-write ratios and I/O sizes, such as `70/30
    read write` or `32 KB` block sizes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 单独的IOPS并不能完全反映磁盘的性能，应该谨慎对待。重要的是要查看I/O请求的大小以及读取和写入操作的比例。例如，复杂的存储系统是为特定的读写比例和I/O大小设计的，例如`70/30
    读写`或`32 KB`块大小。
- en: Different applications have different requirements and expectations from the
    underlying drives. It is important to have a rough estimate of the percentage
    of types of I/O operations that will be performed on a storage device. For instance,
    online transaction processing applications usually consist of the `70/30 read
    write` ratio. On the other hand, a logging application might always be busy writing
    and might require fewer reads.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的应用程序对底层驱动器有不同的需求和期望。了解存储设备上将执行的I/O操作类型的百分比非常重要。例如，在线事务处理应用程序通常由`70/30 读写`比例组成。另一方面，日志应用程序可能总是忙于写入，并可能需要较少的读取操作。
- en: The size of the I/O request by an application also varies, depending upon the
    type of the application. In some cases, it is a far more effective approach to
    transmit larger blocks. The time required to process such a request is longer
    than a single smaller request. On the other hand, considering the same amount
    of data, the combined processing and response time of many smaller requests might
    be greater than a single larger request.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序的I/O请求大小也会有所不同，这取决于应用程序的类型。在某些情况下，传输较大的数据块是一种更有效的方法。处理此类请求所需的时间比单个较小的请求要长。另一方面，考虑相同的数据量，许多较小请求的综合处理和响应时间可能会大于单个较大请求的时间。
- en: Disk cache
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 磁盘缓存
- en: Modern drives come with an onboard **disk cache** or **buffer**. The disk buffer
    is the embedded memory in a disk drive that acts as a buffer between the **host
    bus adapter** (**HBA**) and the disk platter or flash memory that is used for
    storage.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现代硬盘通常配备有内置的**磁盘缓存**或**缓冲区**。磁盘缓冲区是磁盘驱动器中嵌入的内存，它充当**主机总线适配器**（**HBA**）与磁盘盘片或用于存储的闪存之间的缓冲区。
- en: 'The following table highlights the effect of cache on different types of I/O
    patterns:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下表突出了缓存对不同类型I/O模式的影响：
- en: '| **I/O type** | **Read** | **Write** |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **I/O类型** | **读取** | **写入** |'
- en: '| Random | This is difficult to cache and pre-fetch, as a pattern cannot be
    predicted. | Caching is extremely effective, as random writes require a lot of
    disk seek time. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 由于模式无法预测，因此很难缓存和预取。 | 缓存非常有效，因为随机写入需要大量的磁盘寻道时间。 |'
- en: '| Sequential | Caching is extremely effective, as data can be easily pre-fetched.
    | Caching is effective and can be flushed quickly, as data is to be written to
    contiguous locations. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 顺序 | 缓存非常有效，因为数据可以轻松预取。 | 缓存有效且可以迅速刷新，因为数据会被写入连续的位置。 |'
- en: Table 9.1 – The effect of a cache on read/writes
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 – 缓存对读/写的影响
- en: The use of a cache speeds up the process of storing and accessing data from
    the hard disk. Enterprise storage arrays usually have a huge amount of cache available
    for this purpose.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用缓存可以加速从硬盘存储和访问数据的过程。企业级存储阵列通常为此目的提供大量的缓存。
- en: IOPS and throughput
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IOPS和吞吐量
- en: 'Along with latency, IOPS and throughput define the fundamental characteristics
    of physical storage:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与延迟一起，IOPS和吞吐量定义了物理存储的基本特性：
- en: '**IOPS**: IOPS represent the rate at which I/O operations can take place within
    a specific time period. The measurement of IOPS will give you the operations per
    second that the storage system currently delivers.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IOPS**：IOPS表示在特定时间段内可以执行的I/O操作的速率。IOPS的测量值可以告诉你存储系统当前每秒能处理的操作数。'
- en: '**Throughput**: Throughput refers to the volume of data that is transferred
    from or to the disk drive – in other words, the amount of pizza that you can eat
    at once. This is also referred to as bandwidth. As throughput measures the actual
    amount of data transfer, it is expressed in MB or GB per second.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**吞吐量**：吞吐量指的是从硬盘驱动器传输到或从硬盘驱动器传输的数据量——换句话说，就是你一次能吃掉多少披萨。这也叫带宽。由于吞吐量衡量的是实际的数据传输量，因此它以每秒MB或GB为单位。'
- en: 'Here are a couple of important things to remember:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几点重要的内容需要记住：
- en: The IOPS figure should always be correlated with latency, read-write ratios,
    and the I/O request size. When used independently, it does not have much value.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IOPS指标应始终与延迟、读写比例和I/O请求大小相关联。如果单独使用IOPS，它的价值不大。
- en: When processing large amounts of data, the bandwidth statistics might be more
    relevant than IOPS.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理大量数据时，带宽统计可能比IOPS更为相关。
- en: Queue depth
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 队列深度
- en: The **queue depth** dictates the number of I/O requests that can be concurrently
    handled at one time. In general, this value will not need to be altered. For large-scale
    SAN environments, in which hosts are connected to storage arrays using Fibre channel
    HBAs, this becomes a significant value. In that case, there are separate queue
    depth values for disks, HBAs, and the storage array ports.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**队列深度**决定了每次可以并发处理的I/O请求数量。一般来说，这个值不需要修改。在大规模SAN环境中，主机通过光纤通道HBA连接到存储阵列时，队列深度变得尤为重要。在这种情况下，硬盘、HBA和存储阵列端口都有独立的队列深度值。'
- en: If the number of issued I/O requests exceeds the supported queue depth, any
    new requests will not be entertained by the storage device. Instead, it will return
    a “queue full” message to the host. Once there is room in the queue, the host
    will have to resend the failed I/O request. The queue depth settings can impact
    both mechanical drives and SSDs. Mechanical drives and SSDs that use SATA and
    SAS interfaces only support a single queue, with 32 and 256 commands. Conversely,
    NVMe drives have 64,000 queues and 64,000 commands per queue.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果发出的I/O请求数量超过了支持的队列深度，任何新的请求将不会被存储设备处理。相反，存储设备会返回“队列已满”的消息给主机。一旦队列中有空位，主机需要重新发送失败的I/O请求。队列深度的设置会影响机械硬盘和固态硬盘。使用SATA和SAS接口的机械硬盘和SSD只支持一个队列，且每个队列分别支持32个和256个命令。相反，NVMe硬盘支持64,000个队列，每个队列有64,000个命令。
- en: In most cases, the default settings for queue depth might be sufficient. Each
    component in a storage environment has some queue depth settings. For instance,
    a RAID controller also has its own queue depth, which can be larger than the combined
    queue depth of the individual disks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，队列深度的默认设置可能已经足够。存储环境中的每个组件都有自己的队列深度设置。例如，RAID控制器也有自己的队列深度，且这个队列深度可能大于单个硬盘的组合队列深度。
- en: Determining disk busyness
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定硬盘忙碌程度
- en: 'There are a couple of concepts that determine how much the disk is actually
    used. They are described as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个概念决定了硬盘的实际使用程度，具体如下所述：
- en: '**Utilization**: Disk utilization is a fairly common metric that you’ll see
    being reported by various tools. Utilization means that the disk was actively
    used for a given interval. This value is represented as a percentage of time.
    For example, a 70% utilization value indicates that if the kernel looked up the
    disk 100 times, on 70 occasions, it was busy while performing some I/O request.
    Similarly, a disk that is being 100% utilized means that it constantly serves
    I/O requests. Again, a fully utilized disk may or may not become a bottleneck.
    This value needs to be correlated with a few other metrics, such as the associated
    latency and queue depth. It could be that, although the I/O requests are issued
    continuously, they’re fairly small and sequential; hence, the disk is able to
    serve them in a timely manner. Similarly, RAID arrays have the ability to handle
    requests in parallel, and as such, a 100% utilized disk might not be problematic.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用率**：磁盘利用率是一个非常常见的指标，你会看到各种工具报告这个数据。利用率意味着在给定的时间间隔内磁盘被积极使用。这个值以时间的百分比表示。例如，70%的利用率意味着，如果内核检查磁盘100次，70次磁盘在执行某些I/O请求时是忙碌的。类似地，一个100%利用率的磁盘意味着它不断地处理I/O请求。同样，完全利用的磁盘可能成为瓶颈，也可能不会。这个值需要与其他一些指标相关联，如关联的延迟和队列深度。可能是，尽管I/O请求持续不断，但它们相当小且是顺序的，因此磁盘能够及时处理它们。类似地，RAID阵列能够并行处理请求，因此一个100%利用的磁盘也许不会成为问题。'
- en: '**Saturation**: Saturation means that the amount of requests issued to a disk
    might be more than what it can actually deliver. This means that we’re trying
    to exceed its rated capacity. When saturation happens, the applications have to
    wait before being able to read from or write data to disk. Saturation will result
    in increased response times and impact the overall performance of a system.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**饱和度**：饱和度意味着发给磁盘的请求数量可能超过它实际能够处理的能力。这意味着我们正试图超过磁盘的额定容量。当发生饱和时，应用程序必须等待才能从磁盘读取或写入数据。饱和将导致响应时间增加，影响系统的整体性能。'
- en: I/O wait
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I/O等待
- en: Quite understandably, **I/O wait** is often the most misunderstood metric when
    checking for performance issues. Although it has an *I/O* in name, I/O wait time
    is actually a CPU metric, but it doesn’t indicate issues with CPU performance.
    Get it?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易理解的是，**I/O等待**通常是检查性能问题时最容易被误解的指标。尽管它的名称中有*I/O*，但I/O等待时间实际上是一个CPU指标，它并不表示CPU性能的问题。明白了吗？
- en: 'I/O wait time is the percentage of time that a CPU was idle, during which the
    system had pending disk I/O requests. What makes this difficult to comprehend
    is that it is possible to have a healthy system with a high I/O wait percentage,
    and it is also possible to have a slow-performing system without a low I/O wait
    percentage. A high I/O wait means that the CPU is idle while waiting for disk
    requests to be completed. Let’s explain this with a couple of examples:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: I/O等待时间是CPU空闲的时间百分比，在这些空闲时间内系统有待处理的磁盘I/O请求。之所以难以理解，是因为即使系统I/O等待百分比较高，也可能是健康的系统；同样，即使系统I/O等待百分比较低，也可能是性能较慢的系统。高I/O等待意味着CPU在等待磁盘请求完成时处于空闲状态。我们可以通过几个例子来解释这一点：
- en: For instance, if a process has sent some I/O requests and the underlying disk
    is unable to immediately fulfill that request, the CPU is said to be in a waiting
    state, as it is waiting for the request to be completed. Here, the waiting indicates
    that CPU cycles are wasted and the underlying disk might be slow to respond to
    I/O requests.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，如果一个进程发送了一些I/O请求，而底层磁盘无法立即满足该请求，那么CPU就处于等待状态，因为它在等待请求的完成。在这里，等待意味着CPU周期被浪费，且底层磁盘可能响应I/O请求较慢。
- en: Then, there’s the opposite case. Let’s say that process A is extremely CPU-intensive
    and constantly keeps the CPU busy. Another process running on the system, process
    B is I/O-intensive and occupies the disk. Even if the disk is slow to respond
    to requests of process B and becomes a source of a bottleneck for the system,
    the I/O wait value will be very low in this case. Why? Because the CPU is not
    idle, as it is wrapped up while serving process A. Therefore, although the I/O
    wait is on the low side, there could be a potential bottleneck with the storage.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，有一个相反的情况。假设进程A极度依赖CPU，持续让CPU忙碌。另一个在系统上运行的进程B是I/O密集型的，占用了磁盘。即使磁盘响应进程B的请求很慢，且成为系统瓶颈的来源，I/O等待值在这种情况下也会非常低。为什么？因为CPU并没有空闲，它一直在为进程A提供服务。因此，尽管I/O等待值较低，但存储可能仍然存在潜在的瓶颈。
- en: 'High I/O wait values can be caused by anyone or a combination of the following
    factors:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 高 I/O 等待值可能是以下任何一个或多个因素导致的：
- en: Bottlenecks in the physical storage
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理存储中的瓶颈
- en: A large queue of I/O requests
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大量的 I/O 请求排队
- en: Disks nearing saturation or fully saturated
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘接近饱和或已完全饱和
- en: Processes in an uninterruptible sleep state, known as the `D` state (this is
    fairly common when storage is accessed through **network** **filesystem** (**NFS**))
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处于不可中断睡眠状态的进程，称为 `D` 状态（当存储通过 **网络文件系统** (**NFS**) 访问时，这种情况相当常见）
- en: Slow network speed in the case of NFS
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 NFS 的情况下，网络速度慢
- en: High swapping activity
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高交换活动
- en: I think we’ve covered quite a few things to watch out for when doing an analysis
    of storage devices. Again, if your storage environment contains all the components
    in a traditional SAN environment, then you need to look for a few more things,
    such as **fibre channel** (**FC**) switches and any potential bottlenecks on the
    storage array. To troubleshoot FC switches, you need to establish a basic understanding
    of the FC protocol.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们已经涵盖了在分析存储设备时需要注意的许多事项。再次强调，如果你的存储环境包含传统 SAN 环境中的所有组件，那么你还需要关注更多内容，例如 **光纤通道**
    (**FC**) 交换机以及存储阵列中的潜在瓶颈。要排查 FC 交换机的问题，你需要对 FC 协议有基本的了解。
- en: Let’s see how we can identify these red flags using the available tools.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用现有工具识别这些红旗。
- en: Using disk I/O analysis tools
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用磁盘 I/O 分析工具
- en: We now have developed a basic understanding of what to look for when diagnosing
    problems with the underlying storage. Most of the time, the problematic behavior
    is first reported at the application layer, and multiple layers are checked before
    the actual identification of the issue. The problematic scenario can also be intermittent
    in nature, which could make it even more difficult to detect. Fortunately, Linux
    has a broad range of utilities in its toolbox that can be used to identify such
    problematic behavior. We’ll take a look at them one by one and highlight the things
    of value to look for when troubleshooting performance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经对诊断底层存储问题时需要关注的内容有了基本了解。大多数时候，问题行为首先是在应用层报告的，通常需要检查多个层次才能最终识别问题。问题情况可能也会是间歇性的，这使得检测起来更加困难。幸运的是，Linux
    有一系列强大的工具可以帮助识别这些问题行为。我们将逐一查看它们，并重点介绍在故障排除过程中需要关注的有价值的内容。
- en: Establish a baseline using top
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 top 建立基准
- en: '`top` is one of the most frequently used commands when troubleshooting performance
    issues. What makes it so effective is that it can quickly give you the current
    status of a system and possibly give you a hint about the potential problem. Although
    most people use it for CPU and memory analysis, there is one particular field
    that can indicate a problem with underlying storage. As shown in the following
    output, the `top` command can quickly provide a summarized view of the current
    system state:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`top` 是故障排除性能问题时最常用的命令之一。它之所以有效，是因为它能够快速展示系统的当前状态，并可能给出潜在问题的线索。尽管大多数人使用它来进行
    CPU 和内存分析，但有一个特定的字段可以表明底层存储存在问题。如下所示，`top` 命令可以快速提供当前系统状态的总结视图：'
- en: '[PRE2]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we discussed earlier, a high I/O wait is an indication of a bottleneck at
    the storage layer. The `wa` field is the wait average and indicates the potion
    of time that the CPU had to wait because of the disk. High wait averages mean
    that the disk does not respond in a timely manner. Although not discussed here,
    load averages can also increase because of higher wait averages. This is because
    load averages include disk waiting activity.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，高 I/O 等待是存储层瓶颈的一个迹象。`wa` 字段是等待平均值，表示 CPU 因磁盘原因而等待的时间比例。高等待平均值意味着磁盘响应不及时。尽管这里没有讨论，负载平均值也可能因为等待平均值的增加而上升。这是因为负载平均值包括了磁盘等待活动。
- en: The top utility has several options that can provide insight into CPU and memory
    consumption, but we’re not going to focus on them. As our primary concern here
    is storage, we need to watch out for high values in the `wa` column and the load
    averages.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`top` 工具有几个选项可以提供有关 CPU 和内存消耗的洞察，但我们不会在这里重点关注这些。因为我们主要关注的是存储问题，所以需要特别留意 `wa`
    列中的高值以及负载平均值。'
- en: The iotop utility
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: iotop 工具
- en: The `iotop` command is a `top`-like utility to monitor disk-related activity.
    The `top` command, by default, sorts output on the basis of CPU usage. Similarly,
    the `iotop` command sorts processes by the amount of data read and written by
    each process. It displays columns that highlight the top disk bandwidth consumers
    in your system. Additionally, it also displays the proportion of time that the
    thread/process was engaged in swapping and waiting for I/O operations. The I/O
    priority, both in terms of class and level, is indicated for each process.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`iotop`命令是一个类似于`top`的工具，用于监控与磁盘相关的活动。`top`命令默认按 CPU 使用率对输出进行排序。同样，`iotop`命令按每个进程读取和写入的数据量对进程进行排序。它显示了突出显示系统中磁盘带宽消耗的列。此外，它还显示了线程/进程在交换和等待
    I/O 操作过程中所占用的时间比例。每个进程的 I/O 优先级，包括类别和级别，也会显示。'
- en: 'It’s better to run `iotop` with the `-o` flag, as it will show processes that
    currently write to disk:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最好使用`-o`标志运行`iotop`，这样它会显示当前向磁盘写入的进程：
- en: '[PRE3]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`iotop` command shows the amount of data being read from or written to the
    disk by a process. Check the supported disk read and write speeds, and compare
    them with the throughput of the top processes. This can also help to identify
    unusual disk activity by applications and determine whether any process can read
    or write an abnormal amount of data to underlying disks.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`iotop`命令显示了进程从磁盘读取或写入的数据量。检查支持的磁盘读写速度，并将其与占用带宽最多的进程的吞吐量进行比较。这也有助于识别应用程序的异常磁盘活动，并确定是否有进程正在向底层磁盘读取或写入异常的数据量。'
- en: 'Sometimes, the `iotop` command might complain that delay accounting is not
    enabled in the kernel. This can be fixed as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，`iotop`命令可能会提示内核中未启用延迟计数。这可以通过以下方式修复：
- en: '`sysctl kernel.task_delayacct = 1`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`sysctl kernel.task_delayacct = 1`'
- en: The iostat utility
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: iostat 工具
- en: The `iostat` command is the most popular tool for disk analysis, as it displays
    a wide variety of information that can be of help to analyze performance issues.
    Most of the metrics that were explained earlier, such as disk saturation, utilization,
    and I/O wait, can be analyzed through `iostat`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`iostat`命令是最受欢迎的磁盘分析工具，因为它显示了各种有助于分析性能问题的信息。之前解释的多数指标，如磁盘饱和、利用率和 I/O 等待，都可以通过`iostat`进行分析。'
- en: 'The first line of the output in `iostat`’s disk statistics is a summary since
    the most recent boot, which shows the mean for the entire time the system has
    been up. Subsequent lines are displayed as per-second statistics, calculated using
    the interval specified on the command line, as shown in the following screenshot:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`iostat`的磁盘统计输出的第一行是自系统最近启动以来的总结，显示了系统运行期间的平均值。随后的行则按秒统计显示，根据命令行指定的时间间隔计算，如下图所示：'
- en: '![Figure 9.6 – The iostat command](img/B19430_09_06.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – iostat 命令](img/B19430_09_06.jpg)'
- en: Figure 9.6 – The iostat command
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – iostat 命令
- en: 'This is what to look for:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是：
- en: The first line, `avg-cpu`, shows the percentage of CPU utilization that occurred
    while executing in each state.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一行，`avg-cpu`，显示了在每个状态下执行时 CPU 利用率的百分比。
- en: The `r/s` and `w/s` numbers give a breakdown of the number of read and write
    requests issued to the device per second.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r/s`和`w/s`数字提供了每秒发往设备的读写请求数。'
- en: The `avgqu-sz` represents the count of operations that were in either a queued
    state or actively being serviced. The `await` value corresponds to the average
    duration between placing a request in a queue and its completion. The `r_await`
    and `w_await` columns show the average wait time for read and write operations.
    If you see consistently high values here, the device might be nearing saturation.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`avgqu-sz`表示处于队列状态或正在被服务的操作的计数。`await`值对应于请求被放入队列到完成之间的平均时间。`r_await`和`w_await`列显示了读写操作的平均等待时间。如果这里的值持续较高，设备可能接近饱和。'
- en: The `%util` column shows the amount of time during which the disk was busy serving
    at least one I/O request. The utilization value might be misleading if the underlying
    storage is a RAID-based volume.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%util`列显示了磁盘在服务至少一个 I/O 请求期间的繁忙时间。若底层存储是基于 RAID 的卷，利用率值可能会产生误导。'
- en: The general assumption is that as a device’s utilization approaches 100%, it
    becomes more saturated. This holds true when referring to a storage device that
    represents a single disk. However, SAN arrays or RAID volumes consist of multiple
    drives and can serve multiple requests simultaneously. The kernel does not have
    direct visibility on how the I/O device is designed, which makes this a dubious
    figure in some cases.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一般假设，当设备的使用率接近100%时，它变得更为饱和。这适用于表示单一磁盘的存储设备。然而，SAN阵列或RAID卷由多个驱动器组成，可以同时处理多个请求。内核无法直接看到I/O设备的设计方式，这使得在某些情况下，这个数字是值得怀疑的。
- en: Performance Co-Pilot
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能协同驾驶员
- en: '`sysstat` package. The PCP tools also include a GUI application to create graphs
    from the generated metrics and have the ability to save historical data for later
    viewing. A couple of tools that can assist in storage analysis are as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`sysstat`包。PCP工具还包括一个图形用户界面应用程序，可以根据生成的指标创建图表，并能够保存历史数据供以后查看。以下是几个可以协助存储分析的工具：'
- en: '`pcp atop`: This provides information similar to both the `iotop` and `atop`
    commands. The command lists the processes that perform I/O, along with the disk
    bandwidth they use. Like `ioto` and `top`, `pcp atop` is a good tool for quickly
    grasping changes occurring on a system.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pcp atop`：此命令提供的信息类似于`iotop`和`atop`命令。该命令列出了执行I/O的进程以及它们使用的磁盘带宽。像`iotop`和`top`一样，`pcp
    atop`是快速了解系统变化的好工具。'
- en: '`pcp iostat`: The `pcp iostat` command reports live disk I/O statistics, much
    like the `iostat` command we saw earlier. As shown in *Figure 9**.7*, the columns
    in the output are similar to that of `iostat`:'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pcp iostat`：`pcp iostat`命令报告实时磁盘I/O统计信息，类似于我们之前看到的`iostat`命令。如*图 9.7*所示，输出中的列与`iostat`类似：'
- en: '![Figure 9.7 – pcp iostat](img/B19430_09_07.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7 – pcp iostat](img/B19430_09_07.jpg)'
- en: Figure 9.7 – pcp iostat
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – pcp iostat
- en: When troubleshooting disk performance or resource-related issues, `vmstat` can
    provide valuable information, as it can help to identify disk I/O congestion,
    excessive paging, or swapping activity.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在排查磁盘性能或资源相关问题时，`vmstat`可以提供有价值的信息，因为它有助于识别磁盘I/O拥堵、过度分页或交换活动。
- en: The vmstat command
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: vmstat命令
- en: 'The `vmstat` command, derived from “virtual memory statistics,” is a native
    monitoring utility included in nearly all Linux distributions. As shown in *Figure
    9**.8*, it reports information about processes, memory, paging, disks, and processor
    activity:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`vmstat`命令，源自“虚拟内存统计”，是几乎所有Linux发行版中都包含的原生监控工具。如*图 9.8*所示，它报告有关进程、内存、分页、磁盘和处理器活动的信息：'
- en: '![Figure 9.8 – The vmstat command](img/B19430_09_08.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8 – vmstat命令](img/B19430_09_08.jpg)'
- en: Figure 9.8 – The vmstat command
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – vmstat命令
- en: '`b` column in the output shows the number of processes blocked while waiting
    for a resource, such as disk I/O. Additional information most useful for troubleshooting
    I/O issues is as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中的`b`列显示的是等待资源（如磁盘 I/O）时被阻塞的进程数。对排查 I/O 问题最有用的附加信息如下：
- en: '`si`: This field represents the amount of memory, in kilobytes, that is swapped
    in from the swap space on the disk to the system’s memory per second. A higher
    value in the `si` field indicates increased swapping activity, suggesting that
    the system frequently retrieves data from the swap space.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`si`：该字段表示每秒从磁盘交换空间交换到系统内存的内存量（以千字节为单位）。`si`字段的值越高，表示交换活动越频繁，表明系统经常从交换空间检索数据。'
- en: '`so`: This field represents the amount of memory, in kilobytes, that is swapped
    out from the system’s memory to the swap space on disk per second. A higher value
    in the `so` field indicates increased swapping activity, which can occur when
    the system is under memory pressure and needs to free up physical memory.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`so`：该字段表示每秒从系统内存交换到磁盘交换空间的内存量（以千字节为单位）。`so`字段的值越高，表示交换活动越频繁，这可能发生在系统内存紧张，需要释放物理内存时。'
- en: '`bi`: This field specifically refers to the data transfer rate from disk to
    memory. A higher value in the `bi` field indicates increased disk read activity.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bi`：该字段专门指从磁盘到内存的数据传输速率。`bi`字段的值越高，表示磁盘读取活动越频繁。'
- en: '`bo`: This reflects the output activity or the amount of data written from
    the system’s memory to the disk. A higher value in the `bo` field suggests increased
    disk write activity, indicating that data is written from memory to the disk frequently.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bo`：此项反映了系统内存写入到磁盘的输出活动或数据量。`bo` 字段的值越高，表示磁盘写入活动增加，说明数据经常从内存写入磁盘。'
- en: '`wa`: This field represents the percentage of time that the CPU is idle while
    the system waits for I/O operations to complete. A higher value in the `wa` field
    suggests that the system experiences I/O bottlenecks or delays, with the CPU frequently
    waiting for I/O operations to complete.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wa`：此字段表示 CPU 空闲时，系统等待 I/O 操作完成的时间百分比。`wa` 字段的值越高，表示系统经历了 I/O 瓶颈或延迟，CPU 经常等待
    I/O 操作完成。'
- en: Pressure Stall Index
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 压力停滞指数
- en: '**Pressure Stall Index** (**PSI**) is a relatively new addition to the troubleshooting
    toolset in Linux and offers a new way to obtain utilization metrics for memory,
    CPU, and disk I/O. Latency spikes can occur when there is contention for CPU,
    memory, or I/O devices, resulting in increased waiting times for workloads. The
    PSI feature identifies this and prints a summarized view of this information in
    real time.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**压力停滞指数**（**PSI**）是 Linux 中故障排除工具集的一个相对较新的功能，提供了一种新的方式来获取内存、CPU 和磁盘 I/O 的利用率指标。当
    CPU、内存或 I/O 设备发生竞争时，可能会出现延迟峰值，导致工作负载的等待时间增加。PSI 功能可以识别这种情况，并实时打印出这些信息的总结视图。'
- en: 'The PSI values are accessed through the `/proc pseudo` filesystem. The raw
    global PSI values appear in the `/proc/pressure` directory in files called `cpu`,
    `io`, and `memory`. Let’s take a look at the `io` file:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: PSI 值通过 `/proc pseudo` 文件系统访问。原始的全局 PSI 值出现在 `/proc/pressure` 目录下的 `cpu`、`io`
    和 `memory` 文件中。我们来看一下 `io` 文件：
- en: '[PRE4]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`avg` fields represent the *percentage* of time in the last 10, 60, and 300
    seconds, respectively, that processes were starved of disk I/O. The line prefixed
    with `some` represents the portion of time during which one or more tasks were
    delayed due to insufficient resources. The line prefixed with `full` represents
    the percentage of time when all tasks were delayed due to resource constraints,
    indicating the extent of unproductive time.This is a bit similar to the load averages
    in `top`. The output here shows high values for the `10`-, `60`- and `300`-second
    interval averages, which indicates that processes are being stalled.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`avg` 字段表示最近 10 秒、60 秒和 300 秒内，进程因磁盘 I/O 而被饿死的*百分比*时间。以 `some` 为前缀的行表示由于资源不足，某些任务被延迟的时间部分。以
    `full` 为前缀的行表示所有任务因资源限制而被延迟的时间百分比，表明无效时间的程度。这与 `top` 命令中的负载平均值有些相似。这里的输出显示，`10`、`60`
    和 `300` 秒间隔的平均值较高，表明进程正在被暂停。'
- en: To summarize, Linux offers a plethora of utilities to analyze the performance
    of your system. The tools that we covered here are not only used for storage analysis
    but also to establish an overall picture of a system, including its processor
    and memory subsystem. Every tool offers a wide range of options that can be used
    if we want to analyze a particular aspect. We highlighted the major indicators
    to look for when using each of these tools, but since every environment consists
    of different variables, there is no *fixed* approach to troubleshooting.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Linux 提供了许多工具来分析系统性能。我们在这里介绍的工具不仅用于存储分析，还用于建立系统的整体视图，包括处理器和内存子系统。每个工具提供了广泛的选项，可以用于分析特定方面。我们强调了使用这些工具时需要关注的主要指标，但由于每个环境中都有不同的变量，因此没有*固定*的故障排除方法。
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Troubleshooting performance issues is a complex matter, as it can take a long
    time to diagnose and analyze them. Of the three major components in an environment
    – storage, compute, and memory – storage is the slowest. There is always going
    to be a mismatch in their performance, and any degradation in disk performance
    can impact the overall operation of the system.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 故障排除性能问题是一个复杂的过程，因为它可能需要很长时间才能诊断和分析。在环境中的三个主要组件——存储、计算和内存——中，存储是最慢的。它们的性能总是会存在不匹配的情况，任何磁盘性能的下降都可能影响系统的整体运行。
- en: Keeping this objective in mind, we divided this chapter into two sections. In
    the first section, we explained the most important metrics that you should understand
    before troubleshooting any issues. We discussed the time-related metrics related
    to storage devices, CPU wait averages, disk saturation, and disk utilization,
    and the different access patterns when reading from or writing to physical disks.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此目标，我们将本章分为两部分。在第一部分，我们解释了在排查任何问题之前，您应该了解的最重要的指标。我们讨论了与存储设备相关的时间相关指标、CPU等待平均值、磁盘饱和度和磁盘利用率，以及从物理磁盘读取或写入时的不同访问模式。
- en: In the second part, we saw the different ways in which we can analyze the metrics
    highlighted in the first section. There are a lot of mechanisms available in Linux
    that can assist to identify potential bottlenecks in overall system performance.
    We used tools such as `top`, `PSI`, `iostat`, `iotop`, and `vmstat` to analyze
    disk performance.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们看到了解析第一部分中突出的指标的不同方式。Linux中有许多可用的机制，可以帮助识别整体系统性能中的潜在瓶颈。我们使用了`top`、`PSI`、`iostat`、`iotop`和`vmstat`等工具来分析磁盘性能。
- en: In the next chapter, we will continue our analysis of the storage stack and
    focus on the higher layers, such as the block layer and filesystems. For this
    purpose, we’ll make use of the different tracing mechanisms available in Linux.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将继续分析存储堆栈，重点关注更高层次的部分，如块层和文件系统。为此，我们将利用Linux中提供的不同跟踪机制。
