# 第三章：系统存储最佳实践

**系统管理**中可能最复杂且最难理解的部分涉及**存储**领域。存储往往被忽视，极少教授，且常常被当作神话看待而非科学。存储也是最令人恐惧的领域，因为在存储中，我们的错误可能导致数据丢失，而没有什么比数据丢失更严重的失败了。

存储决策影响性能、容量、寿命，最重要的是*耐用性*。存储是我们误差最小的领域，也是我们能产生最大影响的地方。在其他规划和设计领域，我们通常能够获得相当大的*容错空间*，错误往往表现得比较优雅，比如系统没有达到预期的速度，或者成本略高于必要，但在存储领域，过度建设可能会使总成本翻倍，错误也很容易导致系统无法正常运行。失败通常远非优雅。

我们将讨论如何在 Linux 系统中查看和理解存储，并揭开存储的神秘面纱，让你能够以系统化和经验主义的方法进行操作。在本章结束时，你应该能够根据工作负载的所有需求，决定最适合的存储产品和设计。

在本章中，我们将探讨以下关键主题：

+   探讨存储中的关键因素

+   了解块存储：本地和 SAN

+   调查文件系统和网络文件系统

+   了解**逻辑卷管理**（**LVM**）

+   利用 RAID 和 RAIN

+   了解本地存储的复制

+   分析存储架构和风险

# 探讨存储中的关键因素

在考虑系统管理中的存储时，我们关注的是**成本**、**耐用性**、**可用性**、**性能**、**可扩展性**、**可访问性**和**容量**。在面对如此多的因素时，存储会让人感到不知所措，这也让它成为一个风险点，我们可能会忽视我们真正想要实现的目标。在每个存储决策中，我们需要始终关注这些因素。最重要的是，关注所有这些因素。我们很容易只关注其中几个，从而失去对整个图景的把握。

在大多数情况下，如果你研究那些未能满足业务需求的存储系统的事后分析，你几乎总是会发现，设计阶段忘记了一个或多个这些因素。很容易只专注于一两个关键因素，而忽视其他因素，但我们必须始终关注所有因素，才能确保存储的成功。

我们应该从单独分析每个因素开始。

## 成本

也许你会觉得，存储的成本不可能被忽视，但相信我，这种情况确实存在，并且发生得很频繁。通常，IT 是一个业务职能，所有的企业本质上都是为了赚钱，提供基础设施的成本必须始终考虑到利润。因此，基于这一点，IT 中的任何决策（或任何企业中的决策）都不应该在没有考虑成本的情况下做出。我们绝不应该让成本被遗忘，或者更糟的是，允许有人说*成本不是问题*，因为那永远不可能成立，完全没有意义。成本可能不是最主要的关注点，预算限制可能具有一定灵活性，但成本始终是重要的。

存储通常是生产系统中最昂贵的组成部分之一，因此，在处理存储时，我们往往比处理其他物理系统设计部分（如 CPU 和内存）时更加敏感成本。存储也通常是通过简单地投入更多资金来解决的，因此许多人在硬件规划时，倾向于过度建设，因为这比较容易。当然，我们可以始终这样做，只要我们足够了解自己的存储需求以及存储的工作原理，它就能*正常工作*，但不会过于昂贵。不过，当然，如果我们不具备成本效益，作为系统管理员就很难有效工作——这两者是相辅相成的。

## 耐用性

在存储方面，没有什么比耐用性更重要了：即存储机制抵抗数据丢失的能力。耐用性是可靠性的两个方面之一。对于大多数工作负载和大多数系统场景，耐用性是最为关键的。我们很少需要存储那些即使检索缓慢、延迟或昂贵，也无法可靠地恢复的数据。诸如数据可用性或性能等概念在数据丢失的情况下毫无意义。

耐用性还指的是数据能够抵抗损坏或衰退。在存储中，我们必须担心数据集的某些部分可能会失去完整性，而这可能是我们能够或无法检测到的。仅仅因为我们能检索到数据，并不能告诉我们我们所检索的数据是否完全符合预期。数据损坏可能意味着文件无法读取，数据库无法访问，操作系统无法启动，甚至更糟糕，它可能意味着会计应用中的一个数字变成了另一个不同但有效的数字，而这种变化几乎不可能被检测到。

## 可用性

传统上，我们主要从数据可用性的角度考虑数据的可靠性，尤其是在检索数据时。可用性通常被称为*正常运行时间*，如果存储不可用，那么你的工作负载也无法使用。因此，尽管可用性通常会让位于耐用性，但它仍然极为重要，是整体存储可靠性的两个关键方面之一。

有时可用性和性能会交织在一起。有些情况下，存储性能会显著下降，导致数据实际上变得无法使用。可以想象一下淋浴水滴每隔几秒才滴落一次，从技术上讲，水仍然存在，但它流经管道的速度太慢，无法有效使用。

我们稍后会深入讨论 RAID，但可用性和性能是很好的现实例子。一个著名的情况是，当一个或两个硬盘出现故障并被替换后，RAID 6 阵列仍然在线，并且正在进行主动重建（即通过元数据重新计算丢失的数据的过程）。RAID 系统常常因处理和写入的数据量过大而不堪重负，虽然阵列从技术上讲是在线且可用的，但其速度非常慢，以至于无法以任何有意义的方式使用，操作系统或应用程序试图使用时，由于极端的慢速，不仅无用，甚至可能因响应时间过长而错误报告存储设备已离线。*可用性*可能变得模糊不清，如果我们不小心的话。

## 性能

对于 21 世纪的计算机来说，存储几乎总是我们系统中最重要的性能瓶颈。CPU 和 RAM 几乎总是需要等待存储，而不是反过来。现代采用固态技术的存储设备在缩小存储系统与其他组件之间的性能差距方面做出了很大贡献，但这一差距依然相当大。

性能可能很难衡量，因为有许多不同的视角，而不同类型的存储介质通常具有截然不同的性能特性。比如延迟（数据检索开始前的时间）、吞吐量（也称为*带宽*，衡量数据流的速度）以及每秒输入输出操作数（IOPS，表示在特定时间内可以执行的存储相关活动次数）。大多数人仅从吞吐量的角度考虑存储，但传统上，IOPS 是大多数工作负载下衡量性能的最有用指标。

总是很容易将因素简化为一些易于理解和比较的内容。但是如果我们拿汽车做比较，我们可以将三种车辆进行对比：一种加速快但最高速度低，一种加速慢但最高速度高，另一种是拖车，虽然加速慢且最高速度低，但一次可以搬运大量物品。如果我们只关心延迟：即第一个数据包到达的时间，那么第一辆车会脱颖而出。如果我们关心如何快速地将一个小的工作负载从一个地方移动到另一个地方，第二辆车则表现得最好，这最像是在测量 IOPS。拖车则在我们关心的是在系统之间传输的数据量时无可匹敌，这就是我们的吞吐量或带宽。对于汽车，大多数人认为*快车*是指最高速度最快的车，但在存储领域，大多数人认为拖车的例子是他们想要衡量的目标，而不是*在使用时感觉*快的东西。实际上，性能是一个视角问题，不同的工作负载对性能的感知是不同的。

例如，备份涉及到稳定、线性的数据显示，并且最受益于以吞吐量为设计核心的存储系统。因此，磁带在备份性能方面表现优异，也解释了为什么旧的光学介质如 CD 和 DVD 曾经可以接受。但其他工作负载，比如数据库，则非常依赖于 IOPS 和低延迟，并且很少受益于总体吞吐量，因此更适合使用固态存储。其他工作负载，如文件服务器，通常需要性能和容量的平衡，使用旋转硬盘就可以很好地满足需求。你必须了解你的工作负载，才能设计出合适的存储系统来支持它。

当我们开始考虑突发性与持续性速率时，性能变得更加复杂。需要考虑的因素非常多，而且你无法绕过这个过程。

## 可扩展性

典型的物理系统部署如今预计在生产中使用四到八年，且听说有些系统的使用寿命远超这个范围也并不罕见。只要在 IT 行业工作一段时间，你很可能会遇到一些仍在运行并对公司成功至关重要的系统，这些系统已经连续使用了二十年甚至更久！由于存储系统预期拥有如此长的使用寿命，我们必须考虑该系统在这一潜在时间段内如何能够增长或变化。

大多数工作负载随着时间的推移会出现容量增长需求，设计一个可以根据需要扩展容量的存储系统，对于应对未知的挑战非常有益，同时也能让我们在前期投资较少，仅在*需要*和*当*额外容量变得必要时才增加支出。一些存储系统也可能在性能方面进行扩展。这种情况较少见，也较少被认为是关键需求，但即便是一个工作负载仅仅增加了容量需求，而不是性能需求*本身*，仅仅是更大的容量也可能要求提升性能，尤其是处理备份等任务时，因为大容量意味着更长的备份和恢复时间。

理论上，你也可能遇到一种情况，其中对可靠性的需求（持久性、可用性或两者）可能随着时间的推移而增加。这种情况是可能的，但往往会更加复杂。

存储是一个在灵活调整配置方面通常最难、但也是最重要的领域。我们无法总是预见到未来的需求。我们需要尽力规划，以便在可能的情况下留有调整的灵活性。

## 容量

最后，我们来看看容量，系统中可以存储的数据量。容量看似简单，但有时也会令人困惑。即使是简单的磁盘阵列，我们也必须考虑原始容量（所有设备容量的总和）和结果容量（可用于存储目的的可用容量）。许多存储系统有冗余设计，以提供可靠性和性能，而这会消耗原始容量。因此，我们必须了解存储配置如何影响最终结果。存储管理员会同时讨论原始容量和可用容量。

现在我们已经掌握了存储方面的关键要素，接下来我们可以深入学习存储组件如何组合成**企业存储子系统**。

# 理解块存储：本地存储和 SAN

我们今天会遇到的任何标准存储机制的根本概念是**块设备**。块设备是允许存储非易失性数据并可以按任意顺序存储和检索的存储设备。从实际意义上讲，*标准*块设备可以理解为硬盘。硬盘是典型的块设备，我们可以将任何其他块设备视为像硬盘一样工作。我们也可以将其称为实现驱动接口或*外观*。

许多设备都是块设备。传统的旋转硬盘、固态硬盘（SSD）、软盘、光盘、DVD-ROM、磁带驱动器、RAM 磁盘、RAID 阵列等都是块设备。从计算机的角度来看，所有这些设备都是一样的。这使得作为系统管理员的工作变得简单：一切都建立在块设备之上。

从系统管理员的角度来看，我们通常简单地将块设备称为*磁盘*，因为从操作系统的角度，我们无法准确识别设备，只知道我们正在使用块存储。这种块存储可能是物理磁盘，是建立在多个磁盘之上的逻辑设备，或者是建立在内存之上的抽象，磁带驱动器，或者远程系统，你可以随意猜测。我们其实无法真正分辨出来。对我们来说，它只是一个块设备，而由于块设备通常代表磁盘，我们称它们为磁盘。这虽然不一定准确，但却很有用。

## 本地附加块存储

最简单的块存储设备类型是那些物理附加到我们系统的设备。比如，我们对标准的内部硬盘就比较熟悉。如今，本地块设备通常通过 SAS、SATA 和 NVMe 连接。近年来，**并行 SCSI**（当时仅称为**SCSI**）和**并行 ATA**（即**PATA**，当时仅称为**ATA**或**IDE**）是标准。这些技术，以及一些较为冷门的技术，允许物理块设备直接附加到计算机系统上。

这是一种本地附加存储，我们大部分时间都会使用它。所有块设备都必须以某种方式本地附加才能被使用。所以这种技术始终具有相关性。

本地附加的块设备相较于其他替代方案有很多固有的优势。由于本地附加，它具有天然的性能和可靠性优势：系统尽可能简单，这意味着出错的机会较少。其他条件相同的情况下，简单的总是优于复杂的。存储就是一个很好的例子。更少的运动部件和更短的连接路径意味着我们能够获得最低的延迟、最高的吞吐量和最高的可靠性，同时成本最低！

当然，本地附加存储也有一些限制，否则没人会选择其他方案。本地附加存储的缺点是灵活性。确实有一些场景是本地附加存储无法满足的，因此我们有时必须选择其他替代方案。

## 存储区域网络（SAN）

本地附加设备的逻辑替代方案是远程附加设备，虽然我们可能会认为我们会简单地以这种方式称呼这些类型的块设备，但实际上并不是这样。远程附加设备使用网络协议将*远程性*的概念引入存储，而远程设备所使用的网络被称为**存储区域网络**，因此，常见的术语通常将所有远程块存储称为**SAN**。

## SAN 的可怕术语

从技术角度讲，存储区域网络（SAN）应该是指一个专用网络，用于传输块设备流量，在非常专业的圈子里，这个术语是这样使用的。SAN 上的设备可以是直接的块设备、磁盘阵列以及其他类似的*网络上的块设备*。SAN 是网络，而不是一个你可以买到的*东西*。

然而，在日常用语中，通常将任何提供存储、实现块设备接口并连接到网络而非直接连接到计算机的设备称为 SAN。你每天都会听到这样的表达，比如*你买了 SAN 吗？*、*我们需要一个 SAN 工程师*、*我和我们的 SAN 供应商谈过了*、*我们应该升级 SAN 吗？*以及*我们的 SAN 在哪里？*去最近的 IT 硬件供应商那里，问他们卖不卖 SAN，他们会毫不犹豫地卖给你，这个术语已经如此标准，以至于可以打赌，如果你试图把 SAN 当成不是硬件设备的东西，他们会完全困惑，而这种设备是你放入硬盘并通过某种电缆连接到网络的设备。

由于存储既复杂又令人困惑，而且令人害怕，存储区域网络又在基础之上增加了额外的复杂性，这整个领域迅速被视为充满魔法和术语的黑匣子，术语也迅速恶化，大多数关于 SAN 的认知都基于误解和神话。常见的误区包括一些不可能的想法，比如 SAN 不可能出现故障、SAN 比没有网络层的相同技术更快、SAN 是其他技术的必需品等等。

只有当我们理解技术如何工作并避免陷入神话（和市场营销）时，才能成为有效的系统管理员。例如，如果我们认为某个设备是魔法般的，而不考虑其实际的风险状况，我们就无法做出有意义的风险分析或性能决策。

理论上，SAN 是一个非常简单的概念。我们将任何块设备，无论是像实际硬盘这样的物理设备，还是像磁盘阵列这样更复杂的概念，都将其封装在标准块设备协议（如 SCSI 或 ATA）中，并通过网络协议（如 TCP/IP、以太网或*FiberChannel*）传输。网络协议充当了一个简单的隧道，将块协议传输到远距离。就这么简单。归根结底，它仍然只是一个基于 SCSI 或 ATA 的设备，但现在可以在远距离使用。

当然，我们刚刚增加了一些复杂性，因此 SAN 自然比本地存储更脆弱。所有本地存储的风险和复杂性仍然存在，并且还增加了网络层和设备带来的复杂性和风险。风险是累积的。而且，额外的网络层、处理和距离都必然会给存储事务增加额外的延迟。

正因为这些因素，基于 SAN 的存储总是比其他相同配置的本地存储要慢且更脆弱。大多数神话所用来宣传 SAN 的那些因素，正是它们的弱点。

当然，SAN 的做法是有其优势的，否则它就没有存在的意义了。SAN 提供了三个关键特性：距离、整合和共享连接。

距离可以是从几英尺到跨越全球的任何距离。当然，距离越远，延迟越高，而存储通常对延迟非常敏感，因此远程块存储在超出本地连接技术范围的情况下很少有用。如果你必须通过广域网（WAN）获取块存储数据，你可能会遇到至少导致严重性能问题的延迟，并且通常会面临不可承受的带宽限制。典型的生产级块存储假定吞吐量是每秒多个 GB（大写 B，而不是小写 b）且延迟在毫秒级别，但 WAN 连接通常连一个 Gb/s 都难以达到，即便是最好的延迟通常也会是几毫秒，甚至几十毫秒以上！

整合曾是 SAN 的传统驱动力。因为许多系统可以通过单一网络连接到一个存储阵列，这使得第一次可以投资一个昂贵的存储系统，让多个物理分离的计算机系统同时使用。设备上的存储会被*切分*，每个连接到它的设备都能看到属于自己的独特存储部分。

### 当本地存储不再是本地存储

在 IT 领域，接口、抽象和不准确的术语层出不穷，这使得我们在很多时候很容易迷失，不清楚到底发生了什么。SAN 就是这样一个很容易让人感到困惑的地方。SAN 的特点是，它将远程的块设备呈现给计算机时，仿佛这个设备是本地的。但它也可以把本地设备表现为本地设备，尽管实际上它是远程的。我刚才说的是什么意思？

最好的例子就是外部 USB 硬盘。我们都在使用它们，它们非常常见。去任何本地的商场，你都可以买到。或者在线上订购。你大概在某个架子上放了五个，已经忘记了它们。虽然 USB 硬盘是外部的，但显然它仍然是本地的，对吧？

其实，说起来并不那么简单。没错，它在物理上是很接近的。但在技术术语中，远程意味着某个东西是*通过网络连接*的，而本地则是*不通过网络连接*的。无论某个东西有多远，决定本地和远程的因素是网络方面。否则，我位于德克萨斯的桌面和位于纽约的爸爸的桌面之间就因为有一整条电缆连接而被认为是物理连接的。

这带来了一个有趣的挑战，因为你看，USB 实际上是一个非常简单的网络协议，IEEE 1394 和 Thunderbolt 也是如此。如果你物理地解剖一个外部硬盘，你可以在某种程度上看到这一点。它们由标准硬盘组成，通常带有 SATA 接口，以及一个微型网络适配器，将 SATA 协议封装成 USB 网络协议并通过网络传输（通常只需两英尺的总距离）。

USB 及其类似设备可能不会让你觉得它是一个网络协议，但它确实是。它是一个二层网络协议，与以太网竞争，能够将多个设备连接到多个计算机，并且甚至能使用类似交换机的设备。它是一个真正的网络平台，这意味着通过 USB 连接的外部硬盘实际上是微型 SAN！这很难相信，但确实如此。考虑一下，你的思维可能会被震撼。

存储是大多数系统中最大的一笔开销，能够更高效地共享和切割存储降低了部署新物理计算机系统的成本。例如，硬盘可能有 1TB 的容量，但单个系统可能只需要 80GB 或 300GB，或者其他容量，而通过共享 SAN，数百台计算机系统可能共享一个存储阵列，并且每台计算机只使用它所需要的部分。今天，通过虚拟化，我们主要通过本地存储来实现这种效率，但在虚拟化广泛普及之前，只有像 SAN 这样的系统才能解决这个成本节省问题。因此，在 SAN 的早期，重点是成本节省，其他特性则是在后期才出现的。如今，这一价值大多已发生反转，因此通常比过度配置的本地存储更昂贵，但在某些情况下仍然存在。

最后一个价值是共享连接。这是指两个或多个计算机访问同一设备上相同存储部分的情况——看到相同的数据。这可能听起来有点像传统的文件共享，但实际上完全不是那样。

在文件共享中，我们习惯于计算机拥有一个*智能*的门控设备，用于调解对文件的访问。而在 SAN 中，我们必须记住，这是一个*无脑*的块设备，没有自己的逻辑。使用 SAN 将两个或多个计算机系统连接到单个逻辑块设备，意味着每台计算机都把存储看作是自己的私有、完全隔离的系统，并且对可能也连接到它的其他系统一无所知。这可能导致各种问题，从丢失的更改到损坏的文件，再到摧毁的文件系统。当然，有一些机制可以用于实现共享存储空间，但按照定义，它们并非由 SAN 实现，必须在计算机系统的更高层次提供。

### 共享 SCSI 连接

在 SAN 之前，或者说在 SAN 尚未普及和广泛应用的日子里，还有另一种技术允许两台计算机共享同一硬盘池：共享 SCSI。

使用这种技术，一根单独的 SCSI 带状电缆（通常可以连接八个、十六个，甚至三十二个设备。一个设备需要是控制器，通常在计算机的主板上。其他连接用于连接硬盘。但另一个连接器可以连接到另一台计算机上的控制器，两个计算机可以同时看到并访问相同的硬盘。  

由于需要在两台物理计算机之间共享一根带状电缆的限制，这种技术非常有限且笨拙，但仍然可行。这种设置的主要价值在于允许一台计算机系统故障时，另一台计算机可以接管，或者将分配给单一数据集的 CPU 和 RAM 资源加倍，超出单一服务器机箱的容量。但存储组件的可靠性和性能限制使得该系统通常不够实用，因此这种技术在实际中很少实施。但从历史角度来看，它非常重要，因为它是现代共享块存储的基础，它曾是 1990 年代末期系统培训中的标准知识，也有助于我们理解今天 SAN 的工作方式——更优雅、更灵活，但本质上仍然相同。  

今天，共享块存储连接的最大使用场景是为集群系统提供支持，这些系统设计用来将这种存储作为虚拟化的共享后端存储。大约在 2010 年时，这一方式最为流行，但如今已经被其他方法取而代之，来解决这种需求。现在这会是一个相对特殊的系统设计。然而，我们很快会看到，这里使用的技术将被改造用于其他存储模型。

SAN 的世界有许多流行的连接技术。有些超简单的 SAN 传输，简单到没有人认为它们是 SAN，包括 USB、Thunderbolt 和 IEEE1394/Firewire。然后是一些常见的企业级 SAN 协议，如 iSCSI（通过 IP 的 SCSI）、光纤通道、FCoE（以太网光纤通道）、*FC-NVMe*（光纤通道上的 NVMe）等。每种 SAN 协议都有其自身的优点和挑战，通常供应商只提供他们自己设备中的一小部分选择，因此选择供应商通常会限制你的 SAN 选项，而选择 SAN 选项则会限制你的供应商选择。理解所有这些协议将我们从系统世界带入网络世界。作为系统管理员，你很少有机会选择或影响 SAN 设计中的选择，通常这将由存储、网络和/或平台团队为你决定。如果你确实能够在这个领域产生影响，那么就需要对这些技术、它们的优点以及它们如何适应你的工作负载进行深入研究，但这远远超出了本书的范围。  

块存储并不会消失。尽管我们对新的存储技术（如对象存储）感到兴奋，但块存储仍然是所有其他存储类型的基础。我们必须在物理上和逻辑上了解块设备，因为在我们的存储平台的各种方式中，我们将使用它们作为构建块。块存储功能强大且普遍存在。它代表了我们在工程阶段中与之交互的大多数存储，并且预计将在未来几十年内继续保持核心地位。

当决定使用本地存储还是远程块存储时，有一个有用的经验法则：*在本地存储能够满足需求之前，您总是希望使用本地存储。或者在没有其他选择之前，您从不想使用远程存储。*

# 调查文件系统和网络文件系统

在块存储的顶部，我们通常找到一个**文件系统**。文件系统是计算机系统上主要（我指的是，它们构成了 99.999%或更多用例的）最终数据存储的方式。文件系统是我们所知的，在计算机存储中保存文件的工具。

文件系统是一种位于块存储之上的数据组织格式，提供了一种使用文件类比来组织、识别、存储和检索数据的机制。您每天在所有设备上都在使用文件系统。即使在您看不到它们的情况下，无论是在您的桌面、手机甚至 VoIP 电话或微波炉上！文件系统无处不在。

### 文件系统实际上是数据库。

如果你想和我一起稍微“极客”一下，并且诚实地说，你正在阅读一本关于系统管理最佳实践的书，所以我们都知道你喜欢深入了解一些严肃的细节，我们可以看看文件系统的真正含义。在其核心，文件系统是一个 NoSQL 数据库，具体来说是一个文件数据库（本质上是一种专门的文档数据库），它使用原始块设备作为其存储机制，只能存储和检索文件。

还有其他专业数据库直接使用块设备（在处理数据库术语时通常称为原始存储），但它们很少见。文件系统是一种如此常见的数据库类型，比所有其他数据库类型加起来都要常见，以至于没有人谈论或认为它们实际上是数据库。但在幕后，从各个方面来看，它们都是真正的数据库。

为了进行直接对比，无论类型如何，标准数据库都有标准化的存储格式、检索格式、数据库引擎（驱动程序），并且在某些情况下，还会有数据库管理层（它通常允许在同一系统接口中使用多个数据库工程师），以及用于访问数据的查询接口。无论你比较 MongoDB 还是 MS SQL Server，你会发现它们的文件系统行为是一样的。磁盘上选择的文件系统格式就是存储格式，检索格式就是 *文件*，数据库引擎就是文件系统驱动，Linux 中的数据库管理系统是虚拟文件系统（我们稍后会讨论），查询语言是实现于 C 语言中的一系列底层 POSIX 命令（同时我们还可以使用一些简单的基于 Shell 的抽象命令来提高便利性）。与标准数据库相比，根本无法区分它们！非常酷的技术。

一旦计算机系统部署完成，几乎所有从存储角度做的操作都涉及到文件系统的操作。在工程阶段，我们往往会过多关注块存储，而在运维阶段，则会更多关注文件系统。但显然，在将系统部署到生产环境之前，我们必须妥善规划文件系统。文件系统的正确规划其实是一个常被忽视的环节，很多人只是接受默认设置，很少考虑文件系统的设计。

大多数操作系统都原生支持几种不同的文件系统。在大多数情况下，一个操作系统有一个明显标准的主文件系统，并且还会有一些特殊的文件系统，这些文件系统通常用于特殊硬件设备，或者是为了兼容其他系统。例如，Apple macOS 在执行所有正常功能时使用 APFS（Apple 文件系统），但在处理光盘时使用 ISO 9660，在与 Windows 存储设备（如 USB 存储棒或外部硬盘）兼容时使用 FAT32 和 **exFAT**。Windows 操作系统也类似，不过它使用的是 NTFS，而不是 APFS。最近，Windows 添加了一个替代文件系统 ReFS，用于特殊需求，但它并不常见，且使用者不多。

然而，在 Linux 中，我们有多个主要的文件系统选项和数十种专用文件系统选项。我们无法在此一一介绍它们，但我们将讨论几个最重要的文件系统，因为理解它们的存在意义以及选择合适文件系统的时机非常重要。幸运的是，在生产环境中，我们实际上只需要关注几个关键的文件系统。如果你对文件系统感兴趣，可以深入研究许多 Linux 文件系统选项，以了解更多关于文件系统设计和历史的信息，或许你还会找到一个适合特殊用途的文件系统！

目前我们需要关注的关键 Linux 文件系统有：XFS、EXT4、ZFS 和 BtrFS。这四种文件系统几乎涵盖了我们日常使用的所有操作。还有一些不那么流行，但已经很好集成并且工作非常稳定的文件系统，如 JFS 和 ReiserFS，虽然它们几乎不会在生产环境中出现。还有一些较旧的文件系统，如 EXT2 和 EXT3，已被更新版本所取代。另有一些标准文件系统，如 Windows 的 NTFS 或 BSD 系列的 UFS，可以在 Linux 上使用。还有像 ISO 9660 和 FAT32 这样的标准小众文件系统，我们之前提到过。Linux 在每个方面都提供了选择，文件系统的选择便是其灵活性的一个典型例子。

### EXT：Linux 文件系统家族

几乎每个操作系统都有自己的、独特的文件系统，作为其本地或默认使用的文件系统，并且与操作系统紧密相关，Linux 也不例外……开个玩笑，实际上 Linux 正是一个例外，这一点令人惊讶，因为 Linux 在文件系统选项上的强大远超其他任何操作系统。Illumos 有 ZFS，FreeBSD 有 UFS，Windows 有 NTFS，macOS 有 APFS，AIX 有 JFS，IRIX 曾有 XFS，等等。Linux 确实没有属于自己的文件系统，但它几乎包含了所有其他系统的文件系统。

大多数人都把 EXT 文件系统家族看作是 Linux 的本地文件系统，确实没有其他文件系统能与之相提并论。在 Linux 开发初期，早在任何人实际运行它之前，MINIX 文件系统被移植到 Linux 上，成为默认的文件系统，随着这个新操作系统的崛起而广泛使用。但是，正如名字所示，MINIX 文件系统原本是 MINIX 的本地文件系统，早于 Linux 出现。

就在 Linux 发布的第一年，EXT 文件系统（或 MINIX 扩展文件系统）应运而生，它以 MINIX 文件系统为基础，并且如你所料，加入了许多新特性，主要集中在时间戳处理上。

随着 Linux 的发展，EXT 文件系统也在不断发展，仅仅在 EXT 发布一年后，它的继任者 EXT2 就作为一次重要的升级发布，将 Linux 的文件系统生态系统从一个爱好型系统转变为一个严肃的企业级系统。EXT2 从 1993 年推出开始，几乎独占了 Linux 生态系统，直到 2001 年 Linux 经历了一次文件系统革命。EXT2 是如此重要的进步，以至于它被回移植到 MINIX 本身，并且出现了 Windows 和 macOS 等其他操作系统的驱动程序。可能没有任何文件系统比 EXT2 更加*标志性*地与 Linux 相关联了。

到了 2001 年，许多操作系统开始寻求更先进的文件系统技术，以在市场中获得竞争优势，Linux 通过引入更多的文件系统选项，并且在 EXT2 上添加日志功能，将其版本更新为 EXT3，从而实现了这一点。这为 EXT 文件系统家族带来了急需的稳定性。

又过了七年，我们对 Linux 的准原生文件系统进行了一个重要的升级，推出了 EXT4。令人惊讶的是，EXT3 和 EXT4 的主要开发者表示，尽管 EXT4 是一个巨大的进步，但它本质上仍然是 1980 年代技术的权宜之计，主要是在原有基础上进行改进。文件系统设计原则在 2000 年代初期有了飞跃发展，而 EXT 系列很可能已经走到了发展的尽头，但它仍然具有很长的使用寿命。

我将深入探讨每种主要文件系统选项的一些细节，但需要明确的是，这只是一个粗略的了解。文件系统的细节会迅速变化，并且在不同版本或实现中有所不同，因此对于最大文件大小、文件数量、文件系统大小等非常具体的细节，请参阅 Wikipedia 或文件系统的文档。你不需要记住这些细节，也很少需要了解它们。在 1990 年代，文件系统的限制非常明显，你必须时刻警惕并在每个环节绕过这些限制。如今，我们将使用的任何文件系统几乎都能应对我们遇到的任何挑战，因此我们真正需要理解的是不同产品的优缺点，以及何时在高层次上选择使用哪一个。

## EXT4

Linux 作为一个类别，并没有像其他操作系统那样有默认的文件系统，但如果你今天想要主张某个文件系统应当被称为默认文件系统，那么这个荣誉应该归于**EXT4**。目前，部署最广泛的 Linux 操作系统选择 EXT4 作为默认文件系统，而不是其他任何文件系统。但这种情况正在开始发生变化，因此 EXT4 在未来几年内保持主导地位的可能性不大。

EXT4 的速度合理，可靠性强，灵活性高，广为人知，能够满足几乎任何部署的需求。它是 Linux 文件系统中的全能选手。对于典型的部署，EXT4 的表现相当不错。

EXT4 是我们所说的*纯文件系统*，这意味着它仅仅是一个文件系统，不做其他任何事情。这使得它更易于理解和使用，但也使得它的功能更加有限。

## XFS

与 EXT 系列一样，**XFS** 源自 1990 年代初期，来自 Linux 的竞争者——SGI 的 IRIX UNIX 系统。它具有悠久的历史和强大的稳定性，并且在 2001 年被移植到 Linux，这与 EXT3 发布的年份相同。二十年来，EXT3/4 和 XFS 一直在 Linux 管理员（以及 Linux 发行版的开发者）中竞争，争夺默认文件系统的位置。

XFS 也是一个*纯文件系统*，并且非常常用。XFS 以其极高的性能和可靠性而著称。有时，高性能应用程序（如数据库）会特别推荐使用 XFS，以保持它们在高峰期的运行。

当系统管理员故意选择文件系统而不是简单地接受默认设置时，XFS 可能是部署最多的文件系统，也可能是应用程序供应商最推荐的文件系统，而且它是我自己在大多数存储需求非凡的工作负载中的个人选择。

多年来，EXT4 和 XFS 在流行度上一直在交替变化。我个人的观察是，XFS 在这些年中逐渐领先。

与 EXT4 相比，XFS 常被引用的一个警告是 EXT4 能够在部署后对卷进行缩小*或*扩展。XFS 可以扩展，但不能缩小已经部署的卷。然而，在一个正确部署的生产系统中，缩小文件系统几乎是闻所未闻的，因此这通常被视为琐事，与文件系统决策无关（尤其是在薄配置块存储出现之后）。

## ZFS

ZFS 于 2006 年发布给 Solaris，**ZFS** 通常被认为是现代文件系统设计的基础。当 ZFS 的开发工作在 2001 年开始时，业界已经开始非常认真地对待文件系统设计，并且许多新的概念定期被引入，但 ZFS 确实将这些设计范式提升到一个新的高度，至今 ZFS 仍在许多领域中处于领先地位。

ZFS 其实有三个高层次的领域，它试图彻底颠覆文件系统行业。首先是容量：ZFS 能够解决比任何之前的文件系统多几个数量级的存储容量。其次是可靠性：ZFS 引入了比其他文件系统更强大的数据保护机制，使其能够以显著的方式防止数据丢失。第三是集成性：ZFS 是第一个真正的*非纯粹*文件系统，它代表了一个文件系统、一个 RAID 系统和一个逻辑卷管理器，三者都集成在一个单一的文件系统驱动程序中。我们将在本章后面详细讨论 RAID 和 LVM。这个集成性非常重要，因为它使得存储层能够像从未有过的那样相互通信和协调。像 EXT4 和 XFS 这样的纯文件系统可以使用这些技术，但通常是通过外部组件而非集成的方式。

虽然 ZFS 并不新颖，它至少在生产系统中已经使用了十五年，但对于 Linux 的发布而言，它是相当新的。在为 Linux 提供 ZFS 移植版本之前，花费了多年时间，而且又有许多年因许可问题，ZFS 没有以可消费的格式发布给 Linux。如今，唯一一个正式支持并打包 ZFS 的主要 Linux 发行版是 Ubuntu，但 Ubuntu 在市场上的主导地位使得 ZFS 自动广泛可用。目前，距离 ZFS 可以用于 Ubuntu 上可引导的根文件系统还不到两年。因此，ZFS 在生产环境中的 Linux 系统中仍然是以一种广泛可访问的方式相当新的。现在，由于 ZFS 已经可用，它的使用似乎正在迅速增长。

ZFS 代表了截至目前 Linux 平台上最先进、最可靠、最具可扩展性的文件系统。需要注意的是，从纯粹的文件系统性能角度来看，ZFS 并不是以性能突出而闻名。虽然存储性能调整在文件系统层面上很少被认为是有价值的，但当它成为必要时，现代文件系统由于其额外的可靠性，通常无法与较旧、更基础的文件系统竞争。必须指出这一点，因为人们常常默认现代系统也会自动更快，但实际上情况并非如此。

## BtrFS

读作*Butter-F-S*，**BtrFS**是当前 Linux 本地文件系统的重要尝试（在 2000 年代初曾有过一次名为 ReiserFS 的尝试，虽然取得了一些进展，但由于非技术原因最终失败）。BtrFS 旨在模仿 ZFS 的工作方式，但它是本地化的 Linux 文件系统，并且具有兼容的许可证。

BtrFS 在许多特性上落后于 ZFS，许多功能仍未实现，但工作仍在继续。BtrFS 依然充满活力，越来越多的 Linux 发行版开始支持它，甚至选择将其作为默认文件系统。BtrFS 似乎是 Linux 长期未来中最有可能的选择。

像 ZFS 一样，BtrFS 是一个现代化、深度集成的文件系统，开始包含存储堆栈中 RAID 和 LVM 层的功能。当前，BtrFS 的性能是其最弱的环节。

Stratis

鉴于其行业支持，我们需要提到 Stratis。Stratis 本身并不是一个文件系统，*本质上*它更像是一个文件系统。Stratis 试图利用现有的 XFS 和标准 Linux LVM 层，构建集成（或*卷管理文件系统*）功能，如 ZFS 和 BtrFS。

在早期的 IRIX 上，XFS 设计用于与 IRIX 的本地 LVM 一起使用，两者自然地集成，提供的功能与今天的 ZFS 或 BtrFS 非常相似。当 XFS 被移植到 Linux 时，关联的 LVM 层并没有被移植，而是使本地的 Linux LVM 与其兼容。XFS + LVM 一直是业界的标准做法，而 Stratis 仅仅是试图提供一种更易于访问的方式，同时整合最佳实践和简化管理。

这总结了你可能遇到或需要选择的四种当前生产环境中的文件系统选项。请记住，你可以在单一系统上混合使用不同的文件系统。事实上，使用 EXT4 作为基本操作系统功能的启动文件系统，同时依赖 XFS 作为高性能数据库存储文件系统，或者依赖 BtrFS 作为大文件服务器的文件系统，这种情况非常常见。根据每个文件系统层的工作负载选择最合适的文件系统。不要觉得你只能在所有系统上使用同一个文件系统，更不要觉得在一个系统内必须统一使用一个文件系统！

文件系统的许多技术细节都涉及到处理在块设备上查找和存储数据的算法。这些算法的细节远远超出了本书以及系统管理的范围。如果你对文件系统感兴趣，学习数据是如何存储、保护和从磁盘中检索的，可以说是非常迷人的。对于系统管理任务而言，理解文件系统的高层次内容就足够了。

可惜的是，没有办法提供一个关于文件系统选择的真正最佳实践。你不太可能在生产环境中需要认真考虑使用这里未提到的任何稀有文件系统，但这里列出的四个文件系统都有有价值的使用案例，所有这些都应该考虑。通常，文件系统的选择并不是孤立做出的，除非你在处理一个非常特定的产品，该产品需要或推荐使用某个特定的文件系统以支持某个功能。相反，文件系统的选择通常会依赖于许多其他存储决策，包括 RAID、LVM、物理支持需求、驱动介质等。

## 集群文件系统

到目前为止我们讨论的所有文件系统，无论它们多么现代，都是*标准*文件系统或*非共享*文件系统。它们仅在访问保证仅来自单一操作系统时才可行。在几乎所有情况下，这样是完全足够的。

然而，回想一下我们在讨论 SAN 时提到的，我们提到过有些使用场景可能需要多个计算机系统能够同时读取和写入同一区域的存储。集群或*共享存储*文件系统就是能实现这一点的机制。

集群文件系统的工作原理与传统文件系统相同，但它们具有额外的功能，能够将锁定和共享信息写入文件系统，使多个计算机系统能够协调它们之间连接的节点使用文件系统。在标准文件系统中，只有一台计算机在访问文件系统，因此知道哪个文件已打开、文件何时已更新、写入何时被缓存等都在内存中处理。如果两台或更多计算机尝试共享来自传统文件系统的数据，它们不能在内存中共享这些数据，因此不可避免地会因为相互覆盖对方的更改、未能检测到更新的文件，以及各种由于过时的写缓存导致的糟糕情况而导致数据损坏！

由于这些系统唯一共享的组件是文件系统，因此所有访问文件系统的节点之间的通信必须在文件系统内部进行。实际上，除非使用一种不再是共享存储而是共享计算的机制，否则没有其他可行的方法，而这种机制要复杂得多，成本也更高。

为了简单描述集群文件系统的工作原理，我们可以将每台计算机视为通过文件系统得知，某个块设备（硬盘）上的特定区域被严格格式化并预留出一个固定大小的空间，用于记录节点与文件系统交互的当前状态。如果节点 A 需要打开文件 X，它会在该区域记录下它正在保持该文件打开的状态。如果节点 B 删除一个文件，它会在该区域记录下它正在删除该文件，并会在文件删除后进行更新。节点 C 通过读取文件系统中的这一小部分内容，能够知道正在进行什么操作。所有连接的节点都知道不缓存该区域的数据，记录它们计划执行的任何操作，并记录它们所做的任何事情。如果有任何节点行为不当，整个系统会崩溃，数据将丢失。

当然，正如你所看到的，这至少会带来很大的性能开销。而且这个系统必然需要所有连接节点之间的绝对信任，因为访问控制和数据完整性控制完全依赖于各个节点。没有，也不可能有任何机制强制节点正常工作。节点必须自愿遵守规则。这意味着，代码中的任何 bug、内存故障、任何拥有 root 权限的管理员、任何能够访问单个节点的恶意软件等等，都可以绕过*任何*和*所有*控制，读取、修改、销毁、加密等，任意程度地进行操作，并且我们通常认为能够保护我们的所有安全控制都不存在。共享存储非常简单，但由于我们习惯了存储抽象，反而变得难以想象任何存储系统会如此简单。

和常规文件系统一样，Linux 有多个集群文件系统，这些文件系统我们经常会见到。最常见的是 GFS2，其次是 OCFS2。

和 SAN 一样，集群文件系统也遵循相同的规则：你不会想要使用它们，除非你必须使用。

## 网络文件系统

**网络文件系统**总是有点难以描述，但从*你看到的时候就知道了*的现象来看，它们能很好地被理解。与常规文件系统不同，常规文件系统依赖于块设备并提供以文件形式访问存储的方式，而网络文件系统则将文件系统扩展到网络上。这听起来很像 SAN，但它们实际上非常不同。SAN 是通过网络共享一组块设备，而网络文件系统则是通过网络共享文件系统。

网络文件系统非常常见，你可能每天都会看到它们，但我们常常没有意识到它们到底是什么。我们通常把网络文件系统称为*共享*或*映射驱动器*，而标准的协议有 NFS 和 SMB（有时也叫 CIFS，这其实并不完全准确）。实现网络文件系统的服务器被称为文件服务器，如果你将文件服务器做成设备，它叫做 NAS（网络附加存储）。正因如此，网络文件系统也常常被认为是*NAS 协议*，就像块级网络协议被认为是*SAN 协议*一样。

与共享块协议不同，网络文件系统是*智能的*，共享存储的机器拥有一个它理解的本地文件系统，并且对涉及的文件有一定的智能，这样像文件锁定、缓存、文件更新等概念就可以通过一个单一的守门员来处理，该守门员能够执行安全性和完整性验证，而且不需要信任访问节点。关键的区别在于，SAN 只是盲目地连接到网络的存储，它可以简单到只是一个硬盘上连接的网络适配器（实际上它通常就是这样）。而实现网络文件系统的设备，则是一个服务器，要求有 CPU、内存和操作系统才能工作。共享块存储几乎只在少数受控的服务器上使用。网络文件系统可以在几乎任何 SAN 可以使用的地方使用，但它们也常用于直接向最终用户设备共享存储，因为它们的强大安全性、易用性和对终端节点信任的缺乏，使它们在 SAN 无法部署的地方变得非常有用。

网络文件系统作为一个额外的网络启用层运行在传统文件系统之上，并不取代我们已经拥有的*磁盘上的*文件系统。在谈到接口时，我们会描述网络文件系统为*消耗文件系统接口*，同时也*呈现文件系统接口*。基本上，它就是文件系统进，文件系统出。

就像传统文件系统一样，Linux 实际上提供了许多网络文件系统选项，其中许多都是历史性的或极其小众的。一个常见的例子是**Apple 文件协议**或**AFP**（也叫*AppleTalk*），Linux 也提供这种协议，但今天已经没有任何生产操作系统在使用它。如今，只有 NFS 和 SMB 在任何实际工作中有应用。

### NFS

原始的*网络文件系统*在广泛使用中，字面上也就是这个名称的来源，**NFS**可以追溯到 1984 年！NFS 不能是 Linux 的原生协议，因为它早于 Linux 出现七年，但自诞生以来，NFS 一直是所有基于 UNIX 或受其启发的操作系统中的默认网络文件系统，并因此代表了一个相当重要的标准。由于今天 Linux 如此突出，大多数人把 NFS 当作是*Linux 的协议*。

NFS 几乎在所有系统上都有提供。任何 UNIX 系统，甚至 macOS，都提供 NFS，Windows Server 也支持 NFS！NFS 是一种开放标准，几乎是普遍存在的。NFS 通过简单易用、网络稳健且通常表现良好的特点保持了其流行性。NFS 在服务器中仍然被广泛使用，尤其是在需要系统之间直接文件共享的备份系统中。

### SMB

**Server Message Block**（**SMB**）协议早于 NFS，最初于 1983 年推出。这些确实是非常古老的协议。直到 1990 年代微软开始积极推广 SMB，随着 Windows NT 平台在 1990 年代的崛起，SMB 才开始广泛使用并逐渐流行起来。

SMB 受益于微软大量使用映射驱动器在其服务器与工作站之间进行连接，这使得 SMB 协议在传统用户和技术用户中都非常显眼。

在 Linux 中，SMB 协议的支持由 Samba 包提供（Samba 这个名字其实是对 SMB 的一个玩笑）。Linux 对 SMB 的支持很好，但使用它比起 NFS 更为复杂。

在 Linux 上选择 NFS 或 SMB 进行文件共享，通常取决于具体的使用场景。如果主要使用 UNIX 系统，通常选择 NFS 更为合理。如果主要使用 Windows 系统，则一般选择 SMB。两者都非常强大且稳健，可以满足各种需求。

做出决策时，难度通常非常大，尤其是在可以通过完全不同的方式提供相同需求的情况下。例如，如果需要为虚拟化提供共享存储后端，可能有网络文件系统如 NFS，或者在 SAN 上使用集群文件系统如 GFS2。由于这两种方法涉及的每个方面可能都不同，包括供应商和硬件，因此通常需要在整个技术栈层面进行比较，而不仅仅是在网络技术层面进行比较。

现在，我们已经探索了文件系统技术，了解了 Linux 系统中多种实际的文件系统选择，并且看到文件系统可以是本地的或远程的，可以是单一访问的，也可以是集群的以支持多重访问。在此过程中，我们对如何做出文件系统选择和配置决策有了更好的理解。我们知道何时选择不同的文件系统技术，并且知道在我们可能没有专门探讨的新的或替代的系统中应寻找什么。文件系统不必令人害怕或困惑，它们可以是我们工具箱中的宝贵工具，用来微调我们的系统，提升安全性、可扩展性、访问性或性能。接下来，我们将探讨存储领域中最难理解的部分之一——*逻辑卷*。

# 了解逻辑卷管理（LVM）

我不喜欢用*新*这种词来形容那些在 1980 年代末就已经在使用的技术，但与大多数计算机存储概念相比，**逻辑卷管理**（**LVM**）确实相对较新，并且对大多数系统管理员来说，远不如其他标准存储技术那么知名。在 Linux 于 1998 年推出第一个广泛可用的产品之前，LVM 曾经仅限于极高端的服务器系统，微软在 2000 年跟进。如今，LVM 已经无处不在，并且通常在大多数操作系统中是原生且默认提供的。

LVM 是目前使用的主要存储虚拟化技术。LVM 允许我们将任意数量的块设备（通常称为*物理卷*）合并、分割或以其他方式修改它们，并将它们呈现为任意数量的块设备（通常称为*逻辑卷*）供系统使用。听起来可能很复杂，但实际上并非如此。通过一个实际的例子，我们会发现它实际上相当简单。

一个例子是，当我们有一个计算机系统并连接了三块硬盘时。它们可以是完全相同的，也可以是不同的。事实上，其中一个可能是传统的旋转硬盘，另一个是现代的固态硬盘，还有一个是外部 USB 硬盘（或者是 RAID 阵列、SAN，随你取名）。我们可以将这三块硬盘作为物理卷添加到我们的 LVM 中。LVM 允许我们将其视为一个单一的存储池，并将其转化为任何我们想要的配置。我们可能会将它们合并成一个逻辑卷，这样我们就能得到这三块硬盘的总存储空间。或者，也许我们会创建十几个逻辑卷，并将每个卷用于不同的目的。我们可以有任意数量的物理卷，也可以创建任意数量的逻辑卷。逻辑卷的大小可以是我们想要的任何大小（在某些情况下甚至可以超过总物理大小！）我们不再受到传统磁盘大小的限制。使用逻辑卷时，我们通常发现创建更多、更小的卷非常有用，这样可以更好地管理和隔离。

使用 LVM 时，我们可以将系统视为一个消费块设备并呈现一个块设备。由于 LVM 使用并提供块设备（也称为磁盘外观），它们是*可堆叠*的，意味着如果你愿意，你可以在一个块设备上使用 LVM 来创建一个逻辑卷，然后这个逻辑卷又被另一个 LVM 使用，再创建一个新的逻辑卷，以此类推。虽然这种方式并不实际，但它有助于我们理解 LVM 在*存储栈*中的位置。它总是处于中间，但除了在中间之外，它非常灵活。

LVM 只需要提供基本的*块输入，块输出*功能，就能成为一个 LVM，但通常还会添加其他功能，使 LVM 更加实用。一些常见的标准功能包括逻辑卷的在线调整大小、*热插拔*物理设备、快照功能、缓存选项和精简配置。

在 Linux 中，和大多数其他事情一样，我们不仅有一个，而是有多个逻辑卷管理器！随着近年来创建集成文件系统及其独立 LVM 的趋势，这种做法越来越普遍。目前在 Linux 生产环境中，我们有 LVM2、ZFS 和 BtrFS。你当然会认出后两者是我们之前提到过的文件系统。当大多数人在谈论 Linux 上的逻辑卷管理器时，他们指的是 LVM2，通常简称为 LVM。但 ZFS 和 BtrFS 的集成逻辑卷管理器也越来越受欢迎。

由于 LVM 的*可堆叠*特性，即它能够消费和提供块设备，我们可以选择将 LVM2 与 ZFS 或 BtrFS 一起使用，并且可以禁用它们的集成 LVM 层（因为它们不必要），或者如果它们有我们希望利用的功能，我们也可以继续使用它们！这简直是灵活性极致体现。

## 分区到底怎么了？

如果你还记得上世纪 90 年代在 IT 领域的工作，我们曾经经常讨论磁盘分区。那是一个常见的话题。如何设置分区，分多少个，基本分区和扩展分区，使用什么分区软件，等等。可以肯定的是，分区仍然存在，只是我们已经很久没有需要它们了（例如自 Windows 2000 或 Linux 2.4 以来）。

分区是一种非常僵硬的*磁盘上*系统，用于将物理磁盘切割成多个区域，每个区域都可以作为单独的块设备（即驱动器）呈现给系统。这样，分区就像是没有灵活性的超级基础 LVM。分区只能作为单个块设备的一部分存在，哪个区域属于哪个分区的映射信息保存在设备开头的一个简单分区表中。

分区是逻辑卷的前身，至今仍有人使用它们（但只是因为他们不熟悉逻辑卷）。分区缺乏灵活性，缺少诸如精简配置和快照等逻辑卷所能提供的重要选项，而且虽然技术上可以调整分区大小，但这既不灵活，又困难且极其危险。

LVM 提供了分区所提供的所有功能，并且拥有更多功能，而不需要放弃任何东西。多年来，分区（即将块设备分割成多个文件系统）的需求显著减少。在 1990 年代末期，即使是最简单的服务器，甚至是台式机，通常也需要有充分的理由将其划分为多个文件系统。如今，将多个块设备合并为一个文件系统已变得更为常见。这主要是因为文件系统的性能和可靠性发生了彻底的变化，而分区的驱动因素逐渐消失。今天仍然有必要进行文件系统划分的理由，但我们已经不需要那么频繁地这么做了。

目前许多机制，如备份工具，利用 LVM 层的功能执行任务，例如冻结块设备的状态，以便进行完整的备份。由于 LVM 在最终的文件系统层之下运行，因此它具备其他层所不具备的某些能力。LVM 是存储层，它提供的功能相对较少关键，但正是这里发生了许多神奇的事情。LVM 为我们提供了灵活性，使得在初始部署后可以修改存储布局，并且能够在块级别与存储系统进行交互。LVM 是现代二十一世纪操作系统提供使用体验的核心技术组件。

当然，任何新技术层都会有一些限制。LVM 增加了一个额外的复杂层，也给系统管理员带来了更多的理解内容。学习管理 LVM 并不是什么大工程，但比起没有 LVM 时需要学习的内容，它还是要多得多。LVM 还会带来一定的性能开销，因为它需要在物理设备和逻辑设备之间进行转换。通常，这个开销非常小，但依然存在。

总的来说，LVM 的好处远远超过了成本，越来越多的系统开始直接部署 LVM 层，而不询问最终用户是否需要它，因为越来越多的功能已经依赖于 LVM 层，而不允许系统在没有 LVM 的情况下部署，往往会让客户陷入困境，无法理解他们的系统为何无法达到预期，通常这种情况可能在初次部署后几个月或几年才会被发现。

像其他形式的虚拟化一样，存储虚拟化和 LVM 最重要的是 *防止未知的风险*。如果我们知道一个系统在整个生命周期内的所有使用情况，诸如调整大小、备份、整合等功能就几乎没有意义，但现实世界并非如此。

关于 LVM 的最佳实践通常认为是：除非你能明确提供充分的技术理由，证明 LVM 的开销会产生显著影响，并且这种影响超出了 LVM 提供的保护，否则始终部署 LVM。

逻辑卷管理器为强大的存储解决方案提供了一个关键构建块，在许多方面，它们将现代存储与传统计算系统区分开来。了解逻辑卷如何抽象存储概念，并使我们能够以我们想要的方式操作和构建存储，为我们提供了许多选择，并引导我们进入 RAID 和 RAIN 等附加概念，接下来我们将讨论这些概念如何利用 LVM 构建数据保护、扩展和性能能力。

# 使用 RAID 和 RAIN

我们已经看过了许多与存储接口的方式。但最令人兴奋的可能是当我们开始处理**RAID**（**廉价磁盘冗余阵列**）及其后代**RAIN**（**独立节点冗余阵列**）时。在深入讨论之前，必须指出，RAID 是一个庞大的话题，要真正有意义地讨论它需要一本书。理解 RAID 的工作原理以及理解其性能和风险细节所需的所有计算是一个独立的重大课题。我的目标是介绍这个概念，解释它如何融入设计，展示关于它的最佳实践，并为进一步的研究做好准备。

RAID 和 RAIN 是将多个*存储设备*（块设备）结合起来，利用设备的自然多重性（通常误称为冗余）来提供在单一硬盘中无法实现的性能、可靠性或可扩展性的一些组合。与 LVM 类似，RAID 和 RAIN 是*中间层*技术，消费块设备接口并提供块设备接口，因此可以*叠加*在 LVM 之上，LVM 之下，另一个 RAID 之上，或者在各种硬件设备的组合上，等等。非常灵活。

事实上，RAID 和 RAIN 实际上是 LVM 的特殊形式！没有人会这样讨论这些技术，如果你在公司圣诞派对上开始讨论 RAID 作为一种特殊的 LVM，你可能会引来一些奇怪的眼光，但它实际上就是。RAID 和 RAIN 是 LVM 功能的极端子集，关注点非常狭窄。实际上，通用 LVM 通常会内置 RAID 功能，而且集成文件系统的趋势是将 LVM 和 RAID 层都与文件系统集成。

## RAID

RAID 是*廉价磁盘冗余阵列*（Redundant Array of Inexpensive Disks）的缩写，最初作为一套在块设备级别工作的技术被提出，用来将多个设备合并为一个。类似 RAID 的技术可以追溯到 1960 年代，RAID 这个术语和现代定义来自 1988 年，这意味着 RAID 实际上比更通用的 LVM 要早出现。

RAID 本质上是将多个块设备放在一起，并通过许多不同的数据存储机制（称为*级别*）使它们保持同步。每个 RAID 级别的工作方式不同，并采用不同的机制将底层的物理磁盘合并成一个虚拟磁盘。通过使多个硬盘像一个硬盘一样工作，我们可以根据需要扩展存储的不同方面，但在一个领域的收益通常会带来另一个领域的成本，因此理解 RAID 的工作方式非常重要。

RAID 是一个非常依赖系统管理员深入了解存储子系统内部工作原理的领域。令人惊讶的是，这也是一个极少数系统管理员真正理解其系统工作原理的领域。

虽然 RAID 被定义为一系列*级别*，但不要被迷惑。级别只是不同类型的存储，它们共享 RAID 的基本原理。RAID 级别并不是相互递进的，较高的级别并不代表某种内在上更优越的产品。

因为 RAID 实际上是一种 LVM，它可以位于存储栈中的任何位置，并且在实际应用中几乎无处不在。一些非常流行的 RAID 级别实际上是*堆叠 RAID*，利用了其设计的这一固有特性，最著名的就是 RAID 10。

RAID 也有硬件和软件两种变体。硬件 RAID 很像一个直接连接到显示器的显卡，它将工作从主计算机系统中卸载，并直接与硬件通信。硬件 RAID 卡正是执行这一任务，减轻主计算机系统的负载，直接与硬件存储设备接口，并可能提供一些特殊功能（如缓存）。而软件 RAID 则利用通常更强大的系统 CPU 和内存，并具有更灵活的配置。两种方法都是完全可行的。

每种 RAID 级别都有一组独特的属性，并且在不同的时刻使用各自有意义。RAID 是一个复杂的话题，值得单独成册深入探讨。RAID 并不是我们可以草率了解的一个话题，这曾经是存储领域的一个隐患。RAID 的风险因素常常被浓缩成一些毫无意义的陈述，比如*一个 RAID X 级别的阵列可以从多少个驱动器故障中恢复？* 这并没有任何意义，它的目的只是将一个非常复杂的东西简化为可以轻松记住或者直接放在图表上的内容。RAID 并不是这样的。每种 RAID 级别在性能、可靠性、成本、可扩展性、实际应用等方面都有一个复杂的背景故事。

## RAIN

随着系统变得更大、更复杂，RAID 方法的局限性开始显现。RAID 简单且易于实现，但 RAID 缺乏灵活性，对于一些关键特性，如简单的大小调整、自动重平衡和灵活的节点大小，它处理得很差。因此，需要一种新的技术家族。

RAIN 摒弃了 RAID 基于的*全块设备*阵列方法，改为通过较小的块（通常是块）将存储分割，并在该级别处理复制。为了有效地做到这一点，RAIN 不仅必须理解这些块的概念，还必须理解它们所在的块设备（或*磁盘*），以及它们存在的节点。这种节点感知赋予了 RAIN 其名字：*独立节点冗余阵列*。

奇怪的是，在 RAIN 中，实际上并不是节点必须冗余，而是块，实际上你可以在单一物理设备上实现 RAIN，直接与传统 RAID 的最简单形式竞争，但这很少见。

由于 RAIN 处理的是块级复制，因此相对于 RAID，它可以有许多优势。例如，它可以灵活地使用不同大小的设备。不同大小的驱动器可以非常高效地被放入服务器并结合使用。

在 RAID 下，如果一个硬盘驱动器发生故障，我们需要一个能够接替它在阵列中位置的替代硬盘。这通常是个问题，尤其是在旧的阵列中。RAIN 可以通过允许阵列中的任何可用容量来吸收故障硬盘的丢失容量，从而避免这个问题。

RAIN 有多种实现方式，每种实现方式本质上都是独特的，因此我们无法以任何标准化的方式讨论它。今天大多数解决方案都是专有的，虽然一些著名的开源产品已经问世，并成为 Linux 生态系统的标准组件，但它们通常是独立于发行版的，且在我们使用时的行为很像专有产品。

未来，随着这些技术变得更加普及和被理解，我们可能会看到 RAIN 市场内的显著整合，或者至少是标准化。在那之前，我们需要以理解块复制*如何工作*的方式来接触 RAIN，并知道每种实现可能做出截然不同的设计选择。RAIN 可能内建在内核中，也可能作为更高层次的应用程序运行在堆栈中，在某些情况下，它甚至可能运行在虚拟机中，通过虚拟化在虚拟机监控器上运行！RAIN 如何应对丢失的硬盘、负载均衡、位置亲和性、故障丢失后的再平衡、修复后的重建等问题并没有任何标准来定义。使用 RAIN 时，必须深入研究和学习你将考虑的任何解决方案，并批判性地思考其工件如何随着时间的推移影响你的工作负载。

RAIN 几乎可以确定将是系统存储的未来。随着我们越来越多地朝着集群、超融合、云和类云设计发展，RAIN 显得越来越自然。随着理解的增加，RAIN 的采用将只会增加。尽管这项技术本身并不新颖，但这显然需要时间。

几乎所有我们将设计或支持的生产系统，无论是本地还是远程，都将使用某种形式的 RAID 或 RAIN。到现在为止，我们已经准备好思考选择何种 RAID 级别、配置或 RAIN 实现会如何影响我们的系统。花时间深入理解这些多设备聚合框架中存储因素如何相互作用，是系统管理员在各个领域最有价值的高级知识之一。在下一部分，我们将基于这些技术，看看如何通过外部系统或节点使本地存储实现冗余。

# 学习复制本地存储

可能是最关键的存储类型，也是最难理解的存储类型之一，就是**复制本地存储**或**RLS**。RLS 并不是一个难懂的概念，它其实很简单。但围绕着其他概念（如 SAN）有许多误解，这些误解模糊了 RLS 的功能。例如，许多人开始将*共享存储*作为*外部存储*或*SAN*的代名词。但外部存储并不意味着它是或可以被共享的，本地存储也不意味着它不能或不会被共享。

“复制本地存储”这一术语指的是两个或多个计算机系统，它们之间的本地存储是相互复制的。从每个计算机系统的角度来看，存储是本地附加的，就像正常情况一样。但是有一个过程会将数据从一个系统复制到另一个系统，这样在一个系统上进行的更改就会出现在另一个系统上。

复制本地存储可以通过多种方式实现。最简单且最早的方式是使用**网络 RAID**，即 RAID 技术通过网络来使用。**镜像 RAID**（也称为**RAID 1**）是实现这一目标的最简单技术，也是最好的示例。

有两种方式来处理这种情况，一种是热/冷配对，其中一个节点是*热*的，可以访问并写入存储，而另一个节点（或多个节点）可以从中读取，并且在原本的热节点失败或放弃控制时，假设其他节点能够接管成为热写节点。这种模型很简单，类似于许多传统的 SAN 共享存储模型。这种方法允许使用常规（非集群）文件系统，如 XFS 或 ZFS。

另一种方法是活/活系统，其中所有复制存储的节点都可以随时读取和写入。这要求与任何共享块存储一样的集群文件系统支持。就像两个节点同时使用 SAN 一样，RLS 集群中的节点将需要通过在集群文件系统的特殊区域存储其活动数据来进行通信。

复制本地存储可以为我们带来许多通常与 SAN 或其他外部存储相关的好处，即多个节点可以同时访问数据的能力。同时，它还具有本地性的好处，包括提高性能和弹性，因为依赖关系较少。当然，复制流量有其自身的开销，这需要考虑。有许多方式可以配置复制，有些开销非常小，有些则开销很大。

常见的误解是认为复制本地存储是新的、创新的或在某种程度上不寻常的。事实恰恰相反。实际上，鲜为人知的是，对于高可靠性的存储系统，RLS 始终被使用。无论是本地使用（即直接连接到计算系统），还是远程使用（意味着远程存储使用 RLS 以提高其可靠性），RLS 都是几乎所有真正高可用存储层的核心。

RLS 有多种形式，主要有网络 RAID 和 RAIN。我们可以在这种情况下称之为网络 RAIN，但我们并不这样做。与几乎总是仅限于本地的 RAID 不同，RAIN 几乎总是在 RLS 环境中使用，因此它的网络特性几乎是默认的，或者至少是网络选项。

Linux 上有许多形式的 RLS，因此我们无法真正讨论所有选项。我们将专注于一些更常见的选项。RLS 是一个领域，既有开源解决方案，也有商业和专有解决方案，性能、可靠性和功能各异，且实施方式常常截然不同。RLS 可能会为任何存储场景增加新的复杂性，因为你必须考虑本地存储通信、复制通信、节点间和远程存储之间的潜在网络通信（即本地存储位于其他节点上）以及算法和协议如何相互作用。

## DRBD

Linux 上的第一个也是最简单的 RLS 技术是**DRBD**，即**分布式复制存储系统**，它是一个直接集成到 Linux 内核中的网络 RAID 层。维基百科表示 DRBD 不是*网络 RAID*，但随后又将其描述为完全的网络 RAID。它是否真的是网络 RAID 可能仅仅是语义问题，实际上在使用中，它与网络 RAID 在使用、描述，甚至底层实现上没有区别。像所有 RAID 一样，DRBD 消耗块设备并作为块设备出现，允许它像常规 RAID 和 LVM 一样，堆叠在存储栈的任何位置。

DRBD 基于 RAID 1（镜像 RAID）机制构建，因此支持两个或多个节点，每个节点获取一份数据副本。

DRBD 非常灵活且可靠，由于其简洁性，大多数系统管理员更容易清楚地理解它是如何工作的，以及如何融入存储基础设施。然而，由于 DRBD 限于通过镜像进行完整块设备复制，就像 RAID 1 一样，因此其扩展能力相当有限。单独使用时，DRBD 非常专注于经典的双节点集群或有大量计算节点需要共享少量相同数据的特定用途案例。

### 使 DRBD 更加灵活

由于 DRBD 本质上只是一个软件 RAID 工具，而且因为你对其有完全的管理权限，并且因为 RAID 作为一个 LVM 技术，具备在堆栈中任意位置灵活部署的能力，你可以将 DRBD 转变为比初看上去更具可扩展性的工具。但目前这个过程完全是手动的，尽管理论上你可以通过脚本或工具自动化这些过程。

我们可以使用的一项强大技术是通过额外的逻辑块设备来*错开*我们的 RAID 1，从而模拟 RAID 1E，其基本上类似于 RAID 1，但具有可扩展性。该技术的原理是将单个节点上的物理存储空间通过 LVM 技术逻辑性地划分为两部分（或理论上更多）。在标准的网络 RAID 设置中，节点 1 上的整个存储空间会镜像到节点 2 上的整个存储空间。现在由于我们在每个节点上划分了存储空间，我们将节点 1 的第一部分镜像到节点 2 的一部分；节点 2 也会这样做，但是是与节点 3；节点 3 同样与节点 4 进行镜像；这个过程将无限延续，直到终端节点编号的机器与节点 1 完成*循环*，每台机器的数据都采用 RAID 1，并且数据在两个其他节点之间分割作为镜像对。通过这种方式，我们可以使网络 RAID 1 环扩展到无限大。

这个技术确实非常强大。但它在文档编写和维护方面非常繁琐。如果你有一个静态的集群且不会变化，它可以非常有效。但如果你经常扩展或修改集群，它可能会成为一个相当棘手的问题。

DRBD 和大多数网络 RAID 技术通常具有良好的整体性能，更重要的是，其性能相当可预测。由于 DRBD 本质上提供最终的块设备，它本质上是本地的。为了远程访问 DRBD 资源，必须将 DRBD 作为构建块应用于一个 SAN 设备，然后将其共享到远程。这个过程当然只是语义上的问题。DRBD 始终是本地的，因为对 DRBD 来说，SAN 就是本地计算节点，SAN 接口是堆栈中的另一个更高层次，因此虽然 SAN 是远程的，但 DRBD 仍然是本地的！

## Gluster 和 CEPH

**Gluster** 和 **CEPH** 完全是两种不同的技术，但它们都是免费的、开源的现代 RAIN 解决方案，专为 Linux 设计，提供高可靠性和高度可扩展性。这两种解决方案至少都提供了将存储设置为与计算节点本地化的选项。这两者都是非常复杂的解决方案，具有多种部署选项。我们不能简单地假设使用这两种技术中的任何一种就意味着存储是本地的还是远程的。就应用来说，本地化是更常见的情况，但两者都提供了构建一个可以远程访问的独立存储层的选项，如果设计成这样，它可以通过网络进行访问。

这些技术过于复杂，选项也太多，因此我们无法在此深入探讨。我们只能从非常高的层次上对其进行处理，尽管这应该足够满足我们的需求。

这种 RAIN 存储是处理大规模服务器池（计算节点）共享存储池的最常见方法。该技术使存储能够本地化，在发生故障时自动重新平衡，但很少能够保证数据本地性。整个组中的存储只是一个池。因此，数据可能有本地化的亲和性，但不像 DRBD 那样严格强制本地化。这给了 DRBD 更多控制，但 Gluster 或 CEPH 提供了更多的灵活性和更好的利用率。

## 专有和第三方开源解决方案

除了操作系统中自带或可能包含的 Linux 发行版组件外，还有许多第三方组件可以安装。几乎所有这些产品都属于 RAIN 类别，价格、支持和功能各异。一些值得了解的名字包括*LizardFS*、*MooseFS*和*Lustre*。

不可能涵盖所有可能的商业产品，这些产品可能已经存在或未来可能出现。RAIN 存储是一个正在发展的领域，仍然有许多厂商在这个领域生产产品，但并未广泛提供。在某些情况下，你可能会找到商业 RAID 或 RAIN 系统，这些系统只有在与某种类型的设备一起提供或在某些其他项目中包含时才能使用。但所有这些存储系统遵循相同的基本概念，一旦你了解了这些概念以及它们的工作方式，即使你之前没有使用过某个具体的实现，也能做出关于存储系统的正确决策。

## 存储的虚拟化抽象

在谈论存储时，往往很容易迷失方向，忘记了大多数情况下，存储根本不是我们作为系统管理员需要担心的问题！至少不像我们一直以来处理存储的方式。

### 存储管理员

在大型组织中，将存储和系统职责分开并不罕见，因为存储本身有许多复杂性和细微之处，成立一个专门团队来理解它们是有意义的。如果你曾在财富 500 强企业工作过，你可能见过这种情况。

这一问题的最大困境之一在于，将深刻理解工作负载的人与一些决定这些工作负载性能和可靠性的重要因素分开。分离往往还意味着核心架构决策需要通过政治手段而非技术手段来做出。如果使用本地存储，就无法在任何实际情况下将存储和系统团队分开。因此，许多组织使用了通常糟糕的技术设计决策，在组织内创建技能孤岛，而没有考虑这种做法会如何影响工作负载。SAN 技术的部署通常就是为了这个目的。

无论这种方法的有效性如何，实际使用时通常意味着存储脱离了系统管理员的控制。这极大地简化了我们的工作，同时在提供最终价值的能力上却使我们受到了很大的制约。我们可以请求某些性能或可靠性水平，并且必须信任我们的需求会得到满足，或者至少不会因存储失败而被追究责任。

同样，系统和平台团队分离也很常见。在这种情况下，我们看到了同样的效果。管理底层虚拟化的虚拟化平台团队将提供存储容量给系统团队，而系统团队则必须简单地使用提供给他们的资源。

在这两种情况下，存储都从系统中抽象出来，并简单地作为*盲目块设备*提供给我们系统团队。当这种情况发生时，我们仍然需要理解底层组件的工作原理，知道该问哪些问题，最终仍然需要在提供的块设备上管理文件系统。块设备接口依然是存储或平台团队与系统团队之间的通用*交接*接口。

另一个附带说明：同样的情况通常也会发生在平台团队。它们可能需要从存储团队获取盲目存储，应用到虚拟化层，然后将该块设备划分成更小的块设备，提供给系统团队！

今天在大多数情况下，我们的 Linux 系统会以某种方式被虚拟化。我们必须全面了解整个堆栈的存储，因为 Linux 本身可能是虚拟化管理程序（例如 KVM 的情况），或者被用来控制虚拟化管理程序（例如 Xen 的情况），或者为更高级别的虚拟化管理程序（如 VirtualBox）提供存储，在所有这些情况下，Linux 都有可能管理存储体验的每个方面。Linux 还可能被用来创建 SAN 设备或其他形式的存储层。我们必须全面了解存储的内外，但在大多数情况下，我们的 Linux 系统将从其虚拟化管理程序获取存储，即使我们是该虚拟化管理程序的管理者。

虽然它们可以选择以多种不同的方式行事，但大多数人会设置虚拟化管理程序以充当存储的 LVM 层。它们有点特殊，因为它们将数据从块转换为文件系统，然后再转换为块以传递给虚拟机，但概念上是相同的。一些虚拟化管理程序设置将简单地通过直接块连接到底层存储，无论是本地磁盘、SAN 还是来自 LVM 的逻辑卷。这些都是有效的方法，并且为虚拟机留下更多选择，以决定它将如何与存储交互。但总的来说，预期的虚拟化方式是将存储的块层终止于虚拟化管理程序，将其转换为文件系统，在文件系统之上创建*块设备容器*，并允许虚拟机像使用常规块设备一样消耗这些设备，许多人实际上将这种虚拟化方法的结果称为虚拟化本身的内在特性，但这并不正确。

您也可以在系统内部使用这种技术。例如，挂载文件系统文件类型如*qcow2*、*vhdx*或*iso*文件！这是我们每天都在做的事情，但很少有人会去思考或意识到我们实际上在做什么。

显然，当从虚拟化管理程序获取存储时，对标准（非复制）本地存储、复制本地存储、标准（非复制）远程存储或复制远程存储的关注都是在系统之外的不同层次进行的，但决策仍然是必须的，并且这些决策完全影响我们的系统最终如何运行。

现在我们已经学习了很多存储抽象方法和范式，比如 LVM、RAID 和 RAIN。现在我们需要开始考虑如何将这些技术结合起来构建我们自己的存储解决方案。

# 分析存储架构和风险

没有什么比存储给我们的系统带来更大的风险了。这是不言而喻的，但必须要说。存储是我们作为系统管理员能够产生最大影响的地方，也是我们最有可能失败并且失败惨重的地方。

为了应对存储中的风险和机会，我们必须理解整个存储堆栈，以及每一层和每个组件如何相互作用。存储可能让人感到压倒性，那里有太多的活动组件和可选组件。

我们可以通过提供成功的设计模式，并理解在何种情况下应该考虑不同的模式，来缓解一些压倒性的感觉。

## 一般存储架构

存储架构有两个真正高层次的维度：**本地**与**远程**，以及**标准**与**复制**。

当然，对于大多数人来说，自然的假设是立刻相信复制和远程是显而易见的起点。实际上，这并不正确。这可能是存储中最不合理的起点，因为它结合了最不可能有用的因素。

## 简单的本地存储：砖块

不管信不信，对于各类公司的最常见适用存储设计是本地的、**非复制存储**！请记住，当我们谈论存储架构时，*复制*并不指没有备份或没有本地复制（如 RAID 镜像），而仅仅指存储是否在实时或近实时的情况下复制到第二个完全独立的系统。

当我们从整个系统设计角度来看，而非单独看存储时，我们会再次讨论这个问题，并且会有些不同。就像生活中的大多数事情一样，保持简单通常是最有意义的。复制听起来很惊人，是必备功能，但复制需要花费大量资金，且通常需要很多成本，而且要做好复制，通常会影响性能，可能影响得非常严重。

### 复制灾难

存储设计中的一个常见错误是产生一种情感上的感觉，认为复制越多，我们就越能免受灾难的影响。在某种程度上，这是真的，本地复制一些文件并使用 RAID 1 能有效保护我们免受硬盘故障的影响，远程复制则能保护我们免受整个节点失败的影响，但这两者都无法防止更常见的问题，如文件误删除、恶意文件破坏、文件损坏或勒索病毒。

如果我们做一些简单的事情，比如删除一个不该删除的文件，那么瞬间，我们强大的复制机制将确保我们的删除操作会在一两毫秒内传播到整个系统中。它可能不是保护我们，而是成为比我们反应更快地复制我们的错误的机制。过度构建复制机制通常只对抗硬件故障有保护作用，并且可能迅速变成回报递减的局面。

第一级 RAID 可能非常有价值，因为硬盘故障依然是一个非常现实的风险，即使是最微小的硬盘故障也可能导致重大数据丢失。但节点之间的复制仅能防止整个系统的丢失，而这种情况远不如硬盘故障常见。RAID 保护相对便宜，实施成本通常只有几百美元。然而，节点间的复制需要大大更多的硬件，通常需要花费数千或数万美元，提供的保护也只是 RAID 已经提供的一小部分。

像 RAID 这样的机制，尤其是 RAID 1（镜像），也非常容易实现并且非常直接。在镜像 RAID 中，由人为错误引起的数据丢失是相当罕见的。节点之间的复制存储则不同，出错的可能性更大，且人为错误导致数据丢失的几率也高得多。我们并不是通过选择这条昂贵的路来简单地减少风险，而是引入了其他我们也必须减轻的风险。

许多系统管理员认为他们不能使用简单的本地非复制存储，且公司政治问题不可忽视。如果你的公司要 *搞政治*，并将责任推给你，作为系统管理员，即使错误不在你，并且你的决策对业务来说是最好的，那么你就被迫做出对业务盈利性没有帮助的危险决策。这是系统管理员无法控制的事情。

作为系统管理员，我们可以通过呈现（并做好文档记录）风险和财务决策，来管理这种政治问题，*在某些情况下*，以此证明即使最终导致数据丢失的决策，仍然是正确的决策。没有任何决策可以消除所有风险，作为 IT 专业人员，尤其是系统管理员，我们始终在做出决定，权衡要减少多少风险，以及我们应该以什么样的财务成本来进行。

### 风险评估

IT 和尤其是系统管理员领域最具挑战性且最重要的一个方面就是进行风险评估，以便进行适当的规划。风险很少以正式或自然的方式进行教学。这是几乎所有企业都在此领域惨败的地方，而 IT，作为我们所做的所有事情中至关重要的一部分，通常没有培训、没有资源，也没有支持。

教授风险本身就是一项职业，但这里可以简单介绍一些我们应该始终使用的技术。风险的核心是将成本分配到预计的利润上。

我们有两个风险的关键方面。一个是坏事发生的概率，另一个是事件的影响。第一个我们可以表达为*每年发生 X 次*，如果你觉得这样更方便。第二个可以用金钱来表达，比如*它会花费 5000 美元*。如果某事每十年发生一次，你可以说它是每年 0.1 次。所以，影响我们 5000 美元的事情，其年度成本将是 500 美元。这个方法极其简单，实际上并不是风险运作的真实方式。但它是一个在向管理层表达风险决策时非常有用的工具，他们希望将数百万个因素提炼成一个最终的数字。

现在我们需要拿出显示风险缓解策略成本的数字。例如，如果我们要实施一个复制技术，年许可费为 300 美元，并且每年需要十小时的系统管理员时间，每小时 120 美元，那么我们可以预计风险缓解成本为 1500 美元/年。

下周需要一个缓解效果。没有什么是百分之百的。但是一个好的复制策略可能有 95%的有效性，典型的策略可能有效性在 65%左右。通过这些数字，我们可以做一些精确的数学计算。

我们知道我们每年面临大约 500 美元的损失风险。如果我们每年花费 1500 美元，就可以有 95%的把握阻止这 500 美元的损失。500 美元 * 0.95 = 475 美元。所以，1500 美元-475 美元 = 1025 美元，这是风险缓解策略每年造成的损失。这些数字你可以带给 CFO。做这道数学题，你应该能展示出节省或保护，而不是损失。如果你展示的是损失，那么你真的，真的需要避免这个计划。这意味着风险保护机制，实际上是通过实施它就代表了一个*灾难*。

数学，可能听起来有些老生常谈，但平均的系统管理员甚至平均的 CFO 通常会避开使用基本数学来判断一个想法的好坏，而完全凭借情感做决策。使用数学将保护你。它意味着你可以去找 CFO 和 CEO，站稳脚跟。你无法与数学争论。展示给他们看数学，如果他们认为数学是错的，让他们自己计算。如果他们决定忽视数学，那么你就知道你所在的公司是什么样的，你真的应该认真思考，未来在一个不认为利润是驱动力的公司里会有什么样的前景。如果未来出了问题，你被责备了，你可以拿出数学并问，*如果这个决定不正确，为什么我们在做决策时没能从数学中看出来？*

没有什么比成功地为一个看似疯狂的决策辩护更让人感觉好，这个决策是有数学依据的。证明你正在做最好的工作。不要只是说，不要做没有根据的声明。利用数学并证明你做出决策的理由。将决策从猜测提升到科学的层面。

不是所有工作负载都可以这么简单地处理。但比通常假设的要多得多的工作负载是可以的。除非你有坚实的数学依据来证明相反，否则这应该是你的标准假设。或者你正在处理超出数学范畴的情况，例如生命支持系统，在这种情况下，正常运行时间和可靠性比金钱更为重要。否则，使用数学。

这种所有存储方法中最简单的方法，通常被认为是*砖块*。它简单、稳定、可靠，容易备份和恢复，易于理解和支持。今天，这种解决方案非常有效，尤其是在现代存储技术和硬件的帮助下，我会说它适用于 85-90%的所有生产工作负载，无论公司大小，至少适用于 99%的小型企业工作负载。

## **RLS**：超高可靠性解决方案

那些不适合上述简单架构的工作负载和情况几乎总是属于这一类别：**复制本地存储**（**RLS**）。**RLS**允许以合理的成本提供高可用的存储，且性能出色。没有任何解决方案能与我们最初提到的直接本地存储在性能和成本上匹敌，但如果你需要比该方案提供的更高的可用性和更好的可靠性，这几乎肯定是适合你的解决方案。

**RLS**提供了可用的最高可靠性，因为它的移动部件更少。远程高可用性解决方案必然需要处理距离、电缆和网络协议，至少需要这些额外的组件，通常还会额外涉及网络设备（如交换机），这些都比**RLS**的风险更大。记住，远程存储解决方案如果要实现真正的高可用性，必须通过在本地集群中使用**RLS**来实现，然后将该存储集群通过网络远程提供，这样你就会遇到所有与**RLS**相关的复杂性和潜在问题，再加上远程访问技术的开销和风险。

如果直接本地存储没有远程复制，占用了所有工作负载的九成，那么标准的**RLS**（复制本地存储）将占据剩下的九成（这两者加起来应该占据大约 99%的工作负载）。在进行合理规划时，这两种选项是如此简单、具成本效益且安全，正常情况下它们几乎无法被超越。这些是你的核心选项。

### 替代存储可靠性

虽然**RLS**可能感觉像是存储保护的终极解决方案，但它并不是。它在你必须依赖存储层来处理可靠性时非常出色。但从这个角度来看，它更像是一个权宜之计或临时解决方案，而不是终极方案。

在一个理想的世界中，我们有比我们实际存储层更高的存储机制，例如数据库。数据库管理系统能够更好地维护可用性，并协调节点之间的数据复制，而这是一个盲目的块复制机制永远无法做到的。在应用智能所在的地方放置复制是非常合理的选择。

理想情况下，应用程序应该使用它们的数据库作为存储和状态可靠性的层，并让了解数据使用方式的智能系统复制重要数据。数据库是复制的最理想机制之一，因为它们了解它们拥有的数据，并能够做出明智的决策。

因此，许多企业应用程序根本不使用任何形式的存储，甚至没有系统复制。使用*不可靠*的系统和高度可靠的*应用程序*是一种稳固的策略，提供了其他方式无法获得的好处。因此，有时我们可以忽略原始存储层的高可用性需求，而只关注性能。

## 实验室环境：远程共享标准存储

这种架构非常受欢迎，因为它符合销售人员的所有需求：*昂贵*、*有风险*和*令人困惑*。的确，销售人员推动这种设计比其他任何设计更多，因为它为客户提供了许多附加服务的计费方式，同时将所有责任推给客户。

所有架构在生态系统中更多或少都有合法的位置，这种架构也不例外。但在我们列出用例之前，我们应该先说明它的好处所在：非复制的远程存储比本地存储机制*更慢*、*更有风险*，而且表面上更*昂贵*。我们在这样的设计中找到价值的地方在哪里？

主要的好处在于实验室、测试和其他环境中，数据耐久性的价值可以忽略不计，但是我们可以从大型环境中获益，可以审慎地划分存储空间，以实现大规模的最大成本节约。

几乎每个大型环境在其工作负载的某个阶段都需要这种类型的存储。关键是要确定这种需求混合起来是合理的，而不是试图在不合适的地方应用它。因为这种存储在大规模时很便宜（但在小规模时非常昂贵），而且因为经理们经常错误地将远程共享存储误认为是*不可能出错的神奇盒子*，所以它经常被用在最不适合的地方。没有魔法，对于大多数生产工作负载来说这是一种不良技术，应该极度谨慎地选择使用。

这里的经验法则是：只有在你能够决定性地证明它是有意义的情况下，才应该使用这种类型的存储——即使是标准的可靠性和性能也不再有价值。如果你有任何疑虑，或者组织中有人有疑虑，那么在使用这种架构之前选择其他存储架构，这样可以避免最大风险。性能是*软性故障*，在事后很容易纠正，而且如果做错了，通常影响也较小。数据丢失是*硬性故障*，做错了就不是优雅的失败，而是灾难性的失败，事后几乎没有纠正的余地。

优雅失败

我们不能总是避免失败。事实上，很多时候规避失败是我们工作的重要组成部分。为了让这一切有效，我们必须理解如何优雅地失败，而这一点的关键部分就是*优雅失败*的概念。在存储领域，这个概念比其他领域更为突出。

优雅失败的理念是，如果我们失败了，我们希望它是以一种小的、渐进的方式，而不是悲剧性的全面灾难的方式发生。我们做出的许多存储决策正是基于这个理念。我们知道我们可能会出错。所以，我们要确保尽可能接近正确，同时也要考虑到*如果*我们出错了怎么办。

通过这种方式，我们通常倾向于在 RAID 中使用 RAID 10，并且在架构中倾向于使用本地存储。如果我们错了，我们希望结果是更加安全，而不是因为我们认为无关紧要而丢失数据。

虽然决定一个完整架构不仅仅依赖于错误的存储决策，但远程非复制存储是许多供应商和经销商常用设计的基础，他们通常称之为**3-2-1 架构**，而 IT 从业者则称之为**倒金字塔毁灭架构**。

这种架构传统上被广泛部署，但它无疑是在生产环境中最不适用的架构。它慢、复杂、昂贵，而且风险最大。它主要在实验室环境中有意义，因为在这种环境下，重建存储的数据最多也只是耗时。这种架构真正是围绕通常不在生产环境中的工作负载需求设计的。

## 巨大的规模：远程复制存储

我们需要考虑的最后一个主要架构是*最大*的一种：远程复制存储。看起来这可能是每个企业都会采用的存储架构，虽然它并不算稀有，但实际上它的使用频率远低于你想象的。

远程复制存储在小规模部署时实现成本最高，但在大规模时可以变得相对更实惠。它在复杂性上比 RLS 更高（这意味着可靠性较差），性能也低于 RLS 或直接本地存储，因此只有在节省成本至关重要、但仍需一定可靠性时才有意义。这是一种相对小众的选择。

考虑到这两种本地存储架构在工作负载部署中占据了 99%的比例（当然是我亲自做的），因此这个架构承担了剩余的最后那一小部分，至少在生产环境中是这样。

### 最安全的系统也只能做到这么安全。

我在系统管理员的数十年经历中，最具戏剧性的故事之一，发生在我作为顾问在一所大学图书馆工作的那段时间。我被请来负责处理一些大规模的数据库，该图书馆的系统只有两名管理员。高级管理员正在度假，只有初级管理员在场。这个环境使用了一套高度冗余且高可用性的 SAN 系统，所有的系统都依赖于它。为了保护这个系统，从 UPS 到发电机，再到硬件冗余，投入了巨大的成本。

在我在那里时，初级管理员决定对 SAN 进行一些维护，但由于某种原因，不小心点击了删除 SAN 上所有卷的操作。这当然是一次意外，但却是一个非常粗心的错误，发生在一个假设所有可能的故障场景都已被仔细防范的人身上。然而，所有的高可用性、所有的冗余、所有的特殊技术，都未能解决简单的人为错误问题。

只需她点击一下鼠标，整个图书馆的计算机系统就消失了。应用程序、数据库、用户、日志——所有的一切。所有内容都依赖于一个单一的存储系统，而这个系统几乎不费力就能被有权限的人关闭或删除。所有的鸡蛋都放在了一个篮子里，虽然这个篮子做得非常坚固，但却有一个大开口，轻易就能被颠倒。

更糟糕的是，初级系统管理员并未做好情绪上的准备，因害怕丢掉工作而感到恐惧，以至于必须住院，而所有可能协助的人才都忙着给她叫救护车。由于技术规划不足，再加上人力规划不当，本应仅是一个小规模恢复灾难的轻微事件，变成了一次巨大的故障停机。幸运的是有良好的备份，系统得以较快恢复。但这件事深刻反映出，我们常常投入大量资源去防范机械故障，却忽视了人的脆弱。实际上，人为错误往往比机械故障更可能引发灾难。

存储架构和风险是存储设计中最困难的部分。深入挖掘文件系统的细节通常是有趣的，并且对于我们作为系统管理员来说风险极小。如果我们选择了 EXT4 而 BtrFS 本来更好，惩罚可能是名义性的，以至于我们永远不会预料到任何人会发现我们没有做出完美的决策。但选择错误的存储架构可能导致巨大的成本、长时间的停机或严重的数据丢失。

我们确实需要花时间了解我们业务和工作负载的需求。什么样的风险是可以接受的，我们需要什么样的性能，如何以最优成本满足所有需求。如果我们不知道答案，那么我们就需要找出来。

最佳实践始终是花时间了解所有业务需求，学习所有可用因素并加以应用。但在实践中这很难做到。一些经验法则可以帮助我们很多。

## 存储最佳实践

试图将存储简化为最佳实践实际上是相当困难的。在最高层次上，存储的基本规则是没有捷径，你需要了解存储基础设施的所有方面，理解工作负载，并应用这些综合知识，同时了解机构风险规避，以确定工作负载的理想情况。

更进一步，最佳实践包括：

+   RAID：如果数据值得存储，那么在 RAID（或 RAID）上拥有数据是值得的。如果你对在服务器上使用 RAID（至少）存储数据的价值产生疑问，那么重新考虑是否存储数据。

+   RAID 物理性：硬件和软件 RAID 实施同样可行。确定哪些因素最适合你的需求。

+   LVM：就像我们将在未来章节中讨论的一般虚拟化一样，存储虚拟化不是提供我们在第一天就需要的具体功能集。它是提供机制来保护未知并且对未来发生的任何事情具有灵活性。除非你能提出非常强有力的论据证明 LVM 是不需要的，否则请使用它。

+   文件系统：不要被炒作或潮流所困扰。研究今天对你重要并有可能在未来保护你的功能，并选择一个可靠、健壮且经过测试的文件系统，让你对自己的文件系统选择感到放心，长期不会受到阻碍。

+   存储架构：除非你能证明你需要或从远程存储中获得显著的财务利益，否则保持你的存储访问本地化。除非你能展示从节点复制中获得明显的好处，否则不要在节点之间复制。简单胜过复杂。

作为系统管理员，你可能很少需要处理存储设计决策。但无论有多不频繁，存储设计决策对我们长期系统的影响是所有决策中最为深远的。花时间认真确定每个工作负载所需的内容。

# 存储示例

我们应该退后一步，举个例子，看看这些组件如何在现实场景中组合起来。我们不能合理地为每个常见的，甚至是可能的存储场景做出示例，但希望通过这一章的内容，你能对我们讲的内容有个初步的理解。

为了保持事情尽可能简单，我将尽可能采用最常见的小型和中型企业设置，或者至少是应该是它们最常见的设置。

小型企业通常受益于保持其设计相当简单。由于缺乏大规模、经验丰富的员工，并且通常面临较高的员工流动风险，小型企业需要那些需要较少维护并且能够由没有环境内在知识的顾问或员工轻松维护的系统。

对于这些环境，尤其是对于许多更大型的环境，硬件 RAID 和热插拔硬盘非常重要。它们允许硬件维护由专业人员完成，而不需要参与系统管理任务。在处理机房托管或其他远程设施时，这一点尤为重要。在这些情况下，IT 部门的人员可能根本无法亲自参与。

所以，我们从一些通用的假设开始。在物理层面上，我们有八个硬盘。这些硬盘可以是旋转硬盘、SSD、NVMe，任何块设备都可以。其实并不重要，重要的是我们有多个硬盘，但我们希望它们作为一个单一单元来使用。

接下来，我们添加了一个硬件 RAID 控制器。我们用这个 RAID 控制器将所有硬盘连接起来，并将它设置为将它们放入适当的 RAID 级别。这可以是任何一个选项，但在这个示例中，我们假设它们使用的是 RAID 10。

从我们示例的一开始，即使我们还没有安装 Linux 或类似的系统，我们已经利用系统的硬件实现了前两层存储！包括物理设备和第一个抽象层。

我们在这里不会展示实际插入硬盘的过程，因为这纯粹是一个手动过程，而且你的服务器供应商可能已经为你完成了这一步。

至于设置 RAID 本身，每个控制器和供应商可能略有不同，但基本原理是相同的，任务始终非常简单。这正是 RAID 控制器的主要目的之一 - 将 RAID 操作的必要知识和规划最小化，无论是在前期还是在运行阶段。在我们这里的示例中，为了更容易演示，我们假设我们正在处理一个单一数组，而这个数组不是我们操作系统正在运行的数组，这样我们可以更容易地从命令行中展示一些步骤。但这只是一个例子。

请记住，硬件 RAID 对于每个供应商甚至每个供应商的每个产品都是不同的。因此，您始终需要了解您的特定产品的工作方式。

此外，我们应该注意到，对于 RAID 控制器来说，连接到它的每个驱动器都是一个块设备。这是唯一一种情况，其中块设备接口假装是一个物理硬盘，实际上确实是一个硬盘！在随后的每个情况中，我们将使用软件来实现硬盘的接口，但我们代表的驱动器将是逻辑的，而不是物理的：

```
=>ctrl slot=0 create type=ld drives=2I:1:5,2I:1:6,2I:1:7,2I:1:8 raid=1+0
```

这是一个真实的世界 RAID 控制器的语法。通常情况下，您将通过 GUI 以图形方式执行此任务。但有时您可能希望使用命令行实用程序。在可能的情况下，我会从命令行工作，这样做更可重复，也更容易进行文档化。

一旦我们完成了初始硬件配置阶段，我们就可以继续进行示例的 Linux 特定部分。

硬件 RAID 控制器通常在/dev 文件系统中创建自己的命名约定。在我们的示例中，控制器使用了`cciss`语法，并在该系统下创建了设备`c0d1`。所有这些都会根据您使用的控制器和配置而异。

接下来，我们将在 RAID 层之上创建一个逻辑卷层，以在存储系统中提供更大的灵活性。为此，我们必须首先将新创建的设备添加到 Linux 的 LVM 系统中作为*物理设备*。我们使用`pvcreate`命令和我们新设备的路径来完成这一步骤：

```
# pvcreate /dev/cciss/c0d1
```

非常快速和简单。现在 LVM 子系统已经意识到我们的新 RAID 数组设备。但当然，所有 LVM 知道并关心的只是它是一个块设备。它是一个特定的 RAID 数组实际上并不是 LVM 可以检测到的，也无关紧要。关键在于它被抽象化处理，无论它是什么，都可以使用相同的方式进行利用。速度、容量和安全特性都封装在 RAID 层中，现在我们可以将其视为一个硬盘在前进过程中。

另一个有趣的地方在于，当使用硬件 RAID 控制器时，这种抽象和虚拟化的硬盘表示只是一个逻辑硬盘，但作为块设备，它实际上是物理的！这真是令人惊讶，不是吗？它确实是硬件，只是不是硬件硬盘。花点时间思考这个。

现在我们的 RAID 阵列已由 LVM 管理，我们可以将这个驱动器添加到一个卷组中。在本示例中，我们将其添加到一个新的卷组中，这个卷组只是为了本示例而创建的。我们将这个新组命名为 `vg1`。以下是执行此操作的示例命令：

```
# vgcreate vg1 /dev/cciss/c0d1
```

好的，现在我们有所进展了。由 RAID 控制器组合的各个物理硬盘的容量，现在在一个容量池或*卷组*中，我们可以开始切割这个容量的真正有用的子集来在我们的服务器上使用。

卷组创建完成后，剩下的就是制作实际的*逻辑卷*。请记住，逻辑卷已取代*分区*，成为将块设备划分为可消耗部分的主要方式。在我们的示例中，我们将做最简单的事情，告诉 LVM 系统只创建一个尽可能大的逻辑卷；也就是说，使用卷组可用容量的 100%（因为这是我们第一次使用它，目前使用率为 0%）：

```
# lvcreate -l 100%FREE -n lv_data vg1
```

此命令告诉 LVM 创建一个新的逻辑卷，使用可用的所有空闲空间的 100%，在卷组 `vg1` 中，并命名新的逻辑卷为 `lv_data`。就是这样。现在我们有一个可以使用的逻辑卷！显然，我们也可以创建一个更小的逻辑卷，比如可用空间的 50%，然后再创建第二个逻辑卷，使用剩余空间的 100%，这样我们就得到了两个大小相等的逻辑卷。

记住，在 Linux 中找到的 LVM 系统会为我们提供灵活性，如果直接将文件系统应用于物理驱动器甚至 RAID 控制器硬件提供的虚拟驱动器上，我们通常会缺乏这种灵活性。LVM 系统允许我们向卷组添加更多物理设备，从而为制作逻辑卷提供更多容量。它还允许我们调整单个逻辑卷的大小，无论是扩展还是收缩。LVM 还允许我们为逻辑卷创建快照，这对于构建备份机制或准备进行风险系统修改以便快速恢复非常有用。LVM 执行非常重要的功能。

现在 `lv_data` 已创建，我们通常需要使用文件系统对其进行格式化，以使其真正有用。我们将使用 XFS 格式化。在今天的现实世界中，XFS 可能是推荐用于通用需求的文件系统：

```
# mkfs.xfs /dev/vg1/lv_data
```

非常简单。几秒钟之内，我们应该有一个完全格式化的逻辑卷。通过应用文件系统格式，我们停止了块设备接口的链条，现在呈现的是文件系统接口，这种变化使应用程序能够以标准方式使用存储，而不是使用存储系统组件所使用的块设备。

当然，还需要最后一步，我们必须将新的文件系统挂载到一个文件夹中以使其可用：

```
# mkdir /data 
# mount /dev/vg1/lv_data /data
```

就是这样！我们刚刚为其中一个最常见的系统场景实施了一个基于多层抽象的存储系统。我们在多个个别物理硬盘的顶部构建了一个硬件 RAID 控制器上的 LVM 逻辑卷管理上的 XFS 文件系统。

因为每一层都使用块设备接口，我们本可以混合和匹配更多的附加功能。比如使用两个 RAID 控制器，并将它们的容量与卷组合并。或者创建多个卷组。我们本可以创建许多逻辑卷。我们本可以使用软件 RAID（在 Linux 中称为 MD RAID）来使用两个 RAID 控制器的输出创建 RAID！实际上可以做到很多，但实用性让我们脚踏实地。

此时，如果你*cd /data*，你可以像它一直存在一样使用新的文件系统。它是一个新的文件系统，建立在所有这些抽象层之上，有多个物理设备使所有这些魔术发生在这一点上对你来说是完全隐藏的。它只是工作。

现在，在过去，如果这是 2004 年，我们通常会到此为止，并说我们已经描述了一个如果实施得当的真实世界服务器可能会看起来像什么。但现在已经不是 2004 年了，我们确实需要更多地讨论我们如何可能在最常见的场景中使用我们的存储。因此，今天我们需要考虑我们的虚拟化层如何使用这些存储，因为这里的事情变得更加有趣。

我们将假设我们刚创建的/data 文件系统将用于存储几台虚拟机的驱动器映像。当然，这些驱动器映像只是我们存储在文件系统中的个别文件。非常简单。与在/data 中创建和存储文本文件没有什么不同（除了 VM 驱动器映像往往稍微大一些）。

关于驱动映像的好处（这可以是 QCOW、VHD、ISO 或其他格式）是它们位于文件系统的顶部，但是当它们被能够读取它们的特殊驱动程序打开时，它们再次呈现一个块接口！没错。我们从块到块再到块再到块再到文件系统再到块！在某些独特的情况下，我们甚至可能不使用虚拟化程序，而是在我们的常规服务器中的某个地方使用这个新的块设备文件。Windows 通常使用 VHD 文件在某些情况下传递数据。MacOS 使用它作为创建安装程序包的标准方式。在 Linux 上，这种情况远不常见，但同样可用。

假设我们正在做正常的操作，我们可以假设我们正在运行一个虚拟化管理程序，几乎可以肯定是 KVM，并且在 KVM 上运行的虚拟机将使用我们新创建的文件系统上的磁盘镜像文件存储。在这种情况下，我们在这里做的大部分工作，很可能会在那个虚拟机内部再次发生。

有些部分可能不太适合重新创建。物理硬盘已经由物理 RAID 控制器管理。存储的速度、容量和可靠性已经由该系统确立，不需要在这里重复。标准方法是将单个驱动器镜像文件作为单一块设备呈现给在虚拟机中运行的操作系统。这与我们从硬件 RAID 控制器中获得的块设备没有什么不同。

现在在虚拟机内部，我们通常会进行相同的操作。我们将呈现的块设备添加为 LVM 物理卷。然后我们将该物理卷添加到卷组中。接着我们从该卷组中划分出一个或多个逻辑卷。然后我们使用选择的文件系统对该逻辑卷进行格式化并挂载它。当然，通常这些步骤并不是像我们在这里做的那样手动完成，而是由安装过程自动化处理。

我们可以增加更多步骤，例如使用 MD RAID，或者减少一些步骤，比如完全跳过 LVM。我们也可以仅用一个硬盘而没有 RAID 控制器完成所有相同的步骤。虽然在物理上这会弱一些，但从技术层面上看，所有的示例仍然有效。我们可以在物理机上使用 LVM，但在虚拟机中不使用，或者反过来！灵活性在于根据需求来进行操作。关键在于理解块设备如何层叠，文件系统如何应用于块设备，以及块文件如何将文件系统转回块设备！

抽象和封装是我们 IT 武器库中非常强大的工具，而且它们通常是如此具体可触。

# 摘要

如果你坚持到本章结束并且还在跟我一起学习，恭喜你，我们做到了！存储在系统管理中至关重要，可能在你管理的其他领域，没有任何地方能为你的组织带来如此大的价值。

我们已经覆盖了存储基础知识，基于块设备接口、抽象技术、文件系统及其接口的概念，并运用这些概念探讨了多设备冗余及其如何用于构建复杂且健壮的数据存储，及如何处理跨设备的存储访问，以满足任何潜在需求。我的目标是为你提供必要的知识，使你能够独立思考在任何给定工作负载下的存储需求，并理解可用性技术以及如何有效地应用这些技术来实现这些目标。

再也不应该将存储视为神秘的黑匣子或一项令人畏惧的任务，你可以将其视为展示机会，展示如何将适当的系统管理最佳实践应用于最大化你的工作负载中最重要的存储因素的机会，而不仅仅是投入资金来解决挑战，或者更糟糕的是，简单地忽视它，希望问题在事情崩溃之前能找到另一份工作。

在我们的下一章中，我们将更深入地探讨系统架构。本章中的许多最有趣的概念将在那里反复出现。系统架构非常依赖存储架构，并且许多冗余和系统保护范式是共享的。看到良好的存储设计元素如何导致真正高性能、高可用性和成本效益的最终解决方案，这实在是令人兴奋的事情。
