<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer220">
<h1 class="chapterNumber">18</h1>
<h1 class="chapterTitle" id="_idParaDest-228">Container Orchestration</h1>
<p class="normal">In the previous chapter, we started learning concepts around containerization. We’ve learned what containers are and how they differ from <strong class="keyWord">Virtual Machines</strong> (<strong class="keyWord">VMs</strong>), as well as how to run two different types of containers (Docker and LXD). As you are now aware, containers are typically lightweight (which means you run a larger number of them than VMs on the same hardware) and are easy to manage with a command syntax that’s rather logical, such as docker run myapp to launch a container named myapp. Depending on the size of your organization, you may only need to run one or two containers to suit your needs, or perhaps you plan on scaling up to hundreds of them. While it’s rather simple to maintain a small number of containers, the footprint can quickly expand and become much harder to keep track of.</p>
<p class="normal">In this chapter, we’ll start looking into the concept of<strong class="keyWord"> Container Orchestration</strong>, which can help us to better maintain the containers that we run. Without such orchestration, the onus is on you, the administrator, to ensure your containers are running properly. While someone still needs to be responsible for mission-critical containers, orchestration provides us with tools we can use to manage them much more efficiently. In addition, orchestration allows us to create an entire cluster that’s easily scalable, so we’re better equipped to handle the demand of our users or clients. As we navigate this topic, we will work on:</p>
<ul>
<li class="bulletList">Container orchestration</li>
<li class="bulletList">Preparing a lab environment for Kubernetes testing</li>
<li class="bulletList">Utilizing MicroK8s</li>
<li class="bulletList">Setting up a Kubernetes cluster</li>
<li class="bulletList">Deploying containers via Kubernetes</li>
</ul>
<p class="normal">Excited? I know I am—containerization can be a lot of fun to learn and work with. We’ll even create our very own cluster in this chapter! But before we can do that, let’s make sure we have an understanding of what container orchestration is.</p>
<h1 class="heading-1" id="_idParaDest-229">Container orchestration</h1>
<p class="normal">In the last chapter, we<a id="_idIndexMarker944"/> covered the basics of running containers on your server. Of special note is the coverage of Docker, which will play a very important role in this chapter. We saw how to pull a Docker image as well as how to use such an image to create a container. There are many more advanced concepts we can learn when it comes to Docker, but understanding the essentials is good enough for the scope of this chapter. And now that we know how to run containers, looking further into how to more efficiently manage them is a logical next step.</p>
<p class="normal">Traditionally, as an administrator, you’ll ensure the critical apps and services for your organization are always healthy and available. If a critical resource stops working for any reason, it falls on you to return it to a healthy state. Regardless of whether we’re utilizing applications on a physical server, or in a VM or container, this need doesn’t change—production apps need to be available at all times with minimal or no downtime. Specific to containers, what orchestration does for us is help us maintain them much more efficiently. Orchestration allows us to not only manage our containers in one place, but it also provides us with additional tooling we can use to more intelligently handle load and recover from faults.</p>
<p class="normal">Consider this example: let’s assume you work at an organization that has an important website that needs to be available online at all times, and it’s currently set up inside a VM. As the administrator, you test the application by running it inside a Docker container and discover that it not only functions the same as in the VM, it also requires less of the server’s memory to run it in a container, and it’s even more responsive than before. Everyone at your company is impressed, and the project of converting your company’s site to run inside a container is a complete success.</p>
<p class="normal">Now, let’s assume your organization is getting ready to release a brand new version of your company’s product. It’s expected that the demand for your website will increase ten times for at least the first few weeks after the debut. To address this, you can launch however many additional containers you feel will handle the expected load, and then set up a load balancer to spread traffic evenly between them. When the excitement over the new release winds down, you can remove the newly added containers to return to your original population. We haven’t gone over load balancers yet, but these are useful for spreading traffic between multiple nodes. This feature is built into Kubernetes, so you won’t need to install anything extra to take advantage of this.</p>
<p class="normal">If the launch of your organization’s newly updated product is expected to go live at 1:00am, you’d have to make sure you’re awake then and execute the necessary commands to launch the extra containers and deploy the load balancer. You would then watch the containers for a while and ensure they’re stable. Perhaps you’ve already tested this in a testing environment, so you have reasonable confidence that the maintenance will go smoothly. After you successfully launch the containers, you set a reminder for yourself in your calendar to log back in after two weeks to undo the changes.</p>
<p class="normal">While that approach can technically work and result in a successful rollout, it’s not very efficient. Sure, you <em class="italic">can</em> log in at 1:00am to launch the extra containers and deploy the load balancer, but is that an effective use of your time? What if you’re very sleepy at that time and make a mistake that results in a failure during an important launch? And when the launch window is over and the load returns to normal, you may or may not see the alert that you’ve intended to use to remind yourself and lower the container count. </p>
<p class="normal">In that situation, you could end up with an expensive bill since your server would be using extra energy to run containers that were no longer needed. Even worse, manually managing your containers means that it can’t handle a situation where the load increases unexpectedly. Despite our best intentions, any process that is run manually may or may not work <a id="_idIndexMarker945"/>out well; we’re only human after all.</p>
<p class="normal">With container orchestration, we essentially delegate the process of running containers to an application that will automatically create additional containers as demand increases and remove unneeded containers when demand winds down. Orchestration also simplifies the process of setting up applications, by giving us tools we can use to ensure a particular number of containers are running at a minimum and we can set a maximum for when load and demand spikes. This empowers us to have infrastructure that grows and shrinks automatically to match the needs of our users. In addition, container orchestration allows us to automatically heal from failures. If a container runs into a problem and fails for some reason, it will be deleted and recreated from the image.</p>
<p class="normal">As you can see, orchestration gives us additional value, it can actually save you time and effort. To be fair, some of the features mentioned are part of containerization itself and not necessarily specific to orchestration, but the latter does give us the ability to manage these tools much more efficiently. Is container orchestration for everyone? No technology meets 100% of all use-cases, but it’s certainly something to consider. If you only run a single container and have no plans to run another, then a technology such as orchestration is overkill—it would take you more time to manage the cluster than it would to manage the container itself. Use your best judgment.</p>
<p class="normal">If you do decide to implement <a id="_idIndexMarker946"/>containerization, <strong class="keyWord">Kubernetes</strong> (often abbreviated <strong class="keyWord">K8s</strong>) is a very popular solution for container orchestration and is what we’ll be exploring in this chapter. What Kubernetes allows us to do is create deployments for our applications, providing us with fine-tuned control we can leverage to determine the actual behavior of our applications. It can automatically re-spawn a container if it stops working for any reason, ensure the number of containers running meets the demand, and spread our workloads across many worker nodes so that no one server becomes overwhelmed with running too many containers. Kubernetes is very popular and is not specific to Ubuntu Server. However, utilizing a technology such as Kubernetes in Ubuntu is one of many ways that we can use to extend the platform.</p>
<p class="normal">What do you <a id="_idIndexMarker947"/>need in order to set up a Kubernetes cluster? That’s likely a logical question to have at this point. In the next section, we’ll cover some of the considerations to make before deciding how to set it up.</p>
<h1 class="heading-1" id="_idParaDest-230">Preparing a lab environment for Kubernetes testing</h1>
<p class="normal">In an<a id="_idIndexMarker948"/> organization, planning <a id="_idIndexMarker949"/>a roll-out of an entire Kubernetes cluster can be fairly involved—you may have to purchase hardware and also analyze your existing environment and understand how containerization will fit in. It’s possible that some applications you want to run aren’t a good fit for containers; some don’t support running in a container at all. Assuming you’ve already checked the documentation for the applications you are wanting to run in containers and came to the conclusion that such a technology is supported, the next step is procuring the hardware (if you don’t already have a place to run it) and then setting up the cluster.</p>
<p class="normal">Specific to us in this book, we don’t need to contact a server vendor and submit a purchase order to simply test out the technology. If you are actually involved with the rollout of container orchestration at your organization, then it’s a fun project to work on. But for the purposes of this book, let’s discuss some of the details around what’s needed to set up a personal lab in order to set up Kubernetes and start learning the platform.</p>
<p class="normal">The best rule of thumb when setting up a lab for testing software is to try to use what you have available. To create a Kubernetes cluster, we’ll need one Ubuntu machine to act as the controller, and one or more additional servers to be used as worker nodes. As you’ll learn later in the book, the goal of Kubernetes is to schedule containers to run in <strong class="keyWord">Pods</strong> on worker nodes. Pods in Kubernetes are a collection of one or more containers, and every container is run in a pod. With more than one worker node, you can benefit from being able to run more pods (and as an extension, run more containers) and having load spread between multiple workers to have the most control over how your applications are served to the end-user.</p>
<p class="normal">But on what type of device should you run Kubernetes on in order to test it out and learn it? For this, we have the same possibilities as we discussed when installing Ubuntu Server back in <em class="chapterRef">Chapter 1</em>, <em class="italic">Deploying Ubuntu Server</em>—we can use VMs, physical servers, spare desktops or laptops, as well as Raspberry Pi’s (or any combination of those). Again, use whatever you have available. For a rollout in an organization, you may end up using a virtualization server for the controller and worker nodes, or perhaps physical servers. One of my favorite platforms for Kubernetes is the Raspberry Pi, believe it or not. I’ve been running a Kubernetes cluster in production for several years that consists of only Raspberry Pi 4 units with complete success. If nothing else, purchasing a few Pi’s is a relatively <a id="_idIndexMarker950"/>low barrier to entry. You can <a id="_idIndexMarker951"/>even utilize a cloud provider if you’d like, though doing so goes beyond the scope of this chapter as cloud providers generally have their own tools in place to manage clusters.</p>
<p class="normal">In general, it’s recommended to have a controller node and a handful of workers. A single worker will suffice, but in this chapter, I’ll show the process of setting up two workers to better understand how the platform scales. On your end, you can use a single controller and worker, or set up however many workers you’d like. One of the best things about Kubernetes is that you aren’t stuck with the number of workers you set up initially, you can add more nodes to your cluster as time goes on. An organization may start with just a few workers but add additional ones as the need arises.</p>
<p class="normal">As far as resources go, requirements for Kubernetes are relatively modest. The node that’s designated as the controller should have a minimum of two CPUs or one CPU with at least two cores. Obviously, it’s better if you have more than that, such as a quad-core CPU for the controller, but two cores are the minimum. The worker nodes can have a single CPU core each, but the more cores they have, the more Pods they’ll be able to run. When it comes to RAM, I recommend a minimum of 2 GB of memory on each, but again, if you have more than that on each node then that’s even better.</p>
<p class="normal">It’s important that the IP addresses for the nodes in your cluster do not change. If an IP of a cluster node changes, it can cause the cluster nodes to be unable to find each other. In <em class="chapterRef">Chapter 10</em>, <em class="italic">Connecting to Networks</em>, we learned how to set up a static IP address, which is a good solution for preventing IP addresses from changing. As an alternative, you can set up a static lease in your DHCP server if you wish. It doesn’t matter which solution you use, so long as you prevent the IP addresses of cluster nodes from changing.</p>
<p class="normal">For some readers, a more accessible method of setting up comes in the form of <strong class="keyWord">MicroK8s</strong>, which allows you to even set up Kubernetes on a single machine. If your only goal is to set up a simple test installation, it’s one of the best and easiest methods of getting started. MicroK8s is not recommended for running a production cluster in an organization, but it’s definitely a great way to learn. I do recommend that you work through the<a id="_idIndexMarker952"/> standard<a id="_idIndexMarker953"/> Kubernetes procedure with multiple nodes if you can, but in the next section we’ll walk through setting up MicroK8s for those of you that would like to utilize that method.</p>
<h1 class="heading-1" id="_idParaDest-231">Utilizing MicroK8s</h1>
<p class="normal">If you don’t have <a id="_idIndexMarker954"/>more than one machine or enough memory on your laptop or desktop to run multiple nodes inside virtualization software such as VirtualBox, MicroK8s is a simple way to set up a Kubernetes instance for testing the platform, as well as going through the examples in this chapter. MicroK8s is actually provided by Canonical, the makers of Ubuntu. That just goes to show you how important Kubernetes is to the Ubuntu distribution, its own creator is going the extra mile to contribute to the platform. MicroK8s is available for Linux, macOS, as well as Windows. So regardless of which operating system you’re running on your laptop or desktop, you should be able to install and use it. If nothing else, it gives you a great test installation of Kubernetes that will come in handy as you learn.</p>
<p class="normal">To set it up, follow along with one of the subsections below that matches the operating system installed on your computer.</p>
<h2 class="heading-2" id="_idParaDest-232">Installing MicroK8s on Linux</h2>
<p class="normal">When it<a id="_idIndexMarker955"/> comes to<a id="_idIndexMarker956"/> Linux, MicroK8s is distributed as a snap package. We covered snap packages back in <em class="chapterRef">Chapter 3</em>, <em class="italic">Managing Software Packages</em>, and it’s a great solution for cross-distribution package management. If you run a recent version of Ubuntu on your computer, then you should already have support for snap packages and you can proceed to install MicroK8s. If you’re running a distribution of Linux on your computer other than Ubuntu, then you may not have access to the <code class="inlineCode">snap</code> command by default. If in doubt, you can use the <code class="inlineCode">which</code> to see if the command is available:</p>
<pre class="programlisting con"><code class="hljs-con">which snap
</code></pre>
<p class="normal">If you do have the ability to install snap packages, then your output should be similar to the following:</p>
<figure class="mediaobject"><img alt="" height="137" src="../Images/B18425_18_01.png" width="589"/></figure>
<p class="packt_figref">Figure 18.1: Checking if the snap command is available</p>
<p class="normal">If you see<a id="_idIndexMarker957"/> no<a id="_idIndexMarker958"/> output when you run <code class="inlineCode">which snap</code>, then that means your distribution doesn’t have support for this package type installed. Canonical makes the <code class="inlineCode">snap</code> command available to distributions outside of Ubuntu, so if you’re using a distribution other than Ubuntu, it’s usually just a matter of installing the required package. </p>
<p class="normal">Canonical has additional information available on the following site, which shows the process of enabling <code class="inlineCode">snap</code> for several distributions: <a href="https://snapcraft.io/docs/installing-snapd"><span class="url">https://snapcraft.io/docs/installing-snapd</span></a>.</p>
<p class="normal">For example, the documentation on that site gives the following command to install the required package for Fedora:</p>
<pre class="programlisting con"><code class="hljs-con">sudo dnf install snapd
</code></pre>
<p class="normal">Essentially, as long as you follow along with the instructions for setting up <code class="inlineCode">snap</code> that are specific to the distribution of Linux you’re running, the process should be simple enough. If for some reason your distribution isn’t supported, you can simply use the same Ubuntu installation as you’ve been using during this book, which will have <code class="inlineCode">snap</code> support built in.</p>
<p class="normal">Once you either confirm you already have support for <code class="inlineCode">snap</code> on your computer, or you successfully enable the feature, you can install MicroK8s with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">sudo snap install microk8s --classic
</code></pre>
<p class="normal">If successful, you should see a confirmation in your terminal that the process was successful:</p>
<figure class="mediaobject"><img alt="" height="143" src="../Images/B18425_18_02.png" width="791"/></figure>
<p class="packt_figref">Figure 18.2: Setting up MicroK8s on a Linux installation</p>
<p class="normal">Now that we<a id="_idIndexMarker959"/> have<a id="_idIndexMarker960"/> MicroK8s installed on your Linux computer, we can proceed through the chapter. Or, you can check out the next section to see the installation process for macOS.</p>
<h2 class="heading-2" id="_idParaDest-233">Installing MicroK8s on macOS</h2>
<p class="normal">On macOS, the <a id="_idIndexMarker961"/>installation <a id="_idIndexMarker962"/>process for MicroK8s is similar. The process for Mac will utilize <strong class="keyWord">Homebrew</strong>, which <a id="_idIndexMarker963"/>is an addon for macOS. Homebrew isn’t specific to Kubernetes or MicroK8s; it gives you the ability to install additional packages on your Mac that aren’t normally available, with MicroK8s being one of many. Homebrew is essentially a command-line utility with syntax similar to Ubuntu’s <code class="inlineCode">apt</code> command.</p>
<p class="normal">To install <a id="_idIndexMarker964"/>Homebrew, visit <a href="https://brew.sh"><span class="url">https://brew.sh</span></a> in your browser, and the command required to set it up will be right there on the site. I could insert the command to install Homebrew right here on this page, but the process may change someday, so it’s better to get the command right from the official website for the utility. At the time of writing, the page looks like this:</p>
<figure class="mediaobject"><img alt="" height="685" src="../Images/B18425_18_03.png" width="874"/></figure>
<p class="packt_figref">Figure 18.3: The Homebrew website</p>
<p class="normal">In the <a id="_idIndexMarker965"/>screenshot, you’ll <a id="_idIndexMarker966"/>notice the command with white text on a black highlight. It’s cut off a bit, due to the command being wider than this page. You should see the entire command on the site, so you can copy it from there and paste it into your Mac’s terminal.</p>
<p class="normal">It’s becoming increasingly popular for application developers to provide a command on their website to install their application that you paste directly into your terminal. This is very convenient and allows you to set up an app quickly. But you should always research and inspect such commands before pasting them into your terminal (regardless of your operating system). It’s possible that an outside actor could hijack the command on a website to trick you into running something nefarious. You can use the <code class="inlineCode">wget</code> command to download the script their command will end up running, so you can check it to ensure it’s not doing something evil. Since this method of deploying software is getting more and more common, I recommend getting into the habit of checking commands before running them (especially if they use <code class="inlineCode">sudo</code>).</p>
<p class="normal">Once you have Homebrew installed on your Mac, you can proceed to install MicroK8s with Homebrew with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">brew install ubuntu/microk8s/microk8s
</code></pre>
<p class="normal">Once that<a id="_idIndexMarker967"/> command <a id="_idIndexMarker968"/>finishes, you should have MicroK8s installed on your Mac. In the next sub-section, we’ll see the same process in Windows.</p>
<h2 class="heading-2" id="_idParaDest-234">Installing MicroK8s on Windows</h2>
<p class="normal">If you’d like<a id="_idIndexMarker969"/> to try <a id="_idIndexMarker970"/>out MicroK8s on a laptop or desktop computer running Windows 10, there’s a specific installer available that will allow you to get it up and running. On the MicroK8s website, at <a href="https://microk8s.io"><span class="url">https://microk8s.io</span></a>, you should see a large green button labeled <strong class="screenText">Download MicroK8s for Windows</strong>, and if you click on it, you’ll be able to download the installer:</p>
<figure class="mediaobject"><img alt="" height="374" src="../Images/B18425_18_04.png" width="884"/></figure>
<p class="packt_figref">Figure 18.4: The MicroK8s website, with a button to download the installer for Windows</p>
<p class="normal">Once the download finishes, you can then launch the installer, which will have several different sections, all of which you can accept the defaults for. Click <strong class="screenText">Next</strong> and then the <strong class="keyWord">Install</strong> button to begin the actual installation:</p>
<figure class="mediaobject"><img alt="" height="677" src="../Images/B18425_18_05.png" width="876"/></figure>
<p class="packt_figref">Figure 18.5: The MicroK8s installer for Windows</p>
<p class="normal">After<a id="_idIndexMarker971"/> the <a id="_idIndexMarker972"/>installation begins, another installation will also launch in parallel that will ask you to select a hypervisor:</p>
<figure class="mediaobject"><img alt="" height="554" src="../Images/B18425_18_06.png" width="878"/></figure>
<p class="packt_figref">Figure 18.6: The MicroK8s installer for Windows, selecting a hypervisor</p>
<p class="normal">For this, you can<a id="_idIndexMarker973"/> leave <a id="_idIndexMarker974"/>the default selection on Microsoft Hyper-V, and click <strong class="screenText">Next</strong>. You can keep the remaining options at their defaults and click <strong class="screenText">Next</strong> through any remaining prompt that comes up. Depending on the setup of your computer, it may require you to reboot in order to finalize the required components before the installation can be completely finished. If you do see a prompt to reboot, do so, and then launch the installer again.</p>
<p class="normal">You’ll see the following window appear, asking you to set up the parameters for MicroK8s, specifically how many virtual CPUs to utilize, how much RAM to provide, disk space, and so on:</p>
<figure class="mediaobject"><img alt="" height="617" src="../Images/B18425_18_07.png" width="876"/></figure>
<p class="packt_figref">Figure 18.7: The MicroK8s installer for Windows, allocating resources</p>
<p class="normal">You can<a id="_idIndexMarker975"/> simply <a id="_idIndexMarker976"/>choose the defaults for this screen as well and click <strong class="screenText">Next</strong>. Once the process is complete, you can click the <strong class="screenText">Finish</strong> button to exit the installer.</p>
<h2 class="heading-2" id="_idParaDest-235">Interacting with MicroK8s</h2>
<p class="normal">At this point, if you’ve <a id="_idIndexMarker977"/>decided to utilize MicroK8s, you should have it installed on your computer. How you actually interact with it depends on your operating system. In each case, the <code class="inlineCode">microk8s</code> command (which we’ll cover shortly) is used to control MicroK8s. Which app you use in order to enter <code class="inlineCode">microk8s</code> commands differs from one platform to another.</p>
<p class="normal">On Windows, you can use the Command Prompt app, which is built into the operating system. The <code class="inlineCode">microk8s</code> command is recognized and available for use after you install MicroK8s. When you enter the <code class="inlineCode">microk8s</code> command by itself, it should display basic usage information:</p>
<figure class="mediaobject"><img alt="" height="382" src="../Images/B18425_18_08.png" width="761"/></figure>
<p class="packt_figref">Figure 18.8: Executing the microk8s command with no options in a Windows Command Prompt</p>
<p class="normal">With macOS, you<a id="_idIndexMarker978"/> can use the built-in terminal app that’s provided with the operating system. You may have another step to complete though before you can actually use MicroK8s. If you enter the <code class="inlineCode">microk8s</code> command, and you receive an error informing you that <code class="inlineCode">support for multipass needs to be set-up</code>, then you can run the following command to fix it:</p>
<pre class="programlisting con"><code class="hljs-con">microk8s install
</code></pre>
<p class="normal">If you’re curious, <strong class="keyWord">Multipass</strong> is a technology that’s also created by Canonical that allows you to quickly set up an Ubuntu instance for testing. It’s not specific to MicroK8s or even Kubernetes, but in our case it’s used in the background to facilitate MicroK8s. Multipass is not covered in this book, but it’s worth taking a look at if you think you’ll benefit from the ability to quickly set up Ubuntu instances to test applications and configurations on. It’s available for all of the leading operating systems.</p>
<p class="normal">When it comes to Linux, if you’ve already gone through the process of setting up MicroK8s, you should be ready to use it immediately. Open up your distribution’s terminal app and try the <code class="inlineCode">microk8s</code> command to see if it’s recognized. If it is, you’re ready to move on.</p>
<p class="normal">To check the status of MicroK8s, the following command can be used to give you an overview of the various components. Before you run it, note the inclusion of <code class="inlineCode">sudo</code> at the beginning. By default, this is required if your underlying operating system is Linux. </p>
<p class="normal">If you’re running Windows 11 or macOS, you shouldn’t need <code class="inlineCode">sudo</code>. This difference has to do with how the underlying operating system handles permissions, which is different from operating system to operating system. So, if you’re not using Linux on your PC, feel free to omit <code class="inlineCode">sudo</code> from <code class="inlineCode">microk8s</code> commands:</p>
<pre class="programlisting con"><code class="hljs-con">sudo microk8s kubectl get all --all-namespaces
</code></pre>
<p class="normal">Don’t worry about the details regarding the individual components of Kubernetes at the moment, we’ll cover what you need to know as we go through the rest of the chapter. For now, so long as you don’t see errors when you run the previous command and check the status, you <a id="_idIndexMarker979"/>should be in good shape to continue.</p>
<p class="normal">By default, our MicroK8s installation comes with only the addons that are required for it to run. There are other addons we can enable, such as <code class="inlineCode">storage</code>, which gives us the ability to expose a path on the host (the underlying operating system) to Kubernetes; <code class="inlineCode">gpu</code>, which allows us to utilize NVIDIA GPUs within the containers; and others. We don’t need to worry about these for now, but we should at least enable the <code class="inlineCode">dns</code> addon, which sets up DNS within our cluster. It’s not necessarily required, but not having it can create issues down the road when it comes to name resolution, so we may as well enable it now:</p>
<pre class="programlisting con"><code class="hljs-con">sudo microk8s enable dns
</code></pre>
<p class="normal">You should see output similar to the following:</p>
<figure class="mediaobject"><img alt="" height="378" src="../Images/B18425_18_09.png" width="818"/></figure>
<p class="packt_figref">Figure 18.9: Enabling the DNS addon in MicroK8s on a Linux installation</p>
<p class="normal">In my tests, I find that the command to enable the addon seems to stop responding for a while, with no output. Don’t close the window; it should catch up and then display information about the process of installing the addon eventually. If this happens, just wait it out a bit.</p>
<p class="normal">With the previous <a id="_idIndexMarker980"/>command, we’ve enabled the <code class="inlineCode">dns</code> addon. As I’ve mentioned, there are other addons available, but that’s enough for now. At this point, just keep in mind that you can extend MicroK8s with addons, so it may be worth exploring in more detail later on if you wish. A list of addons is included in a link at the end of this chapter.</p>
<p class="normal">As mentioned earlier, if you’re using Linux to run MicroK8s, then you would’ve used <code class="inlineCode">sudo</code> with the <code class="inlineCode">microk8s</code> command we used earlier. If you’d like to remove the requirement of using <code class="inlineCode">sudo</code> on Linux, you can do so by adding your user to the <code class="inlineCode">microk8s</code> group. That can be done with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">sudo usermod -a -G microk8s &lt;yourusername&gt;
</code></pre>
<p class="normal">After you log out and then log in again, you should be able to run <code class="inlineCode">microk8s</code> commands without <code class="inlineCode">sudo</code>.</p>
<p class="normal">As we’ll discover as we proceed through the rest of the chapter, the <code class="inlineCode">kubectl</code> command is generally used to interact with a Kubernetes cluster and manage it. <code class="inlineCode">kubectl</code>, short for <strong class="keyWord">Kube Control</strong>, is the <a id="_idIndexMarker981"/>standard utility you use to perform many tasks against your cluster. We’ll explore this more later on. But specific to MicroK8s, it’s important to understand that it uses its own version of <code class="inlineCode">kubectl</code> that’s specific to it. So with MicroK8s, you would run <code class="inlineCode">microk8s kubectl</code> instead of simply <code class="inlineCode">kubectl</code> to interact with the cluster. Having a separate implementation of <code class="inlineCode">kubectl</code> with MicroK8s allows you to target an actual cluster (the <code class="inlineCode">kubectl</code> command by itself) or specifically your installation of MicroK8s (prefix <code class="inlineCode">kubectl</code> commands with <code class="inlineCode">microk8s</code>), so one won’t conflict with the other. As we work through the chapter, I’ll call out <code class="inlineCode">kubectl</code> by itself when we get to the actual examples, so it will be up to you to remember to use <code class="inlineCode">microk8s</code> in front of such commands as needed.</p>
<p class="normal">Now that we have our very own installation of MicroK8s on our laptop or desktop, we can proceed through <a id="_idIndexMarker982"/>the examples in this book. You can skip ahead to the <em class="italic">Deploying containers via Kubernetes</em> section later in this chapter to begin working with Kubernetes. In the next section though, we’ll take a look at setting up a Kubernetes cluster manually without MicroK8s, which is closer to the actual process you’d use in an actual production implementation.</p>
<h1 class="heading-1" id="_idParaDest-236">Setting up a Kubernetes cluster</h1>
<p class="normal">In the previous<a id="_idIndexMarker983"/> section, we set up MicroK8s, which provides us with a Kubernetes cluster on a single machine, which is great for testing purposes. That might even be all you need in order to learn Kubernetes and see how it works. If you can, I still recommend setting up a cluster manually, which will give you even more insight into how the individual components work together. That’s exactly what we’re going to do in this section.</p>
<p class="normal">Before we do get started, it’s important to synchronize our mindset a bit. Of all of the activities we’ve worked through so far, setting up a Kubernetes cluster manually is easily the most complex. Kubernetes itself is made up of many components, as well as settings. If any one component is incorrect or a setting is misconfigured, the entire process can fail. In this section, a great deal of care and attention was spent to ensure (as much as possible) that the process works to the point where it’s completely reproducible and has been broken down to only the required components to simplify everything. However, if the process doesn’t work out well when you go to try it, don’t worry - it’s perfectly fine if we encounter an issue. We’re still learning, and fixing things is the best way to learn. So the bottom line is to have fun, and be patient.</p>
<p class="normal">What does a typical Kubernetes cluster look like? When it comes to a production installation in an actual data center, having Kubernetes installed on multiple servers is commonplace. Typically, one of them will act as the controller node, and then you can add as many worker nodes as you need. </p>
<p class="normal">As your needs expand, you can add additional servers to provide more worker nodes to your cluster. Setting up a controller Kubernetes node and then individual workers is a great way to see the actual relationship in action. And that’s exactly what we’re going to do in this section.</p>
<p class="normal">As I walk you through the process, I’m going to do so utilizing three VMs. The first will be the controller, and the remaining two will be workers. On your end, it doesn’t really matter how many workers you decide to go with. You can have a single worker, or a dozen—however many you’d like to set up is fine. If you do set up more nodes than I do, then you would simply repeat the commands in this section for the additional nodes. For the nodes on your end, feel free to use whatever combination of physical or VMs makes sense and fits within the resources of the equipment you have available.</p>
<p class="normal">Before we get started, you’ll want to have already installed Ubuntu Server on each machine and install all of the updates. It’s also a good idea for you to create a user for yourself if you haven’t already done so. The remainder of this section will assume you’ve already set up Ubuntu on each. After you’ve installed Ubuntu on everything, it’s a good idea to also configure the hostnames on each node as well to make it easier to tell them apart. For example, on the node that I’m intending to use as the controller, I’m going to set the hostname as <code class="inlineCode">controller</code>. For the workers, I’ll name them <code class="inlineCode">node-01</code> and <code class="inlineCode">node-02</code> respectively. You <a id="_idIndexMarker984"/>can name yours in whatever theme makes sense, but the reason I specifically mention it is to make sure that you name them something, as by default they’ll each show the same hostname in the command prompt, and that may be confusing.</p>
<p class="normal">Now that you have everything you need, let’s set up Kubernetes!</p>
<h2 class="heading-2" id="_idParaDest-237">Preliminary setup</h2>
<p class="normal">In this section, we’re <a id="_idIndexMarker985"/>going to complete some miscellaneous setup tasks before we actually start building our cluster. These are basically the prerequisites for our cluster to function.</p>
<p class="normal">We should first ensure that each of the servers we intend on using for the cluster has either a static lease, or a static IP that will not change. You can change your server’s IP address at any time, but with Kubernetes, there’s some additional steps involved if you do change the IP later. For this reason, I suggest setting your static lease or static IP right now, <em class="italic">before</em> we build the cluster. That way, you won’t have to worry about that later. The process was covered in <em class="chapterRef">Chapter 11</em>, <em class="italic">Setting Up Network Services</em>, so I won’t repeat that here. On your side, be sure to set up a static lease or static IP, and then continue further with our setup here.</p>
<p class="normal">Next, if the devices you’ve selected for this exercise includes servers that are being built using the Raspberry Pi, then there are some special requirements that are specific to the Pi. If you are using one or more Raspberry Pi units, be sure to complete this step (otherwise, skip this if you’re NOT using the Raspberry Pi).</p>
<p class="normal">To set boot parameters specific to the Raspberry Pi, open up the following file in your editor of choice:</p>
<pre class="programlisting con"><code class="hljs-con">sudo nano /boot/firmware/cmdline.txt
</code></pre>
<p class="normal">What you’re about to do in this file, is add some cgroup options to control how Kubernetes is able to<a id="_idIndexMarker986"/> interact with the hardware of your device. You’ll want to add the following options to the existing line within the file. Don’t create a new line. Instead, add these options to the very end of the first (and only) line within that file:</p>
<pre class="programlisting code"><code class="hljs-code"><code class="inlineCode">cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1</code>
</code></pre>
<p class="normal">Save and exit the file, and that should be the only change we’ll be making that’s specific to the Raspberry Pi.</p>
<p class="normal">In order to build a Kubernetes cluster, we’ll need to set up something called a <strong class="keyWord">Container Runtime</strong>. A <a id="_idIndexMarker987"/>Container Runtime is the means by which Kubernetes will run containers within the cluster. Without a runtime, there’s no ability for Kubernetes to run even a single container. While Kubernetes is a container orchestration solution, it doesn’t mandate which container runtime you use. The suggested runtime known <a id="_idIndexMarker988"/>as <strong class="keyWord">containerd</strong>, so that’s what we’ll use in our cluster. Other container runtimes include (but aren’t limited to) Docker, CRI-O, and there’s others. We’ve already worked with Docker in this book, but for our cluster, we’re going to deploy it with the suggested containerd runtime.</p>
<p class="normal">When it comes to the containers we can run, there’s no difference - containerd for example can run Docker containers just like we’ve been doing. So even though we’re going to go with a different container runtime, it shouldn’t really make a difference at all when it comes to the end result.</p>
<p class="normal">In this section, the commands that I’m going to have you run should be executed on each of the instances you plan on using with Kubernetes, regardless of whether or not you’re working with the intended controller or a node; these commands need to be run on each. There will be commands specific to the controller and the nodes later on, but I’ll mention the intended target if it’s something I’d like you to do on specifically one or the other.</p>
<p class="normal">The first thing we’re going to do is install the <code class="inlineCode">containerd</code> package:</p>
<pre class="programlisting con"><code class="hljs-con">sudo apt install containerd
</code></pre>
<p class="normal">After the package is installed, we’ll check the status of it and ensure that it’s running:</p>
<pre class="programlisting con"><code class="hljs-con">systemctl status containerd
</code></pre>
<p class="normal">We should see<a id="_idIndexMarker989"/> that the unit is running:</p>
<figure class="mediaobject"><img alt="" height="230" src="../Images/B18425_18_10.png" width="876"/></figure>
<p class="packt_figref">Figure 18.10: Checking the status of the containerd runtime</p>
<p class="normal">We’re not quite done with <code class="inlineCode">containerd</code>, though. It’s going to need some default configuration in order to function properly, but by default, there’s no sample config provided automatically. Let’s create that config, starting with adding a directory to our server to house the configuration file:</p>
<pre class="programlisting con"><code class="hljs-con">sudo mkdir /etc/containerd
</code></pre>
<p class="normal">Next, we’ll create the config file for <code class="inlineCode">containerd</code> with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">containerd config default | sudo tee /etc/containerd/config.toml
</code></pre>
<p class="normal">That command will not only create the default configuration file for <code class="inlineCode">containerd</code>, it also prints the configuration information to the screen as well so we can see what the default values are. However, while most of the default settings are fine, we’ll need to make some adjustments to this file. Open up the <code class="inlineCode">/etc/containerd/config.toml</code> file in your editor of choice.</p>
<p class="normal">The first change to make to this file, is to set the cgroup driver to <code class="inlineCode">systemd</code>. To do so, look for the following line:</p>
<pre class="programlisting con"><code class="hljs-con">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
</code></pre>
<p class="normal">Several lines below that, you’ll see the following line:</p>
<pre class="programlisting con"><code class="hljs-con">SystemdCgroup = false
</code></pre>
<p class="normal">Change that line to <code class="inlineCode">true</code> instead:</p>
<pre class="programlisting con"><code class="hljs-con">SystemdCgroup = true
</code></pre>
<p class="normal">At this point, we’re <a id="_idIndexMarker990"/>done with <code class="inlineCode">containerd</code> for now. Let’s move on and configure some important system tweaks. The first of these is disabling swap. Although it’s usually not a good idea to run a server without swap, setting up a Kubernetes cluster is the exception to this. In fact, when we go to initialize the cluster later, the process will actually abort if swap is enabled. So let’s take care of disabling it now.</p>
<p class="normal">First, we’ll run the following command to disable swap:</p>
<pre class="programlisting con"><code class="hljs-con">sudo swapoff -a
</code></pre>
<p class="normal">After doing this, if we run <code class="inlineCode">free -m</code> we should see all zero’s for swap:</p>
<figure class="mediaobject"><img alt="" height="138" src="../Images/B18425_18_11.png" width="880"/></figure>
<p class="packt_figref">Figure 18.11: Running the free -m command to check that swap is disabled</p>
<p class="normal">However, the command that we’ve just run to disable swap was not a permanent change. When we reboot our server, swap will be automatically re-enabled. In order to disable swap for good, we’ll need to edit the <code class="inlineCode">/etc/fstab</code> file and comment out the line that activates swap during boot. </p>
<p class="normal">The following screenshot shows an example configuration line for swap, but it’s been commented out by inserting the <code class="inlineCode">#</code> character at the beginning of that <a id="_idIndexMarker991"/>line:</p>
<figure class="mediaobject"><img alt="" height="182" src="../Images/B18425_18_12.png" width="878"/></figure>
<p class="packt_figref">Figure 18.12: The /etc/fstab file, with the swap line commented out</p>
<p class="normal">At this point, swap should be turned off, and considering we’ve commented out the swap line in the <code class="inlineCode">/etc/fstab</code> file, swap won’t be re-enabled the next time we start our server.</p>
<p class="normal">Next, we’ll need to make a change to the <code class="inlineCode">/etc/sysctl.conf</code> file to enable bridging. This is similar to what we worked through as we set up an internet gateway back in <em class="chapterRef">Chapter 11</em>, <em class="italic">Setting Up Network Services</em>. Once we’ve opened up the <code class="inlineCode">/etc/sysctl.conf</code> file in an editor, we’ll need to uncomment the following line:</p>
<pre class="programlisting con"><code class="hljs-con">#net.ipv4.ip_forward=1 
</code></pre>
<p class="normal">It should now look like this:</p>
<pre class="programlisting con"><code class="hljs-con">net.ipv4.ip forward=1
</code></pre>
<p class="normal">With that line uncommented, bridging should be enabled the next time we start our server.</p>
<p class="normal">Next, we’ll create the following file, which we’ll use to ensure a required kernel module is loaded when the server starts:</p>
<pre class="programlisting con"><code class="hljs-con">sudo nano /etc/modules-load.d/k8s.conf
</code></pre>
<p class="normal">Inside that file, we’ll add the following:</p>
<pre class="programlisting con"><code class="hljs-con">br_netfilter
</code></pre>
<p class="normal">That’s actually it for that file, so save and exit the editor. The <code class="inlineCode">br_netfilter</code> module assists with <a id="_idIndexMarker992"/>networking for our eventual Kubernetes cluster, and will need to be enabled in order for the cluster to function. By creating the <code class="inlineCode">/etc/modules-load.d/k8s.conf</code> file, we’re ensuring this kernel module is loaded automatically when we start our server.</p>
<p class="normal">Before we continue, further, let’s take a moment and ensure we’ve done everything we need to do up to now. At this point, on each of the servers you intend to use with Kubernetes (the controller as well as the nodes) you should’ve accomplished the following:</p>
<ul>
<li class="bulletList">Installed all updates</li>
<li class="bulletList">Set a hostname</li>
<li class="bulletList">Set up a static IP or lease on each node</li>
<li class="bulletList">Set Raspberry Pi-specific boot options (if you’re using a Raspberry Pi)</li>
<li class="bulletList">Installed <code class="inlineCode">containerd</code></li>
<li class="bulletList">Created the <code class="inlineCode">/etc/containerd/config.toml</code> file for <code class="inlineCode">containerd</code> and also set the cgroup driver</li>
<li class="bulletList">Disabled swap</li>
<li class="bulletList">Added the <code class="inlineCode">/etc/modules-load.d/k8s.conf</code> file with the <code class="inlineCode">br_netfilter</code> module listed inside</li>
<li class="bulletList">Edited the <code class="inlineCode">/etc/sysctl.conf</code> file to enable bridging</li>
</ul>
<p class="normal">All of the above should’ve been completed on every server you plan to use with your cluster. Once you’re sure that you’ve performed each of those tasks, we should reboot each of our nodes:</p>
<pre class="programlisting con"><code class="hljs-con">sudo reboot
</code></pre>
<p class="normal">Once your nodes <a id="_idIndexMarker993"/>finish starting back up, we can proceed to actually install Kubernetes.</p>
<h2 class="heading-2" id="_idParaDest-238">Installing Kubernetes</h2>
<p class="normal">Now it’s finally time <a id="_idIndexMarker994"/>to start building our cluster, and we’ll start by installing the required packages. To do so, we’ll need to add the appropriate repository so we can have access to the required packages for installing Kubernetes. To add the repository, we’ll first add the key for the repository so our server will accept it as a trusted resource:</p>
<pre class="programlisting con"><code class="hljs-con">sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
</code></pre>
<p class="normal">To add the actual repository itself, we will run the following command:</p>
<pre class="programlisting con"><code class="hljs-con">echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
</code></pre>
<p class="normal">After adding the repository, we’ll update our local index:</p>
<pre class="programlisting con"><code class="hljs-con">sudo apt update
</code></pre>
<p class="normal">What you may notice is that the repository URL references <code class="inlineCode">xenial</code> instead of the actual codename for Ubuntu 22.04, which is <code class="inlineCode">jammy</code> (short for Jammy Jellyfish). At the time of writing, there is no dedicated Kubernetes repository for Ubuntu 22.04 yet, but the process will still work just fine. It’s possible that by the time you’re reading this, there will be a dedicated repository for 22.04. But for now, we can use the line mentioned above for our repository file.</p>
<p class="normal">Next, we can install the packages required for Kubernetes:</p>
<pre class="programlisting con"><code class="hljs-con">sudo apt install kubeadm kubectl kubelet
</code></pre>
<p class="normal">We’re installing<a id="_idIndexMarker995"/> three packages, <code class="inlineCode">kubeadm</code>, <code class="inlineCode">kubectl</code>, and <code class="inlineCode">kubelet</code>:</p>
<ul>
<li class="bulletList"><strong class="keyWord">kubeadm</strong>: The <code class="inlineCode">kubeadm</code> package<a id="_idIndexMarker996"/> gives us tools we can use to “bootstrap” our cluster. We can use this tool to initialize a new cluster, join a node to an existing clusterand upgrade the cluster to a newer version.</li>
<li class="bulletList"><strong class="keyWord">kubectl</strong>: This <a id="_idIndexMarker997"/>package provides us with the Kubernetes command-line tool, <code class="inlineCode">kubectl</code>. We will use this tool to interact with our cluster and manage it.</li>
<li class="bulletList"><strong class="keyWord">kubelet</strong>: The <a id="_idIndexMarker998"/>Kubernetes <code class="inlineCode">kubelet</code> acts as an agent that facilitates communication between nodes. It also exposes API endpoints that can be used to enable additional communication and features.</li>
</ul>
<p class="normal">Unlike all of the previous commands we’ve run through so far while setting up our cluster, the following will be entered and ran only on the node that you’ve designated as the controller (before you run it, read the paragraph that follows to know how to customize the command):</p>
<pre class="programlisting con"><code class="hljs-con">sudo kubeadm init --control-plane-endpoint=172.16.250.216 --node-name controller --pod-network-cidr=10.244.0.0/16
</code></pre>
<p class="normal">The previous <code class="inlineCode">kubeadm init</code> command should be customized a bit, before you run it. There’s a few things you should change to ensure they match your environment. In fact, I bolded the individual settings you should customize. I’ll also describe some additional details<a id="_idIndexMarker999"/> about those particular settings now.</p>
<p class="normal"><code class="inlineCode">--control-plane-endpoint=172.16.250.216</code>: For this option, the IP address that is mentioned here should be the same IP address that’s assigned to your server. I filled in the value I used for my test server, but you should make sure this matches the actual IP on your side. Since this is a very specific setting, this is one of the reasons why I recommended you finalize your IP address for your nodes <em class="italic">before</em> you set them up in the cluster.</p>
<p class="normal"><code class="inlineCode">--node-name controller</code>: When I set up my test server, I simplified its hostname down to simply <code class="inlineCode">controller</code>. On your side, you can set this to match your controller’s hostname as well.</p>
<p class="normal">Note that I didn’t bold the second IP address in the command, <code class="inlineCode">10.244.0.0/16</code>. The reason for this, is that you should not change that particular IP declaration. That’s actually the IP scheme for<a id="_idIndexMarker1000"/> the <strong class="keyWord">Pod Network</strong>, which is an internal network to Kubernetes and is not accessible from the outside. You can actually customize this to your own scheme, but then you’d have to change other settings as well. Since this is a dedicated internal network, there shouldn’t be any need to change this so I recommend to leave it as-is.</p>
<p class="normal">The <code class="inlineCode">kubeadm init</code> command that I mentioned earlier, after you customize it, will initialize the cluster and assign it a Pod Network. If the initialization process is successful, you’ll see a <code class="inlineCode">join</code> command printed to the terminal at the end. If you do see it, then congratulations! You’ve successfully set up a Kubernetes cluster. It’s not a very usable cluster yet, because we haven’t added any worker nodes to it at this point. But you do, by definition, have a Kubernetes cluster in existence at this point. On my controller node, I see the following output:</p>
<figure class="mediaobject"><img alt="" height="245" src="../Images/B18425_18_13.png" width="756"/></figure>
<p class="packt_figref">Figure 18.13: Successful initialization of a Kubernetes cluster, showing a join command</p>
<p class="normal">The <code class="inlineCode">join</code> command <a id="_idIndexMarker1001"/>that’s shown to you after the process is complete is very special—you can copy that command and paste it into the command prompt of a worker node to join it to the cluster, but don’t do that just yet. For now, copy that command and store it somewhere safe. You don’t want to allow others to see it, because it contains a hash value that’s specific to your cluster and allows someone to join nodes to it. Technically, showing my <code class="inlineCode">join</code> command with the hash value is the last thing I should do in a book that will be seen and read by many people, but since this is just a test cluster with no actual value, I don’t mind you seeing it.</p>
<p class="normal">Also, in the same output as the <code class="inlineCode">join</code> command, are three additional commands we should run on the controller node. If you scroll up in your terminal window to before the <code class="inlineCode">join</code> command, you should see output similar to the following:</p>
<figure class="mediaobject"><img alt="" height="182" src="../Images/B18425_18_14.png" width="867"/></figure>
<p class="packt_figref">Figure 18.14: Output of the initialization process for Kubernetes showing additional commands to run</p>
<p class="normal">For those three commands, you can copy and paste them as-is right back into the terminal, one by one. Go ahead and do that. Essentially what you’re doing is creating a local config directory for <code class="inlineCode">kubectl</code> right in your user’s home directory. Then, you’re copying the <code class="inlineCode">admin.conf</code> file from <code class="inlineCode">/etc/kubernetes</code> and storing it in the newly created <code class="inlineCode">.kube</code> folder in your home directory with a new name of <code class="inlineCode">config</code>. Finally, you’re changing ownership of the <code class="inlineCode">.kube</code> directory to be owned by your user. This will allow your user account to manage Kubernetes with the <code class="inlineCode">kubectl</code> command, without needing to be logged in as <code class="inlineCode">root</code> or use <code class="inlineCode">sudo</code>.</p>
<p class="normal">Kubernetes itself <a id="_idIndexMarker1002"/>consists of multiple <strong class="keyWord">Pods</strong>, within which your containers will run. We haven’t deployed any containers yet, we’ll do that later. But even though we didn’t deploy any containers ourselves, there are some that are actually running already. See for yourself:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get pods --all-namespaces
</code></pre>
<p class="normal">On my end, I see the following:</p>
<figure class="mediaobject"><img alt="" height="193" src="../Images/B18425_18_15.png" width="879"/></figure>
<p class="packt_figref">Figure 18.15: Checking the status of pods running in our Kubernetes cluster</p>
<p class="normal">Notice that the namespace for each of these Pods is <code class="inlineCode">kube-system</code>. This is a special namespace, where Pods related to the Kubernetes cluster itself will run. We didn’t explicitly ask for these to run, they’re run as part of the cluster and provide essential functionality. Another important column is <code class="inlineCode">STATUS</code>. In the screenshot, we see that most of them are <code class="inlineCode">Running</code>. The first two, though are in a state of<strong class="keyWord"> </strong><code class="inlineCode">Pending</code>, which means that they’re not quite ready yet. This is actually expected, we haven’t deployed an overlay network yet, which is required for the <code class="inlineCode">coredns</code> pods to function.</p>
<div class="note">
<p class="normal">Overlay networks are virtual networks that are created to serve as a network on top of another network. The concept is not specific to Kubernetes, but when used in Kubernetes, the overlay network manages communication between nodes.</p>
</div>
<p class="normal">Other Pods may also show a status of <code class="inlineCode">Pending</code>, and those should automatically switch to <code class="inlineCode">Running</code> when they finish setting themselves up. How long this takes depends on the speed of your hardware, but it should only take a few minutes.</p>
<p class="normal">The others will <a id="_idIndexMarker1003"/>eventually change to a state of <code class="inlineCode">Running</code> once their setup is finished – but they might also crash as well, and respawn. Since there’s no overlay network yet, the pods aren’t able to fully communicate yet. So if you see some errors, don’t worry about that yet.</p>
<p class="normal">Let’s deploy an overlay network so we can make the cluster more stable:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
</code></pre>
<p class="normal">When we check the status again, we should see an additional Pod running (<code class="inlineCode">kube-flannel-ds</code>), and the Pods that had a state of <code class="inlineCode">Pending</code> previously should now be <code class="inlineCode">Running</code>:</p>
<figure class="mediaobject"><img alt="" height="223" src="../Images/B18425_18_16.png" width="879"/></figure>
<p class="packt_figref">Figure 18.16: Checking the status of Pods running in our Kubernetes cluster again</p>
<p class="normal">So, what exactly did we do? <strong class="keyWord">Flannel</strong> is <a id="_idIndexMarker1004"/>a networking layer that can run on Kubernetes. Networking is required for Kubernetes to function, as Pods within the cluster need to be able to communicate with one another. There are multiple different types of networking models you can implement in Kubernetes, Flannel is just one available option. If you’re using a cloud provider, such as AWS, then the networking model is typically chosen for you. Since we’re building a cluster from scratch, we have to choose a networking model—Kubernetes itself doesn’t come with one.</p>
<p class="normal">Setting up networking in a cluster that was manually created can be quite an involved task, Flannel is easy to set up (we simply deploy it) and its defaults meet the needs of Kubernetes and will get us up and running quickly. There are definitely some other options for the networking layer to consider for your cluster, but Flannel is good enough for us for what we need right now.</p>
<p class="normal">Next, it’s time to <a id="_idIndexMarker1005"/>watch the magic happen and join worker nodes to our cluster. We can check the status of all of our nodes with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get nodes
</code></pre>
<p class="normal">Right now though, we only have the controller showing up in the output since we haven’t yet added any nodes:</p>
<figure class="mediaobject"><img alt="" height="131" src="../Images/B18425_18_17.png" width="719"/></figure>
<p class="packt_figref">Figure 18.17: Checking the status of the nodes within the cluster</p>
<p class="normal">To add a worker to our cluster, we can enter the <code class="inlineCode">join</code> command we saw earlier on a node designated as a worker. If you recall, the <code class="inlineCode">join</code> command was shown in the terminal when we first initialized our cluster. The command will look something like the following, which I’ve shortened a bit to fit on this page (I’ve added <code class="inlineCode">sudo</code> to the beginning):</p>
<pre class="programlisting con"><code class="hljs-con">sudo kubeadm join 172.16.250.216:6443 --token zu5u3x.p45x0qkjl37ine6i \
    --discovery-token-ca-cert-hash sha256...1360c
</code></pre>
<p class="normal">You should consider the join command as private, and not show it to anyone, nor should you upload it to a Git repository or a documentation server. Reason being, it could be used by an outside actor to join something to your cluster. In my case, I truncated the hash so it’s impossible to reproduce, but keep this in mind going forward.</p>
<p class="normal">For you, the command will be very different. The IP address in the command is for the controller, which will no doubt be different on your end. The hash value will be different as well. Basically, just copy the <code class="inlineCode">join</code> command you were provided while initializing the cluster on the controller and paste it into each of your worker nodes. You should see a message in the<a id="_idIndexMarker1006"/> output that the node was successfully added to the cluster:</p>
<figure class="mediaobject"><img alt="" height="183" src="../Images/B18425_18_18.png" width="872"/></figure>
<p class="packt_figref">Figure 18.18: Successfully adding a worker node to the Kubernetes cluster</p>
<p class="normal">After you’ve run the join command on each of your worker nodes (on however many you decided to create), you can run the <code class="inlineCode">kubectl get nodes</code> command again on the controller and verify the new nodes appear on the list. I added two nodes, so I see the following output on my end:</p>
<figure class="mediaobject"><img alt="" height="182" src="../Images/B18425_18_19.png" width="719"/></figure>
<p class="packt_figref">Figure 18.19: Output of kubectl showing worker nodes now added to the cluster</p>
<p class="normal">Once all of the nodes you plan on deploying show a <code class="inlineCode">STATUS</code> of <code class="inlineCode">Ready</code>, then your cluster setup is complete!</p>
<p class="normal">If you run into any trouble joining a node to the cluster, you can try to regenerate the join token. After a while, it’s possible the original could’ve expired and the certificates won’t be valid. To regenerate the join command, run the following from the controller node:</p>
<pre class="programlisting con"><code class="hljs-con">kubeadm token create --print-join-command
</code></pre>
<p class="normal">This will print a brand new join command you can use in place of the original.</p>
<p class="normal">At this point, our <a id="_idIndexMarker1007"/>cluster exists and has one or more worker nodes ready to do our bidding. The next step is to actually use the cluster and deploy a container. That’s exactly what we’ll do in the next section.</p>
<h1 class="heading-1" id="_idParaDest-239">Deploying containers via Kubernetes</h1>
<p class="normal">Now it’s <a id="_idIndexMarker1008"/>time to <a id="_idIndexMarker1009"/>see our work pay off, and we can successfully use the cluster we’ve created. At this point, you should have either set up MicroK8s, or manually created a cluster as we’ve done in the previous section. In either case, the result is the same: we have a cluster available that we can use to deploy containers.</p>
<p class="normal">Keep in mind that if you’re using MicroK8s, you might need to prepend <code class="inlineCode">microk8s</code> in front of <code class="inlineCode">kubectl</code> commands, depending on how you set up MicroK8s. I’ll leave it up to you to add <code class="inlineCode">microk8s</code> to the front of such commands as you go along, if you’re using MicroK8s and you don’t have it set up to simplify <code class="inlineCode">microk8s kubectl</code> to <code class="inlineCode">kubectl</code>.</p>
<p class="normal">Kubernetes utilizes files created in the YAML format to receive instructions. Does that sound familiar? In <em class="chapterRef">Chapter 15</em>, <em class="italic">Automating Server Configuration with Ansible</em>, we worked with YAML files as that’s the format that Ansible playbooks are written in. YAML isn’t specific to Ansible; it’s used with many different applications and services, and Kubernetes will also recognize the YAML format to contain instructions for how to deploy something. Here’s an example YAML file to get us started. This one in particular is intended to<a id="_idIndexMarker1010"/> launch<a id="_idIndexMarker1011"/> an NGINX container within our cluster:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Pod
metadata:
  name: nginx-example
  labels:
    app: nginx
spec:
  containers:
    - name: nginx
      image: linuxserver/nginx
      ports:
        - containerPort: 80
          name: "nginx-http"
</code></pre>
<p class="normal">Before I show you how to run it, let’s walk through the file and understand what’s going on.</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Pod
</code></pre>
<p class="normal">First, we’re identifying the API version we intend to use, and then we’re setting the <code class="inlineCode">kind</code> of deployment we intend to set up. In this case, we’re deploying a <code class="inlineCode">pod</code>, which is what our containers run inside of in Kubernetes. One or more containers can run in a Pod, and a worker node can run one or more Pods at the same time.</p>
<pre class="programlisting code"><code class="hljs-code">metadata:
  name: nginx-example
  labels:
    app: nginx
</code></pre>
<p class="normal">Next, we add some metadata. Metadata allows us to set special parameters specific to our deployment. The first item of metadata we’re customizing is the <code class="inlineCode">name</code>, we’re naming the Pod <code class="inlineCode">nginx-example</code> in this case. We’re also able to set up labels with metadata, which is a “name: value” key pair that allows us to add additional values to our Pod that we can refer to later. In this case, we’re creating a label called <code class="inlineCode">app</code> and setting it to <code class="inlineCode">nginx</code>. This name is arbitrary; we could call it <code class="inlineCode">potato</code> if we wanted to. Setting this to <code class="inlineCode">nginx</code> is a descriptive label that makes it obvious to someone else what we are intending to run here.</p>
<pre class="programlisting code"><code class="hljs-code">spec:
  containers:
    - name: nginx       image: linuxserver/nginx
</code></pre>
<p class="normal">Moving on, the <code class="inlineCode">spec</code> section allows us to specify what exactly we want to run in our Pod, and how we <a id="_idIndexMarker1012"/>want<a id="_idIndexMarker1013"/> it to run. We want to run a container in our Pod, and specifically a container that we’ll name <code class="inlineCode">nginx</code>, which we’ll retrieve from a registry called <code class="inlineCode">linuxserver</code>, and we’re requesting a container from that registry by the name of <code class="inlineCode">nginx</code>.</p>
<p class="normal">The registry we’re fetching the container image from deserves some extra explanation. This registry in particular is located at <a href="https://linuxserver.io"><span class="url">https://linuxserver.io</span></a>, which is a special service that makes container images available for us to download and use. The site has a documentation section that gives us information about each of the container images they offer. Why use the <a href="http://linuxserver.io"><span class="url">linuxserver.io</span></a> registry? The reason is that their service makes available various container images that support a variety of architectures, including x86 as well as ARM. The latter is especially important, because if you’re using Raspberry Pi units for your cluster, they won’t be able to utilize container images created for x86. If you do attempt to run a container image that does not support ARM, then the container will fail to launch on a Pi. Since <a href="http://linuxserver.io"><span class="url">linuxserver.io</span></a> makes container images available for multiple architectures, they should work fine regardless of the type of device you decided to use for your cluster. Whether you’re using an x86 physical server or VM for your worker node, the <code class="inlineCode">nginx</code> container we’re retrieving from that registry should function just fine.</p>
<pre class="programlisting code"><code class="hljs-code">      ports:
        - containerPort: 80
          name: "nginx-http"
</code></pre>
<p class="normal">In the last few lines, we’re setting up a container port of <code class="inlineCode">80</code>, which is the standard for a web server. This is the default port that NGINX listens on when it runs, and NGINX is what we’re intending to run inside our container. We’re applying a name to this port declaration, and calling it <code class="inlineCode">nginx-http</code>. We can refer to that name in a subsequent YAML file (if we have more than one) and that works better than having to type the same port in each file. Referring to a port by name is not all that different from variables in scripting or programming languages.</p>
<p class="normal">Before we continue, there’s a bit of difference regarding how you deploy resources to a cluster within MicroK8s compared to a cluster that was set up manually on actual servers. The commands going forward will assume you’ve set up a cluster manually. If you’re using MicroK8s on Windows or macOS, you’ll need to copy any deployment files you create into the MicroK8s VM that’s created as part of the installation of MicroK8s. If you try to save a<a id="_idIndexMarker1014"/> file <a id="_idIndexMarker1015"/>locally and deploy it as we’re about to do in the next paragraph, it will fail, because the file will not be found on the VM. To copy a deployment file to the MicroK8s VM to enable you to deploy it, you can use the following command to copy the file into the VM first:</p>
<pre class="programlisting con"><code class="hljs-con">multipass transfer &lt;filename.yml&gt; microk8s-vm:
</code></pre>
<p class="normal">As we go along, keep in mind that whenever we’re deploying a file, you’ll have to make sure to transfer it first. Also, a manually created cluster uses the <code class="inlineCode">kubectl</code> command, while MicroK8s requires you to prefix <code class="inlineCode">kubectl</code> with <code class="inlineCode">microk8s</code>, so such commands become <code class="inlineCode">microk8s kubectl</code> instead of just <code class="inlineCode">kubectl</code>.</p>
<p class="normal">Let’s go ahead and deploy the container to our cluster. Assuming you named the YAML file as <code class="inlineCode">pod.yml</code>, you can deploy it with the following command on the controller node:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl apply -f pod.yml
</code></pre>
<p class="normal">As mentioned previously, the <code class="inlineCode">kubectl</code> command allows us to control our cluster. The <code class="inlineCode">-f</code> option accepts a file as input, and we’re pointing it to the YAML file that we’ve created.</p>
<p class="normal">Once you’ve run the command, you’ll be able to check the status of the deployment with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get pods
</code></pre>
<p class="normal">For me, I see the following when I run it:</p>
<figure class="mediaobject"><img alt="" height="148" src="../Images/B18425_18_20.png" width="792"/></figure>
<p class="packt_figref">Figure 18.20: Checking the status of a container deployment</p>
<p class="normal">From the output in my case, I can see that the process was successful. The <code class="inlineCode">STATUS</code> is <code class="inlineCode">Running</code>. What we don’t see is what worker node the Pod is running on. It’s nice to know that it’s <code class="inlineCode">Running</code>, but where? We can get more information by adding the <code class="inlineCode">-o wide</code> option to the end of the command:</p>
<pre class="programlisting con"><code class="hljs-con"> kubectl get pods -o wide
</code></pre>
<p class="normal">The output<a id="_idIndexMarker1016"/> contains <a id="_idIndexMarker1017"/>much more information, so much that a screenshot of it won’t fit on this page. The focus though, is that among the extra fields we have with this version of the command is the <code class="inlineCode">node</code> field, which shows us which worker node is handling this deployment. We’ll even see an IP address assigned to the Pod as well.</p>
<p class="normal">We can use the IP address shown for the Pod to access the application running inside the container. In my case, my Pod was given an IP address of <code class="inlineCode">10.244.1.2</code>, so I can use the <code class="inlineCode">curl</code> command to access the default NGINX web page:</p>
<pre class="programlisting con"><code class="hljs-con">curl 10.244.1.2
</code></pre>
<p class="normal">The output that’s shown in the terminal after running that command should be the HTML for the default NGINX web page. Note that the <code class="inlineCode">curl</code> command may not be available in your Ubuntu installation, so you may need to install the required package first:</p>
<pre class="programlisting con"><code class="hljs-con">sudo apt install curl
</code></pre>
<p class="normal">We have one potential issue though: if you want to be able to access an application running inside the cluster on a machine other than the controller or worker nodes, it won’t work. By default, there’s nothing in place to route traffic from your LAN to your cluster. This means that in my case, the IP address of <code class="inlineCode">10.244.1.2</code> that was given to my Pod was provided to it by the Pod Network, the router on my network doesn’t understand that IP address scheme so trying to access it from another machine on the LAN will fail. What’s interesting is that you can access the application from any other node within the cluster. In my case, the NGINX Pod is running on the first worker node. However, I can actually run the previous <code class="inlineCode">curl</code> command from worker #2 even though the Pod isn’t running there, and I’ll get the same exact output. This works because the Pod Network is cluster-wide; it’s not specific to any one node. </p>
<p class="normal">The IP address of <code class="inlineCode">10.244.1.2</code> is unique to the entire cluster, so if I run another container it won’t receive that same IP address, and every node within the cluster knows how to internally route to IPs within that network.</p>
<p class="normal">Not allowing<a id="_idIndexMarker1018"/> outside<a id="_idIndexMarker1019"/> devices to access applications within the cluster is great for security purposes. After all, a hacker can’t break into a container that they can’t even route to. But the entire point of running a Kubernetes cluster is to make applications available to our network, so how do we do that? This can be a very confusing topic for newcomers, especially when we’re setting up a cluster manually. If we’re using a cloud service such as AWS or Google Cloud, they add an additional layer on top of their Kubernetes implementation that facilitates routing traffic in and out of the cluster. Since we set up our cluster manually, we don’t have anything like that in place.</p>
<p class="normal">When it comes to building services and networking components designed to handle networking to bridge our LAN and Kubernetes cluster, that’s an expansive topic that can span a few chapters in and of itself. But a simple solution for us is to create a <strong class="keyWord">NodePort Service</strong>. These are<a id="_idIndexMarker1020"/> two new concepts here, a Service as well as NodePort. When it comes to a Service, a Pod isn’t the only thing we can deploy within our cluster. There are several different things we can deploy, and a Service is a method of exposing a Kubernetes Pod, and a NodePort is a specific type of service that gives us a specific method for facilitating a way of accessing it. What NodePort itself does is expose a port running inside a Pod to a port on each cluster node.</p>
<p class="normal">Here’s a file we can deploy to create a NodePort Service for our Pod:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Service
metadata:
  name: nginx-example
spec:
  type: NodePort
  ports:
    - name: http
      port: 80
      nodePort: 30080
      targetPort: nginx-http
  selector:
    app: nginx
</code></pre>
<p class="normal">As you can see, the <code class="inlineCode">kind</code> is <code class="inlineCode">Service</code>; as this time, we’re deploying a service to our cluster to complement the Pod we’ve deployed previously. The <code class="inlineCode">type</code> of service is <code class="inlineCode">NodePort</code>, and we are mapping port <code class="inlineCode">80</code> within the Pod to port <code class="inlineCode">30080</code> in our cluster. I chose port <code class="inlineCode">30080</code> arbitrarily. With NodePort, we can utilize ports <code class="inlineCode">30000</code> through <code class="inlineCode">32767</code>. The <code class="inlineCode">selector</code> in this file is <a id="_idIndexMarker1021"/>set<a id="_idIndexMarker1022"/> to <code class="inlineCode">nginx</code>, which is the same selector we used while creating the Pod. Selectors allow us to “select” Kubernetes resources via a name we assign them, to make it easier to refer to them later.</p>
<p class="normal">Let’s deploy our service:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl apply -f service-nodeport.yml
</code></pre>
<p class="normal">Assuming we’ve typed everything in the YAML file for the service properly, we can retrieve the status of services running within our cluster with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get service
</code></pre>
<p class="normal">If everything has gone well, you should see output similar to the following:</p>
<figure class="mediaobject"><img alt="" height="142" src="../Images/B18425_18_21.png" width="874"/></figure>
<p class="packt_figref">Figure 18.21: Checking the status of a service deployment</p>
<p class="normal">In the output, we should see the status of our <code class="inlineCode">service</code> deployment, which is the second line in the screenshot. We can also see the port mapping for it, showing that port <code class="inlineCode">30080</code> should be exposed to the outside. To test that it’s actually working, we can open a web browser on a machine that is within our LAN, and it should be able to access the controller’s IP address at port <code class="inlineCode">30080</code>:</p>
<figure class="mediaobject"><img alt="" height="364" src="../Images/B18425_18_22.png" width="788"/></figure>
<p class="packt_figref">Figure 18.22: Accessing a web page served by an NGINX container in a cluster from within a web browser</p>
<p class="normal">Another <a id="_idIndexMarker1023"/>interesting <a id="_idIndexMarker1024"/>benefit is that we didn’t actually have to use the IP address of our controller node, we can use the IP address of a worker node as well and see the same default page. The reason this works is that the mapping of port <code class="inlineCode">30080</code> to port <code class="inlineCode">80</code> inside the Pod is cluster-wide, just as the internal Pod Network is also cluster-wide. Accessing a resource on one is the same as accessing the same resource on any other, as the request will be directed toward whichever node is running the Pod that matches the request.</p>
<p class="normal">When it comes to removing a Pod or a Service, assuming you want to decommission something running in your cluster, the syntax for that is fairly straightforward. To remove our <code class="inlineCode">nginx-example</code> Pod, for example, you can run this:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl delete pod nginx-example
</code></pre>
<p class="normal">Similarly, to delete our service, we can run this:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl delete service nginx-example
</code></pre>
<p class="normal">At this point, you not only have a working cluster, but you also have the ability to deploy containers to it and set them up to be accessible from outside the Pod Network. From here, I recommend <a id="_idIndexMarker1025"/>you practice with this a bit and <a id="_idIndexMarker1026"/>attempt to run additional containers and apps to have a bit of fun with it.</p>
<h1 class="heading-1" id="_idParaDest-240">Summary</h1>
<p class="normal">In this chapter, we took containerization to the next level and implemented Kubernetes. Kubernetes provides us with orchestration for our containers, enabling us to more intelligently manage our running containers and implement services. Kubernetes itself is a very expansive topic, and there are many additional features and benefits we can explore. But for the goal of getting set up on Kubernetes and running containers with it, we did what we needed to do. I recommend that you continue studying Kubernetes and expand your knowledge, as it’s a very worthwhile subject to dig deeper into.</p>
<p class="normal">Speaking of subjects that are worthwhile to learn, in the next chapter, we’re going to learn how to deploy Ubuntu in the cloud! Specifically, we’ll get started with Amazon Web Services, which is a very popular cloud platform.</p>
<h1 class="heading-1" id="_idParaDest-241">Relevant videos</h1>
<ul>
<li class="bulletList">Setting up a Kubernetes Cluster (LearnLinuxTV): <a href="https://linux.video/setup-k8s"><span class="url">https://linux.video/setup-k8s</span></a></li>
</ul>
<h1 class="heading-1" id="_idParaDest-242">Further reading</h1>
<ul>
<li class="bulletList">MicroK8s website: <a href="https://learnlinux.link/mk8s"><span class="url">https://learnlinux.link/mk8s</span></a></li>
<li class="bulletList">MicroK8s addons: <a href="https://learnlinux.link/mk8s-addons"><span class="url">https://learnlinux.link/mk8s-addons</span></a></li>
</ul>
<h1 class="heading-1">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the author and other readers: </p>
<p class="normal"><a href="https://packt.link/LWaZ0"><span class="url">https://packt.link/LWaZ0</span></a></p>
<p class="normal"><img alt="" height="138" src="../Images/QR_Code50046724-19558751561.png" width="177"/></p>
</div>
</div></body></html>