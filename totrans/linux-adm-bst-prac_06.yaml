- en: '*Chapter 4*: Designing System Deployment Architectures'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How we deploy systems determines so much about how those systems will perform
    and how resilient they will be for years to come. A good understanding of design
    components and principles is necessary for us to understand in order to approach
    the design of the platforms that will carry our workloads. Remember, at the end
    of the day, only the applications running at the very top of the stack matter
    - everything beneath the applications, whether the operating system, hypervisor,
    storage, hardware, and others are just tools used to enable the final application-level
    workloads to do what they need to do best. It is easy to feel that these other
    components matter individually, but they do not. To put it another way, what matters
    is the results rather than the path taken to get to the results.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to start by looking at the building blocks of
    systems (other than storage which we tackled extensively in our last chapter before
    taking all of those components as a whole and looking at them, to see how they
    can form robust carriers for our application workloads. Next, we will look at
    need analysis. Then finally we will move on to assembling those pieces into architectural
    designs to meet those needs.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should feel confident that, while Linux is potentially
    only one slice in the middle of our application stack, you are prepared to design
    the entire stack properly to meet workload goals. While technically much of this
    design is not strictly systems administration (or engineering) it most often falls
    to the system administrators to handle as only the rarest of organizations have
    highly skilled and end to end knowledgeable staff from other departments. The
    systems team sits at the nexus of all components and has the greatest single role
    visibility in both directions (up the stack to the applications and down the stack
    to hypervisors, hardware, and storage). It is natural that systems teams are tasked
    with the greater design tasks as there is no one else capable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we are going to learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Virtualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud and **Vitual Private Server** (**VPS**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On premises, hosted, and hybrid hosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System design architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Risk assessment and availability needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Availability strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Twenty years ago, if you asked the average system administrator what virtualization
    was they would look at you with a blank stare. We have had virtualization technologies
    in IT since 1965 when IBM first introduced them in their mainframe computer systems,
    but for your average company these technologies were relatively rare and out of
    reach until vendors like *VMware* and *Xen* brought these to the mainstream market
    around the turn of the millennium. The enterprise space did have many of these
    technologies by the 1990s, but knowledge of them did not disseminate far.
  prefs: []
  type: TYPE_NORMAL
- en: Times have changed. Since 2005, virtualization has been broadly available and
    widely understood, with options for every platform and at all price points, leaving
    no one with a need to avoid implementing the technology because it is out of technical
    or financial reach. At its core, virtualization is an abstraction layer that creates
    a computer *in software* (on top of the actual hardware) and presents a standard
    set of virtual hardware. Software that performs virtualization is called a **hypervisor**
  prefs: []
  type: TYPE_NORMAL
- en: In the last chapter we spoke repeatedly about interfaces and how something consumes
    or presents itself as a disk drive or file system, for example. A hypervisor is
    software that presents a *computer interface*, meaning it doesn't just present
    a hard drive, but it acts like an entire computer. If you have never used or thought
    about virtualization this might seem extraordinarily complex and confusing, but
    in reality, this is an abstraction that often makes computing far simpler and
    more reliable. Just like technologies that abstracted storage (like **Logical
    Volume Managers** and **RAID** systems) proved to be incredibly valuable once
    they were mature and understood, so has computer level virtualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of hypervisors that we will talk about in this chapter,
    and they are simply known as Type 1 and Type 2 hypervisors. All hypervisors present
    the same thing: a *computer*. But what makes a Type 1 and a Type 2 hypervisor
    different is what they consume.'
  prefs: []
  type: TYPE_NORMAL
- en: Type 1 hypervisor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes called a *bare metal* hypervisor, Type 1 hypervisors are intended
    to run directly on the system hardware but, of course, can run on anything presenting
    itself as a capable piece of hardware (such as another hypervisor!) As such, a
    Type 1 Hypervisor is not an application and does not run on top of an operating
    system and so only needs to worry about hardware compatibility with the physical
    device on which it will be installed.
  prefs: []
  type: TYPE_NORMAL
- en: Type 1 hypervisors are generally the only type considered true ready for production
    because they install directly without any unnecessary software layers and so can
    be faster, smaller, and more reliable.
  prefs: []
  type: TYPE_NORMAL
- en: The Type 1 hypervisor was more difficult to initially engineer and so the earliest
    hypervisors were generally other types that could pass off work to the operating
    system. But effectively it was the introduction of the Type 1 hypervisor and enough
    vendors with disparate products to warrant a mature market designation that encouraged
    the extreme move to virtualization in the 2000s.
  prefs: []
  type: TYPE_NORMAL
- en: Type 2 hypervisor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the bare-metal hypervisor, a Type 2 hypervisor is an application that
    you install onto an operating system. This means that the hypervisor has to wait
    for the operating system to give it resources, competes with other applications
    for resources, and requires that the operating system itself is stable, in addition
    to the hypervisor being stable, in order to keep workloads running on top of it.
  prefs: []
  type: TYPE_NORMAL
- en: When virtualization was relatively new, especially in the microcomputer arena,
    Type 2 Hypervisors were much more common because they were cheaper and easier
    to make and have little need for hardware support to do what they do. A Type 2
    Hypervisor lets the bare metal operating system do the heavy lifting of supplying
    drivers and hardware detection, task scheduling, and so forth. So, in much of
    the 2000s we saw Type 2 Hypervisors taking a principal role in driving virtualization
    adoption. They are easy to deploy and very easy to understand and because they
    are just an application that gets deployed on top of an operating system anyone
    can just install one on an existing desktop or even laptop to try out virtualization
    for themselves.
  prefs: []
  type: TYPE_NORMAL
- en: By the late 2000s, technology had changed rather significantly, the software
    had advanced and matured, and nearly all computers had gained some degree of hardware
    assistance for virtualization, allowing hypervisors to use less code while gaining
    much better performance. Type 1 hypervisors rapidly proliferated, and, before
    2010, the idea of using a Type 2 hypervisor in production was all but unthinkable.
    Type 1 hypervisors provide a single, standard operating system installation target,
    moving the heavy lifting away from the operating system and over to the hypervisor,
    where it is generally accepted to be better positioned. Because the hypervisor
    controls the bare metal, it is able to properly schedule system resources and
    eek maximum performance out of a system. Hypervisors are expected to be only a
    small fraction of the size of an operating system. This means little more than
    a shim between virtualized operating systems and the physical system (a tiny layer
    of code doing the bare minimum, and being essentially invisible to the operating
    system running on top of it). This minimizes bloat and features, while operating
    systems need to be large, complex, and feature-rich to do their jobs well in most
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: Type 2 Hypervisors have proven to be useful in lab environments, especially
    for situations where testing or learning is best done from a personal computing
    environment such as a desktop or laptop or can be useful for special case temporary
    workloads where there is a need to completely disable or possibly even to remove
    the hypervisor when it is no longer needed. But for production server environments
    only Type 1 Hypervisors are really appropriate today.
  prefs: []
  type: TYPE_NORMAL
- en: There are two best practices commonly associated with virtualization
  prefs: []
  type: TYPE_NORMAL
- en: Virtualize every system, unless a requirement makes you unable to do so. In
    practical terms, you will never realistically see a valid exception to this rule.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always use a Type 1 (Bare Metal) Hypervisor for servers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hypervisor types are confusing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the real world detecting what is and is not a Type 1 Hypervisor can be rather
    difficult. A hypervisor, by definition, really does not have any kind of end user
    interface of its own. This makes it something that we have to explain, but not
    something that we really see. Even a true operating system is hard to point to
    and say *see, there it is* because it is really a shell or a desktop environment
    running as an application on top of the operating system, rather than the operating
    system itself, that we see and touch. With a hypervisor, any interface that we
    see, of any sort, has to be being presented by something running on an operating
    system, not something running directly on the hypervisor.
  prefs: []
  type: TYPE_NORMAL
- en: Hypervisors, of course, need some sort of interface for us to interact with
    them. How they handle this varies wildly and not all hypervisors, even of the
    same type, are built the same. Under the hood, of course, they are always running
    on the bare metal, but they can use several different architectures to handle
    all of the functions that they need. Each different architecture has a different
    opportunity for how it will seem to appear to an end user – meaning that an end
    user sitting down to the system may experience wildly different interfaces that
    pretend to be things that they are or possibly are not.
  prefs: []
  type: TYPE_NORMAL
- en: In early Type 1 Hypervisors it was common to run a virtual machine (the name
    for a virtualized computer running on top of a hypervisor of any type) that had
    privileges to control the hypervisor given to it. This allows the hypervisor to
    be as lean as possible and allows big tasks like presenting a user interaction
    shell to be done using existing tools and no one had to reinvent the wheel. Using
    this approach meant that hypervisor engineering work was minimal in the early
    days allowing the virtualization itself to be the key focus.
  prefs: []
  type: TYPE_NORMAL
- en: As time has progressed, alternative approaches have begun to emerge. Running
    a full operating system in a virtualized environment on top of the hypervisor
    just to act as an interface for the end user felt like a waste of resources. Later
    hypervisors used creative ways to get around this making the hypervisor itself
    heavier but reducing the overall weight of the total system.
  prefs: []
  type: TYPE_NORMAL
- en: Virtualization tends to be confusing and vendors have little reason to want
    to expose the inner workings of their systems. It has therefore become commonplace
    to misuse terms or to suggest that hypervisors work differently than they do either
    for marketing reasons or to attempt to simplify the system for less knowledgeable
    customers. There are many hypervisors today and potentially more will arise in
    the future. In production enterprise environments, however, there are four that
    we expect to see with any regularity and we will briefly break down each one to
    explain how it works. No one approach is best, these are simply different ways
    to skin the same cat.
  prefs: []
  type: TYPE_NORMAL
- en: VMware ESXi
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The market leader of virtualization today. VMware is one of the oldest virtualization
    products and has changed its design over time. Originally VMware followed the
    classic design of an extremely lean hypervisor and a *hidden* virtual machine
    that ran on top of it running a stripped down copy of Red Hat Enterprise Linux
    which provided the end user facing shell for interaction with the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Today VMware ESXi instead builds a tiny shell into the hypervisor itself that
    provides only enough potential user interaction to handle the simplest of tasks
    such as detecting the IP address that is in use or setting the password. Everything
    else is handled through an API that is called from an external tool allowing for
    the heaviest portions of the user interface to be kept completely on the physical
    client workstation rather than on the hypervisor.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Hyper-V
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although late to the enterprise Type 1 virtualization game, Microsoft opted
    for the classic approach and always runs a virtual machine in which is contained
    a stripped down copy of Windows which provides the graphical user interface that
    end users will see when having installed Hyper-V. This first virtual machine is
    installed automatically, by default requires no paid licensing, does not contain
    Windows branding, and is contrarily named the *physical* machine which together
    can make it appear that there is no VM at all, but rather a Type 2 hypervisor
    running on top of a Windows install, but this is not the case. It simply appears
    so based on tricky naming and the unnecessarily convoluted standard install processes.
    Doubt not, Hyper-V is a true Type 1 hypervisor running in the most classic way.
  prefs: []
  type: TYPE_NORMAL
- en: Xen
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Coming from the same early era as VMware, Xen started with the classic approach,
    but, unlike Vmware, stuck with it over the years. However, unlike Hyper-V, Xen
    installations tend to be more manual, and the use of a first virtual machine for
    the purpose of providing end user interactions is not in any way hidden and, in
    fact, is completely exposed. What this means is that during the hypervisor installation
    process a virtual machine is created automatically (so it is always the first
    one) and that virtual machine is given special access to directly manipulate the
    console. So, what you see once it turns on is the console of the virtual machine
    itself as the hypervisor does not have one besides that.
  prefs: []
  type: TYPE_NORMAL
- en: You *can* even choose between different operating systems to use in the management
    virtual machine! In practice, however, Xen is always used with Linux as its control
    environment. Other operating systems are mostly theoretical. This exposure makes
    hidden classic systems, like Hyper-V, all the more confusing because in Xen it
    is so obvious how it all works.
  prefs: []
  type: TYPE_NORMAL
- en: Because Xen and Linux go together so tightly, it can be valuable for a Linux
    system administrator to have at least some knowledge of Xen, Xen management via
    Linux, and Xen architecture. This tight coupling does not ensure that systems
    and platform teams will become intertwined when using Xen, but it makes the likelihood
    higher.
  prefs: []
  type: TYPE_NORMAL
- en: KVM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we come to the **Kernel-based Virtual Machine** (**KVM**). KVM is special
    for a few reasons. First because it uniquely takes the approach of merging the
    hypervisor into the operating system itself. And second because it does so with
    Linux. Unlike other hypervisors where you have a clear separation between the
    platform administration who manages the hypervisor and the system administrator
    who manages the operating system level. Here, the two roles must be merged into
    one because the two components have been merged into one. You cannot separate
    the operating system and the hypervisor when using KVM. KVM bakes the hypervisor
    right into the Linux kernel. It is simply part of the kernel itself and always
    there.
  prefs: []
  type: TYPE_NORMAL
- en: This approach has clear benefits. It simplifies the entire system and provides
    most of the advantages of each different approach with relatively few caveats.
    There are caveats, of course, such as that there is a real risk of bloat in the
    hypervisor install which increases the potential attack surface. KVM's biggest
    benefit is probably that it leverages the ecosystem of Linux system administrators
    and existing knowledge so that some of the more complex aspects of managing a
    system that runs on bare metal such as filesystem and other storage architecture
    decisions, driver support, and hardware troubleshooting are all shared with Linux
    giving an enormous base platform and support network from which to begin.
  prefs: []
  type: TYPE_NORMAL
- en: Because of KVM's easy and well-known licensing (due to its inclusion in Linux),
    well known development potential, and broad availability of components it has
    become far and away the most popular means of building your own hypervisor platform
    when vendors want to create something of their own. Many large vendors in the
    cloud, hypervisor management, or hyperconvergence space have leveraged KVM as
    the base of their systems with customizations layered on top.
  prefs: []
  type: TYPE_NORMAL
- en: Is virtualization only for consolidation?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ask most people why you *bother* to virtualize and consistently the same answer
    is repeated: *Because it allows you to run several disparate workloads on a single
    physical device.* There is no doubt that consolidation is a massive benefit, when
    it applies, but stating this as the only, or even the key, benefit means we are
    missing the big picture.'
  prefs: []
  type: TYPE_NORMAL
- en: The enduring myth that we virtualize to save money through consolidation is
    one that it seems no one is going to be able to dispel. The nature of virtualization
    is simply too complex for the average person, even the average IT professional,
    and what it really provides remains broadly misunderstood. The real value of virtualization
    is in the abstraction layer that it creates which provides a hedge against the
    unknown - a way to make system deployments more flexible, and more reliable, while
    incurring less overall effort. Virtualization gives you more options for that
    unknown event happening sometime in the future that you cannot plan for.
  prefs: []
  type: TYPE_NORMAL
- en: A core challenge for virtualization is that it offers simply too many benefits
    that do not always relate to one another. Most people want a simple, stable answer
    and do not want to understand how exactly virtualization works and why adding
    a layer of additional code actually makes systems simpler and more reliable. It
    is all a bit too much. The reality is that virtualization has many benefits, each
    of which is typically enough to justify always using it, and essentially no caveats.
    Virtualization has no cost, essentially no performance overhead, does not add
    management complexity (it does, but only in some areas while reducing it in others
    resulting in an overall reduction.)
  prefs: []
  type: TYPE_NORMAL
- en: Inevitably people will ask if special case workloads exist for which virtualization
    is not ideal. Of course special cases exist. But the next response from every
    IT shop on earth is to proclaim that they and they alone are unique in their server
    needs and that they are the one solitary case where virtualization does not make
    sense - and then they reliably state a stock and absolutely ideal workload for
    standard virtualization that applies to nearly everyone and is as far from a special
    case or an exemption from best practices as can be. Trust me, you are not the
    exception to this rule. Virtualize every workload, every time. No exceptions.
  prefs: []
  type: TYPE_NORMAL
- en: Most examples that people give of why they avoid virtualization is normally
    examples of virtualization done wrong. Other, non-virtualization related mistakes,
    such as selecting a bad vendor, improperly sizing a server, or choosing a large
    overhead storage layer when something lean is needed could happen with or without
    virtualization. Everyone from the system administrator to the platform team to
    the hardware purchasers still have to do the same quality job that they would
    without virtualization. Virtualization is not a panacea but failing to be the
    silver bullet that removes the need to do our jobs well in no way excuses not
    using it every time. That is just bad logic.
  prefs: []
  type: TYPE_NORMAL
- en: Now we should have a good understanding of what virtualization really is, instead
    of simply a passing knowledge of its utility, and know why we use it for all production
    workloads. Virtualization should become second nature very quickly whether you
    are working in your home lab or running a giant production environment. Make it
    a foregone conclusion that you will virtualize and only worry about little details
    like storage provisioning and hypervisor selection or management tools. Next we
    look at virtualizations alternative and close cousin, containerization.
  prefs: []
  type: TYPE_NORMAL
- en: Containerization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some people consider containers to be a form of virtualization, sometimes called
    Type-C virtualization or OS-level virtualization. In recent years, containers
    have taken on a life of their own and very specific container use cases have become
    such buzz-worthy topics that containers as a general concept have been all but
    lost. Containers, however, represent an extremely useful form of (or alternative
    to) traditional virtualization.
  prefs: []
  type: TYPE_NORMAL
- en: Container-based virtualization varies from traditional virtualization in that
    in traditional virtualization every aspect of system hardware is replicated in
    software by the hypervisor and exists uniquely to every instance or virtual machine
    (often called a Virtual Environment (VE) when talking about containers) running
    on top of it. There is nothing shared between the virtual machines and by definition
    any operating system that supports the hardware virtualized can run on it exactly
    as if it was running on bare metal.
  prefs: []
  type: TYPE_NORMAL
- en: Container-based virtualization does not use a hypervisor at all, but rather
    is built from software that heavily isolates system resources in an operating
    system allowing individual virtual machines to be installed and operate as if
    they are fully unique instances, but behind the scenes all virtual machines running
    as containers share the host's kernel instance. Because of this, the ability to
    install any arbitrary operating system is limited as only operating systems capable
    of sharing the same kernel can be installed on a single platform.
  prefs: []
  type: TYPE_NORMAL
- en: Because there is no hypervisor and only a single kernel shared between all systems,
    there is nearly zero overhead in most container systems making it perfect for
    many highly demanding tasks. Containers are especially popular in Linux where
    many container options exist today. The almost total lack of system overhead in
    container systems used to be a key feature of the approach, but as systems have
    moved from resource tight to having a surplus of power in many cases, the value
    of squeezing every last drop out of hardware has begun to wane in comparison to
    the greater flexibility and isolation of full virtualization. Because of this,
    what sounds like the greatest virtualization option ever is often overlooked or
    even forgotten about!
  prefs: []
  type: TYPE_NORMAL
- en: Containers do not require any special hardware support and are implemented completely
    in software allowing them to exist on a broader variety of platforms (it is easy
    to implement on old 32bit Intel hardware or a Raspberry Pi, for example) at lower
    cost. This made them important in the era before hardware acceleration was broadly
    available for full virtualization technologies.
  prefs: []
  type: TYPE_NORMAL
- en: In the Linux world, which is what we care about, we can run many disparate Linux-based
    operating systems on a single container host because only the kernel needs to
    be shared. So, running virtual machines of Ubuntu, Debian, Red Hat Enterprise
    Linux, SUSE Tumbleweed, and Alpine Linux all on a single container host is no
    problem at all.
  prefs: []
  type: TYPE_NORMAL
- en: Linux is blessed (and cursed) with a plethora of options for nearly every technology,
    and containerization is no exception. Multiple open source and commercial container
    products exist for Linux, but today the undisputed reigning champion is **LinuX
    Containers** (**LXC**). LXC is unique in that it is fully built into the Linux
    kernel so utilizing it is simply a matter of doing so, it does not require additional
    software or kernel modifications. If you are going to be implementing real containers
    on Linux, chances are it is going to be LXC. LXC is fully supported in nearly
    all Linux-based operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: A little history of containers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Full virtualization was introduced by IBM in the 1960s but proved to be complex
    and did not make it into general availability for mainstream servers for decades
    as high end hardware support and extensive special case software was necessary
    to make the magic happen until the very end of the 1990s.
  prefs: []
  type: TYPE_NORMAL
- en: Containers were first introduced in System 7 UNIX in 1979 using a mechanism
    called *chroot jails* which is rudimentary by today's standards, but is functionally
    pretty close to modern containers. In the UNIX world containers, of one type or
    another, have almost always been available. In 1999 what we might consider truly
    modern containers, starting with FreeBSD's Jails, were introduced and rapidly
    other UNIX platforms like Solaris with Zones and Linux with OpenVZ and then LXC
    began to emerge. By the mid-2000s containers were everywhere and quite popular
    before truly effective full virtualization had taken off.
  prefs: []
  type: TYPE_NORMAL
- en: Containers saw a Renaissance of sorts in 2013 with the introduction of Docker.
    Docker is not exactly a container, however, even though the term container has
    become more associated with Docker than with actual, true containers. Prior to
    Docker, containers were never really seen as being very sexy but rather basic
    process isolation workhorses doing a rudimentary security job for the operating
    system with the possible short lived exception of Solaris Zones which were heavily
    promoted for a short time in conjunction with the release of ZFS.
  prefs: []
  type: TYPE_NORMAL
- en: Today, because of the popularity of Docker and its association with containers,
    the majority of people (even including system administrators!) think of Docker
    when someone mentions containers rather than true containers which have been around
    for decades.
  prefs: []
  type: TYPE_NORMAL
- en: We cannot talk about containers without mentioning Docker. Docker, today, is
    the name most associated with containers, and for good reason. Docker originated
    as a set of extensions built on top of LXC to provide for extreme process isolation
    with a *packaged* library environment for said applications. While Docker uses
    containers, first LXC and now their own container library, it itself is an application
    isolation environment providing a more limited range of services than something
    like LXC will provide. With LXC, you deploy an operating system (sans kernel)
    and treat it all but identically to traditional full virtualization. With Docker
    you are deploying an application or service, not an operating system. The scope
    is different and so Docker really finds itself more applicable to [*Chapter 5*](B16600_05_Final_ASB_ePub.xhtml#_idTextAnchor128),
    *Patch Management Strategies*. Because Docker is an application layer containerization,
    it would most often be run on top of a virtual machine in either full virtualization
    or containerization to provide its underlying operating system component.
  prefs: []
  type: TYPE_NORMAL
- en: Containers, in general, represent a trusted, mature, and ultra-high performance
    virtualization option (or alternative.) While more limited in their capabilities
    (a Linux container host can only run Linux VEs, while FreeBSD VEs are not possible,
    for example) they are otherwise easier to maintain, faster, and generally more
    stable (there is simply less to go wrong.) They can be created faster, turned
    on or off faster, patched faster, are more flexible in their resource usage (they
    don't require the strict CPU and RAM assignments typically needed with full virtualization),
    need fewer skills, and have less overhead when running. The only significant caveats
    to containers are the inability to mix operating system workloads, or even kernel
    versions. If anything, that you do requires a specific kernel version (that is
    not uniform across the entire platform), or custom compilation of the kernel,
    a GUI, ISO or similar based full installs then containers simply are not flexible
    enough for you. But if you are dealing with a pure Linux environment where all
    workloads are Linux and can share the kernel, which is not that uncommon, then
    containers can be ideal for you.
  prefs: []
  type: TYPE_NORMAL
- en: Containers, at least thus far, are not able to leverage the graphical interface
    of the operating system and so are relegated to server duties where pure text
    based (aka TTY) interfaces are used. Thus, they are not an option for graphical
    terminal servers or **virtual desktop instance** (**VDI**) deployments. Those
    kinds of workloads still need full virtualization until someone builds a workaround
    for that. But as that is generally not a heavily desired workload to support,
    there is little chance that someone is going to invest heavily in tackling that
    already easy to solve problem just to be able to say that they did it with containers.
  prefs: []
  type: TYPE_NORMAL
- en: Through the use of containers, you allow the operating system itself to act
    as a hypervisor, but one that is still the operating system as well and can be
    managed using all of the normal Linux tools and techniques because the system
    is still Linux. There is no separate hypervisor to learn or maintain. In this
    way the ability to leverage existing Linux skills and tools is very good.
  prefs: []
  type: TYPE_NORMAL
- en: It has to be noted that, because KVM and containers both use a standard, bare-metal
    Linux installation as their base, and because both are baked directly into the
    stock vanilla Linux kernel, it is not only possible but actually not uncommon
    for systems to run both full virtualization and containerization on the same host
    at the same time with vanilla Linux workloads running in containers. This ensures
    the lower overhead and extra flexibility and non-Linux (primarily Windows) or
    custom-kernel Linux systems running in full virtual machines on KVM. There is
    no reason to have to choose only one approach or the other if a blend is better
    for you. Container technology has become extremely popular and important today,
    but the use of containers in their traditional sense has dwindled heavily. This
    is mostly because of deep misunderstandings of the terminology and technology,
    which has caused it to be ignored even when it is highly appropriate. As a Linux
    system administrator especially, it may be very beneficial to consider containers
    instead of traditional virtualization in your environments. Having containers
    as another tool in your proverbial toolbelt makes you more flexible and effective.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will apply what we have learned about virtualization and containers,
    and add management, to learn about cloud computing.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud and VPS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any discussion of virtualization today is inevitably going to lead us to cloud.
    Cloud has become, that hot decade-long buzz-worthy concept that everyone wants,
    most people use, and no one has a clue what it is, what it means, or why anyone
    uses it. Few technologies are more totally misunderstood, yet widely talked about,
    than cloud. So we have a lot to cover here, much of it clearly up misconceptions
    and the misuse of terms.
  prefs: []
  type: TYPE_NORMAL
- en: The bizarre confusion of Cloud
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is a rare combination of being vastly technical and non-applicable to normal
    business conversations while being constantly discussed as if it were a casual
    high level non-technical business decision at nearly all levels. Considering only
    a minuscule fraction of IT professionals have any serious grasp of what cloud
    is, and even fewer have a clear understanding of when to choose it, that the average
    non-technical mid-level manager will toss around the term as if they were discussing
    the price of postage stamps is mind-boggling. What do those people even think
    that they are discussing? No one truly knows. And I mean that, sincerely.
  prefs: []
  type: TYPE_NORMAL
- en: Ask a group of people who have been throwing about the term cloud. Separate
    them so that they are not stealing each other's answers. Now, ask them to describe
    what cloud means to them. Mostly you will get gibberish, obviously. When you drill
    down, however, you will get a variety of descriptions and answers that are nothing
    like one another, and yet people listening to others discussing cloud will generally
    state that they believe that all of those people were meaning the same thing and
    often that *one thing* is something none of them actually meant, let alone more
    than one of them. Truly cloud means something different, random, and meaningless
    to nearly every human being with no rhyme or reason to it.
  prefs: []
  type: TYPE_NORMAL
- en: If cloud as a term had a musical equivalent, it would be Alanis Morisette's
    Ironic, the song where the only thing ironic about it is the title. The term cloud
    is the same, everyone uses it, no one knows what it means. Just like ironic.
  prefs: []
  type: TYPE_NORMAL
- en: For quite some years, cloud was commonly used to mean *hosted*. Simply replacing
    a long established, well known industry term with another one for no good reason.
    Of course, cloud means nothing of the sort. This horrific misinterpretation led
    to the meme of *there is no cloud, just someone else's computer*. Of course, even
    a passing knowledge of cloud would make one cringe to hear someone state something
    so profound while getting what cloud means so incredibly wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Today, you are more likely to hear cloud used to mean *built with HTML*, I kid
    you not. Or sometimes it means *platform independent*. Other times it means *subscription
    pricing*. You name it, and cloud has been used to refer to it. The only thing
    you can be confident in is that no one, ever, actually means cloud. It could mean
    almost anything else, but it never means what the term actually is meant to refer
    to.
  prefs: []
  type: TYPE_NORMAL
- en: The only benefit to the mass hysteria around cloud definitions is that no alternative
    definition is used commonly enough to rise and overtake true cloud. The problem,
    however, is so bad that there is no reasonable way for you, as an IT professional,
    to use the term *cloud* with anyone except a truly well read and trusted technical
    colleague that you know actually knows what it means.
  prefs: []
  type: TYPE_NORMAL
- en: What is truly amazing is that the use of *cloud* has replaced terms like *synergy*
    as the default joke in business – that is, as a term only used by those who are
    truly lost. It is such common knowledge that *cloud* is complex and completely
    misunderstood that you can never use it to communicate an idea. It has become
    a standard example of a *marker* in the language for someone who is just spouting
    off management-speak without having any idea what they are saying and not realizing
    that everyone else is silently laughing at their ineptitude, and yet you hear
    it repeated almost constantly! No matter how much everyone knows that they are
    misusing it, somehow it remains addictive and in constant use.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important takeaways from this seeming rant (and rant it is)
    is that you cannot use the term cloud to any but the most elite professionals
    and you cannot explain cloud to any but well educated IT professionals. Avoid
    using the term because, no matter how much you think that you can use it in the
    same mistaken way that everyone else is using it, you cannot. There is no way
    to use cloud in a way that can be understood because everyone believes that they
    know what it means, even though they all think that it is something unique.
  prefs: []
  type: TYPE_NORMAL
- en: When you absolutely must refer to cloud for some reason, use more complete terms
    like *cloud computing* or *cloud architecture* to clarify that you actually mean
    cloud and not just throwing out a word for the listener to interpret at will.
  prefs: []
  type: TYPE_NORMAL
- en: In many ways, it is almost easier to describe cloud by what it is not, rather
    than what it is, because everyone thinks that it is one thing or another. Cloud
    has no association with hosting, none with the web, not even any with the Internet
    (the thing sometimes referred to as THE cloud, as opposed to A cloud.) We cannot
    here go into all details explaining every possible aspect of cloud computing,
    nor would much of it be applicable as the majority of cloud is not related to
    systems, and therefore not related to Linux. But we should address, to a small
    degree, what it means to the Linux system administrator, when it is applicable,
    and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: First, we must start with the **NIST** (the **National Institute of Standards
    and Technology** in the USA) cloud abstract definition which works from Amazon's
    original definition. Always keep in mind that cloud computing is a real, strict,
    technical term created by Amazon for a real-world architecture and therefore has
    a strict, non-fungible definition and no amount of misuse or misunderstanding
    or attempts to co-opt its use by others changes that it means an extremely specific
    thing. It is common for those who do not understand cloud to argue that it is
    a loose term that can mean what you want it to mean, but it is not. That is simply
    not the case, it is not a random English language word making its way into the
    lexicon organically, it was defined carefully within the industry before first
    use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NIST definition is as follows: *Cloud computing is a model for enabling
    ubiquitous, convenient, on-demand network access to a shared pool of configurable
    computing resources (e.g., networks, servers, storage, applications, and services)
    that can be rapidly provisioned and released with minimal management effort or
    service provider interaction. This cloud model is composed of five essential characteristics,
    three service models, and four deployment models.*'
  prefs: []
  type: TYPE_NORMAL
- en: The most important parts of this definition to us, as system administrators,
    are the parts that include *pool of resources*, with *rapid provisioning and release*.
    So shared resources (meaning servers, CPUs, RAM, storage, networking) that we
    can create and destroy access to quickly. While cloud certainly means more than
    that, those are the basics. If you immediately thought to yourself *that sounds
    a lot like what virtualization already does*, you are correct, there is an extreme
    degree of overlap, and virtualization is the key building block of cloud computing
    (both full virtualization and/or containers.) If you thought to yourself *wait,
    pooled resources that I can build AND destroy rapidly - those do not sound like
    useful characteristics to me or any environment I have worked in previously*,
    you are also correct. Cloud computing is not logically applicable to traditional
    workloads or environments, it is designed around extremely specific needs of purpose-built
    application architectures that few businesses are prepared to leverage on any
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: What makes the use of *cloud* more confusing is that many vendors (and quite
    logically at that) use cloud themselves as a component of their products. So,
    when you ask your vendor if a product is cloud-based, they might be answering
    if they are providing you cloud itself as a product, or they might be answering
    if they use cloud in the building of the tool somewhere under the hood. The two
    are both applicable to how most people ask the question, and since no one knows
    what cloud really is, no one is sure what you are really asking or want to know.
    Let me give a contrived, but reasonable, example.
  prefs: []
  type: TYPE_NORMAL
- en: If I were to purchase a legacy application, say a client-server application
    that uses MS SQL Server and an old Delphi (Objective Pascal) front end and then
    use a true cloud product to create a virtual machine and deploy the server-side
    components so that we can truly say that we built the solution on a cloud. Yet
    the resultant product is not cloud computing, in any sense. Just because one piece
    of an architecture is built on cloud does not imply that the final product is
    cloud. Cloud is a layer in the stack.
  prefs: []
  type: TYPE_NORMAL
- en: For us, as system administrators, we are concerned with the type of cloud knows
    as **Infrastructure as a Service** (**IaaS**). That's a fancy way of saying cloud-based
    virtual machines. Other types of cloud, like **Platform as a Service** (**PaaS**)
    and **Software as a Service** (**SaaS**), are very important in the cloud space
    but exist only when the system administrator is *somewhere else*. If we are the
    system administrator for PaaS or SaaS then, to us, cloud is the workload, and
    it is not cloud to us. If we are not the system administrator for a PaaS or SaaS
    system, then it is of no concern to us as system administrators to talk about
    those systems as they do not apply to our role.
  prefs: []
  type: TYPE_NORMAL
- en: From our systems perspective, cloud is almost like an advanced, flexible virtualization
    layer. Of course, like virtualization, we may find ourselves tasked with being
    the ones to implement the cloud platform. That is a completely different animal
    and worthy of a book (or two) on its own. But in practical terms system administrators
    may consume platform resources from a hypervisor, a container engine, or either
    one orchestrated through a cloud interface. To us it is all the same - a mechanism
    on which we deploy an operating system. So, from a pure system administrator perspective,
    think of cloud computing no differently than any other virtualization because
    it is exactly the same. The only difference is how it is managed and handed off
    to us.
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, we likely have to be heavily involved in decision making
    around the recommendation for the use of cloud versus other paths to acquire virtualization.
    Like any business decision, this simply comes down to evaluating the performance
    and features offered at a price point and comparing to the performance and features
    at the same price point with other options. It is that simple. But, with cloud,
    due to all the reasons that we mentioned before, we are often fighting against
    a mountain of misinformation and a belief in magic. So, we need to talk a little
    about cloud in a way that we should not have to simply because these misunderstandings
    are so common and deeply rooted.
  prefs: []
  type: TYPE_NORMAL
- en: First, there is a belief that cloud computing is cheap. And while in special
    cases cloud computing will potentially save a lot of money, this is rarely the
    case. Cloud computing is typically an extreme price premium product chosen because
    it allows a great degree of flexibility so that less is needed to be purchased
    overall to meet the same needs. Cloud computing is very expensive to provide and
    so vendors are forced to charge more than for other architectures in order to
    provide it to customers (keep in mind that the *vendor* might be in your internal
    cloud department, nothing about cloud implies an external vendor.)
  prefs: []
  type: TYPE_NORMAL
- en: Horizontally scalable elastic workloads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attempting to describe what workloads cloud computing was built to address can
    be a challenge for those not already familiar with certain types of application
    architecture. We must take an aside and dive into some application concepts here
    to really understand how we related as the systems team and to see why different
    approaches play such a large role in our platform decision making at this level.
  prefs: []
  type: TYPE_NORMAL
- en: In a traditional application design the expectation is that we will run the
    entire application on just one or maybe just a few operating system instances.
    Typically, one instance would be used as the database server and one as the application
    server. More roles might exist, and you could have a redundant database server
    or similar, but essentially the number of instances usable was quite limited and
    static. Once deployed, the number of instances would not change. In many cases
    the entire application would exist on a single operating system instance.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling a traditional application mostly focuses on increasing the power of
    a single operating instance. This might be done through some combination of faster
    CPUs, more CPUs, more CPU cores, more memory, more storage, or faster storage.
    Or as we would typically say, if you need your server to do more, you need a bigger
    server. This kind of performance improvement can go a really long way as top end
    servers are very powerful, and few companies need to run any workloads that exceed
    the performance capabilities of a single large server. This style of scaling is
    called vertical scaling, meaning that we improve the performance of the single
    thread or server *within the box*. This kind of scaling is by far the easiest
    to do and works for any kind of application no matter how it is designed (this
    is how you improve video game performance or any desktop workload).
  prefs: []
  type: TYPE_NORMAL
- en: For most people, workloads designed for vertical scaling are the only types
    of workloads that they know. Of course, for end users working on desktops, everything
    is vertical. Even system administrators almost exclusively have to oversee applications
    that are built for this kind of scaling only. Nearly all deploy in house applications
    assume that this is how you will scale and only recently do many developers know
    alternatives well and still many (potentially most) still do not, even though
    those that do are the more prominent in media.
  prefs: []
  type: TYPE_NORMAL
- en: The alternative approach is to design applications that allow for the application
    to scale by adding additional, isolated operating instances. For example, running
    multiple database server instances (likely in a cluster) not just for resilience,
    but also performance. Running multiple application servers with the ability to
    simply add more operating system instances running the application while keeping
    each individual instance small. In a traditional application architecture, we
    might require a single application server with four high performance CPUs and
    1TB of RAM to handle our application workload. A horizontally scalable application
    might use sixteen smaller servers each with 64GB of RAM and a smaller CPU to handle
    the same load. Traditionally we would say that our systems *scaled up*, but in
    adding more instances we say that they *scale out*. Now, of course, you can always
    scale both *up and out* which would mean increasing the resources of each individual
    instance while also increasing the number of instances.
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, few applications that we work with in the real world as
    end users or as system administrators are designed for or could leverage horizontal
    or *scale out* platforms effectively, if at all. It requires that the application
    architectures, analysts, and developers plan for this style of deployment from
    the very beginning. And no amount of planning or desire makes every workload capable
    of scaling in this manner.
  prefs: []
  type: TYPE_NORMAL
- en: Some applications like common web-based business processes, most websites, email
    systems, and so forth are very conducive to this type of design and you can easily
    find or make these kinds of applications to take advantage of these resources.
    Other applications like financial processing or inventory control systems may
    struggle with the design limitations and take far more work to be able to work
    in this way, if at all.
  prefs: []
  type: TYPE_NORMAL
- en: Just because a workload is designed by the development team to be horizontally
    scalable does not mean that the workload itself will, however. This is easiest
    with a simple example. You create a website that helps people choose a healthy
    breakfast. You market to the United States. From 6am Eastern until about 2PM eastern
    (when California is wrapping up breakfast) you are really busy, but outside of
    those hours your website is really slow. But another website helps people choose
    food for any meal and is marketed worldwide. This second site never gets quite
    as busy as the first but stays roughly as busy all day long. The first site can
    leverage scalability, the second site cannot because its resource needed never
    really change.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key advantages to horizontally scalable workloads are that they can be
    grown rapidly. Adding an additional operating system instance (or an additional
    one hundred!) is easy and non-disruptive. Adding more CPU or RAM to your existing
    server, is hard and slow by comparison. The next step of horizontal scaling is
    making it elastic. To be elastic, your system does not only have to scale out
    quickly but allow you to also scale back in quickly: that is to spin down and
    destroy unneeded operating system instances when capacity has changed. This is
    the unique proposition of cloud computing, to provide capacity on demand for elastic,
    horizontally scalable workloads so that you can use resources only when needed
    and stop using them when you do not.'
  prefs: []
  type: TYPE_NORMAL
- en: Vertically scaled resources are far less expensive to provide than horizontally
    scaled ones. You can test this by trying to assemble several computers with roughly
    the same specifications. With only rare exceptions, it is a fraction of the cost
    to build a single large server than several smaller ones using real-world economics.
    A single system requires only a single operating system and application instance,
    but multiple systems require the overhead of the same operating system and applications
    loaded into memory in each case wasted many resources there, as well. It just
    takes less management power to oversee something that is *faster*, rather than
    many slower tasks. It is not unlike managing humans. It takes less overhead to
    manage one really fast, efficient employee than it does to manage several slower
    employees trying to coordinate doing the same work that the faster one was doing.
  prefs: []
  type: TYPE_NORMAL
- en: So a horizontally scaled system, in order to make sense to choose, has to be
    able to leverage both scaling out, as well as in, have a workload use case that
    actually leverages this, and that does so to an extent large enough to overcome
    the lower cost, lower overhead, and great simplicity of traditional designs. Unless
    your workload meets all of those requirements, cloud computing should not be a
    consideration for you at all. It simply does not apply. And while contrary to
    how the media and trend-happy IT professionals want to portray cloud, it is only
    a small percentage of workloads that can effectively leverage cloud and only a
    small percentage of businesses have those rare workloads at all.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we are talking IaaS aspect of cloud. Other cloud aspects where only
    the application portion is exposed to the business will often be cloud based.
    But this is essentially unrelated and certainly a decision process totally different
    from anything we would be looking at in a tomb such as this.
  prefs: []
  type: TYPE_NORMAL
- en: Second, there is a belief that cloud computing is reliable. Absolutely nothing
    in cloud definitions or designs implies reliability in any way. In fact, this
    runs completely contrary to all standard cloud thinking. Cloud computing, because
    it is useful exclusively with scale out design, is built on the assumption that
    any redundancy or reliability is built into the application itself as scale out
    implies - as you have to have this inside the application itself in order for
    scale out to work properly. So including any redundancy at the system or platform
    level would be nonsensical and counterproductive. A basic understanding of cloud
    computing should, with any thought, make us surprised if anyone expected redundancy
    beyond the minimum at this level. In the real world, cloud computing resources
    tend to be far more fragile than traditional server resources for exactly this
    reason. Cloud computing assumes that either reliability is of trivial importance
    or that it is provided elsewhere in the stack. Cloud computing is just a building
    block of the resulting system, it is not in any way a complete solution by itself.
    Of course, theoretically, a high availability cloud provider could arise, but
    their cost and performance caveats would make it hard to compete in a marketplace
    driven almost entirely by price.
  prefs: []
  type: TYPE_NORMAL
- en: Third, there is a belief that cloud computing is broadly applicable, that every
    company should be using it, and that it is replacing all other architectures.
    This is not true at all. Cloud has been around now for more than fifteen years
    (at the time of writing) and it made its inroads quite quickly towards the beginning
    of that cycle. Today, cloud computing is mature and well known. Companies and
    workloads that are going to move to cloud (or design for it) have largely already
    done so, and new workloads are created on cloud at a roughly constant rate. The
    industry saturation rate for cloud computing has been more or less achieved. Some
    new workloads will go there as older ones or anti-cloud holdouts retire or give
    in to other pressures. Some will come back as overzealous cloud fanboys and buzz-word
    driven managers learn their lessons of having gone to cloud without any understanding
    or planning. Sending standard workloads to cloud computing without redesign is
    typically costly and risky. But by and large cloud computing has already settled
    into a known saturation rate and the computing world is as it will be until another
    exciting paradigm shift occurs. Basically, what we see today in advanced bespoke
    internal software and grand scale multi-customer software is ideal for the cloud
    paradigm, and traditional workloads for single customers remain the most beneficial
    on traditional paradigms. This is all as originally predicted when cloud computing
    was first announced long ago.
  prefs: []
  type: TYPE_NORMAL
- en: Using cloud does not require any specific skills or training, as many in the
    industry would like us to believe in order to sell certifications and training
    classes. In fact, just knowing what cloud truly is often enough to enable you
    to utilize it effectively. That said, individual cloud vendor platforms (such
    as **Amazon**'s **AWS** or **Microsoft**'s **Azure**) are so large and convoluted
    that there can be real value to getting vendor certifications and training to
    understand how to work with their product interfaces. But to be clear, the value
    in the training is learning how to work with the vendor in question, not in learning
    about cloud.
  prefs: []
  type: TYPE_NORMAL
- en: That does not change the fact that most organizations seeking to get significant
    value out of cloud computing will most likely need to do so with deep vendor integrations
    that will almost certainly require an investment in specific vendor product knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud is an amazing set of technologies that serves an incredibly important
    purpose. When your workload is right for cloud computing, nothing else can come
    close to it. Whether you build your own private cloud or use a public shared one,
    whether you host your cloud in house or let a hosting firm handle the data center
    components for you, cloud might be the right technology for some of your workloads.
    With your understanding here you should be able to evaluate your needs to know
    if cloud is likely to play any reasonable role, and be able to look at real work
    vendors and costs and make solid, math-based valuations of cloud in comparison
    to other options.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know cloud computing, we can step back and look at the older concept
    of virtual private servers and see why they are so closely tied with, but not
    actually related to, cloud computing today.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual Private Servers (VPS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to and, on the Venn Diagram of things, nearly overlapping with IaaS
    cloud computing is the modern concept of virtual private servers or VPS. VPS actually
    predates cloud and comes from simpler virtualization (or containerization) allowing
    a vendor (which could be an internal department, of course) to carve out single
    virtual machines for customers from a larger, shared environment. Instead of needing
    to provide an entire server of their own, customers need only buy a small slice
    or set of slices of the vendor's server(s) to use for their needs.
  prefs: []
  type: TYPE_NORMAL
- en: As I mentioned, this sounds very similar to IaaS cloud that we just described,
    and certainly it is. So much so, that most people using IaaS cloud actually use
    a VPS aspect of it without realizing so. The idea behind VPS is to allow companies
    to purchase server-class resources at a fraction of the scale typically required
    to a single physical server. If you think back to our discussion on virtualization
    and how by using a hypervisor we might be able to take a single physical server
    and, for example, create one hundred virtual machines that run on top of it, each
    with their own operating system, then we could sell those resources to one hundred
    separate customers, each of which could run their own small server inside its
    own secure space. This allows small companies, or companies with small needs,
    to buy enterprise level datacenter and server hardware capacity and prices within
    any realistic budget.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we go further, we need to do a quick breakdown and comparison of VPS
    against IaaS cloud to see why VPS is so commonly confused with cloud computing
    and why they often compete:'
  prefs: []
  type: TYPE_NORMAL
- en: '**First, the goals**: An IaaS Cloud''s goal is to provide rapid creation and
    destruction of resources on demand via automation - primarily used by the largest
    organization or workloads. The goal behind VPS is to carve up traditional server
    resources in such a way that they can be affordable to be used by small organizations
    and/or workloads. So, one goes after the biggest scale and the most complexity,
    the other after the smallest scale and least complexity. One expects custom engineering
    effort on both the application and infrastructure team''s sides where the other
    expects traditional applications and no special knowledge or accommodation from
    any team.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Second, the interface**: In cloud computing the expectation and purpose is
    for systems to be self-provisioning (and self-destroying.) Cloud is not designed
    for humans to have to interact manually to request resources, nor to configure
    them, nor to decide when more (or less) are needed, nor to destroy them when done.
    So, cloud''s focus is on APIs to allow software to handle provisioning. VPS is
    meant to work just like any normal virtual machine with a human initiating the
    build, installing the operating system, maybe configuring the operating system,
    and turning the VM off, and then deleting it when no longer needed. It is standard
    for cloud products to not offer any interaction directly with the virtualized
    hardware such as access to a console so any system requiring console level interaction
    (such as a GUI) are impossible. To qualify as a VPS console and GUI access are
    required to completely mimic a hardware device in a standard way. If you can use
    a normal server, you can use a VPS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Third, the provisioning**: Cloud assumes a need for rapid provisioning. Of
    course, rapid is a relative term. But it is just expected that in a cloud ecosystem
    that systems must be able to go from first request to fully functional in minutes,
    and sometimes seconds. In the VPS world, while we always want everything available
    as quickly as possible, having to wait a few minutes before being given the access
    to begin a manual operating system install that could take potentially tens of
    minutes is common. We assume that a cloud instance will be created via software,
    but a VPS instance we assume will be created manually by a human.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fourth, the billing**: Because the value of cloud computing is assumed to
    come from its ability to be created and destroyed rapidly in order to keep costs
    managed it follows that billing must be granular to accomplish this. To this end
    billing it generally handled in increments of minutes or possibly hours, or in
    other extremely short measurements like processor cycles. VPS will sometimes charge
    in these short increments but may easily use longer intervals such as daily or
    monthly as it is not a rapid create and destroy intended service. (We can say
    that cloud leans towards stateless and VPS leans towards stateful.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What often makes VPS and IaaS Cloud harder to distinguish is that today, VPS
    providers almost always use IaaS Cloud as their own mechanism for provisioning
    the VPS under the hood, and most IaaS Cloud providers have opted to offer VPS
    additionally. This was bound to happen for two reasons. First, VPS providers use
    cloud because it is a really logical way to build a VPS (if you think about the
    requirements that the VPS *vendor* would have, they would sound a lot like what
    cloud is meant to do) and because by being built on top of cloud computing, you
    can advertise the VPS as being cloud in a sense and be a sort of *simple interface
    to cloud resources*. It makes sense for cloud providers to offer VPS because the
    majority of customers who look for cloud have no idea what it is and only use
    it for political, not business or technical reasons, so offering something simple
    that allows them to purchase from you and use your resources (because cloud is
    so hard and complex) allows you to capture the majority of revenue.
  prefs: []
  type: TYPE_NORMAL
- en: Vendors like Amazon used to offer no VPS services and using their resources
    if you were not truly in need of cloud was difficult, at best. To address this,
    Amazon added LightSail as a VPS product layered on top of their cloud product.
  prefs: []
  type: TYPE_NORMAL
- en: Other cloud providers, such as *Digital Ocean*, *Linode*, and *Vultr* use VPS
    as their primary product offering and focus on it almost entirely while keeping
    their cloud interface quietly to the side so that customers truly looking for
    cloud can find it, but those seeking cloud when they intended to use VPS will
    be able to get what they need right away.
  prefs: []
  type: TYPE_NORMAL
- en: VPS is one of the most popular and effective ways for real world companies to
    run workloads, especially smaller companies, but companies of any size can leverage
    them. Cloud is effective but primarily for special case workloads. The majority
    of companies talking about already leveraging cloud are actually using VPS and
    not even aware that they missed cloud computing entirely.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth noting that when we talk about rates of cloud adoption we have
    a fundamental problem: no one knows what cloud is, including people who think
    that they are or are not using it currently! Vendors like Amazon can tell you
    how many customers that they have, but they cannot tell you if their customers
    are using their products as cloud or just using cloud in some other way. In a
    survey about cloud adoption you have zero reasonable chance that the person being
    asked about their adoption, the person doing the asking, and the person reading
    about the adoption rates all understand enough about cloud to answer or ask meaningfully
    and, in reality, generally none of them know at all what is being asked. So any
    information about cloud adoption rates border on being totally meaningless. There
    is no honest mechanism by which any person or organization could possibly know
    what the cloud ecosystem really looks like. You would get just as meaningful data
    if a group of squirrels surveyed a bunch of hamsters about astrophysics and then
    handed the results to a bunch of hyper puppies to interpret. People love reports
    and data and rarely care if the survey in question was real in any way.'
  prefs: []
  type: TYPE_NORMAL
- en: You should, at this point, feel both overwhelmed and depressed about the state
    of cloud understanding within both IT and business, but you as the Linux system
    administrator should now be prepared to explain it, evaluate it, understand what
    is built upon it, and know when and how to choose to use it for your own workloads
    and when to look at VPS instead.
  prefs: []
  type: TYPE_NORMAL
- en: On premises, hosted, and hybrid hosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have talked about so many aspects of the underlying components that
    are used to provide us with a platform on which to deploy an operating system,
    we can finally talk about where those systems should exist!
  prefs: []
  type: TYPE_NORMAL
- en: This is, at least, the simplest of all our topics. Physical location is easy
    to explain, even if many businesses get confused about it in practice. Conceptually
    we really only think about two locations for a workload and that is as being either
    on premises or off premises. This can be a little convoluted, though, as companies
    own multiple locations so what is off premises to one site might be see as on
    premises to another. But we generally consider on premises to be all of a company's
    owned sites and off premises being any sites that are operated by a third party.
    Because of this we generally refer to off premises physicality as being hosted,
    as physical systems are being hosted on our behalf. However, there are reasons
    why this can prove to be very misleading.
  prefs: []
  type: TYPE_NORMAL
- en: Most people assume that when a system is kept on premises that that also implies
    that it will be being operated by an internal team. This is most often true, but
    having on premises systems managed by third party teams is not unheard of, especially
    in very high performance or high security environments. For example, if you required
    Amazon's specific range of cloud computing products, but could not allows for
    any off-premises hosting, you can have Amazon operate an AWS cloud instance on
    your own premises. This is anything but low cost or simple and requires housing
    at minimum a small, self-contained data center and all of its associated staff!
  prefs: []
  type: TYPE_NORMAL
- en: 'Hosting gets more complicated in practice, but the core issue remains the same:
    the demarcation points. When we decide to start having our systems be hosted off
    premises the questions rapidly become about defining what portions of the hosting
    will be provided by the hosting provider and which by us.'
  prefs: []
  type: TYPE_NORMAL
- en: In its most extreme (and impractical sense) you could rent a house, office,
    or storage unit and provide your own rack, servers, Internet, cooling, power,
    and so forth as needed, but if we did this one would rightfully argue that we
    had basically made the site our own premises. Touche.
  prefs: []
  type: TYPE_NORMAL
- en: Classically it was assumed that nearly all workloads should be run on premises.
    This was for the very simple reason that early business networks had no Internet
    connectivity so hosting elsewhere was effectively impossible or at least impractical.
    Follow that in the early days of the Internet wide area network links were slow
    and unreliable keeping remote servers almost unusable. And software was built
    around LAN networking characteristics, unlike today when enterprise software of
    any quality assumes that it needs to perform adequately over a long distance connection,
    most likely on the Internet.
  prefs: []
  type: TYPE_NORMAL
- en: Because of these old assumptions, the tribal knowledge that servers need to
    be local to an office where people work has been passed down by rote generation
    to generation without many people evaluating it. This information went from generally
    true to seldom true pretty quickly during the early 2000s.
  prefs: []
  type: TYPE_NORMAL
- en: Today most workloads work effectively over the Internet and so can be located
    almost anywhere. Using some form of off-premises hosting or centralized hosting
    that is not based at any specific company location is now the norm rather than
    the exception.
  prefs: []
  type: TYPE_NORMAL
- en: 'In all on-premises and off-premises evaluations we have certain factors that
    are universal: who will access the data and from where, how does latency and bandwidth
    impact application performance, which people accessing the data have priority
    and what is the cost of performance issues at different locations.'
  prefs: []
  type: TYPE_NORMAL
- en: There is no hard and fast rules, we simply have to carefully consider as many
    factors as possible. On and Off Premises solutions are just locations and should
    be treated as such. The ability to use an enterprise datacenter off premises might
    be significant, especially if we do not have a real server room on premises. And
    disaster recovery might be better at an off premises location. But will the user
    experience be good enough if the server is far away? These questions are all situational
    and need to be answered not just about the status of the business infrastructure
    today but also for the near future.
  prefs: []
  type: TYPE_NORMAL
- en: Colocation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a site provides the real estate, cooling, power, Internet, racking, networking,
    and so on but we provide our own physical servers it is called a colocation facility.
    Colocation is one of the most popular and effective ways that we, as system administrators,
    can acquire enterprise class datacenter services outside of our own premises while
    retaining the flexibility to use any hardware that makes sense for our organization
    and its workloads. Colocation is effective for very small businesses up to the
    most absolutely massive. No company outgrows the value that colocation may bring,
    nor does any government. It is a strategy that lacks a *top end* size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Colocation is one of the most useful and effective forms of moving IT equipment
    off premises because it allows the IT department to retain full control of hardware
    purchasing and configuration not just for systems but for networking and appliances
    as well. Only non-IT functions necessary to support the technology hardware is
    provided. This allows the colocation facility to focus on a strong facilities
    management skillset and IT to retain literally all IT functions and flexibility:
    basically doing remotely with a third party what you would hope you would be doing
    internally with your own teams assuming that you had enough volume to do so. It
    is expected that a colocation provider will also have *remote hands* to assist
    with bench tasks when necessary, such as changing or verifying cabling, power
    cycling devices, and things of that nature.'
  prefs: []
  type: TYPE_NORMAL
- en: Flexibility is key with colocation. Whether it is because you want to custom
    build your own hardware, maintain legacy systems, use special case hardware (for
    example, IBM Power hardware), or what the freedom to do a lift and shift of an
    entire existing environment from on premises to the colocation facility you can
    do it all. Most colocation facilities allow for a range of scales as well, from
    housing a single 1U server on your behalf to fraction racks (tenth, quarter, half,
    full rack sizes are common) to providing cages that can house many racks to even
    renting entire floors of the datacenter!
  prefs: []
  type: TYPE_NORMAL
- en: The biggest challenges that colocation faces is that there is no effective way
    to go extremely small because the smallest size you can reasonable host is a single
    server. If your needs are smaller than this, then colocation will generally struggle
    to be cost effective for you. But do not simply reject colocation with an assumption
    of it being expensive. I run these calculations often for companies who were ignoring
    colocation as too costly for them only to find out that it would be less than
    half of the cost of their alternative propositions while having more flexibility
    for growth without additional expenditures. Most people assume that servers are
    more expensive to buy than they really are and that colocation costs are higher
    than they really are. Colocation costs are often inappropriately associated with
    legacy systems, as if only twenty year old equipment can go into a datacenter
    today, and decades old cost models are often envisioned. Twenty years ago servers
    were much more expensive than they are today and had noticeably shorter operational
    lives and datacenter space was more costly than today as well. Like everything
    in IT, cost over time have come down and for workloads of any size colocation
    tends to be much less costly than most alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: Colocation is just one approach to hosting systems off premises. Other approaches
    like public, hosted cloud and cloud-based VPS systems are standard alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest challenges around locality are really all associated with understanding
    current marketing pricing for different approaches and being able to evaluate
    the benefits and caveats of hosting equipment on premises or off premises, and
    if that equipment should be dedicated or shared. You should now be ready to make
    that evaluation and choose appropriately for your deployments. Next, we dig into
    the much more complicated topic of platform level system design architectures.
  prefs: []
  type: TYPE_NORMAL
- en: System Design Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the more challenging aspects of system administration is tackling the
    broad concept of system architecture. In some cases, we have it easy, our budget
    is so low or our needs so simplistic that we simply do not need to consider any
    but the most basic options. But for many systems, we have broader needs and a
    great number of factors to consider making system architecture potentially challenging
    in many ways.
  prefs: []
  type: TYPE_NORMAL
- en: We now understand platform concepts, locality, and the range of services normally
    associated with providing an environment onto which we can install an operating
    system. Now we have to begin describing how we can combine these concepts into
    real world, usable designs. Most of system design is just common sense and practicality.
    Remember nothing should feel like magic or a black box and if we get services
    from a vendor, they are using the same potential range of technology and options
    that we are.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to talk about risk and availability in the next section, but before
    we do, we should mention here to make it more clear why system designs rely on
    this data, that any redundancy (whether for performance or risk reduction) that
    we add to our overall system can be done at different layers. Principally in the
    system layer (where we are looking now) or at the application layer (which we
    do not control.) So even in the most demanding of high availability workload situations,
    we may have no need for a robust underlying system design and have to consider
    this when thinking about design options.
  prefs: []
  type: TYPE_NORMAL
- en: I am going to break down common design approaches so that we can understand
    how they best apply to different scenarios. These are physical system architectures
    that include both the storage and the compute. It is assumed that some sort of
    abstraction, meaning virtualization and/or containerization, is used in every
    case and so will not be mentioned case by case.
  prefs: []
  type: TYPE_NORMAL
- en: Standalone server, aka the snowflake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You really cannot get more basic than this design. The simple server, the baseline
    against which all else must be measured. The self-contained, all in one server
    with storage and compute in a single chassis. No external dependencies, no clustering,
    no redundancy (external to the single box.) Of course, we assume standard hardware
    practices are followed such as minimums like RAID and dual, hot swappable power
    supplies.
  prefs: []
  type: TYPE_NORMAL
- en: Today, many IT people are going to frown on attempting to use a stand alone
    server, but they should not. The classic single server is a powerful, effective
    design appropriate for the majority of workloads. This should be the *go to* design,
    the default starting point, unless you have a specific need to do something else.
  prefs: []
  type: TYPE_NORMAL
- en: Because of its simplicity, single server designs have the best cost ratios to
    all other factors, are more robust than they appear, and have excellent performance.
    Many people think of servers as being rather fragile creatures, and years ago,
    they were; but that impression stems from servers of the 1980s and early 1990s.
    By the late 1990s server technology was becoming mature and reliable and today
    failure rates on well maintained servers are extremely low. The idea that a single
    server is a high risk is an antiquated one, but like many things in our industry
    old feelings often linger and get taught mentor to student without reevaluation
    to see if factors remain true (and in many cases without initial evaluation to
    know if they were ever true.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Single servers benefit from having many fewer components and lowered complexity
    compared to any other approach and with fewer parts to fail and fewer configurations
    to get wrong it is that much easier to make really reliable: hence why we sometimes
    refer to this as the *brick* approach. Bricks are simple but effective, while
    they can fail, they rarely do. Emotionally it is common to associate complexity
    with robustness, but in practice simplicity is far more desirable. Complexity
    is its own enemy and an unnecessarily complex system takes on unnecessary risk
    (and cost.)'
  prefs: []
  type: TYPE_NORMAL
- en: While hard to measure for many, many reasons, we generally assume that a properly
    maintained and supported stand alone server can delivery average availability
    rates close to five nines (that is around one hour of downtime per year.) It is
    a rare workload in any business that cannot function well with that kind of downtime.
    What is difficult in stand alone servers is that this is an average only (of course)
    and we will have isolated systems experiencing much higher downtime, and others
    experiencing none.
  prefs: []
  type: TYPE_NORMAL
- en: Simplicity also brings us performance. By having fewer components in the path
    single servers have excellent performance. Attempting to gain total performance
    greater than what can be achieved using a single server is difficult. Single servers
    give us the lowest latency and nearly the best throughput of any approach.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to single server systems, use math and logic to explain why it
    may or may not make sense. Many people rely on emotions when it comes to system
    architecture and this should never happen. Our concerns with system design are
    about performance and availability and these are purely mathematical components.
    Emotions have no role here and are, in fact, our enemy (as they are the enemy
    of any business process.)
  prefs: []
  type: TYPE_NORMAL
- en: Single servers can scale far larger than most people assume. I often hear arguments
    that they cannot look at single servers because their needs are *so large*, but
    then deploy systems only a tiny fraction of the standard scale, let alone the
    maximum scale, that a single server can achieve. Remember that vertical scaling
    is highly effective compared to horizontal, and generally cost effective as well.
    The biggest single server systems can support hundreds of the most powerful CPUs,
    and many terabytes (or more) of RAM. The challenge is not finding a single server
    that is large enough for a task, but rather finding any workload that can leverage
    so much power usefully in a single location!
  prefs: []
  type: TYPE_NORMAL
- en: A big advantage to standalone servers is that each physical device can be scaled
    and custom designed to address the needs of its workload(s). So different servers
    can use different CPU to RAM to storage ratios, different servers can use different
    CPU generations or architectures, one system might use large hard drives while
    another uses small but screaming fast solid-state storage. Tuning is very easy.
  prefs: []
  type: TYPE_NORMAL
- en: Simple does not necessarily mean simple
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having a standalone server does not mean that we give up all of the options
    and flexibility that we might believe that we need from more complex designs.
    It simply means that we have to think about them differently. Many of the concerns
    that one may have about standalone servers likely stems from a pre-virtualization
    world with relatively slow networks. Today we have virtualization, fast storage,
    and fast networks and these can move the goal line by a bit.
  prefs: []
  type: TYPE_NORMAL
- en: We refer to servers as being standalone in reference to their architecture,
    everything is self contained in a single piece of hardware. This does not mean
    that we do not have (or cannot have) more than one server. On the contrary, giant
    Fortune 100 firms will often have thousands of standalone servers. What makes
    then standalone is that they have no dependencies on each other. The complete
    failure (or theft) of one does not negatively impact another.
  prefs: []
  type: TYPE_NORMAL
- en: A tiny organization might choose to rely on a single standalone server for their
    entire business and depend completely on backups and the ability to restore to
    replacement hardware should disaster strike. This is a totally valid approach
    and quite common.
  prefs: []
  type: TYPE_NORMAL
- en: If your organization is larger, or workloads require more immediate protection
    against loss of availability, then it is standard to run multiple standard servers.
    This spreads load between physical hardware devices and, because of virtualization,
    provides an opportunity for there to be natural ways of mitigating hardware failure
    by rapidly rebuilding lost workloads on other hardware. If deployment density
    is too high, spare hardware is an option as well. With modern storage, networks,
    system management, and backup techniques restoring many workloads can be done
    in as little as minutes allowing even complete hardware failures to often carry
    only the tiniest of system impacts. In fact, keeping backups stored on other standalone
    nodes can allow for essentially instant recovery of lost systems while maintaining
    strong decoupling.
  prefs: []
  type: TYPE_NORMAL
- en: Standalone servers also do not imply that there is no form of unified management.
    Tools like ProxMox or VMware vSphere allow a consolidation of management while
    keeping system hardware independent. Modern tooling has made managing sprawling
    fleets of standalone servers very simple indeed.
  prefs: []
  type: TYPE_NORMAL
- en: Most every aspect of a stand-alone server can be improved by adding more to
    it and making it more complex, the two things that it always leads on are cost
    and simplicity. No other approach will reliably be able to keep our costs or our
    simplicity as low and in business, these are generally the factors that matter
    most.
  prefs: []
  type: TYPE_NORMAL
- en: Many to many servers and storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As companies grow there can be an opportunity to consolidate different aspects
    of the architecture in order to save money. Separating networking and storage
    is the most common approach to this. Creating a layer of compute nodes, and a
    layer of storage nodes allows for a lot of flexibility. The primary benefit is
    allowing for the easy movement of resources and better system utilization.
  prefs: []
  type: TYPE_NORMAL
- en: For example, an organization maybe need fifteen physical compute nodes (traditional
    servers) but only half a dozen storage nodes (SAN or NAS) to support them. Each
    individual system can be easily custom scaled and does not need to match other
    systems in the pool. In this way this approach is not so different from the standalone
    server approach.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that when doing this, the storage layer is the greater risk,
    compared to the compute layer, for two key reasons. First, it is stateful where
    compute is stateless, which means that here we not only have to protect the availability
    (uptime) of the system, but this is also where we have to protect the data as
    it is stored so we have the risk of data loss as well - there is simply more to
    lose here. Second, storage is more complex than compute and equivalent hardware
    and software at both layers means that the storage layer is just more likely to
    fail due to complexity. This is all risk that also exists in the standalone server,
    but when combined into a single chassis it can be more difficult to understand
    where the risk is occurring, even if we know what the overall resultant risk is.
  prefs: []
  type: TYPE_NORMAL
- en: In its simplest incarnation, we would have a single compute server node and
    a single storage node (typically a SAN array) and would connect them directly
    via a straight cable (Ethernet, eSATA, FC, and so on.) This is really more of
    a hypothetical scenario as it is so obviously bloated and illogical without any
    scale, but we can learn from the example to see how we take the single standalone
    server design and, without any benefits of scale, simply double the chassis to
    manage and increase the physical, as well as, logical complexity of the system
    design.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, this type of design is leveraged most to consolidate heavily on storage,
    pushing as much storage into a single node as possible, while having many small
    to medium sized (one to two CPU) servers that allow workloads to move between
    them in order to best balance said workloads. This approach is flexible and generally
    cost-effective, and makes large scalability quite simple.
  prefs: []
  type: TYPE_NORMAL
- en: Moving past standalone servers means we start to inject dependencies that need
    to be discussed. At a minimum, when we move to a multi-nodal system, we have the
    complexities of the interconnections (which might be as simple as just a cable,
    or more complex like going through a switching fabric of some sort), any complexities
    that come from configuring the nodes to speak to each other, and the risks of
    the extra components that might fail.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of design really does nothing to address risk, and actually is far
    riskier than standard standalone servers. This is why it is important to use standalone
    servers as a baseline and discover risk variation from that point. In a *network*
    system design, there is no redundancy, so each workload has a full dependency
    on both its associated compute node and its associated storage node(s). This risk
    may be uniform across a compute node or each workload located there might have
    unique storage configurations so that risks may vary widely between different
    workloads on a single server. This is where risk becomes much more complicated
    to measure because we have to deal with the cumulative risks of the compute node,
    the storage node, the connection between the two, and the configuration! Each
    individual piece is extremely difficult to measure on its own – putting them together,
    we mostly have to look in relative terms only and understand that it is much riskier
    than a standalone server.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the world as a workload
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: System architecture is a *by the workload* task and there is no specific necessity
    for all workloads in your organization, or even all workloads running on a single
    compute node, to share architectures. Mixing and matching is totally doable and
    somewhat common. Each workload should be being evaluated as to its own needs,
    and then the overall architecture evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: Often overlooked is the ability to use a complex and less reliable (but potentially
    less expensive at scale) option like network design for workloads that are less
    important, while on the same compute nodes also having local storage that is extremely
    fast and/or reliable for more critical workloads. Mixing and matching can be a
    strong strategy in a large environment where storage consolidation is considered
    necessary without endangering an isolated number of highly critical services in
    order to do so.
  prefs: []
  type: TYPE_NORMAL
- en: In the same vein, each workload can have its own backup, replication, failover
    and other risk mitigation strategies for deal with disaster. Sharing a compute
    node generally dictates very little as to how reliability and availability from
    workload to workload must be handled. Of course typically all workloads are treated
    the same either out of a desire for standardization and simplicity, but also regularly
    out of a misunderstanding of the range of customization available for each individual
    workload. It is often assumed that choosing a system design is an all or nothing
    endeavour, but this is not the case.
  prefs: []
  type: TYPE_NORMAL
- en: The main challenges of network system design is that any efficiency gained has
    to offset the additional cost created by needed more overall nodes (separating
    compute and storage means that additional hardware chassis and operating systems
    are necessary for the same tasks) and at any scale additional networking equipment
    is needed to handle the interconnects. Networking equipment can be as simple as
    a single Ethernet switch or as complex as clusters of Fiber Channel or Infiniband
    switches. Switches represent no only additional cost to purchase, but also additional
    points of failure both for hardware and, to a much lesser extent, configuration.
    Often redundant switches are purchased reducing hardware risk but increasing cost
    and configuration complexity. Even in extremely large environments this represents
    additional cost and risk that is very hard to overcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Inverted Pyramid of Doom: Clustered Compute with Risky Storage, aka the
    3-2-1'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sadly the Inverted Pyramid of Doom (aka 3-2-1 or IPOD) has traditionally, for
    the majority of the 2000s and 2010s, been the most commonly deployed architecture
    in small and medium business and is also the prime example of the absolutely worst
    possible design decision for normal workload needs. It is also the design that
    maximizes profits for vendors and resellers, so it is what everyone wants you
    to buy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The IPOD design is differentiated from the network system design above in that
    the compute layer is clustered for high availability, but the storage layer is
    not. As we discussed in the last design storage is both more important to protect
    and more likely to fail. Typically the networking layer (the layer providing connectivity
    between compute and storage) is also clustered for high availability. This nodal
    count by layer creates the naming conventions used: 3-2-1 refers to the design
    having three (or more) compute nodes, connected to two redundant switches, all
    relying on a single storage device which is most typically a SAN.'
  prefs: []
  type: TYPE_NORMAL
- en: When viewed in an architectural drawing, the IPOD is a pyramid with the wide
    portion on top and everything balanced on the point. Hence the term *inverted
    pyramid*, this design is designed to be as costly and risky as possible, hence
    the moniker *of doom*.
  prefs: []
  type: TYPE_NORMAL
- en: Top-down redundancy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Why is a design so obviously impractical as the IPOD so traditionally popular?
    The answer requires us to understand several factors. First, redundancy, risk,
    and system design are all areas that most businesses, and even most IT departments
    within those businesses, have received no training and are generally completely
    unaware and so represent an easy target for vendors to be manipulative.
  prefs: []
  type: TYPE_NORMAL
- en: 'The real trick comes from two things: linguistics and the simplification enabled
    by top down viewing. The linguistic trick happens because the term *redundancy*
    does not mean what most people believe that it means and this system *has redundancy*,
    but in an all but meaningless way. So, when a customer says, *I need redundancy*
    they actually mean *I need high availability*, but this allows the vendor to state
    that there is redundancy and ignore actual needs. Semantics are super important
    in all business, and IT more than most.'
  prefs: []
  type: TYPE_NORMAL
- en: The top-down aspect of the system comes from how we view the architecture. As
    IT professionals, we know that we should view our architecture *from the side*,
    that is seeing the reliability as it stands layer by layer, knowing that compute
    is built on top of the network, and the network on top of the storage. But a vendor
    wanting to steer a customer to believe that there is strong redundancy will demonstrate
    the system *top down* showing a view that only sees the compute layer where there
    is redundancy. The other layers are overly complex and tend to be happily ignored
    by all parties as being *black boxes that do magic*. Ignoring the hard parts and
    just focusing on the trivial, easy part where redundancy is least important makes
    it really easy to mislead a customer.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if we really stop and think about it, what matters is the overall
    reliability of the entire system. Getting distracted by any single layer will
    simply lead us astray. We need to understand all of the layers, and how they interact
    with each other, in order to determine overall reliability, but there is a strong
    emotional drive to see one layer as being extremely reliable (as the compute layer
    here often is) and then feeling that the overall system must therefore be extremely
    reliable. But this is anything but true. The overall reliability of the system
    is driven primarily by the most fragile layer, not the most reliable. The system
    risk is, if you recall from earlier, cumulative. You combine all of the risks
    together because each layer depends one hundred percent on every other layer,
    if any layer fails everything fails. You can demonstrate this easily with a thought
    experiment... if one layer has a 100% chance of failure, and all other layers
    have a 0% chance of failure, the system will still fail 100% of the time. The
    impossibly reliable layers do literally nothing to offset the unreliable layer.
  prefs: []
  type: TYPE_NORMAL
- en: Redundancy itself is a dangerous word to use. In general English usage, the
    word redundant simply means that you have multiple of something when fewer are
    needed. This can mean that one is a replacement or backup should the other fail,
    but that is not implied and often the term is used to mean something else. In
    RAID, for example, RAID 0 has multiple disks (redundant) but the more redundancy
    the higher the risk, not the lower. RAID 1 is the opposite. Redundancy is polar
    opposite there, even within a single context. This really shows the importance
    of semantics in IT (and business, or really, life in general.) People often use
    redundancy as a proxy work for reliability, but the two mean very different things.
    Use the term you mean and you will get far better information.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest problem with the IPOD design is one of practicality. If we were
    to look at it purely from a reliability standpoint we could state that it is safer
    than the network system design because at least some of the layers contain high
    availability measures, even if not all of them do. And this is totally correct,
    but tends to be misleading. Network system design is meant to trade high risk
    for cost savings versus the simple stand alone server design, using the idea of
    *safer than* something that is not even designed to be safe is not exactly wrong,
    but talking about it in that context is done to evoke an emotional response -
    to make the IPOD feel safe, which is not the same as *safer*. If we compare the
    reliability of an IPOD to the stand alone server, it feels quite unsafe and remember,
    we stated at the beginning, the low cost, simple, stand alone server is our baseline
    for comparisons. The problem with the IPOD is that the risk is extremely high,
    approaching the risk of the network system design, while its costs are much higher
    than the network system design and generally much higher than the stand alone
    server design all while having more complexity and effort for the IT team. It
    is the consistent combination of high risk and high cost that makes it problematic
    and generally accepted as the worst design to encounter in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Outside of production environments, the IPOD is often ideal for large lab environments
    where capacity matters most and reliability does not matter at all. The ability
    to flexibly scale compute with a single consolidated, low cost, highly unreliable
    storage layer can make sense to make large scale labs more affordable.
  prefs: []
  type: TYPE_NORMAL
- en: Layered high availability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The logical system design derived from what we have already seen is to take
    the separate layers of the network system design, and the high availability clustering
    from the compute layer of the Inverted Pyramid of Doom and apply it to all layers
    giving us a high availability storage layer, a high availability networking layer,
    and a high availability compute layer. In this way we can have large scale compute,
    storage, and networking without any individual layer being a high level of concern.
  prefs: []
  type: TYPE_NORMAL
- en: where each layer still depends on every other layer, that three highly available
    layers must still be evaluated with the risk of each layer added together. So,
    while we can almost certainly make any individual layer more, or even far more,
    reliable than a single standalone server would be when we accumulate the risk
    of each layer, and then add in the risk of the additional complexities from incorporating
    the layers together, it may or may not remain more reliable than the standalone
    server would be.
  prefs: []
  type: TYPE_NORMAL
- en: Reliability is relative
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When discussing reliability and these different architectures we have to remember
    to think in terms of apples to apples, not apples to oranges. When we say that
    a single server is a certain level of reliability, and that servers clustered
    with standard high availability technologies have a certain relatively higher
    reliability, we are assuming that all of those servers are roughly identical in
    their individual reliability. In most situations this is true. Whether we are
    talking compute nodes, networking hardware, or storage nodes, for roughly the
    same price range we get similar quality hardware and software with roughly similar
    failure characteristics. So, these different devices *of the same quality* are
    all about the same level of reliability with networking hardware being the most
    reliable (least complex) and storage nodes being the least reliable (because they
    are the most complex.)
  prefs: []
  type: TYPE_NORMAL
- en: However we can manipulate this dramatically. A five thousand dollar server will
    generally be much less reliable (and performant) than a five hundred thousand
    dollar server. Yet each is an individual, stand alone server. So clearly we have
    to think in terms both of architectural reliability (the reliability of the system
    design that we make) and in terms of the individual components.
  prefs: []
  type: TYPE_NORMAL
- en: A common problem found here is that *you get what you pay for* applies not at
    all and you can easily find extremely expensive single-chassis systems for both
    compute and storage nodes that are not highly available at all and may not even
    be as reliable as average devices! As reliability is hard to measure and even
    harder to prove, vendors have little incentive to tell us the truth. Vendors are
    highly incentivized to tell us whatever is likely to make us spend more money
    with them whether it is making us feel that traditional servers are more fragile
    than they really are, or by making wild high availability claims for devices that
    are essentially built from straw (and by pigs.)
  prefs: []
  type: TYPE_NORMAL
- en: So we must be careful that we consider all of the factors. And we must understand
    that the ability to protect a single chassis (vertical reliability) compared to
    multiple chassis (horizontal reliability) is different. Single chassis reliability
    tends to be incredibly powerful for certain components (such as redundant power
    supplies, high quality components, and mirrored RAID for storage) but tends to
    be complex and problematic for others (CPU, RAM, Motherboards.) And single chassis
    systems, while easier to operate, cannot address some key concerns like physical
    damage (water, fire, forklift) in the same way that multiple chassis can.
  prefs: []
  type: TYPE_NORMAL
- en: We must also be keenly aware that marketers and sales people often use confusion
    around reliability as a sales tactic and will push concepts such as *dual controller*
    systems as being essentially impossible to fail but without science or math to
    back it up. Dual controller systems are simply horizontally scaled systems inside
    a single chassis with all of the complexity of the former and the lack of physical
    protection of the later. And any product sold based on being misleading is that
    much more likely to be poorly made as it means that the vendor is unlikely to
    be being held accountable to quality design.
  prefs: []
  type: TYPE_NORMAL
- en: It has become known, especially in the early 2010s, that server vendors were
    regularly pushing products branded as high availability or *cannot fail* that
    did not even begin to approach the baseline reliability of traditional servers.
    Since customers could not verify this for themselves, they often just take the
    vendor's word for it and if the businesses loses money, finger pointing is the
    natural recourse.
  prefs: []
  type: TYPE_NORMAL
- en: This approach necessarily is the most expensive design we can reasonably assemble
    because we need multiple devices at each layer, as well as technology to create
    the clustering at each layer. This is best for very large systems where each layer
    is able to scale so large that cost benefits of scale come in at every point.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that essentially all cloud based systems run on this architecture
    due to their enormous scale. Certainly not all as cloud can run using any architecture,
    but this is far and away the most likely to be used in a large, public cloud implementation
    and is most generally what would be found even in a moderate scale private implementation.
    Many clouds do, however, run on stand alone servers even at massive scale.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperconvergence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last architectural type that we will look at is hyperconvergence and we
    have now come full circle from increasingly complex designs to one of the least.
    Hyperconvergence as an architecture is anything but new, but for the last few
    decades it has been almost completely ignored before having a sort of renaissance
    in the mid-2010s and is now, along with stand alone servers, the bulwark of system
    architectural design.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperconvergence, also called HC or HCI, takes the compute and storage nodes
    of other, more complex architectures, and recombines them back into single servers
    (or you can view it as taking stand alone servers and adding high availability
    through engineering redundancy without adding unnecessary complexity.) Hyperconvergence
    gives us the best of both worlds, simplicity like stand alone servers, but options
    for high availability like Layered High Availability.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperconvergence is both so incredibly simple, but so effective that it can
    be hard to explain. The key strategy is taking the existing power and cost savings
    of the stand alone approach and doing as little as possible while still being
    able to add high availability clustering. By having multiple stand alone nodes
    that are clustered together (are they still stand alone, then?) we make all of
    the pieces highly available, while also reducing how many pieces are needed in
    total.
  prefs: []
  type: TYPE_NORMAL
- en: When done correctly, data can also be guaranteed to be kept locally to a compute
    node, even though storage is replicated between nodes to create storage high availability,
    which not only means that we can get the high performance for our storage like
    we can with stand alone servers, but also that we can avoid a cross-node dependency
    allowing any node to keep working on its own, even if all other nodes and/or the
    network connecting them fails! That means that unlike all other system designs,
    we are only adding more resilience on top of the stand alone server design! That
    is huge. All other designs must put in the bulk of their efforts to overcoming
    their own introduced fragilities and risk failing to adequately do so potentially
    leaving them riskier than they would have been had we done nothing.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperconvergence, therefore, acts as the logical extension of the stand alone
    design and represents probably the most applicable system design for any large
    scale system. It is common to see hyperconvergence as being limited to small systems,
    but it is able to scale to the limits of the clustering technologies - the same
    limits affecting all design options. So all standard designs can go to roughly
    the same size, which is generally far larger than anyone would want to go in practical
    terms within the confines of a single system.
  prefs: []
  type: TYPE_NORMAL
- en: High availability vs basic clustering
  prefs: []
  type: TYPE_NORMAL
- en: In this section we talk about clustering with the assumption that clustering
    (whether compute, storage, or networking) is done for the purpose of making the
    system highly available, at least within that one layer. High availability clustering
    is not the only kind of clustering, however. In all of these designs, including
    stand alone, we can add generic clustering as a management layer to manage many
    systems together. This can be confusing as the term clustering can be used to
    mean many things.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices in System Design Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best practice for system design is to keep your architecture as simple as
    possible to meet your needs, but no simpler. Remember that simplicity is a benefit,
    not a caveat. Complexity should be avoided when possible as complexity brings
    cost and risk.
  prefs: []
  type: TYPE_NORMAL
- en: In any assessment, start with the brick. Just one, single, solitary stand alone
    server. Simple and effective. Now evaluate, does this meet your needs? How could
    spending more money better meet your needs?
  prefs: []
  type: TYPE_NORMAL
- en: If high availability is needed, then assess hyperconvergence. Nothing can be
    more reliable, architecturally speaking.
  prefs: []
  type: TYPE_NORMAL
- en: If you have special cases where you need cost savings at massive scale, other
    designs might be applicable. But remember, no matter how reliable it might feel
    emotionally or how much a sales person may push the solution, hyperconvergence
    is literally impossible to beat for reliability by design. Make sure that any
    design that is not one of these two starting points is being used with a comprehensive
    understanding of all of the risks and costs involved.
  prefs: []
  type: TYPE_NORMAL
- en: That is a lot of material that we covered and a lot of turning conventional
    thinking on its ear. It is sad that here *conventional thinking* equates to *blindly
    ignoring needs and risk logic*, but it is what it is. This is a very difficult
    topic because it is so foreign to most people in any technical or business realm
    and such a specialty skill to master. And in many cases, you will get a lot of
    pushback from others who struggle to assess or communicate risks, and fail to
    turn risk information into actionable business decision making.
  prefs: []
  type: TYPE_NORMAL
- en: At this point you have the tools and knowledge to design systems physically.
    This is a big topic, and it might be worth revisiting from time to time. This
    is very foundational and gives us the starting point to build reliable systems
    farther up the proverbial stack. And now that we know how to approach different
    designs for different purposes we will go on to look at risk itself and learn
    to ask what risk mitigation is right for us.
  prefs: []
  type: TYPE_NORMAL
- en: Risk assessment and availability needs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the very core of what we do in designing a system architecture is taking
    business needs around performance and availability and applying our understanding
    of risk and, as with everything in business (and therefore IT) assessing against
    costs. In the last section we already talked about risk, a lot. We have to - risk
    and performance pretty much define everything (other than strict capabilities
    and features) for us during our design stages.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we ask our businesses about risk, we almost always receive one of two stock
    answers: *we are not willing to pay anything to mitigate risks* or *we cannot
    afford to go down, it is worth anything to be up one hundred percent of the time*.
    Both answers should be obviously seen as insane and have no reason to ever come
    out of the mouth of any business person or IT professional, and yet they are nearly
    the only answers that you will ever receive providing you with no guidance whatsoever.
    They represent management simply *blowing off* IT and leaving IT to take on all
    decision-making risks without management providing any guidance.'
  prefs: []
  type: TYPE_NORMAL
- en: We have some amount of basic guidance that we can almost always work with. On
    the *low* end of the spectrum the rule of thumb is that if data is valuable enough
    to have stored in the first place, then at a minimum it is worth backing up. This
    is the simplest aspect of data and availability protection, and if your business
    thinks that the data that they store is not worth even backing up, you should
    be asking yourself why you are there yourself. There are extremely special cases
    where storage data is truly ephemeral and does not need a backup, but this situation
    is so unique and rare that it can be safely ignored.
  prefs: []
  type: TYPE_NORMAL
- en: On the other end of the spectrum no system, anywhere, ever is so important that
    it is worth anything to not have downtime. First of all, totally avoiding downtime
    is impossible. No one can do this with any amount of resources. We can make a
    system ridiculously reliable and easily recoverable from nearly endless potential
    scenarios but no government, military, secret cabal, alien species, investment
    bank, or otherwise can possibly meet requirements often demanded by small businesses
    without any discussion whatsoever. The theoretical maximum that can be invested
    into making systems reliable is the entire value of the company in question and
    even if every penny that the largest firms had was invested into reliability and
    nothing else, risk still remains, no matter how small.
  prefs: []
  type: TYPE_NORMAL
- en: Risk and diminishing returns
  prefs: []
  type: TYPE_NORMAL
- en: Attempting to invest in risk mitigation technology is a tough thing to do because
    as systems become more reliable the cost of *moving the needle* significantly
    towards ever improved reliability becomes more and more costly. For example, getting
    a stand alone server that is well built may give us as much as five nines of availability
    without any special *high availability* features.
  prefs: []
  type: TYPE_NORMAL
- en: We may find that we need much higher availability. Perhaps six or seven nines.
    To get those order of magnitude jumps in reliability will require, almost certainly,
    at least double the investment in hardware as the standalone server. This may
    be well justified by our needs, but the cost per workload just jumped significantly.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to move the needle again an order of magnitude beyond that, the price
    jumps yet again. We get less and less protection as we spend more and more money.
  prefs: []
  type: TYPE_NORMAL
- en: And so, because our business will rarely be willing or able to clearly define
    for us what our risk aversion level truly is, it often falls to IT and within
    IT to systems administration to carry out this all-important task on behalf of
    management. This will generally take some math, a lot of interviews with many
    different parts of the company, some common sense, and of course, some guesswork.
    Working with risk requires maintaining a logical view while it is tempting to
    become emotional, which is often the mistake made in business. Business owners
    or managers tend to react emotionally either seeing money spent on protection
    as not generating revenue directly and therefore undesirable, or seeing their
    business as not justifying protection and so tending to spend too much to provide
    an impression that the company is seen as valuable because downtime would be such
    a terrible thing.
  prefs: []
  type: TYPE_NORMAL
- en: Of course good management will always been heavily involved in risk assessment
    tasks. This should not fall to IT. While IT has great insight and is a valuable
    contributor to any risk discussion it is the core management, operations, and
    financial departments that truly have the full risk picture necessary to create
    a corporate infrastructure risk strategy.
  prefs: []
  type: TYPE_NORMAL
- en: When looking at workloads, we must attempt to evaluate what downtime will truly
    cost our business. This is not straightforward in nearly any scenario, but it
    is what we need to understand to have any means of logically discussing risk.
    Most businesses want to simplify downtime cost into the simplest possible terms.
    So dollars per minute or hour is generally how downtime is discussed. For example,
    losing the company's primary line of business application will lose the business
    one thousand dollars per hour of downtime.
  prefs: []
  type: TYPE_NORMAL
- en: While simple, almost no real world workload actually loses revenue evenly hour
    by hour. In the real world it is most common to see a complex curve. For example,
    in the first minutes, and possibly even hours, we might see almost zero revenue
    loss. But then commonly we see a spike as outages go long enough to first be perceived
    by customers, then to cause customer concern. Lack of confidence and lack of operations
    spikes tend to hit a peak relatively quickly. Then long term revenue loss tends
    to start to kick in after days or weeks as customers leave. But this curve is
    different for every business. Of course, making an entire curve graph of all downtime
    scenarios is difficult and probably impractical, but the business should be able
    to predict significant inflection points along a timeline that represent major
    changes in impact behavior.
  prefs: []
  type: TYPE_NORMAL
- en: It is tempting to look at outages as being all or nothing. Basically, ignoring
    workloads and seeing the entire company as completely down, as if the zombie apocalypse
    is happening and all of the staff have been infected. It is a rare workload that
    is going to impact any business in that manner. For example, if a company loses
    an email workload there will likely be an impact, but as email is not real time,
    it might take hours or even days before there is an actual loss of revenue (but
    if email is used to win real time bids, losing even a few minutes might be very
    impactful - it just all depends.) But assuming email could not be restored for
    hours, or days, a normal business would immediately begin mitigating the loss
    of the email workload through other channels. Maybe employees talk to each other
    in person, or use the company's instant messaging products. Perhaps sales teams
    begin to call customers on the phone rather than emailing. Working around a lost
    workload is often far more possible and effective that one realizes until a triage
    process is performed to see what a real world recovery might really look like.
  prefs: []
  type: TYPE_NORMAL
- en: Workload interplay
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Something else that we need to understand is how workloads interact with each
    other. As systems administrators we might have excellent insight into technical
    dependencies such as that a key ERP or CRM system depends on another application,
    such as email or financial, to function and if one is down, the other is down,
    too. That is an important aspect of workload dependency, but one that is well
    known and understood just by mentioning it. What is much harder to understand
    is the human workflow interdependence of systems.
  prefs: []
  type: TYPE_NORMAL
- en: Some workloads may be exceptionally stand alone. Some may depend significantly
    on others. Others may overlap and provide risk mitigation unofficially.
  prefs: []
  type: TYPE_NORMAL
- en: Lets look at the third case first. In many an organization today there might
    be traditional telephones, email, a few types of video conferencing solutions,
    and a handful of instant messaging solutions. Even in a tiny organization it is
    easy to casually end up with several overlapping solutions simply because so many
    things come bundled with so much functionality. In a situation like this, losing
    email might matter little for a very long time as internal communications may
    move to instant messaging and customer communications to telephone or video conference.
    Most organizations have the ability to work around a system that is down by using
    other tools at their disposal.
  prefs: []
  type: TYPE_NORMAL
- en: But the converse is also true. Two technically unrelated systems, again lets
    say CRM and email, might not connect together but the human workflow may require
    that both be used at the same time and the loss of either one of them might be
    functionally equivalent to losing them both. So we have to consider all use cases,
    and all mitigation possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: This interplay knowledge will help us to determine how it makes sense to deploy
    some workloads. For example, if email and instant messaging work to overlap during
    a crisis, it likely makes sense to decouple them as much as possible so that if
    the hardware or software for one was to fail that it would not take down the other.
  prefs: []
  type: TYPE_NORMAL
- en: If we have systems, like our email and CRM example, where one is useless without
    the other, then combining the two workloads to share a failure domain might make
    total sense. Meaning, as an example, if we had two independent servers one running
    the CRM and one running email, then each individual server would carry its own
    risk of failing with near certainty that those failures would not happen at an
    overlapping time. Each workload has an equal amount of expected annual downtime
    of X. The total downtime expected for the combined workloads is 2X. Easy math.
    Combine the two workloads onto a single server as each retains an equal amount
    of annual downtime risk, still X, and the combined is still 2X. But the effective
    downtime in the first case is 2X (or 1.99999X as there is some tiny chance of
    the outages overlapping) but in the second case is just 1X. How did we do that?
    Not by reducing any individual risk, but by reducing the effective risk - that
    is the risk impacting the business as a final result. Under the hood, we did reduce
    risk physically as a single server has half the risk of downtime of two equal
    servers simply because there is half as many devices to fail.
  prefs: []
  type: TYPE_NORMAL
- en: Even a complete company shutdown is not necessary a total loss. Sending staff
    home for a surprise holiday might lower insurance costs and raise morale. Given
    a day or two to spend at home might reinvigorate workers who may be happy to return
    after systems have been restored and work more efficiently or maybe put in a little
    extra time to attempt to recoup lost business. We have to consider mitigation
    strategies when looking at losses from failed workloads. Some businesses may simply
    lose some efficiency, while others may lose customers.
  prefs: []
  type: TYPE_NORMAL
- en: Of course we have to consider the possibility of the opposite. What if you are
    a business that depends heavily on customer perception of high uptime and even
    a tiny outage has a sprawling impact? Maybe your entire business generates only
    one thousand dollars per hour, but loss of customer confidence from even a two
    hour outage (which we might assume could only, at maximum, lose us two thousand
    dollars in this case) resulted in the loss of customers resulting in tens or hundreds
    of thousands of dollars of losses!
  prefs: []
  type: TYPE_NORMAL
- en: All of these losses are just estimates. Even if an outage actually happens,
    there is no guaranteed way to know what revenue would have been without an outage
    having occurred. So if we cannot know this number for certain after something
    has happened then obviously we cannot know it with any certainty before the event
    that only might happen, has happened. Bottom line... estimating risk is very hard.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a large organization, consider playing *what if* games on a weekend with
    some staff from different departments. Run through scenarios of *X or Y has failed*
    and attempt to mitigate in a nearly real-world simulation on a small scale. Can
    you keep working with one tool or another? Which departments become dysfunctional,
    which keep humming along, how do your customers see the situation? This kind of
    *game* is best played with a combination of strong planners who are thinking about
    risk strategies and writing procedures as well as with a group of perceivers (for
    example: triage experts) who do not plan, but work tactically on the ground figuring
    out how to keep working with the tools at their disposal.'
  prefs: []
  type: TYPE_NORMAL
- en: Defining high availability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of my favorite quotes in all of IT comes from John Nicholson who said *High
    availability isn't something that you buy, it is something that you do*. It is
    so tempting to see high availability as an intrinsic need in IT, and then to see
    it as so complicated that we cannot know how to approach it, and so fall prey
    to vendors who slap the unverified name *high availability* onto products, or
    even just slip it into a product name, and act as if buying a product can deliver
    high availability when logically, this is impossible. Imagine buying a high availability
    airplane, as an example. While you can make one airplane must more reliable than
    another, almost all of your overall reliability and safety comes from the pilot,
    not the plane. The same is true in IT. The best made product does little if operated
    poorly. A million-dollar cluster without backups is likely not as safe as a desktop
    with good backups!
  prefs: []
  type: TYPE_NORMAL
- en: So first we need to establish a baseline for measurement. In our last section
    we said that stand alone server infrastructure serves as our baseline. This baseline
    has to represent what we will call *standard availability*. We now have two ways
    that we would hope that we can look at this availability. One is in absolute terms
    by giving a number such as a *nines* number and through industry evidence, it
    appears that well maintained, well-made stand alone servers can approach five
    nines of availability which roughly means six minutes, or less, of unplanned downtime
    per year (planned downtime for maintenance can represent a potential problem,
    but is not itself included in a reliability figure such as this.)
  prefs: []
  type: TYPE_NORMAL
- en: Now keep in mind, when we are talking about a server or a system design architecture,
    we are not including the final workload, only the platform providing an underlying
    system onto which a hypervisor will be installed. So essentially hardware availability.
    Any software running on top may have its own reliability concerns and no amount
    of platform stability will fix instability from bad code in the final workload,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: The other, and generally more useful, way to look at reliability of system architecture
    is not in unmeasurable absolute terms, but in relative terms comparing different
    designs to one another. No one really knows what system reliability really is.
    It is not a big secret that server vendors are keeping from us, they simply do
    not know. Every little system configuration difference produces very different
    reliability numbers and, like we said about planes, the users operating the systems
    create the largest impact in terms of reliability. A company with a pristine datacenter
    and continuous onsite support that responds to alerts immediately and spare parts
    on hand or nearby might be able to squeeze very different reliability numbers
    out of the same server stuck in a closet without air conditioning, lots of dust,
    and generally ignored. There are simply too many factors involved. And even if
    we could somehow account for all of the potential variation, in order to get meaningful
    statistics on systems so complex with failure rates so low, we would need to operate
    thousands or tens of thousands of servers for more than a decade to collect useful
    numbers and then all of the data would be outdated by more than a decade. So,
    for all intents and purposes, it cannot be measured.
  prefs: []
  type: TYPE_NORMAL
- en: So, our most important tool is not talking in terms of *nines*, that is a great
    marketing tool and something that managers steeped in big heavy processes like
    Six Sigma like to repeat, but it means nothing in this context, but rather looking
    at orders of magnitude of systems deviating from our baseline. A system that is
    significantly more available than our baseline can be classified as *high availability*
    and a system that is significantly less available than our baseline can be classified
    as *low availability* and any system that is roughly the same as baseline remains
    *standard availability*. Beyond these general terms it becomes all but impossible
    to discuss.
  prefs: []
  type: TYPE_NORMAL
- en: A system design like hyperconvergence would be generally classified as *high
    availability* as it is the most reliable design approach. And an IPOD would generally
    be classified as *low availability* as it is closer to the least reliable design
    approach, which is the network system design. Layered clustering is generally
    considered high availability, but not *as high* as hyperconvergence. Of course,
    in this case we are only considering the availability of the system design and
    ignoring individual components. If we use extremely highly available individual
    components at every layer of an IPOD, we can theoretically get it back up to standard
    availability, but likely at great cost.
  prefs: []
  type: TYPE_NORMAL
- en: It is far more valuable to think of reliability in relative terms, rather than
    absolute ones. It is almost trivial to look at a standalone server, an IPOD, and
    hyperconvergence and see how there is a clear *high*, *medium*, and *low* availability
    based on nothing but common sense and the location of risk, risk mitigation, and
    risk accumulation in the design. It requires no special training or math to see
    how dramatically each is separated from the next and how improving the overall
    quality of components moves the absolute reliability number, but the relative
    does not change. And at the end of the day, this is all that we can know.
  prefs: []
  type: TYPE_NORMAL
- en: By knowing what downtime impact will look like financially for our business,
    even if it is only a very rough estimate, we have something to work with when
    attempting to decide on how to invest in risk mitigation. We should never invest
    more in risk mitigation than what calculated risk shows as our potential losses.
    This sounds obvious but is a common stumbling point for assessment in many firms.
    For example, if a possible outage might cost us one thousand dollars and protecting
    effectively against that outage would cost two thousand dollars, we should not
    at all entertain paying to mitigate that risk.
  prefs: []
  type: TYPE_NORMAL
- en: We should think of risk mitigation as a form of outage itself, for mathematical
    reasons. This makes calculations easier to understand. With good risk mitigation
    we would incur a minimal financial penalty now (say spending one thousand dollars)
    to protect against a large potential outage (that might cost us one hundred thousand
    dollars.) The upfront cost is guaranteed, the future risk is only a possibility.
    So any risk mitigation must therefore be much smaller than the potential damage
    that it is meant to protect against.
  prefs: []
  type: TYPE_NORMAL
- en: 'An analogy of my own that I have been using for years to describe paying more
    for risk mitigation than the potential damage of the outage itself is: *That''s
    like shooting yourself in the face today to avoid maybe getting a headache next
    year*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When comparing standard availability systems and high availability systems
    we might be talking about a difference of only several minutes per year, on average,
    of downtime. High availability, therefore, has to justify its cost and complexity
    very quickly. A massive public website where just a minute or two of being unavailable
    could cost millions in purchases or worse, erode customer confidence, therefore
    could easily justify a large expenditure in high availability systems even if
    the time saved seems trivial. But an internal system servicing employees where
    customer confidence is not a factor, and downtime does not lead users to turn
    to competitors (for example: email system, financial, CRM, and others) the lose
    of even several minutes a day, let alone a year, is likely to have no real financial
    impact whatsoever and investing heavily to protect those systems would be wasteful.'
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we apply all of this to best practices? The hard answer is that risk
    assessments and resulting system design are very hard things to do. Determining
    risk is a long process involving a lot of math, logic, and to some degree, guessing.
    It requires that we understand our businesses and our technology stacks deeply.
    It demands that we engage the business at all levels, and from all departments,
    and consolidate information that is generally siloed. It forces us to evaluate
    other risk assessments against logic and expected emotional reactions.
  prefs: []
  type: TYPE_NORMAL
- en: Rules of thumb tell us that the majority of systems that we deploy should be
    standalone servers, and nearly all remaining systems should be hyperconverged.
    These two standard patterns represent the near totality of what proper design
    will look like in the real world. All other designs are realistically relegated
    to extremely niche use cases with the IPOD being the ultimate *anti-pattern* of
    what not to do except for the most extreme of special cases.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered a lot of material in this chapter. But now we have an idea of
    how we make risk determinations, how we design our architectures based on that
    assessment. We understand how and why we use different kinds of virtualization,
    and why we always virtualize. And we know how to evaluate the use of cloud and
    locality for our deployments. Now to put all of this together! We use all of these
    tools in deciding the deployment of every workload! So many options, but that
    is what makes our careers challenging and fulfilling (and what makes us worth
    our salaries.)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In summary, system architecture is complex and requires us to really dig into
    business needs, how operations works, talk to key roles throughout the organization
    and elicit input, and take a broad view of technological building blocks to construct
    solutions that deliver the performance and reliability that our workloads need
    at the minimum cost.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at fundamental components with virtual machines and containers and
    should now be able to defend our use of them and choose properly between them,
    as well as be able to use traditional containers without becoming confused with
    more recent application containers. And we learned about locality. You should
    be able to navigate the complicated linguistic minefield that is managers attempting
    to talk about the placement and ownership of server resources, analyze costs and
    risks and find the right option for your organization. Colocation, cloud, traditional
    virtualization, on premises are all options that you understand.
  prefs: []
  type: TYPE_NORMAL
- en: And finally, the big piece, system design and architecture. Taking the physical
    and logical components of our system and building a full functional platform that
    empowers our workloads rather than crippling them. This has been a long chapter
    and touches on a lot of topics that are very rarely taught individual, let alone
    together. These are some really hard topics, and it is probably worth covering
    a lot of this material again before moving on.
  prefs: []
  type: TYPE_NORMAL
- en: For many of us in systems administration we might use the material in this chapter
    almost never. For others it might be nearly everyday skills. These topics are
    often ones that allow you to completely elevate your career by demonstrating a
    concrete ability to take seemingly mundane technical minutia and applying background
    system design decisions to key organizational needs. Of all of our topics in this
    book, this one is probably the one that should empower you more than any other
    to stand out among your peers and cross organizational boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that with the information presented here that you can filter through
    sales and marketing misinformation, apply solid logic and reasoning, and build
    on concepts that will remain timeless. Taking the time to really understand failure
    domains, additive risk, false redundancy, and more will make you better at nearly
    every aspect of your information technology journey whether your goals are purely
    technical, or you dream of sitting in the board room chairs.
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we are going to return to the seemingly more pedestrian
    topic of system patching, and move from the high level system strategies to in
    the trenches security and stability warfare.
  prefs: []
  type: TYPE_NORMAL
