<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deployment Methodologies</h1>
                </header>
            
            <article>
                
<p>So far in this book, we have set the groundwork for a stable foundation for your Enterprise Linux environment. We have discussed in detail how to ensure your Linux environment lends itself well to automation through standardization and how to leverage Ansible and AWX to support you on your automation journey. Before we get started on the really detailed technical work in this chapter, we must take a look at one final piece of detail—your deployment methodology.</p>
<p>We have already established a need for a small number of consistent Linux builds for your environment. There is now a decision-making process for you to go through—how to deploy these builds across your enterprise. Most enterprises have several choices available to them, ranging from the easiest<span>—</span>downloading publicly available template images<span>—</span>through building their own templates, to perhaps the most complex<span>—</span>building from scratch using a pre-boot environment. Alternatively, the best approach might be some hybrid of these approaches. In this chapter, we will explore these options and understand how to ensure you are selecting the best one for your enterprise that supports you in your automation journey and is efficient and easy to implement. In subsequent chapters, we will then go into greater technical depth on each approach. </p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Knowing your environment</li>
<li>Keeping builds efficient</li>
<li>Ensuring consistency across Linux images</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>This chapter assumes that you have access to a virtualization capable environment running Ubuntu 18.04 LTS. Some examples are also performed on CentOS 7. In either of these cases, the examples can be run on either a physical machine (or laptop) running one of the aforementioned operating systems, with a process that has virtualization extensions enabled, or a virtual machine with nested virtualization enabled.</p>
<p class="mce-root">Ansible 2.8 is also used later in this chapter and it is assumed you have this installed on the Linux host you are using.</p>
<p><span>All example code discussed in this book is available from GitHub at:</span><span> <a href="https://github.com/PacktPublishing/Hands-On-Enterprise-Automation-on-Linux">https://github.com/PacktPublishing/Hands-On-Enterprise-Automation-on-Linux</a>.</span></p>
<p> </p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Knowing your environment</h1>
                </header>
            
            <article>
                
<p>No two enterprise environments are the same. Some businesses still rely heavily on bare-metal servers, whilst others now rely on one of a myriad of virtualization or cloud providers (either private or public). Knowing which environments are available to you is a key part of the decision-making process.</p>
<p>Let's explore the various environments and the relevant build strategies for each.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying to bare-metal environments</h1>
                </header>
            
            <article>
                
<p>Bare-metal environments are without a doubt the grandfather of all enterprise environments. Before the revolution in virtualization and then cloud technologies throughout the 21<sup>st</sup> century, the only way to build an environment was on bare metal.</p>
<p>These days it is unusual to find an entire environment which is run on bare metal, though it is common to find ones where certain key components are run on physical hardware, especially databases or computational tasks that require certain physical hardware assistance (for example, GPU acceleration or hardware random number generation).</p>
<p>When building servers from bare metal, two fundamental approaches are suitable in most environments. The first is to build the servers manually using either optical media or, more commonly now, a USB drive. This is a slow, interactive process that is not repeatable at scale, and hence it is not recommended for any environments other than those containing just a handful of physical servers, where the requirement to build new machines is minimal and infrequent.</p>
<p>The other most viable option for building at scale in the repeatable, consistent manner that we have advocated throughout this book so far is to boot physical servers over the network, using a <strong>Pre-eXecution Environment</strong> (<strong>PXE</strong>). This involves loading a tiny boot environment from a network server, and then using this to load the Linux kernel and associated data. In this manner, it is possible to bring up an installation environment without the need for any form of physical media. Once the environment is up, we would use an unattended installation method to allow the installation to complete without any intervention from the user.</p>
<p>We will cover these methods in detail later in this book, as well as repeatable techniques for configuring the servers once they are built. In the meantime, however, it will suffice to simply state that, for building out physical Linux servers in an enterprise, PXE booting coupled with an unattended installation is the route that is easiest to automate and will produce the most repeatable results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying to traditional virtualization environments</h1>
                </header>
            
            <article>
                
<p>Traditional virtualization environments are those that predate what we know today as cloud environments<span>—</span>that is to say, they are straightforward hypervisors on which operating systems are run. Commercial examples such as VMware are common, as well as their open source counterparts such as Xen and KVM (and frameworks built off of these, such as oVirt).</p>
<p>As these technologies were originally built to supplement traditional physical environments, they present several possible options for building out your Enterprise Linux estate. For example, most of these platforms support the same network-booting capabilities as their bare-metal counterparts, and hence we could actually just pretend they are bare metal and continue with a network booting methodology.</p>
<p>However, virtualized environments introduced something that was difficult to achieve in physical environments because of the differences in hardware between the bare-metal devices on which they all ran<span>—</span>templates. A templated virtual machine is quite simply a deployable snapshot of a preconfigured virtual machine. Hence, you might build out the perfect CentOS 7 image for your enterprise, integrate your monitoring platform, perform all of the security hardening required, and then, using tools built into the virtualization platform itself, turn it into a template. The following is a screenshot of the CentOS 7 templates in the author's lab environment:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/18a0df96-60ff-4339-b5c5-04bc9318fdf6.png" style="width:53.50em;height:26.25em;"/></p>
<p>Each of these templates is a fully configured CentOS 7 base image ready to be deployed, with all pre-deployment work such as removal of SSH host keys completed. As a result, all an administrator has to do is to select the appropriate template and click on the<span> </span><span class="packt_screen">New VM</span><span> </span>button<span>—</span>the process will be similar in platforms other than RHV, as most mainstream virtualization solutions provide this functionality in some guise.</p>
<div class="packt_infobox">Note that, to keep the examples accessible, I have used the GUI as the primary process for creating a new VM. Nearly all virtualization and cloud platforms have APIs, command-line interfaces, and even Ansible modules that can be used to deploy virtual machines, and in an enterprise setting, these would scale far better than the GUI itself. Given the wide variety of environments available, this is left as an exercise for you to explore.</div>
<p>This is in itself a fairly straightforward process, but it requires a little care and attention. For example, nearly all Linux servers these days have SSH turned on, and the SSH daemon on each server has a unique host identification key that is used to prevent (amongst other things) man-in-the-middle attacks. If you template a preconfigured operating system, you will also template these keys, which means a distinct possibility of duplicates across your environment. This reduces security quite considerably. It is hence very important to perform several steps to prepare your virtual machine before turning it into a template, and one such common step is to delete the SSH host keys.</p>
<p>Servers created using the PXE method do not suffer from this problem, as they are all installed from scratch and hence there are no historic log entries to clean up and no duplicate SSH keys.</p>
<p>In <a href="3802fb48-9f14-4a52-98c5-280d381260a4.xhtml" target="_blank">Chapter 5</a>, <em>Using Ansible to Build Virtual Machine Templates for Deployment</em>, we will go into detail on creating virtual machine templates suitable for templating using Ansible. Although both the PXE boot and template deployment methodologies are equally valid for virtualized environments, most people find the templated route to be more efficient and easier to manage, and for this reason, I also advocate it (for example, most PXE boot environments need to know the MAC address of the network interface used on the physical or virtual server being deployed<span>—</span>this is not a necessary step in template deployment).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying to cloud environments</h1>
                </header>
            
            <article>
                
<p>The most recent incumbent to Enterprise Linux architectures (barring of course containers, which is another discussion entirely) is the cloud provisioning environment. This might be through a <em>public cloud</em> solution such as <strong>Amazon Web Services</strong> (<strong>AWS</strong>), Microsoft Azure, <strong>Google Cloud Platform</strong> (<strong>GCP</strong>), or one of the myriad of smaller providers that have sprung up in recent years. It might equally be through an on-premise solution such as one of the variants of the OpenStack project or a proprietary platform.</p>
<p>These cloud environments have radically changed the life cycle of Linux machines in the enterprise. Whereas on bare-metal or traditional virtualized architectures, Linux machines were cared for, nurtured, and repaired if ever they failed, cloud architectures are built on the premise that each machine is more or less expendable, and that if it fails, a new one is simply deployed in its place.</p>
<p>As a result, PXE deployment methodologies are not even possible in such environments, and instead they rely on pre-built operating system images. These are in essence just a template either created by a third-party vendor or prepared by the enterprise.</p>
<p>Whether you go with a commercial provider or build an on-premise OpenStack architecture, you will find a catalog of available operating system images for you to choose from. Generally, those provided by the cloud provider themselves are trustworthy, though depending on your security requirements, you may find those provided by external parties suitable as well.</p>
<p>For example, here is a screenshot of the recommended operating system images available for OpenStack:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/27b9c9ea-b59a-4827-a9af-4813deb256ae.png" style="width:56.08em;height:35.42em;"/></p>
<p>As you can see from the table of contents, most of the major Linux distributions are represented here, which immediately saves you the task of building the basic operating system itself. The same is true of AWS:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d9bef8ae-be67-4214-a11d-793c794ae165.png" style="width:47.00em;height:31.83em;"/></p>
<p>In short, if you are using a cloud environment, you will be spoiled for choice for base operating system images from which to get started. Even so, it is unlikely this choice will be sufficient for all enterprises. For example, using a pre-built, cloud-ready image does not negate requirements for things such as enterprise security standards, monitoring, or log forwarding agent integration, and a myriad of other things that<span> </span><span>are so important</span><span> for the</span> <span>enterprise</span><span>. Before we proceed, it is worth noting that you can, of course, create your own images for your chosen cloud platforms. In the interests of efficiency though, why re-invent the wheel? If someone has already completed this step for you, this is something that you can effectively delegate elsewhere.</span></p>
<div class="packt_tip">Although most ready-made operating system images are trustworthy, you should always exercise caution when selecting a new one, especially if it has been created by an author you are unfamiliar with. There is no way to know for sure what the image comprises and you should always carry out due diligence when selecting an image to work with.</div>
<p>Assuming that you do choose to proceed with a pre-made cloud-ready image, the post-installation configuration work can all be handled neatly by Ansible. In fact, the steps required are almost identical to those required to build templates for traditional virtualization platforms, and we shall again cover this process in detail a little later in this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Docker deployments</h1>
                </header>
            
            <article>
                
<p>Docker deployments are a special case in our discussion on Linux environments. In practical terms, they share a lot in common with cloud environments<span>—</span>Docker images are built based upon pre-existing minimal OS images and are often built using the native Docker toolchains, though automation with Ansible is entirely possible.</p>
<p>As Docker is a special case, we will not be focusing on it in this book, though it is important to note that Docker, being a recent incumbent into the presence of Linux in the enterprise, is actually designed around many of the principles we have already considered in this book. Let's briefly consider the Dockerfile used to create the official nginx container.</p>
<div class="packt_tip">For those not familiar with Docker, a Dockerfile is a flat text file that contains all the directives and commands that are required to build up a container image for deployment.</div>
<p>At the time of writing, this file contains the following:</p>
<pre>#<br/># Nginx Dockerfile<br/>#<br/># https://github.com/dockerfile/nginx<br/>#<br/><br/># Pull base image.<br/>FROM ubuntu:bionic<br/><br/># Install Nginx.<br/>RUN \<br/>  add-apt-repository -y ppa:nginx/stable &amp;&amp; \<br/>  apt-get update &amp;&amp; \<br/>  apt-get install -y nginx &amp;&amp; \<br/>  rm -rf /var/lib/apt/lists/* &amp;&amp; \<br/>  echo -e "\ndaemon off;" &gt;&gt; /etc/nginx/nginx.conf &amp;&amp; \<br/>  chown -R www-data:www-data /var/lib/nginx</pre>
<p>Although not based on Ansible, we can see the following in the preceding code block:</p>
<ol>
<li>The<span> </span><kbd>FROM</kbd><span> </span>line near the top defines a minimal Ubuntu base image on which to perform the rest of the configuration<span>—</span>this can be thought of as your SOE Linux image that we have discussed for other platforms.</li>
<li>The<span> </span><kbd>RUN</kbd><span> </span>command then performs the steps necessary to install the<span> </span><kbd>nginx</kbd><span> </span>package and perform some housekeeping to keep the image tidy and minimal (reducing space requirements and clutter).</li>
</ol>
<p>The code then continues as follows:</p>
<pre># Define mountable directories.<br/>VOLUME ["/etc/nginx/sites-enabled", "/etc/nginx/certs", "/etc/nginx/conf.d", "/var/log/nginx", "/var/www/html"]<br/><br/># Define working directory.<br/>WORKDIR /etc/nginx<br/><br/># Define default command.<br/>CMD ["nginx"]<br/><br/># Expose ports.<br/>EXPOSE 80<br/>EXPOSE 443</pre>
<p>Continuing our analysis of this file, we can see the following:</p>
<ol>
<li>The<span> </span><kbd>VOLUME</kbd><span> </span>line defines which directories from the host filesystem can be mounted within the container.</li>
<li>The<span> </span><kbd>WORKDIR</kbd><span> </span>directive tells Docker which directory to run the<span> </span><kbd>CMD</kbd><span> </span>that follows it in<span>—</span>think of it as a boot-time configuration.</li>
<li>The<span> </span><kbd>CMD</kbd><span> </span>line defines the command to run when the container starts<span>—</span>a microcosm of the process of defining which services will start at boot time in a full Linux system image.</li>
<li>Finally, the<span> </span><kbd>EXPOSE</kbd><span> </span>lines define which ports the container should expose to the network<span>—</span>perhaps a little like a firewall might allow certain ports through.</li>
</ol>
<p>In short, the native process to build a Docker container is very much aligned with our defined build process for an Enterprise Linux environment<span>—</span>hence, we can proceed in confidence with this process. With this in mind, we will now explore the process of ensuring our builds are as tidy and efficient as possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keeping builds efficient</h1>
                </header>
            
            <article>
                
<p>Knowing the fundamentals of your Linux environment, as we discussed in the last section, is vital to working out your deployment methodology. Although there exist some similarities between the build processes themselves (especially between traditional hypervisors and cloud environments), knowing these differences enables you to make informed decisions about how to deploy Linux throughout your enterprise.</p>
<p>Once you have chosen the methodologies most appropriate to your environment, it's important to consider a few principles to ensure your process is streamlined and efficient (again, bywords of Enterprise Linux deployments). We will cover these here to proceed into the real in-depth, hands-on work in the remainder of this book. Let's get started by looking at the need for simplicity in our builds.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keeping your builds simple</h1>
                </header>
            
            <article>
                
<p>Let's start to put some practical application of our earlier discussion on the importance of SOEs to our Linux build processes. Whatever route you choose and whatever your environment looks like, one key facet you should consider is to keep your build standard as simple and concise as possible.</p>
<p>No two enterprise environments are the same, and hence the build requirements for each enterprise will certainly be different. Nonetheless, a common set of example requirements is given here to demonstrate the kinds of things that will be needed in the build process:</p>
<ul>
<li>Monitoring agents</li>
<li>Log forwarding configuration</li>
<li>Security hardening</li>
<li>Core enterprise software requirements</li>
<li>NTP configuration for time synchronization</li>
</ul>
<p>This list is just a start, and every enterprise will be different, but it gives you an idea of the kinds of things that will go into a build. However, let's start to look at some of the edge cases to your build process. It is fair to say that each Linux server will be built with a purpose in mind and, as such, will run some form of application stack.</p>
<p>Again, the application stack is certain to vary between enterprises, but examples of the kinds of applications that might commonly be required are as follows:</p>
<ul>
<li>A web server such as Apache or nginx</li>
<li>The OpenJDK environment for Java workloads</li>
</ul>
<ul>
<li>A MariaDB database server</li>
<li>A PostgreSQL database server</li>
<li>NFS file-sharing tools and kernel extensions</li>
</ul>
<p>Now, in your standardization process, when you originally defined your SOE, you may even have gone to the lengths of already specifying the use of (just as an example) OpenJDK 8 and MariaDB 10.1. Does this mean you should actually include these in your build process?</p>
<p>The answer is almost always, <em>no</em>. Quite simply, adding these applications adds to the complexity of the build and to post-install configuration and debugging. It also reduces security<span>—but</span> more on that shortly.</p>
<p>Let's suppose we standardize on MariaDB 10.1 and include that in our base operating system image (and hence every single Linux machine deployed contains it), knowing that only a subset of the machines in operation will actually ever use it.</p>
<p>There are several reasons for not including MariaDB in the base image:</p>
<ul>
<li>An install of just the server components of MariaDB 10.1 takes around 120 MB, depending on your operating system and packaging<span>—</span>there will also be dependency packages but let's just start with this. Although storage is cheap and plentiful these days, if you deploy 100 servers across your environment (actually a small number for most enterprises), that's approximately 11.7 GB of space dedicated to a package you don't need. The actual figure will be far higher as there will be dependency packages to install and so on.</li>
<li>This may also have a knock-on effect on backups and the storage required for these, and indeed any virtual machine snapshots if you use that in the enterprise.</li>
<li>If an application arrives that requires MariaDB 10.3 (or indeed, the business decides to update its standard to 10.3), then the images need to be upgraded or possibly version 10.1 uninstalled before 10.3 is installed. This is an unnecessary level of complexity when a minimal Linux image could just have received an updated MariaDB workload.</li>
<li>You need to ensure that MariaDB is turned off and firewalled off when not required to as to prevent any misuse<span>—</span>this is an additional auditing and enforcement requirement that again is unnecessary on many servers where MariaDB isn't used.</li>
</ul>
<p class="mce-root">There are other security considerations too, but the key message here is that it is wasteful on resources and time. This doesn't, of course, only apply to MariaDB 10.1<span>—</span>that is simply an example, but it serves to show that, as a rule, application workloads should not be included in the base operating system definition. Let's take a more detailed look at the security requirements for our builds now.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making your builds secure</h1>
                </header>
            
            <article>
                
<p>We have already touched on security and not installing or running unnecessary packages. Any running service provides a potential attack vector for an intruder, and whilst hopefully, you will never have one inside your enterprise network, it is still good practice to build the environment in a manner that is as secure as possible. This is especially true of services that come configured with default passwords (and in some cases, with no password configured at all<span>—</span>though this is thankfully becoming rare now). </p>
<p>These principles apply when defining the build itself too. Don't create a build with weak static passwords, for example. Ideally, every build should be configured to obtain even initial credentials from an external source, and although there are a myriad of ways to achieve this, you are encouraged to look up <kbd>cloud-init</kbd> if this is a new concept to you. There are cases, especially in legacy environments, where you may need some initial credentials to allow access to the newly built server, but reusing weak passwords is dangerous and opens up the possibility of the newly built server being intercepted before it is configured and some kind of malware planted on it.</p>
<p>In short, the following list provides some sound guidance on ensuring secure builds:</p>
<ul>
<li>Don't install applications or services that are not required.</li>
<li>Do ensure services that are common to all builds but require post-deployment configuration are disabled by default.</li>
<li>Don't re-use passwords even for initial access and configuration if at all possible.</li>
<li>Do apply your enterprise security policy as early as possible in the process<span>—</span>in the build process of the image or server if possible, but if not, as soon as possible after installation.</li>
</ul>
<p>These principles are simple yet fundamental, and it is important to adhere to them. Hopefully, a situation will never arise where it matters that they have been applied, but if it does, they might just stop or sufficiently impede an intrusion or attack on your infrastructure. This, of course, is a topic that deserves its own book, but it is hoped these pointers, along with the examples in <a href="3d4a9c0a-452f-4fbb-85c8-372149303613.xhtml" target="_blank">Chapter 13</a>, <em><span>Using CIS Benchmarks</span></em>, will point you in the right direction. Let's take a brief look now at ensuring our build processes are efficient.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating efficient processes</h1>
                </header>
            
            <article>
                
<p>Efficient processes are supported heavily by automation, as this ensures minimal human involvement and consistent, repeatable end results. Standardization also supports this, as it means that much of the decision-making process has already been completed, and so all people involved know exactly what they are doing and how it should be done.</p>
<p>In short, stick to these principles outlined in this book and your build processes will, by their very nature, be efficient. Some degree of manual intervention is inevitable, even if it involves choosing a unique hostname (though this can be automated) or perhaps the process of a user requesting a Linux server in the first place. However, from here, you want to automate and standardize wherever possible. We will follow this mantra throughout this book. For now, though, we will take a look at the importance of consistency in our build processes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensuring consistency across Linux images</h1>
                </header>
            
            <article>
                
<p>In <a href="c7596fb8-4971-44d7-943a-7660c5eecb17.xhtml" target="_blank">Chapter 1</a>, <em>Building a Standard Operating Environment on Linux</em>, we discussed the importance of commonality in SOE environments. Now that we are actually looking at the build process itself, this comes back to the fore as we are, for the first time, looking at how to actually implement commonality. Assuming Ansible is your tool of choice, consider the following task. We are writing playbooks for our image build process and have decided that our standard image is to synchronize its time with our local time server. Suppose that our base operating system of choice is Ubuntu 16.04 LTS for historic reasons.</p>
<p>Let's create a simple role to ensure<span> NTP</span> is installed and to copy across our corporate standard<span> </span><kbd>ntp.conf</kbd>,<span> </span>which includes the addresses of our in-house time servers. Finally, we need to restart<span> </span>NTP<span> </span>to pick up the changes.</p>
<div class="packt_infobox">The examples in this chapter are purely hypothetical and given to demonstrate what Ansible code for a given purpose might look like. We will expand on the tasks performed (such as deploying configuration files) in detail in later chapters and provide hands-on examples for you to try out.</div>
<p>This role could look like the following:</p>
<pre>---<br/>- name: Ensure ntpd and ntpdate is installed<br/>  apt:<br/>    name: "{{ item }}"<br/>    update_cache: yes<br/>  loop:<br/>    - ntp<br/>    - ntpdate<br/>- name: Copy across enterprise ntpd configuration<br/>  copy:<br/>    src: files/ntp.conf<br/>    dest: /etc/ntp.conf<br/>    owner: root<br/>    group: root<br/>    mode: '0644'<br/>- name: Restart the ntp service<br/>  service:<br/>    name: ntp<br/>    state: restarted<br/>    enabled: yes</pre>
<p>This role is simple, concise, and to the point. It always ensures the<span> </span><kbd>ntp</kbd><span> </span>package is installed, and also ensures we are copying across the same version of our configuration file, making sure it is the same on every server. We could improve this further by checking this file out of a version control system, but that is left as an exercise for you.</p>
<p>Instantly, you can see the power of writing an Ansible role for this one simple step<span>—</span>there is great consistency to be achieved from including this role in a playbook, and if you scale this approach up to your entire enterprise, then all configured services will be consistently installed and configured.</p>
<p>However, it gets better. Let's say that the business decides to rebase the standard operating system to Ubuntu 18.04 LTS to make use of newer technologies and increase the supported lifespan of the environment. The<span> </span><kbd>ntp</kbd><span> </span>package is still available on Ubuntu 18.04, though by default, the<span> </span><kbd>chrony</kbd><span> </span>package is now installed. To proceed with<span> </span>NTP, the role would need only minor tweaks to simply ensure that<span> </span><kbd>chrony</kbd><span> </span>is removed first (or you could disable it if you prefer)<span>—</span>after this, it is identical, for example, consider the following role code that ensures the correct packages are absent and present:</p>
<pre>---<br/>- name: Remove chrony<br/>  apt:<br/>    name: chrony<br/>    state: absent<br/>- name: Ensure ntpd and ntpdate is installed<br/>  apt:<br/>    name: "{{ item }}"<br/>    update_cache: yes<br/>  loop:<br/>    - ntp<br/>    - ntpdate</pre>
<p>We would then continue this code by adding two further tasks that copy across the configuration and restart the service to ensure it picks up the new configuration:</p>
<pre>- name: Copy across enterprise ntpd configuration<br/>  copy:<br/>    src: files/ntp.conf<br/>    dest: /etc/ntp.conf<br/>    owner: root<br/>    group: root<br/>    mode: '0644'<br/>- name: Restart the ntp service<br/>  service:<br/>    name: ntp<br/>    state: restarted<br/>    enabled: yes</pre>
<p>Alternatively, we could decide to embrace this change and make use of<span> </span><kbd>chrony</kbd><span> </span>on the new base image. Hence, we would simply need to create a new<span> </span><kbd>chrony.conf</kbd><span> </span>to ensure it talks to our enterprise NTP servers, and then proceed exactly as before:</p>
<pre>---<br/>- name: Ensure chrony is installed<br/>  apt:<br/>    name: chrony<br/>    update_cache: yes<br/>- name: Copy across enterprise chrony configuration<br/>  copy:<br/>    src: files/chrony.conf<br/>    dest: /etc/chrony.conf<br/>    owner: root<br/>    group: root<br/>    mode: '0644'<br/>- name: Restart the chrony service<br/>  service:<br/>    name: chrony<br/>    state: restarted<br/>    enabled: yes</pre>
<p>Notice how similar these roles all are? Only minor changes are required even when supporting a change in the base operating system or even underlying service.</p>
<p>Although these three roles differ in places, they are all performing the same basic tasks, which are as follows:</p>
<ol>
<li>Ensure that the correct NTP service is installed.</li>
<li>Copy across the standard configuration.</li>
<li>Ensure the service is enabled at boot time and has started.</li>
</ol>
<p class="mce-root"/>
<p>Hence, we can be sure that, using this approach, we have consistency. </p>
<p>Even when changing the platform entirely, the high-level approach can still be applied. Let's say that the enterprise has now taken on an application that is only supported on CentOS 7. This means an accepted deviation to our SOE, however, even our new CentOS 7 build will need to have the correct time, and as NTP is a standard, it will still use the same time servers. Hence, we can write a role to support CentOS 7:</p>
<pre>---<br/>- name: Ensure chrony is installed<br/>  yum:<br/>    name: chrony<br/>    state: latest<br/>- name: Copy across enterprise chrony configuration<br/>  copy:<br/>    src: files/chrony.conf<br/>    dest: /etc/chrony.conf<br/>    owner: root<br/>    group: root<br/>    mode: '0644'<br/>- name: Restart the chrony service<br/>  service:<br/>    name: chronyd<br/>    state: restarted<br/>    enabled: yes</pre>
<p>Again, the changes are incredibly subtle. This is a significant part of the reason for embracing Ansible as our automation tool of choice for enterprise automation<span>—</span>we can build and adhere to our standards with great ease, and our operating system builds are consistent if we change the version or even the entire distribution of Linux we are using.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>At this stage, we have defined our requirement for standardization, established which tools to use in our journey toward automation, and now taken a practical look at the fundamental types of environments into which enterprises can expect to deploy an operating system. This has set the groundwork for our automation journey and has provided us with the context for the rest of this book<span>—</span>a hands-on journey through the process of building and maintaining a Linux environment in the enterprise.</p>
<p>In this chapter, we learned about the different types of environments into which Linux might be deployed and the different build strategies available to each. We then looked at some practical examples of ensuring that our builds are of a high standard and can be completed efficiently and repeatably. Finally, we started to look at the benefits of automation and how it can ensure consistency across builds, even when we change the entire underlying Linux distribution.</p>
<p>In the next chapter, we will begin our hands-on journey into Enterprise Linux automation and deployments, looking at how Ansible can be employed to build out virtual machine templates, whether from cloud environment images or from scratch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What are the similarities between building a Docker container and an SOE?</li>
<li>Why would you not include MariaDB in your base build if it is only required on a handful of servers?</li>
<li>How would you ensure your base operating system image is as small as possible?</li>
<li>Why should you be careful about embedding passwords in your base operating system image?</li>
<li>How would you ensure all Linux images send their logs to your centralized logging server?</li>
<li>When would you not use a base image provided by a cloud provider and build your own instead?</li>
<li>How would you secure your SSH daemon configuration using Ansible?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li>For an in-depth understanding of Ansible, please refer to <em>Mastering Ansible, Third Edition</em> by <em>James Freeman</em> and <em>Jesse Keating</em> (<a href="https://www.packtpub.com/gb/virtualization-and-cloud/mastering-ansible-third-edition">https://www.packtpub.com/gb/virtualization-and-cloud/mastering-ansible-third-edition</a>).</li>
<li>To gain an understanding of the Docker code and discussion in this chapter, please refer to <em>Mastering Docker, Third Edition</em> by <em>Russ McKendrick</em> and <em>Scott Gallagher</em> (<a href="https://www.packtpub.com/gb/virtualization-and-cloud/mastering-docker-third-edition">https://www.packtpub.com/gb/virtualization-and-cloud/mastering-docker-third-edition</a>).</li>
</ul>


            </article>

            
        </section>
    </body></html>