<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer013">
			<h1 id="_idParaDest-99"><em class="italic"><a id="_idTextAnchor099"/>Chapter 4</em>: Designing System Deployment Architectures</h1>
			<p>How we deploy systems determines so much about how those systems will perform and how resilient they will be for years to come. A good understanding of design components and principles is necessary for us to understand in order to approach the design of the platforms that will carry our workloads. Remember, at the end of the day, only the applications running at the very top of the stack matter - everything beneath the applications, whether the operating system, hypervisor, storage, hardware, and others are just tools used to enable the final application-level workloads to do what they need to do best. It is easy to feel that these other components matter individually, but they do not. To put it another way, what matters is the results rather than the path taken to get to the results. </p>
			<p>In this chapter, we are going to start by looking at the building blocks of systems (other than storage which we tackled extensively in our last chapter before taking all of those components as a whole and looking at them, to see how they can form robust carriers for our application workloads. Next, we will look at need analysis. Then finally we will move on to assembling those pieces into architectural designs to meet those needs.</p>
			<p>By the end of this chapter, you should feel confident that, while Linux is potentially only one slice in the middle of our application stack, you are prepared to design the entire stack properly to meet workload goals. While technically much of this design is not strictly systems administration (or engineering) it most often falls to the system administrators to handle as only the rarest of organizations have highly skilled and end to end knowledgeable staff from other departments. The systems team sits at the nexus of all components and has the greatest single role visibility in both directions (up the stack to the applications and down the stack to hypervisors, hardware, and storage). It is natural that systems teams are tasked with the greater design tasks as there is no one else capable. </p>
			<p>In this chapter we are going to learn about the following:</p>
			<ul>
				<li><a id="_idTextAnchor100"/>Virtualization</li>
				<li>Containerization</li>
				<li>Cloud and <strong class="bold">Vitual Private Server</strong> (<strong class="bold">VPS</strong>)</li>
				<li>On premises, hosted, and hybrid hosting</li>
				<li>System design architecture</li>
				<li>Risk assessment and availability needs</li>
				<li>Availability strategies</li>
			</ul>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor101"/>Virtualization</h1>
			<p>Twenty years ago, if <a id="_idIndexMarker326"/>you asked the average system administrator what virtualization was they would look at you with a blank stare. We have had virtualization technologies in IT since 1965 when IBM first introduced them in their mainframe computer systems, but for your average company these technologies were relatively rare and out of reach <a id="_idIndexMarker327"/>until vendors like <em class="italic">VMware</em> and <em class="italic">Xen</em> brought<a id="_idIndexMarker328"/> these to the mainstream market around the turn of the millennium. The enterprise space did have many of these technologies by the 1990s, but knowledge of them did not disseminate far.</p>
			<p>Times have changed. Since 2005, virtualization has been broadly available and widely understood, with options for every platform and at all price points, leaving no one with a need to avoid implementing the technology because it is out of technical or financial reach. At its core, virtualization is an abstraction layer that creates a computer <em class="italic">in software</em> (on top of the actual hardware) and presents a standard set of virtual hardware. Software that performs virtualization<a id="_idIndexMarker329"/> is called a <strong class="bold">hypervisor</strong></p>
			<p>In the last chapter we spoke repeatedly about interfaces and how something consumes or presents itself as a disk drive or file system, for example. A hypervisor is software that presents a <em class="italic">computer interface</em>, meaning it doesn't just present a hard drive, but it acts like an entire computer. If you have never used or thought about virtualization this might seem extraordinarily complex and confusing, but in reality, this is an abstraction that often makes computing far simpler and more reliable. Just like technologies that abstracted <a id="_idIndexMarker330"/>storage (like <strong class="bold">Logical Volume Managers</strong> and <strong class="bold">RAID</strong> systems) proved <a id="_idIndexMarker331"/>to be incredibly valuable once they were mature and understood, so has computer level virtualization.</p>
			<p>There are two types of hypervisors that we will talk about in this chapter, and they are simply known as Type 1 and Type 2 hypervisors. All hypervisors present the same thing: a <em class="italic">computer</em>. But<a id="_idIndexMarker332"/> what makes a Type 1 and a Type 2 hypervisor different is what they consume. </p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor102"/>Type 1 hypervisor</h2>
			<p>Sometimes<a id="_idIndexMarker333"/> called a <em class="italic">bare metal</em> hypervisor, Type 1 hypervisors<a id="_idIndexMarker334"/> are <a id="_idIndexMarker335"/>intended to run directly on the system hardware but, of course, can run on anything presenting itself as a capable piece of hardware (such as another hypervisor!) As such, a Type 1 Hypervisor is not an application and does not run on top of an operating system and so only needs to worry about hardware compatibility with the physical device on which it will be installed.</p>
			<p>Type 1 hypervisors are generally the only type considered true ready for production because they install directly without any unnecessary software layers and so can be faster, smaller, and more reliable.</p>
			<p>The Type 1 hypervisor was more difficult to initially engineer and so the earliest hypervisors were generally other types that could pass off work to the operating system. But effectively it was the introduction of the Type 1 hypervisor and enough vendors with disparate products to warrant a mature market designation that encouraged the extreme move to virtualization in the 2000s. </p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor103"/>Type 2 hypervisor</h2>
			<p>Unlike the bare-metal<a id="_idIndexMarker336"/> hypervisor, a Type 2 hypervisor is an<a id="_idIndexMarker337"/> application that you install onto an operating system. This means that the hypervisor has to wait for the operating system to give it resources, competes with other applications for resources, and requires that the operating system itself is stable, in addition to the hypervisor being stable, in order to keep workloads running on top of it.</p>
			<p>When virtualization was relatively new, especially in the microcomputer arena, Type 2 Hypervisors were much more common because they were cheaper and easier to make and have little need for hardware support to do what they do. A Type 2 Hypervisor lets the bare metal operating system do the heavy lifting of supplying drivers and hardware detection, task scheduling, and so forth. So, in much of the 2000s we saw Type 2 Hypervisors taking a principal role in driving virtualization adoption. They are easy to deploy and very easy to understand and because they are just an application that gets deployed on top of an operating system anyone can just install one on an existing desktop or even laptop to try out virtualization for themselves.</p>
			<p>By the late 2000s, technology had changed rather significantly, the software had advanced and matured, and nearly all computers had gained some degree of hardware assistance for virtualization, allowing hypervisors to use less code while gaining much better performance. Type 1 hypervisors rapidly proliferated, and, before 2010, the idea of using a Type 2 hypervisor in production was all but unthinkable. Type 1 hypervisors provide a single, standard operating system installation target, moving the heavy lifting away from the operating system and over to the hypervisor, where it is generally accepted to be better positioned. Because the hypervisor controls the bare metal, it is able to properly<a id="_idIndexMarker338"/> schedule system resources and eek maximum performance<a id="_idIndexMarker339"/> out of a system. Hypervisors are expected to be only a small fraction of the size of an operating system. This means little more than a shim between virtualized operating systems and the physical system (a tiny layer of code doing the bare minimum, and being essentially invisible to the operating system running on top of it). This minimizes bloat and features, while operating systems need to be large, complex, and feature-rich to do their jobs well in most cases.</p>
			<p>Type 2 Hypervisors have proven to be useful in lab environments, especially for situations where testing or learning is best done from a personal computing environment such as a desktop or laptop or can be useful for special case temporary workloads where there is a need to completely disable or possibly even to remove the hypervisor when it is no longer needed. But for production server environments only Type 1 Hypervisors are really appropriate today.</p>
			<p>There are two best practices commonly associated with virtualization</p>
			<ul>
				<li>Virtualize every system, unless a requirement makes you unable to do so. In practical terms, you will never realistically see a valid exception to this rule.</li>
				<li>Always use a Type 1 (Bare Metal) Hypervisor for servers.</li>
			</ul>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor104"/>Hypervisor types are confusing</h2>
			<p>In the real world <a id="_idIndexMarker340"/>detecting what is and is not a Type 1 Hypervisor can be rather difficult. A hypervisor, by definition, really does not have any kind of end user interface of its own. This makes it something that we have to explain, but not something that we really see. Even a true operating system is hard to point to and say <em class="italic">see, there it is</em> because it is really a shell or a desktop environment running as an application on top of the operating system, rather than the operating system itself, that we see and touch. With a hypervisor, any interface that we see, of any sort, has to be being presented by something running on an operating system, not something running directly on the hypervisor.</p>
			<p>Hypervisors, of course, need some sort of interface for us to interact with them. How they handle this varies wildly and not all hypervisors, even of the same type, are built the same. Under the hood, of course, they are always running on the bare metal, but they can use several different architectures to handle all of the functions that they need. Each different architecture has a different opportunity for how it will seem to appear to an end user – meaning that an end user sitting down to the system may experience wildly different interfaces that pretend to be things that they are or possibly are not.</p>
			<p>In early Type 1 Hypervisors it was common to run a virtual machine (the name for a virtualized computer running on top of a hypervisor of any type) that had privileges to control the hypervisor given to it. This allows the hypervisor to be as lean as possible and allows big tasks like presenting a user interaction shell to be done using existing tools and no one had to reinvent the wheel. Using this approach meant that hypervisor engineering work was minimal in the early days allowing the virtualization itself to be the key focus.</p>
			<p>As time has progressed, alternative approaches have begun to emerge. Running a full operating system in a virtualized environment on top of the hypervisor just to act as an interface for the end user felt like a waste of resources. Later hypervisors used creative ways to get around this making the hypervisor itself heavier but reducing the overall weight of the total system.</p>
			<p>Virtualization tends to be confusing and vendors have little reason to want to expose the inner workings of their systems. It has therefore become commonplace to misuse terms or to suggest that hypervisors work differently than they do either for marketing reasons or to attempt to simplify the system for less knowledgeable customers. There are many hypervisors today and potentially more will arise in the future. In production enterprise environments, however, there are four that we expect to see with any regularity and we will briefly break down each one to explain how it works. No one approach is best, these<a id="_idIndexMarker341"/> are simply different ways to skin the same cat.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor105"/>VMware ESXi</h2>
			<p>The market <a id="_idIndexMarker342"/>leader of virtualization today. VMware is one of the oldest<a id="_idIndexMarker343"/> virtualization products and has changed its design over time. Originally VMware followed the classic design of an extremely lean hypervisor and a <em class="italic">hidden</em> virtual machine that ran on top of it running a stripped down copy of Red Hat Enterprise Linux which provided the end user facing shell for interaction with the platform. </p>
			<p>Today VMware ESXi instead builds a tiny shell into the hypervisor itself that provides only enough potential user interaction to handle the simplest of tasks such as detecting the IP address that is in use or setting the password. Everything else is handled through an API that is called from an external tool allowing for the heaviest portions of the user interface to be kept completely on the physical client workstation rather than on the hypervisor.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor106"/>Microsoft Hyper-V</h2>
			<p>Although late <a id="_idIndexMarker344"/>to the enterprise Type 1 virtualization game, Microsoft <a id="_idIndexMarker345"/>opted for the classic approach and always runs a virtual machine in which is contained a stripped down copy of Windows which provides the graphical user interface that end users will see when having installed Hyper-V. This first virtual machine is installed automatically, by default requires no paid licensing, does not contain Windows branding, and is contrarily named the <em class="italic">physical</em> machine which together can make it appear that there is no VM at all, but rather a Type 2 hypervisor running on top of a Windows install, but this is not the case. It simply appears so based on tricky naming and the unnecessarily convoluted standard install processes. Doubt not, Hyper-V is a true Type 1 hypervisor running in the most classic way.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor107"/>Xen</h2>
			<p>Coming from<a id="_idIndexMarker346"/> the same <a id="_idIndexMarker347"/>early era as VMware, Xen started with the classic approach, but, unlike Vmware, stuck with it over the years. However, unlike Hyper-V, Xen installations tend to be more manual, and the use of a first virtual machine for the purpose of providing end user interactions is not in any way hidden and, in fact, is completely exposed. What this means is that during the hypervisor installation process a virtual machine is created automatically (so it is always the first one) and that virtual machine is given special access to directly manipulate the console. So, what you see once it turns on is the console of the virtual machine itself as the hypervisor does not have one besides that.</p>
			<p>You <em class="italic">can</em> even choose between different operating systems to use in the management virtual machine! In practice, however, Xen is always used with Linux as its control environment. Other operating systems are mostly theoretical. This exposure makes hidden classic systems, like Hyper-V, all the more confusing because in Xen it is so obvious how it all works.</p>
			<p>Because Xen and Linux go together so tightly, it can be valuable for a Linux system administrator to have at least some knowledge of Xen, Xen management via Linux, and Xen architecture. This tight coupling does not ensure that systems and platform teams will become intertwined when using Xen, but it makes the likelihood higher.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor108"/>KVM</h2>
			<p>Finally, we <a id="_idIndexMarker348"/>come to the <strong class="bold">Kernel-based Virtual Machine</strong> (<strong class="bold">KVM</strong>). KVM<a id="_idIndexMarker349"/> is special for a few reasons. First because it uniquely takes the approach of merging the hypervisor into the operating system itself. And second because it does so with Linux. Unlike other hypervisors where you have a clear separation between the platform administration who manages the hypervisor and the system administrator who manages the operating system level. Here, the two roles must be merged into one because the two components have been merged into one. You cannot separate the operating system and the hypervisor when using KVM. KVM bakes the hypervisor right into the Linux kernel. It is simply part of the kernel itself and always there.</p>
			<p>This approach has clear benefits. It simplifies the entire system and provides most of the advantages of each different approach with relatively few caveats. There are caveats, of course, such as that there is a real risk of bloat in the hypervisor install which increases the potential attack surface. KVM's biggest benefit is probably that it leverages the ecosystem of Linux system administrators and existing knowledge so that some of the more complex aspects of managing a system that runs on bare metal such as filesystem and other storage architecture decisions, driver support, and hardware troubleshooting are all shared with Linux giving an enormous base platform and support network from which to begin.</p>
			<p>Because of KVM's easy and well-known licensing (due to its inclusion in Linux), well known development potential, and broad availability of components it has become far and away the most popular means of building your own hypervisor platform when vendors want to create something of their own. Many large vendors in the cloud, hypervisor<a id="_idIndexMarker350"/> management, or hyperconvergence<a id="_idIndexMarker351"/> space have leveraged KVM as the base of their systems with customizations layered on top.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor109"/>Is virtualization only for consolidation?</h2>
			<p>Ask most <a id="_idIndexMarker352"/>people why you <em class="italic">bother</em> to virtualize and consistently the same answer is repeated: <em class="italic">Because it allows you to run several disparate workloads on a single physical device.</em> There is no doubt that consolidation is a massive benefit, when it applies, but stating this as the only, or even the key, benefit means we are missing the big picture.</p>
			<p>The enduring myth that we virtualize to save money through consolidation is one that it seems no one is going to be able to dispel. The nature of virtualization is simply too complex for the average person, even the average IT professional, and what it really provides remains broadly misunderstood. The real value of virtualization is in the abstraction layer that it creates which provides a hedge against the unknown - a way to make system deployments more flexible, and more reliable, while incurring less overall effort. Virtualization gives you more options for that unknown event happening sometime in the future that you cannot plan for.</p>
			<p>A core challenge for virtualization is that it offers simply too many benefits that do not always relate to one another. Most people want a simple, stable answer and do not want to understand how exactly virtualization works and why adding a layer of additional code actually makes systems simpler and more reliable. It is all a bit too much. The reality is that virtualization has many benefits, each of which is typically enough to justify always using it, and essentially no caveats. Virtualization has no cost, essentially no performance overhead, does not add management complexity (it does, but only in some areas while reducing it in others resulting in an overall reduction.)</p>
			<p>Inevitably people will ask if special case workloads exist for which virtualization is not ideal. Of course special cases exist. But the next response from every IT shop on earth is to proclaim that they and they alone are unique in their server needs and that they are the one solitary case where virtualization does not make sense - and then they reliably state a stock and absolutely ideal workload for standard virtualization that applies to nearly everyone and is as far from a special case or an exemption from best practices as can be. Trust me, you are not the exception to this rule. Virtualize every workload, every time. No exceptions.</p>
			<p>Most examples that people give of why they avoid virtualization is normally examples of virtualization done wrong. Other, non-virtualization related mistakes, such as selecting a bad vendor, improperly sizing a server, or choosing a large overhead storage layer when something lean is needed could happen with or without virtualization. Everyone from the system administrator to the platform team to the hardware purchasers still have to do the same quality job that they would without virtualization. Virtualization is not a panacea but failing to be the silver bullet that removes the need to do our jobs well in no way excuses not using it every time. That is just bad logic.</p>
			<p>Now we should have a good understanding of what virtualization really is, instead of simply a passing knowledge of its utility, and know why we use it for all production workloads. Virtualization should become second nature very quickly whether you are working in your <a id="_idIndexMarker353"/>home lab or running a giant production environment. Make it a foregone conclusion that you will virtualize and only worry about little details like storage provisioning and hypervisor selection or management tools. Next we look at virtualizations alternative and close cousin, containerization.</p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor110"/>Containerization</h1>
			<p>Some people<a id="_idIndexMarker354"/> consider containers to be a form of virtualization, sometimes<a id="_idIndexMarker355"/> called <a id="_idIndexMarker356"/>Type-C virtualization or OS-level virtualization. In recent years, containers have taken on a life of their own and very specific container use cases have become such buzz-worthy topics that containers as a general concept have been all but lost. Containers, however, represent an extremely useful form of (or alternative to) traditional virtualization.</p>
			<p>Container-based virtualization varies from traditional virtualization in that in traditional virtualization every aspect of system hardware is replicated in software by the hypervisor and exists uniquely to every instance or virtual machine (often called a Virtual Environment (VE) when <a id="_idIndexMarker357"/>talking about containers) running on top of it. There is nothing shared between the virtual machines and by definition any operating system that supports the hardware virtualized can run on it exactly as if it was running on bare metal.</p>
			<p>Container-based virtualization does not use a hypervisor at all, but rather is built from software that heavily isolates system resources in an operating system allowing individual virtual machines to be installed and operate as if they are fully unique instances, but behind the scenes all virtual machines running as containers share the host's kernel instance. Because of this, the ability to install any arbitrary operating system is limited as only operating systems capable of sharing the same kernel can be installed on a single platform.</p>
			<p>Because there is no hypervisor and only a single kernel shared between all systems, there is nearly zero overhead in most container systems making it perfect for many highly demanding tasks. Containers are especially popular in Linux where many container options exist today. The almost total lack of system overhead in container systems used to <a id="_idIndexMarker358"/>be a key feature of the approach, but as systems have moved from resource tight to having a surplus of power in many cases, the value of squeezing every last drop out of hardware has begun to wane in comparison to the greater flexibility and isolation of full virtualization. Because of this, what sounds like the greatest virtualization option ever is often overlooked or even forgotten about!</p>
			<p>Containers do not require any special hardware support and are implemented completely in software allowing them to exist on a broader variety of platforms (it is easy to implement on old 32bit Intel hardware or a Raspberry Pi, for example) at lower cost. This made them important in the era before hardware acceleration was broadly available for full virtualization technologies.</p>
			<p>In the Linux world, which is what we care about, we can run many disparate Linux-based operating systems on a single container host because only the kernel needs to be shared. So, running virtual machines of Ubuntu, Debian, Red Hat Enterprise Linux, SUSE Tumbleweed, and Alpine Linux all on a single container host is no problem at all.</p>
			<p>Linux is blessed (and cursed) with a plethora of options for nearly every technology, and containerization is no exception. Multiple open source and commercial container products exist for Linux, but today the undisputed reigning champion is <strong class="bold">LinuX Containers</strong> (<strong class="bold">LXC</strong>). LXC is <a id="_idIndexMarker359"/>unique in that it is fully built into the Linux kernel so utilizing it is simply a matter of doing so, it does not require additional software or kernel modifications. If you are going to be implementing real containers on Linux, chances are it is going to <a id="_idIndexMarker360"/>be LXC. LXC is fully supported in nearly all Linux-based operating systems.</p>
			<h3>A little history of containers</h3>
			<p>Full virtualization was <a id="_idIndexMarker361"/>introduced by IBM in the 1960s but proved to be complex and did not make it into general availability for mainstream servers for decades as high end hardware support and extensive special case software was necessary to make the magic happen until the very end of the 1990s.</p>
			<p>Containers were first introduced in System 7 UNIX in 1979 using a mechanism called <em class="italic">chroot jails</em> which is rudimentary by today's standards, but is functionally pretty close to modern containers. In the UNIX world containers, of one type or another, have almost always been available. In 1999 what we might consider truly modern containers, starting with FreeBSD's Jails, were introduced and rapidly other UNIX platforms like Solaris with Zones and Linux with OpenVZ and then LXC began to emerge. By the mid-2000s containers were everywhere and quite popular before truly effective full virtualization had taken off.</p>
			<p>Containers saw a Renaissance of sorts in 2013 with the introduction of Docker. Docker is not exactly a container, however, even though the term container has become more associated with Docker than with actual, true containers. Prior to Docker, containers were never really seen as being very sexy but rather basic process isolation workhorses doing a rudimentary security job for the operating system with the possible short lived exception of Solaris Zones which were heavily promoted for a short time in conjunction with the release of ZFS.</p>
			<p>Today, because of the popularity of Docker and its association with containers, the majority of people (even including system administrators!) think of Docker when someone mentions containers rather than true containers which have been around for decades.</p>
			<p>We cannot talk about containers without mentioning Docker. Docker, today, is the name most associated with containers, and for good reason. Docker originated as a set of extensions built on top of LXC to provide for extreme process isolation with a <em class="italic">packaged</em> library environment for said applications. While Docker uses containers, first LXC and now their own container library, it itself is an application isolation environment providing a more limited range of services than something like LXC will provide. With LXC, you deploy an operating system (sans kernel) and treat it all but identically to traditional full virtualization. With Docker you are deploying an application or service, not an operating system. The scope is different and so Docker really finds itself more applicable to <a href="B16600_05_Final_ASB_ePub.xhtml#_idTextAnchor128"><em class="italic">Chapter 5</em></a>, <em class="italic">Patch Management Strategies</em>. Because Docker is an application layer containerization, it would most often be run on top of a virtual machine in either full virtualization or containerization to provide its underlying operating system component.</p>
			<p>Containers, in general, represent a trusted, mature, and ultra-high performance virtualization option (or alternative.) While more limited in their capabilities (a Linux container host can <a id="_idIndexMarker362"/>only run Linux VEs, while FreeBSD VEs are not possible, for example) they are otherwise easier to maintain, faster, and generally more stable (there is simply less to go wrong.) They can be created faster, turned on or off faster, patched faster, are more flexible in their resource usage (they don't require the strict CPU and RAM assignments typically needed with full virtualization), need fewer skills, and have less overhead when running. The only significant caveats to containers are the inability to mix operating system workloads, or even kernel versions. If anything, that you do requires a specific kernel version (that is not uniform across the entire platform), or custom compilation of the kernel, a GUI, ISO or similar based full installs then containers simply are not flexible enough for you. But if you are dealing with a pure Linux environment where all workloads are Linux and can share the kernel, which is not that uncommon, then containers can be ideal for you.</p>
			<p>Containers, at least thus far, are not able to leverage the graphical interface of the operating system and so are relegated to server duties where pure text based (aka TTY) interfaces are used. Thus, they <a id="_idIndexMarker363"/>are not an option for graphical terminal servers or <strong class="bold">virtual desktop instance</strong> (<strong class="bold">VDI</strong>) deployments. Those kinds of workloads still need full virtualization until someone builds a workaround for that. But as that is generally not a heavily desired workload to support, there is little chance that someone is going to invest heavily in tackling that already easy to solve problem just to be able to say that they did it with containers.</p>
			<p>Through the use of containers, you allow the operating system itself to act as a hypervisor, but one that is still the operating system as well and can be managed using all of the normal Linux tools and techniques because the system is still Linux. There is no separate hypervisor to learn or maintain. In this way the ability to leverage existing Linux skills and tools is very good.</p>
			<p>It has to be noted that, because KVM and containers both use a standard, bare-metal Linux installation as their base, and because both are baked directly into the stock vanilla Linux kernel, it is not only possible but actually not uncommon for systems to run both full virtualization and containerization on the same host at the same time with vanilla Linux workloads running in containers. This ensures the lower overhead and extra flexibility and non-Linux (primarily Windows) or custom-kernel Linux systems running in full virtual machines on KVM. There is no reason to have to choose only one approach or the other if a blend is better for you. Container technology has become extremely popular and important today, but the use of containers in their traditional sense has dwindled heavily. This is mostly because of deep misunderstandings of the terminology and technology, which has caused it to be ignored even when it is highly appropriate. As a Linux system administrator especially, it may be very beneficial to consider containers<a id="_idIndexMarker364"/> instead of traditional virtualization in your environments. Having containers as another tool in your proverbial toolbelt makes you more flexible and effective.</p>
			<p>Next, we will apply what we have learned about virtualization and containers, and add management, to learn about cloud computing.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor111"/>Cloud and VPS</h1>
			<p>Any discussion of<a id="_idIndexMarker365"/> virtualization today is inevitably going to lead us to cloud. Cloud has become, that hot decade-long buzz-worthy concept that everyone wants, most people use, and no one has a clue what it is, what it means, or why anyone uses it. Few technologies are more totally misunderstood, yet widely talked about, than cloud. So we have a lot to cover here, much of it clearly up misconceptions and the misuse of terms.</p>
			<h3>The bizarre confusion of Cloud</h3>
			<p>It is a rare <a id="_idIndexMarker366"/>combination of being vastly technical and non-applicable to normal business conversations while being constantly discussed as if it were a casual high level non-technical business decision at nearly all levels. Considering only a minuscule fraction of IT professionals have any serious grasp of what cloud is, and even fewer have a clear understanding of when to choose it, that the average non-technical mid-level manager will toss around the term as if they were discussing the price of postage stamps is mind-boggling. What do those people even think that they are discussing? No one truly knows. And I mean that, sincerely. </p>
			<p>Ask a group of people who have been throwing about the term cloud. Separate them so that they are not stealing each other's answers. Now, ask them to describe what cloud means to them. Mostly you will get gibberish, obviously. When you drill down, however, you will get a variety of descriptions and answers that are nothing like one another, and yet people listening to others discussing cloud will generally state that they believe that all of those people were meaning the same thing and often that <em class="italic">one thing</em> is something none of them actually meant, let alone more than one of them. Truly cloud means something different, random, and meaningless to nearly every human being with no rhyme or reason to it.</p>
			<p>If cloud as a term had a musical equivalent, it would be Alanis Morisette's Ironic, the song where the only thing ironic about it is the title. The term cloud is the same, everyone uses it, no one knows what it means. Just like ironic.</p>
			<p>For quite some years, cloud was commonly used to mean <em class="italic">hosted</em>. Simply replacing a long established, well known industry term with another one for no good reason. Of course, cloud means nothing of the sort. This horrific misinterpretation led to the meme of <em class="italic">there is no cloud, just someone else's computer</em>. Of course, even a passing knowledge of cloud would make one cringe to hear someone state something so profound while getting what cloud means so incredibly wrong.</p>
			<p>Today, you are more likely to hear cloud used to mean <em class="italic">built with HTML</em>, I kid you not. Or sometimes it means <em class="italic">platform independent</em>. Other times it means <em class="italic">subscription pricing</em>. You name it, and cloud has been used to refer to it. The only thing you can be confident in is that no one, ever, actually means cloud. It could mean almost anything else, but it never means what the term actually is meant to refer to.</p>
			<p>The only benefit to the mass hysteria around cloud definitions is that no alternative definition is used commonly enough to rise and overtake true cloud. The problem, however, is so bad that there is no reasonable way for you, as an IT professional, to use the term <em class="italic">cloud</em> with anyone except a truly well read and trusted technical colleague that you know actually knows what it means.</p>
			<p>What is truly amazing is that the use of <em class="italic">cloud</em> has replaced terms like <em class="italic">synergy</em> as the default joke in business – that is, as a term only used by those who are truly lost. It is such common knowledge that <em class="italic">cloud</em> is complex and completely misunderstood that you can never use it to communicate an idea. It has become a standard example of a <em class="italic">marker</em> in the language for someone who is just spouting off management-speak without having any idea what they are saying and not realizing that everyone else is silently laughing at their ineptitude, and yet you hear it repeated almost constantly! No matter how much everyone knows that they are misusing it, somehow it remains addictive and in constant use.</p>
			<p>One of the most important takeaways from this seeming rant (and rant it is) is that you cannot use the term cloud to any but the most elite professionals and you cannot explain cloud to any but well educated IT professionals. Avoid using the term because, no matter how much you think that you can use it in the same mistaken way that everyone else is using it, you cannot. There is no way to use cloud in a way that can be understood because<a id="_idIndexMarker367"/> everyone believes that they know what it means, even though they all think that it is something unique.</p>
			<p>When you absolutely must refer to cloud for some reason, use more complete terms like <em class="italic">cloud computing</em> or <em class="italic">cloud architecture</em> to clarify that you actually mean cloud and not just throwing out a word for the listener to interpret at will.</p>
			<p>In many ways, it is almost easier to describe cloud by what it is not, rather than what it is, because everyone thinks that it is one thing or another. Cloud has no association with hosting, none with the web, not even any with the Internet (the thing sometimes referred to as THE cloud, as opposed to A cloud.) We cannot here go into all details explaining every possible aspect of cloud computing, nor would much of it be applicable as the majority of cloud is not related to systems, and therefore not related to Linux. But we should address, to a small degree, what it means to the Linux system administrator, when it is applicable, and so forth.</p>
			<p>First, we must start <a id="_idIndexMarker368"/>with the <strong class="bold">NIST</strong> (the <strong class="bold">National Institute of Standards and Technology</strong> in the USA) cloud abstract definition which works from Amazon's original definition. Always keep in mind that cloud computing is a real, strict, technical term created by Amazon for a real-world architecture and therefore has a strict, non-fungible definition and no amount of misuse or misunderstanding or attempts to co-opt its use by others changes that it means an extremely specific thing. It is common for those who do not understand cloud to argue that it is a loose term that can mean what you want it to mean, but it is not. That is simply not the case, it is not a random English language word making its way into the lexicon organically, it was defined carefully within the industry before first use.</p>
			<p>The NIST definition is as follows: <em class="italic">Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. This cloud model is composed of five essential characteristics, three service models, and four deployment models.</em></p>
			<p>The most <a id="_idIndexMarker369"/>important parts of this definition to us, as system administrators, are the parts that include <em class="italic">pool of resources</em>, with <em class="italic">rapid provisioning and release</em>. So shared resources (meaning servers, CPUs, RAM, storage, networking) that we can create and destroy access to quickly. While cloud certainly means more than that, those are the basics. If you immediately thought to yourself <em class="italic">that sounds a lot like what virtualization already does</em>, you are correct, there is an extreme degree of overlap, and virtualization is the key building block of cloud computing (both full virtualization and/or containers.) If you thought to yourself <em class="italic">wait, pooled resources that I can build AND destroy rapidly - those do not sound like useful characteristics to me or any environment I have worked in previously</em>, you are also correct. Cloud computing is not logically applicable to traditional workloads or environments, it is designed around extremely specific needs of purpose-built application architectures that few businesses are prepared to leverage on any scale.</p>
			<p>What makes the use of <em class="italic">cloud</em> more confusing is that many vendors (and quite logically at that) use cloud themselves as a component of their products. So, when you ask your vendor if a product is cloud-based, they might be answering if they are providing you cloud itself as a product, or they might be answering if they use cloud in the building of the tool somewhere under the hood. The two are both applicable to how most people ask the question, and since no one knows what cloud really is, no one is sure what you are really asking or want to know. Let me give a contrived, but reasonable, example.</p>
			<p>If I were to purchase a legacy application, say a client-server application that uses MS SQL Server and an old Delphi (Objective Pascal) front end and then use a true cloud product to create a virtual machine and deploy the server-side components so that we can truly say that we built the solution on a cloud. Yet the resultant product is not cloud computing, in any sense. Just because one piece of an architecture is built on cloud does not imply that the final product is cloud. Cloud is a layer in the stack.</p>
			<p>For us, as system<a id="_idIndexMarker370"/> administrators, we are concerned with the type of cloud knows as <strong class="bold">Infrastructure as a Service</strong> (<strong class="bold">IaaS</strong>). That's a fancy way of <a id="_idIndexMarker371"/>saying cloud-based virtual machines. Other types of cloud, like <strong class="bold">Platform as a Service</strong> (<strong class="bold">PaaS</strong>) and <strong class="bold">Software as a Service</strong> (<strong class="bold">SaaS</strong>), are very important in the cloud space but exist only when the system administrator is <em class="italic">somewhere else</em>. If we are the system administrator for PaaS or SaaS then, to us, cloud is the workload, and it is not cloud to us. If we are not the system administrator for a PaaS or SaaS system, then it is of no concern to us as system administrators to talk about those systems as they do not apply to our role.</p>
			<p>From our systems perspective, cloud is almost like an advanced, flexible virtualization layer. Of course, like virtualization, we may find ourselves tasked with being the ones to implement the cloud platform. That is a completely different animal and worthy of a book (or two) on its own. But in practical terms system administrators may consume platform resources from a hypervisor, a container engine, or either one orchestrated through a cloud interface. To us it is all the same - a mechanism on which we deploy an operating system. So, from a pure system administrator perspective, think of cloud computing no<a id="_idIndexMarker372"/> differently than any other virtualization because it is exactly the same. The only difference is how it is managed and handed off to us.</p>
			<p>In practical terms, we likely have to be heavily involved in decision making around the recommendation for the use of cloud versus other paths to acquire virtualization. Like any business decision, this simply comes down to evaluating the performance and features offered at a price point and comparing to the performance and features at the same price point with other options. It is that simple. But, with cloud, due to all the reasons that we mentioned before, we are often fighting against a mountain of misinformation and a belief in magic. So, we need to talk a little about cloud in a way that we should not have to simply because these misunderstandings are so common and deeply rooted.</p>
			<p>First, there is a belief that cloud computing is cheap. And while in special cases cloud computing will potentially save a lot of money, this is rarely the case. Cloud computing is typically an extreme price premium product chosen because it allows a great degree of flexibility so that less is needed to be purchased overall to meet the same needs. Cloud computing is very expensive to provide and so vendors are forced to charge more than for other architectures in order to provide it to customers (keep in mind that the <em class="italic">vendor</em> might be in your internal cloud department, nothing about cloud implies an external <a id="_idIndexMarker373"/>vendor.)</p>
			<h3>Horizontally scalable elastic workloads</h3>
			<p>Attempting to<a id="_idIndexMarker374"/> describe what workloads cloud computing was built to address can be a challenge for those not already familiar with certain types of application architecture. We must take an aside and dive into some application concepts here to really understand how we related as the systems team and to see why different approaches play such a large role in our platform decision making at this level.</p>
			<p>In a traditional application design the expectation is that we will run the entire application on just one or maybe just a few operating system instances. Typically, one instance would be used as the database server and one as the application server. More roles might exist, and you could have a redundant database server or similar, but essentially the number of instances usable was quite limited and static. Once deployed, the number of instances would not change. In many cases the entire application would exist on a single operating system instance.</p>
			<p>Scaling a traditional application mostly focuses on increasing the power of a single operating instance. This might be done through some combination of faster CPUs, more CPUs, more CPU cores, more memory, more storage, or faster storage. Or as we would typically say, if you need your server to do more, you need a bigger server. This kind of performance improvement can go a really long way as top end servers are very powerful, and few companies need to run any workloads that exceed the performance capabilities of a single large server. This style of scaling is called vertical scaling, meaning that we improve the performance of the single thread or server <em class="italic">within the box</em>. This kind of scaling is by far the easiest to do and works for any kind of application no matter how it is designed (this is how you improve video game performance or any desktop workload).</p>
			<p>For most people, workloads designed for vertical scaling are the only types of workloads that they know. Of course, for end users working on desktops, everything is vertical. Even system administrators almost exclusively have to oversee applications that are built for this kind of scaling only. Nearly all deploy in house applications assume that this is how you will scale and only recently do many developers know alternatives well and still many (potentially most) still do not, even though those that do are the more prominent in media.</p>
			<p>The alternative approach is to design applications that allow for the application to scale by adding additional, isolated operating instances. For example, running multiple database server instances (likely in a cluster) not just for resilience, but also performance. Running multiple application servers with the ability to simply add more operating system instances running the application while keeping each individual instance small. In a traditional application architecture, we might require a single application server with four high performance CPUs and 1TB of RAM to handle our application workload. A horizontally scalable application might use sixteen smaller servers each with 64GB of RAM and a smaller CPU to handle the same load. Traditionally we would say that our systems <em class="italic">scaled up</em>, but in adding more instances we say that they <em class="italic">scale out</em>. Now, of course, you can always scale both <em class="italic">up and out</em> which would mean increasing the resources of each individual instance while also increasing the number of instances. </p>
			<p>As you can imagine, few applications that we work with in the real world as end users or as system <a id="_idIndexMarker375"/>administrators are designed for or could leverage horizontal or <em class="italic">scale out</em> platforms effectively, if at all. It requires that the application architectures, analysts, and developers plan for this style of deployment from the very beginning. And no amount of planning or desire makes every workload capable of scaling in this manner.</p>
			<p>Some applications like common web-based business processes, most websites, email systems, and so forth are very conducive to this type of design and you can easily find or make these kinds of applications to take advantage of these resources. Other applications like financial processing or inventory control systems may struggle with the design limitations and take far more work to be able to work in this way, if at all.</p>
			<p>Just because a workload is designed by the development team to be horizontally scalable does not mean that the workload itself will, however. This is easiest with a simple example. You create a website that helps people choose a healthy breakfast. You market to the United States. From 6am Eastern until about 2PM eastern (when California is wrapping up breakfast) you are really busy, but outside of those hours your website is really slow. But another website helps people choose food for any meal and is marketed worldwide. This second site never gets quite as busy as the first but stays roughly as busy all day long. The first site can leverage scalability, the second site cannot because its resource needed never really change.</p>
			<p>The key advantages to horizontally scalable workloads are that they can be grown rapidly. Adding an additional operating system instance (or an additional one hundred!) is easy and non-disruptive. Adding more CPU or RAM to your existing server, is hard and slow by comparison. The next step of horizontal scaling is making it elastic. To be elastic, your system does not only have to scale out quickly but allow you to also scale back in quickly: that is to spin down and destroy unneeded operating system instances when capacity has changed. This is the unique proposition of cloud computing, to provide capacity on demand for elastic, horizontally scalable workloads so that you can use resources only when needed and stop using them when you do not.</p>
			<p>Vertically scaled resources are far less expensive to provide than horizontally scaled ones. You can test this by trying to assemble several computers with roughly the same specifications. With only rare exceptions, it is a fraction of the cost to build a single large server than several smaller ones using real-world economics. A single system requires only a single operating system and application instance, but multiple systems require the overhead of the same operating system and applications loaded into memory in each case wasted <a id="_idIndexMarker376"/>many resources there, as well. It just takes less management power to oversee something that is <em class="italic">faster</em>, rather than many slower tasks. It is not unlike managing humans. It takes less overhead to manage one really fast, efficient employee than it does to manage several slower employees trying to coordinate doing the same work that the faster one was doing.</p>
			<p>So a horizontally scaled system, in order to make sense to choose, has to be able to leverage both scaling out, as well as in, have a workload use case that actually leverages this, and that does so to an extent large enough to overcome the lower cost, lower overhead, and great simplicity of traditional designs. Unless your workload meets all of those requirements, cloud computing should not be a consideration for you at all. It simply does not apply. And while contrary to how the media and trend-happy IT professionals want to portray cloud, it is only a small percentage of workloads that can effectively leverage cloud and only a small percentage of businesses have those rare workloads at all.</p>
			<p>Of course, we are talking IaaS aspect of cloud. Other cloud aspects where only the application portion is exposed to the business will often be cloud based. But this is essentially unrelated and certainly a decision process totally different from anything we would be looking at in a tomb such as this.</p>
			<p>Second, there is a belief that cloud computing is reliable. Absolutely nothing in cloud definitions or designs implies reliability in any way. In fact, this runs completely contrary to all standard cloud thinking. Cloud computing, because it is useful exclusively with scale out design, is built on the assumption that any redundancy or reliability is built into the application itself as scale out implies - as you have to have this inside the application itself in order for scale out to work properly. So including any redundancy at the system or platform level would be nonsensical and counterproductive. A basic understanding of cloud computing should, with any thought, make us surprised if anyone expected redundancy beyond the minimum at this level. In the real world, cloud computing resources tend to be far more fragile than traditional server resources for exactly this reason. Cloud computing assumes that either reliability is of trivial importance or that it is provided elsewhere in the stack. Cloud computing is just a building block of the resulting system, it is not in any way a complete solution by itself. Of course, theoretically, a high availability cloud provider could arise, but their cost and performance caveats would make it hard to compete in a marketplace driven almost entirely by price.</p>
			<p>Third, there is a belief that cloud computing is broadly applicable, that every company should be using it, and that it is replacing all other architectures. This is not true at all. Cloud has been <a id="_idIndexMarker377"/>around now for more than fifteen years (at the time of writing) and it made its inroads quite quickly towards the beginning of that cycle. Today, cloud computing is mature and well known. Companies and workloads that are going to move to cloud (or design for it) have largely already done so, and new workloads are created on cloud at a roughly constant rate. The industry saturation rate for cloud computing has been more or less achieved. Some new workloads will go there as older ones or anti-cloud holdouts retire or give in to other pressures. Some will come back as overzealous cloud fanboys and buzz-word driven managers learn their lessons of having gone to cloud without any understanding or planning. Sending standard workloads to cloud computing without redesign is typically costly and risky. But by and large cloud computing has already settled into a known saturation rate and the computing world is as it will be until another exciting paradigm shift occurs. Basically, what we see today in advanced bespoke internal software and grand scale multi-customer software is ideal for the cloud paradigm, and traditional workloads for single customers remain the most beneficial on traditional paradigms. This is all as originally predicted when cloud computing was first announced long ago.</p>
			<p>Using cloud does not require any specific skills or training, as many in the industry would like us to believe in order to sell certifications and training classes. In fact, just knowing what cloud truly is often enough to enable you to utilize it effectively. That said, individual cloud vendor platforms (such as <strong class="bold">Amazon</strong>'s <strong class="bold">AWS</strong> or <strong class="bold">Microsoft</strong>'s <strong class="bold">Azure</strong>) are so large and convoluted that there can be real value to getting vendor certifications and training to understand how to work with their product interfaces. But to be clear, the value in the training is learning how to work with the vendor in question, not in learning about cloud.</p>
			<p>That does not change the fact that most organizations seeking to get significant value out of cloud computing will most likely need to do so with deep vendor integrations that will almost certainly require an investment in specific vendor product knowledge.</p>
			<p>Cloud is an amazing set of technologies that serves an incredibly important purpose. When your workload is right for cloud computing, nothing else can come close to it. Whether you build your own private cloud or use a public shared one, whether you host your cloud in house or let a hosting firm handle the data center components for you, cloud might be the right technology for some of your workloads. With your understanding here you should be able to evaluate your needs to know if cloud is likely to play any reasonable role, and be able to look at real work vendors and costs and make solid, math-based valuations of cloud in comparison to other options.</p>
			<p>Now that we <a id="_idIndexMarker378"/>know cloud computing, we can step back and look at the older concept of virtual private servers and see why they are so closely tied with, but not actually related to, cloud computing today.</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor112"/>Virtual Private Servers (VPS)</h2>
			<p>Similar to and, on <a id="_idIndexMarker379"/>the Venn Diagram of things, nearly overlapping with IaaS cloud computing is the modern concept of virtual private servers or VPS. VPS actually predates cloud and comes from simpler virtualization (or containerization) allowing a vendor (which could be an internal department, of course) to carve out single virtual machines for customers from a larger, shared environment. Instead of needing to provide an entire server of their own, customers need only buy a small slice or set of slices of the vendor's server(s) to use for their needs.</p>
			<p>As I mentioned, this sounds very similar to IaaS cloud that we just described, and certainly it is. So much so, that most people using IaaS cloud actually use a VPS aspect of it without realizing so. The idea behind VPS is to allow companies to purchase server-class resources at a fraction of the scale typically required to a single physical server. If you think back to our discussion on virtualization and how by using a hypervisor we might be able to take a single physical server and, for example, create one hundred virtual machines that run on top of it, each with their own operating system, then we could sell those resources to one hundred separate customers, each of which could run their own small server inside its own secure space. This allows small companies, or companies with small needs, to buy enterprise level datacenter and server hardware capacity and <a id="_idIndexMarker380"/>prices within any realistic budget.</p>
			<p>Before we go further, we need to do a quick breakdown and comparison of VPS against IaaS cloud to see why VPS is so commonly confused with cloud computing and why they often compete:</p>
			<ul>
				<li><strong class="bold">First, the goals</strong>: An IaaS <a id="_idIndexMarker381"/>Cloud's goal is to provide <a id="_idIndexMarker382"/>rapid creation and destruction of resources on demand via automation - primarily used by the largest organization or workloads. The goal behind VPS is to carve up traditional server resources in such a way that they can be affordable to be used by small organizations and/or workloads. So, one goes after the biggest scale and the most complexity, the other after the smallest scale and least complexity. One expects custom engineering effort on both the application and infrastructure team's sides where the other expects traditional applications and no special knowledge or accommodation from any team.</li>
				<li><strong class="bold">Second, the interface</strong>: In cloud computing the expectation and purpose is for systems to be self-provisioning (and self-destroying.) Cloud is not designed for humans to have to interact manually to request resources, nor to configure them, nor to decide when more (or less) are needed, nor to destroy them when done. So, cloud's focus is on APIs to allow software to handle provisioning. VPS is meant to work just like any normal virtual machine with a human initiating the build, installing the operating system, maybe configuring the operating system, and turning the VM off, and then deleting it when no longer needed. It is standard for cloud products to not offer any interaction directly with the virtualized hardware such as access to a console so any system requiring console level interaction (such as a GUI) are impossible. To qualify as a VPS console and GUI access are required to completely mimic a hardware device in a standard way. If you can use a normal server, you can use a VPS.</li>
				<li><strong class="bold">Third, the provisioning</strong>: Cloud assumes a need for rapid provisioning. Of course, rapid is a relative term. But it is just expected that in a cloud ecosystem that systems must be able to go from first request to fully functional in minutes, and sometimes seconds. In the VPS world, while we always want everything available as quickly as possible, having to wait a few minutes before being given the access to begin a manual operating system install that could take potentially tens of minutes is common. We assume that a cloud instance will be created via software, but a VPS instance we assume will be created manually by a human.</li>
				<li><strong class="bold">Fourth, the billing</strong>: Because the value of cloud computing is assumed to come from its ability to be created and destroyed rapidly in order to keep costs managed it follows that billing must be granular to accomplish this. To this end billing it generally handled in increments of minutes or possibly hours, or in other extremely <a id="_idIndexMarker383"/>short measurements like<a id="_idIndexMarker384"/> processor cycles. VPS will sometimes charge in these short increments but may easily use longer intervals such as daily or monthly as it is not a rapid create and destroy intended service. (We can say that cloud leans towards stateless and VPS leans towards stateful.)</li>
			</ul>
			<p>What often makes VPS and IaaS Cloud harder to distinguish is that today, VPS providers almost always use IaaS Cloud as their own mechanism for provisioning the VPS under the hood, and most IaaS Cloud providers have opted to offer VPS additionally. This was bound to happen for two reasons. First, VPS providers use cloud because it is a really logical way to build a VPS (if you think about the requirements that the VPS <em class="italic">vendor</em> would have, they <a id="_idIndexMarker385"/>would sound a lot like what cloud is meant to do) and because by being built on top of cloud computing, you can advertise the VPS as being cloud in a sense and be a sort of <em class="italic">simple interface to cloud resources</em>. It makes sense for cloud providers to offer VPS because the majority of customers who look for cloud have no idea what it is and only use it for political, not business or technical reasons, so offering something simple that allows them to purchase from you and use your resources (because cloud is so hard and complex) allows you to capture the majority of revenue. </p>
			<p>Vendors like Amazon used to offer no VPS services and using their resources if you were not truly in need of cloud was difficult, at best. To address this, Amazon added LightSail as a VPS product layered on top of their cloud product. </p>
			<p>Other cloud providers, such as <em class="italic">Digital Ocean</em>, <em class="italic">Linode</em>, and <em class="italic">Vultr</em> use VPS as their primary <a id="_idIndexMarker386"/>product<a id="_idIndexMarker387"/> offering <a id="_idIndexMarker388"/>and focus on it almost entirely while keeping their cloud interface quietly to the side so that customers truly looking for cloud can find it, but those seeking cloud when they intended to use VPS will be able to get what they need right away.</p>
			<p>VPS is one of the most popular and effective ways for real world companies to run workloads, especially smaller companies, but companies of any size can leverage them. Cloud is effective but primarily for special case workloads. The majority of companies talking about already leveraging cloud are actually using VPS and not even aware that they missed cloud computing entirely.</p>
			<p>It is worth noting that when we talk about rates of cloud adoption we have a fundamental problem: no one knows what cloud is, including people who think that they are or are not using it currently! Vendors like Amazon can tell you how many customers that they have, but they cannot tell you if their customers are using their products as cloud <a id="_idIndexMarker389"/>or just using cloud in some other way. In a survey about cloud adoption you have zero reasonable chance that the person being asked about their adoption, the person doing the asking, and the person reading about the adoption rates all understand enough about cloud to answer or ask meaningfully and, in reality, generally none of them know at all what is being asked. So any information about cloud adoption rates border on being totally meaningless. There is no honest mechanism by which any person or organization could possibly know what the cloud ecosystem really looks like. You would get just as meaningful data if a group of squirrels surveyed a bunch of hamsters about astrophysics and then handed the results to a bunch of hyper puppies to interpret. People love reports and data and rarely care if the survey in question was real in any way.</p>
			<p>You should, at this point, feel both overwhelmed and depressed about the state of cloud understanding within both IT and business, but you as the Linux system administrator should now be prepared to explain it, evaluate it, understand what is built upon it, and know when and how to choose to use it for your own workloads and when to look at VPS instead.</p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor113"/>On premises, hosted, and hybrid hosting</h1>
			<p>Now that<a id="_idIndexMarker390"/> we have <a id="_idIndexMarker391"/>talked <a id="_idIndexMarker392"/>about so many aspects of the underlying components that are used to provide us with a platform on which to deploy an operating system, we can finally talk about where those systems should exist!</p>
			<p>This is, at least, the simplest of all our topics. Physical location is easy to explain, even if many businesses get confused about it in practice. Conceptually we really only think about two locations for a workload and that is as being either on premises or off premises. This can be a little convoluted, though, as companies own multiple locations so what is off premises to one site might be see as on premises to another. But we generally consider on premises to be all of a company's owned sites and off premises being any sites that are operated by a third party. Because of this we generally refer to off premises physicality as being hosted, as physical systems are being hosted on our behalf. However, there are reasons why this can prove to be very misleading.</p>
			<p>Most people assume that when a system is kept on premises that that also implies that it will be being operated by an internal team. This is most often true, but having on premises systems managed by third party teams is not unheard of, especially in very high performance or high security environments. For example, if you required Amazon's specific range of cloud computing products, but could not allows for any off-premises hosting, you can have Amazon operate an AWS cloud instance on your own premises. This is anything but low cost or simple and requires housing at minimum a small, self-contained data center and all of its associated staff!</p>
			<p>Hosting gets more complicated in practice, but the core issue remains the same: the demarcation points. When we decide to start having our systems be hosted off premises the questions rapidly become about defining what portions of the hosting will be provided by the hosting provider and which by us.</p>
			<p>In its most extreme (and impractical sense) you could rent a house, office, or storage unit and provide your own rack, servers, Internet, cooling, power, and so forth as needed, but if we did this one would rightfully argue that we had basically made the site our own premises. Touche. </p>
			<p>Classically it was assumed that nearly all workloads should be run on premises. This was for the very simple reason that early business networks had no Internet connectivity so hosting elsewhere was effectively impossible or at least impractical. Follow that in the early days of the Internet wide area network links were slow and unreliable keeping remote servers almost unusable. And software was built around LAN networking characteristics, unlike today when enterprise software of any quality assumes that it needs to perform adequately over a long distance connection, most likely on the Internet.</p>
			<p>Because of these old assumptions, the tribal knowledge that servers need to be local to an office where people work has been passed down by rote generation to generation without many people evaluating it. This information went from generally true to seldom true pretty quickly during the early 2000s.</p>
			<p>Today most workloads work effectively over the Internet and so can be located almost anywhere. Using some form of off-premises hosting or centralized hosting that is not based at any specific company location is now the norm rather than the exception.</p>
			<p>In all on-premises and off-premises evaluations we have certain factors that are universal: who will access the data and from where, how does latency and bandwidth impact application performance, which people accessing the data have priority and what is the cost of performance issues at different locations.</p>
			<p>There is no hard and fast rules, we simply have to carefully consider as many factors as possible. On and Off Premises solutions are just locations and should be treated as such. The ability to use an enterprise datacenter off premises might be significant, especially if we do not have a <a id="_idIndexMarker393"/>real <a id="_idIndexMarker394"/>server<a id="_idIndexMarker395"/> room on premises. And disaster recovery might be better at an off premises location. But will the user experience be good enough if the server is far away? These questions are all situational and need to be answered not just about the status of the business infrastructure today but also for the near future.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor114"/>Colocation</h2>
			<p>When a site provides<a id="_idIndexMarker396"/> the real estate, cooling, power, Internet, racking, networking, and so on but we provide our own physical servers it is called a colocation facility. Colocation is one of the most popular and effective ways that we, as system administrators, can acquire enterprise class datacenter services outside of our own premises while retaining the flexibility to use any hardware that makes sense for our organization and its workloads. Colocation is effective for very small businesses up to the most absolutely massive. No company outgrows the value that colocation may bring, nor does any government. It is a strategy that lacks a <em class="italic">top end</em> size.</p>
			<p>Colocation is one of the most useful and effective forms of moving IT equipment off premises because it allows the IT department to retain full control of hardware purchasing and configuration not just for systems but for networking and appliances as well. Only non-IT functions necessary to support the technology hardware is provided. This allows the colocation facility to focus on a strong facilities management skillset and IT to retain literally all IT functions and flexibility: basically doing remotely with a third party what you would hope you would be doing internally with your own teams assuming that you had enough volume to do so. It is expected that a colocation provider will also have <em class="italic">remote hands</em> to assist with bench tasks when necessary, such as changing or verifying cabling, power cycling devices, and things of that nature.</p>
			<p>Flexibility is key with colocation. Whether it is because you want to custom build your own hardware, maintain legacy systems, use special case hardware (for example, IBM Power hardware), or<a id="_idIndexMarker397"/> what the freedom to do a lift and shift of an entire existing environment from on premises to the colocation facility you can do it all. Most colocation facilities allow for a range of scales as well, from housing a single 1U server on your behalf to fraction racks (tenth, quarter, half, full rack sizes are common) to providing cages that can house many racks to even renting entire floors of the datacenter!</p>
			<p>The biggest challenges that colocation faces is that there is no effective way to go extremely small because the smallest size you can reasonable host is a single server. If your needs are smaller than this, then colocation will generally struggle to be cost effective for you. But do not simply reject colocation with an assumption of it being expensive. I run these calculations often for companies who were ignoring colocation as too costly for them only to find out that it would be less than half of the cost of their alternative propositions while having more flexibility for growth without additional expenditures. Most people assume that servers are more expensive to buy than they really are and that colocation costs are higher than they really are. Colocation costs are often inappropriately associated with legacy systems, as if only twenty year old equipment can go into a datacenter today, and decades old cost models are often envisioned. Twenty years ago servers were much more expensive than they are today and had noticeably shorter operational lives and datacenter space was more costly than today as well. Like everything in IT, cost over time have come down and for workloads of any size colocation tends to be much less costly than most alternatives.</p>
			<p>Colocation is just one approach to hosting systems off premises. Other approaches like public, hosted cloud and cloud-based VPS systems are standard alternatives.</p>
			<p>The biggest challenges around locality are really all associated with understanding current marketing pricing for different approaches and being able to evaluate the benefits and caveats of hosting equipment on premises or off premises, and if that equipment should <a id="_idIndexMarker398"/>be dedicated or shared. You should now be ready to make that evaluation and choose appropriately for your deployments. Next, we dig into the much more complicated topic of platform level system design architectures.</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor115"/>System Design Architecture</h1>
			<p>One of the<a id="_idIndexMarker399"/> more challenging aspects of system administration is tackling the broad concept of system architecture. In some cases, we have it easy, our budget is so low or our needs so simplistic that we simply do not need to consider any but the most basic options. But for many systems, we have broader needs and a great number of factors to consider making system architecture potentially challenging in many ways.</p>
			<p>We now understand platform concepts, locality, and the range of services normally associated with providing an environment onto which we can install an operating system. Now we have to begin describing how we can combine these concepts into real world, usable designs. Most of system design is just common sense and practicality. Remember nothing should feel like magic or a black box and if we get services from a vendor, they are using the same potential range of technology and options that we are.</p>
			<p>We are going to talk about risk and availability in the next section, but before we do, we should mention here to make it more clear why system designs rely on this data, that any redundancy (whether for performance or risk reduction) that we add to our overall system can be done at different layers. Principally in the system layer (where we are looking now) or at the application layer (which we do not control.) So even in the most demanding of high availability workload situations, we may have no need for a robust underlying system design and have to consider this when thinking about design options.</p>
			<p>I am going to break down common design approaches so that we can understand how they best apply to different scenarios. These are physical system architectures that include both the storage and the compute. It is assumed that some sort of abstraction, meaning virtualization and/or containerization, is used in every case and so will not be mentioned <a id="_idIndexMarker400"/>case by case.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor116"/>Standalone server, aka the snowflake</h2>
			<p>You really cannot <a id="_idIndexMarker401"/>get more basic than this design. The<a id="_idIndexMarker402"/> simple server, the baseline against which all else must be measured. The self-contained, all in one server with storage and compute in a single chassis. No external dependencies, no clustering, no redundancy (external to the single box.) Of course, we assume standard hardware practices are followed such as minimums like RAID and dual, hot swappable power supplies.</p>
			<p>Today, many IT people are going to frown on attempting to use a stand alone server, but they should not. The classic single server is a powerful, effective design appropriate for the majority of workloads. This should be the <em class="italic">go to</em> design, the default starting point, unless you have a specific need to do something else. </p>
			<p>Because of its simplicity, single server designs have the best cost ratios to all other factors, are more robust than they appear, and have excellent performance. Many people think of servers as being rather fragile creatures, and years ago, they were; but that impression stems from servers of the 1980s and early 1990s. By the late 1990s server technology was becoming mature and reliable and today failure rates on well maintained servers are extremely low. The idea that a single server is a high risk is an antiquated one, but like many things in our industry old feelings often linger and get taught mentor to student without reevaluation to see if factors remain true (and in many cases without initial evaluation to know if they were ever true.)</p>
			<p>Single servers benefit from having many fewer components and lowered complexity compared to any other approach and with fewer parts to fail and fewer configurations to get wrong it is that much easier to make really reliable: hence why we sometimes refer to this as the <em class="italic">brick</em> approach. Bricks are simple but effective, while they can fail, they rarely do. Emotionally it is common to associate complexity with robustness, but in practice simplicity is far more desirable. Complexity is its own enemy and an unnecessarily complex system takes on unnecessary risk (and cost.)</p>
			<p>While hard to measure for many, many reasons, we generally assume that a properly maintained and supported stand alone server can delivery average availability rates close to five nines (that is around one hour of downtime per year.) It is a rare workload in any business that cannot function well with that kind of downtime. What is difficult in stand alone servers is that this is an average only (of course) and we will have isolated systems experiencing much higher downtime, and others experiencing none.</p>
			<p>Simplicity also brings us performance. By having fewer components in the path single servers have excellent performance. Attempting to gain total performance greater than what can be achieved using a single server is difficult. Single servers give us the lowest latency and nearly the best throughput of any approach.</p>
			<p>When it comes to single server systems, use math and logic to explain why it may or may not make sense. Many people rely on emotions when it comes to system architecture and this should never happen. Our concerns with system design are about performance and availability and these are purely mathematical components. Emotions have no role here and are, in fact, our enemy (as they are the enemy of any business process.)</p>
			<p>Single servers can scale far larger than most people assume. I often hear arguments that they cannot look at single servers because their needs are <em class="italic">so large</em>, but then deploy systems only a tiny fraction of the standard scale, let alone the maximum scale, that a single server can achieve. Remember that vertical scaling is highly effective compared to horizontal, and generally cost effective as well. The biggest single server systems can support hundreds of the most powerful CPUs, and many terabytes (or more) of RAM. The challenge is not finding a single server that is large enough for a task, but rather finding any workload that can leverage so much power usefully in a single location!</p>
			<p>A big advantage to standalone servers is that each physical device can be scaled and custom designed to address the needs of its workload(s). So different servers can use different CPU to<a id="_idIndexMarker403"/> RAM to storage ratios, different servers<a id="_idIndexMarker404"/> can use different CPU generations or architectures, one system might use large hard drives while another uses small but screaming fast solid-state storage. Tuning is very easy.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor117"/>Simple does not necessarily mean simple</h2>
			<p>Having a standalone<a id="_idIndexMarker405"/> server does not mean that we give up all of the options and flexibility that we might believe that we need from more complex designs. It simply means that we have to think about them differently. Many of the concerns that one may have about standalone servers likely stems from a pre-virtualization world with relatively slow networks. Today we have virtualization, fast storage, and fast networks and these can move the goal line by a bit.</p>
			<p>We refer to servers as being standalone in reference to their architecture, everything is self contained in a single piece of hardware. This does not mean that we do not have (or cannot have) more than one server. On the contrary, giant Fortune 100 firms will often have thousands of standalone servers. What makes then standalone is that they have no dependencies on each other. The complete failure (or theft) of one does not negatively impact another.</p>
			<p>A tiny organization might choose to rely on a single standalone server for their entire business and depend completely on backups and the ability to restore to replacement hardware should disaster strike. This is a totally valid approach and quite common.</p>
			<p>If your organization is larger, or workloads require more immediate protection against loss of availability, then it is standard to run multiple standard servers. This spreads load between physical hardware devices and, because of virtualization, provides an opportunity for there to be natural ways of mitigating hardware failure by rapidly rebuilding lost workloads on other hardware. If deployment density is too high, spare hardware is an option as well. With modern storage, networks, system management, and backup techniques restoring many workloads can be done in as little as minutes allowing even complete hardware failures to often carry only the tiniest of system impacts. In fact, keeping backups stored on other standalone nodes can allow for essentially instant recovery of lost systems while maintaining strong decoupling.</p>
			<p>Standalone servers also do not imply that there is no form of unified management. Tools like ProxMox or VMware vSphere allow a consolidation of management while keeping system hardware independent. Modern tooling has made managing sprawling fleets of standalone servers very simple indeed. </p>
			<p>Most every aspect<a id="_idIndexMarker406"/> of a stand-alone server can be improved by adding more to it and making it more complex, the two things that it always leads on are cost and simplicity. No other approach will reliably be able to keep our costs or our simplicity as low and in business, these are generally the factors that matter most.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor118"/>Many to many servers and storage</h2>
			<p>As companies<a id="_idIndexMarker407"/> grow there can be an opportunity to consolidate different aspects of the architecture in order to save money. Separating networking and storage is the most common approach to this. Creating a layer of compute nodes, and a layer of storage nodes allows for a lot of flexibility. The primary benefit is allowing for the easy movement of resources and better system utilization. </p>
			<p>For example, an organization maybe need fifteen physical compute nodes (traditional servers) but only half a dozen storage nodes (SAN or NAS) to support them. Each individual system can be easily custom scaled and does not need to match other systems in the pool. In this way this approach is not so different from the standalone server approach.</p>
			<p>It should be noted that when doing this, the storage layer is the greater risk, compared to the compute layer, for two key reasons. First, it is stateful where compute is stateless, which means that here we not only have to protect the availability (uptime) of the system, but this is also where we have to protect the data as it is stored so we have the risk of data loss as well - there is simply more to lose here. Second, storage is more complex than compute and equivalent hardware and software at both layers means that the storage layer is just more likely to fail due to complexity. This is all risk that also exists in the standalone server, but when combined into a single chassis it can be more difficult to understand where the risk is occurring, even if we know what the overall resultant risk is.</p>
			<p>In its simplest <a id="_idIndexMarker408"/>incarnation, we would have a single compute server node and a single storage node (typically a SAN array) and would connect them directly via a straight cable (Ethernet, eSATA, FC, and so on.) This is really more of a hypothetical scenario as it is so obviously bloated and illogical without any scale, but we can learn from the example to see how we take the single standalone server design and, without any benefits of scale, simply double the chassis to manage and increase the physical, as well as, logical complexity of the system design.</p>
			<p>Typically, this type of design is leveraged most to consolidate heavily on storage, pushing as much storage into a single node as possible, while having many small to medium sized (one to two CPU) servers that allow workloads to move between them in order to best balance said workloads. This approach is flexible and generally cost-effective, and makes large scalability quite simple. </p>
			<p>Moving past standalone servers means we start to inject dependencies that need to be discussed. At a minimum, when we move to a multi-nodal system, we have the complexities of the interconnections (which might be as simple as just a cable, or more complex like going through a switching fabric of some sort), any complexities that come from configuring the nodes to speak to each other, and the risks of the extra components that might fail.</p>
			<p>This kind of design really does nothing to address risk, and actually is far riskier than standard standalone servers. This is why it is important to use standalone servers as a baseline and discover risk variation from that point. In a <em class="italic">network</em> system design, there is no redundancy, so each workload has a full dependency on both its associated compute node and its associated storage node(s). This risk may be uniform across a compute node or each workload located there might have unique storage configurations so that risks may vary widely between different workloads on a single server. This is where risk becomes much more complicated to measure because we have to deal with the cumulative risks of the compute node, the storage node, the connection between the two, and the <a id="_idIndexMarker409"/>configuration! Each individual piece is extremely difficult to measure on its own – putting them together, we mostly have to look in relative terms only and understand that it is much riskier than a standalone server.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor119"/>Viewing the world as a workload</h2>
			<p>System architecture <a id="_idIndexMarker410"/>is a <em class="italic">by the workload</em> task and there is no specific necessity for all workloads in your organization, or even all workloads running on a single compute node, to share architectures. Mixing and matching is totally doable and somewhat common. Each workload should be being evaluated as to its own needs, and then the overall architecture evaluated. </p>
			<p>Often overlooked is the ability to use a complex and less reliable (but potentially less expensive at scale) option like network design for workloads that are less important, while on the same compute nodes also having local storage that is extremely fast and/or reliable for more critical workloads. Mixing and matching can be a strong strategy in a large environment where storage consolidation is considered necessary without endangering an isolated number of highly critical services in order to do so.</p>
			<p>In the same vein, each workload can have its own backup, replication, failover and other risk mitigation strategies for deal with disaster. Sharing a compute node generally dictates very little as to how reliability and availability from workload to workload must be handled. Of course typically all workloads are treated the same either out of a desire for standardization and simplicity, but also regularly out of a misunderstanding of the range of customization available for each individual workload. It is often assumed that choosing a system design is an all or nothing endeavour, but this is not the case.</p>
			<p>The main challenges of network system design is that any efficiency gained has to offset the additional cost created by needed more overall nodes (separating compute and storage means that additional hardware chassis and operating systems are necessary for the same tasks) and at any scale additional networking equipment is needed to handle the interconnects. Networking equipment can be as simple as a single Ethernet switch or as complex as clusters of Fiber Channel or Infiniband switches. Switches represent no only additional cost to purchase, but also additional points of failure both for hardware and, to a much lesser extent, configuration. Often redundant switches are purchased reducing hardware risk but increasing cost and configuration complexity. Even in extremely large<a id="_idIndexMarker411"/> environments this represents additional cost and risk that is very hard to overcome.</p>
			<h3>The Inverted Pyramid of Doom: Clustered Compute with Risky Storage, aka the 3-2-1</h3>
			<p>Sadly the Inverted Pyramid of <a id="_idIndexMarker412"/>Doom (aka 3-2-1 or IPOD) has traditionally, for the majority of<a id="_idIndexMarker413"/> the 2000s and 2010s, been the most commonly deployed architecture in small and medium business and is also the prime example of the absolutely worst possible design decision for normal workload needs. It is also the design that maximizes profits for vendors and resellers, so it is what everyone wants you to buy.</p>
			<p>The IPOD design is differentiated from the network system design above in that the compute layer is clustered for high availability, but the storage layer is not. As we discussed in the last design storage is both more important to protect and more likely to fail. Typically the networking layer (the layer providing connectivity between compute and storage) is also clustered for high availability. This nodal count by layer creates the naming conventions used: 3-2-1 refers to the design having three (or more) compute nodes, connected to two redundant switches, all relying on a single storage device which is most typically a SAN.</p>
			<p>When viewed in an architectural drawing, the IPOD is a pyramid with the wide portion on top and everything balanced on the point. Hence the term <em class="italic">inverted pyramid</em>, this design is designed to be as costly and risky as possible, hence the moniker <em class="italic">of doom</em>.</p>
			<h3>Top-down redundancy</h3>
			<p>Why is a design<a id="_idIndexMarker414"/> so obviously impractical as the IPOD so traditionally popular? The answer requires us to understand several factors. First, redundancy, risk, and system design are all areas that most businesses, and even most IT departments within those businesses, have received no training and are generally completely unaware and so represent an easy target for vendors to be manipulative.</p>
			<p>The real trick comes from two things: linguistics and the simplification enabled by top down viewing. The linguistic trick happens because the term <em class="italic">redundancy</em> does not mean what most people believe that it means and this system <em class="italic">has redundancy</em>, but in an all but meaningless way. So, when a customer says, <em class="italic">I need redundancy</em> they actually mean <em class="italic">I need high availability</em>, but this allows the vendor to state that there is redundancy and ignore actual needs. Semantics are super important in all business, and IT more than most.</p>
			<p>The top-down aspect of the system comes from how we view the architecture. As IT professionals, we know that we should view our architecture <em class="italic">from the side</em>, that is seeing the reliability as it stands layer by layer, knowing that compute is built on top of the network, and the network on top of the storage. But a vendor wanting to steer a customer to believe that there is strong redundancy will demonstrate the system <em class="italic">top down</em> showing a view that only sees the compute layer where there is redundancy. The other layers are overly complex and tend to be happily ignored by all parties as being <em class="italic">black boxes that do magic</em>. Ignoring the hard parts and just focusing on the trivial, easy part where redundancy is least important makes it really easy to mislead a customer.</p>
			<p>Of course, if we really stop and think about it, what matters is the overall reliability of the entire system. Getting distracted by any single layer will simply lead us astray. We need to understand all of the layers, and how they interact with each other, in order to determine overall reliability, but there is a strong emotional drive to see one layer as being extremely reliable (as the compute layer here often is) and then feeling that the overall system must therefore be extremely reliable. But this is anything but true. The overall reliability of the system is driven primarily by the most fragile layer, not the most reliable. The system risk is, if you recall from earlier, cumulative. You combine all of the risks together because each layer depends one hundred percent on every other layer, if any layer fails everything fails. You can demonstrate this easily with a thought experiment... if one layer has a 100% chance of failure, and all other layers have a 0% chance of failure, the system will still fail 100% of the time. The impossibly reliable layers do literally nothing to offset the unreliable layer.</p>
			<p>Redundancy itself is a dangerous word to use. In general English usage, the word redundant simply means that you have multiple of something when fewer are needed. This can mean that one is a replacement or backup should the other fail, but that is not implied and often the term is used to mean something else. In RAID, for example, RAID 0 has multiple disks (redundant) but the more redundancy the higher the risk, not the lower. RAID 1 is the opposite. Redundancy is polar opposite there, even within a single context. This really shows the importance of semantics in IT (and business, or really, life in general.) People often use redundancy as a proxy work for reliability, but the two mean very different things. Use the term you mean and you will get far better information.</p>
			<p>The biggest problem with the IPOD design is one of practicality. If we were to look at it purely from a reliability standpoint we could state that it is safer than the network system design because at least some of the layers contain high availability measures, even if not all of them do. And this is totally correct, but tends to be misleading. Network system design is meant to trade high risk for cost savings versus the simple stand alone server<a id="_idIndexMarker415"/> design, using the idea of <em class="italic">safer than</em> something that is not even designed to be safe is not exactly wrong, but talking about it in that context is done to evoke an emotional response - to make the IPOD feel safe, which is not the same as <em class="italic">safer</em>. If we compare the reliability of an IPOD to the stand alone server, it feels quite unsafe and remember, we stated at the beginning, the low cost, simple, stand alone server is our baseline for comparisons. The problem with the IPOD is that the risk is extremely high, approaching the risk of the network system design, while its costs are much higher than the network system design and generally much higher than the stand alone server design all while having more complexity and effort for the IT team. It is the consistent combination of high risk and high cost that makes it problematic and generally accepted as the worst design to encounter in the real world.</p>
			<p>Outside of production environments, the IPOD is often ideal for large lab environments where capacity matters most and reliability does not matter at all. The ability to flexibly scale compute with a single consolidated, low cost, highly unreliable storage layer can make sense to make large scale labs more affordable. </p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor120"/>Layered high availability</h2>
			<p>The logical<a id="_idIndexMarker416"/> system design derived from what we have already seen is to take the separate layers of the network system design, and the high availability clustering from the compute layer of the Inverted Pyramid of Doom and apply it to all layers giving us a high availability storage layer, a high availability networking layer, and a high availability compute layer. In this way we can have large scale compute, storage, and networking without any individual layer being a high level of concern.</p>
			<p>where each layer still depends on every other layer, that three highly available layers must still be evaluated with the risk of each layer added together. So, while we can almost certainly make any individual layer more, or even far more, reliable than a single standalone server would be when we accumulate the risk of each layer, and then add in the risk of the additional complexities from incorporating the layers together, it may or may not remain more reliable than the standalone server would be.</p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor121"/>Reliability is relative</h2>
			<p>When discussing<a id="_idIndexMarker417"/> reliability and these different architectures we have to remember to think in terms of apples to apples, not apples to oranges. When we say that a single server is a certain level of reliability, and that servers clustered with standard high availability technologies have a certain relatively higher reliability, we are assuming that all of those servers are roughly identical in their individual reliability. In most situations this is true. Whether we are talking compute nodes, networking hardware, or storage nodes, for roughly the same price range we get similar quality hardware and software with roughly similar failure characteristics. So, these different devices <em class="italic">of the same quality</em> are all about the same level of reliability with networking hardware being the most reliable (least complex) and storage nodes being the least reliable (because they are the most complex.)</p>
			<p>However we can manipulate this dramatically. A five thousand dollar server will generally be much less reliable (and performant) than a five hundred thousand dollar server. Yet each is an individual, stand alone server. So clearly we have to think in terms both of architectural reliability (the reliability of the system design that we make) and in terms of the individual components.</p>
			<p>A common problem found here is that <em class="italic">you get what you pay for</em> applies not at all and you can easily find extremely expensive single-chassis systems for both compute and storage nodes that are not highly available at all and may not even be as reliable as average devices! As reliability is hard to measure and even harder to prove, vendors have little incentive to tell us the truth. Vendors are highly incentivized to tell us whatever is likely to make us spend more money with them whether it is making us feel that traditional servers are more fragile than they really are, or by making wild high availability claims for devices that are essentially built from straw (and by pigs.)</p>
			<p>So we must be careful that we consider all of the factors. And we must understand that the ability to protect a single chassis (vertical reliability) compared to multiple chassis (horizontal reliability) is different. Single chassis reliability tends to be incredibly powerful for certain components (such as redundant power supplies, high quality components, and mirrored RAID for storage) but tends to be complex and problematic for others (CPU, RAM, Motherboards.) And single chassis systems, while easier to operate, cannot address some key concerns like physical damage (water, fire, forklift) in the same way that multiple chassis can.</p>
			<p>We must also be keenly aware that marketers and sales people often use confusion around reliability as a sales tactic and will push concepts<a id="_idIndexMarker418"/> such as <em class="italic">dual controller</em> systems as being<a id="_idIndexMarker419"/> essentially impossible to fail but without science or math to back it up. Dual controller systems are simply horizontally scaled systems inside a single chassis with all of the complexity of the former and the lack of physical protection of the later. And any product sold based on being misleading is that much more likely to be poorly made as it means that the vendor is unlikely to be being held accountable to quality design.</p>
			<p>It has become known, especially in the early 2010s, that server vendors were regularly pushing products branded as high availability or <em class="italic">cannot fail</em> that did not even begin to approach the baseline reliability of traditional servers. Since customers could not verify this for themselves, they often just take the vendor's word for it and if the businesses loses money, finger pointing is the natural recourse.</p>
			<p>This approach necessarily is the most expensive design we can reasonably assemble because we need multiple devices at each layer, as well as technology to create the clustering at each layer. This is best for very large systems where each layer is able to scale so large that cost benefits of scale come in at every point.</p>
			<p>It is worth noting that essentially all cloud based systems run on this architecture due to their enormous scale. Certainly not all as cloud can run using any architecture, but this is far and away the most likely to be used in a large, public cloud implementation and is most generally what would be found even in a moderate scale private implementation. Many clouds do, however, run <a id="_idIndexMarker420"/>on stand alone servers even at massive scale.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor122"/>Hyperconvergence</h2>
			<p>The last architectural<a id="_idIndexMarker421"/> type that we will look at is<a id="_idIndexMarker422"/> hyperconvergence and we have now come full circle from increasingly complex designs to one of the least. Hyperconvergence as an architecture is anything but new, but for the last few decades it has been almost completely ignored before having a sort of renaissance in the mid-2010s and is now, along with stand alone servers, the bulwark of system architectural design.</p>
			<p>Hyperconvergence, also called HC or HCI, takes the compute and storage nodes of other, more complex architectures, and recombines them back into single servers (or you can view it as taking stand alone servers and adding high availability through engineering redundancy without adding unnecessary complexity.) Hyperconvergence gives us the best of both worlds, simplicity like stand alone servers, but options for high availability like Layered High Availability.</p>
			<p>Hyperconvergence is both so incredibly simple, but so effective that it can be hard to explain. The key strategy is taking the existing power and cost savings of the stand alone approach and doing as little as possible while still being able to add high availability clustering. By having multiple stand alone nodes that are clustered together (are they still stand alone, then?) we make all of the pieces highly available, while also reducing how many pieces are needed in total. </p>
			<p>When done correctly, data can also be guaranteed to be kept locally to a compute node, even though storage is replicated between nodes to create storage high availability, which not only means that we can get the high performance for our storage like we can with stand alone servers, but also that we can avoid a cross-node dependency allowing any node to keep working on its own, even if all other nodes and/or the network connecting them fails! That means that unlike all other system designs, we are only adding more resilience on top of the stand alone server design! That is huge. All other designs must put in the bulk of their efforts to overcoming their own introduced fragilities and risk failing to adequately do so potentially leaving them riskier than they would have been had we done nothing.</p>
			<p>Hyperconvergence, therefore, acts as the logical extension of the stand alone design and represents probably the most applicable system design for any large scale system. It is common to see hyperconvergence as being limited to small systems, but it is able to scale to the limits of the clustering technologies - the same limits affecting all design options. So all standard designs can go to roughly the same size, which is generally far larger than anyone <a id="_idIndexMarker423"/>would want to go in practical terms<a id="_idIndexMarker424"/> within the confines of a single system.</p>
			<p class="callout-heading">High availability vs basic clustering</p>
			<p class="callout">In this section <a id="_idIndexMarker425"/>we talk about clustering with the assumption that clustering (whether compute, storage, or networking) is done for the purpose of making the system highly available, at least within that one layer. High availability clustering is not the only kind of clustering, however. In all of these designs, including stand alone, we can add generic clustering as a management layer to manage many systems together. This can be confusing as the term clustering can be used to mean many things. </p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor123"/>Best practices in System Design Architecture</h2>
			<p>The best practice <a id="_idIndexMarker426"/>for system design is to keep your architecture as simple as possible to meet your needs, but no simpler. Remember that simplicity is a benefit, not a caveat. Complexity should be avoided when possible as complexity brings cost and risk. </p>
			<p>In any assessment, start with the brick. Just one, single, solitary stand alone server. Simple and effective. Now evaluate, does this meet your needs? How could spending more money better meet your needs?</p>
			<p>If high availability is needed, then assess hyperconvergence. Nothing can be more reliable, architecturally speaking. </p>
			<p>If you have special cases where you need cost savings at massive scale, other designs might be applicable. But remember, no matter how reliable it might feel emotionally or how much a sales person may push the solution, hyperconvergence is literally impossible to beat for reliability by design. Make sure that any design that is not one of these two starting points is being used with a comprehensive understanding of all of the risks and costs involved.</p>
			<p>That is a lot of material that we covered and a lot of turning conventional thinking on its ear. It is sad that here <em class="italic">conventional thinking</em> equates to <em class="italic">blindly ignoring needs and risk logic</em>, but it is what it is. This is a very difficult topic because it is so foreign to most people in any technical or business realm and such a specialty skill to master. And in many cases, you will get a lot of pushback from others who struggle to assess or communicate risks, and fail to turn risk information into actionable business decision making. </p>
			<p>At this point you have the tools and knowledge to design systems physically. This is a big topic, and it might be worth revisiting from time to time. This is very foundational and gives us the<a id="_idIndexMarker427"/> starting point to build reliable systems farther up the proverbial stack. And now that we know how to approach different designs for different purposes we will go on to look at risk itself and learn to ask what risk mitigation is right for us. </p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor124"/>Risk assessment and availability needs</h2>
			<p>At the very<a id="_idIndexMarker428"/> core of what we do in designing a system <a id="_idIndexMarker429"/>architecture is taking business needs around performance and availability and applying our understanding of risk and, as with everything in business (and therefore IT) assessing against costs. In the last section we already talked about risk, a lot. We have to - risk and performance pretty much define everything (other than strict capabilities and features) for us during our design stages.</p>
			<p>If we ask our businesses about risk, we almost always receive one of two stock answers: <em class="italic">we are not willing to pay anything to mitigate risks</em> or <em class="italic">we cannot afford to go down, it is worth anything to be up one hundred percent of the time</em>. Both answers should be obviously seen as insane and have no reason to ever come out of the mouth of any business person or IT professional, and yet they are nearly the only answers that you will ever receive providing you with no guidance whatsoever. They represent management simply <em class="italic">blowing off</em> IT and leaving IT to take on all decision-making risks without management providing any guidance.</p>
			<p>We have some amount of basic guidance that we can almost always work with. On the <em class="italic">low</em> end of the spectrum the rule of thumb is that if data is valuable enough to have stored in the first place, then at a minimum it is worth backing up. This is the simplest aspect of data and availability protection, and if your business thinks that the data that they store is not worth even backing up, you should be asking yourself why you are there yourself. There are extremely special cases where storage data is truly ephemeral and does not need a backup, but this situation is so unique and rare that it can be safely ignored.</p>
			<p>On the other end of the spectrum no system, anywhere, ever is so important that it is worth anything to not have downtime. First of all, totally avoiding downtime is impossible. No one can do this with any amount of resources. We can make a system ridiculously reliable and easily recoverable from nearly endless potential scenarios but no government, military, secret cabal, alien species, investment bank, or otherwise can possibly meet requirements often demanded by small businesses without any discussion whatsoever. The theoretical maximum that can be invested into making systems reliable is the entire value of the company in question and even if every penny that the largest<a id="_idIndexMarker430"/> firms had was invested into <a id="_idIndexMarker431"/>reliability and nothing else, risk still remains, no matter how small.</p>
			<p class="callout-heading">Risk and diminishing returns</p>
			<p class="callout">Attempting to invest in risk mitigation technology is a tough thing to do because as systems become more reliable the cost of <em class="italic">moving the needle</em> significantly towards ever improved reliability becomes more and more costly. For example, getting a stand alone server that is well built may give us as much as five nines of availability without any special <em class="italic">high availability</em> features.</p>
			<p class="callout">We may find that we need much higher availability. Perhaps six or seven nines. To get those order of magnitude jumps in reliability will require, almost certainly, at least double the investment in hardware as the standalone server. This may be well justified by our needs, but the cost per workload just jumped significantly.</p>
			<p class="callout">If we want to move the needle again an order of magnitude beyond that, the price jumps yet again. We get less and less protection as we spend more and more money. </p>
			<p>And so, because our business will rarely be willing or able to clearly define for us what our risk aversion level truly is, it often falls to IT and within IT to systems administration to carry out this all-important task on behalf of management. This will generally take some math, a lot of interviews with many different parts of the company, some common sense, and of course, some guesswork. Working with risk requires maintaining a logical view while it is tempting to become emotional, which is often the mistake made in business. Business owners or managers tend to react emotionally either seeing money spent on protection as not generating revenue directly and therefore undesirable, or seeing their business as not justifying protection and so tending to spend too much to provide an impression that the company is seen as valuable because downtime would be such a terrible thing.</p>
			<p>Of course good management will always been heavily involved in risk assessment tasks. This should not fall to IT. While IT has great insight and is a valuable contributor to any risk discussion it is the core management, operations, and financial departments that truly have the full risk picture necessary to create a corporate infrastructure risk strategy.</p>
			<p>When looking at workloads, we must attempt to evaluate what downtime will truly cost our business. This is not straightforward in nearly any scenario, but it is what we need to understand to have any means of logically discussing risk. Most businesses want to simplify downtime cost into the simplest possible terms. So dollars per minute or hour is generally how downtime is discussed. For example, losing the company's primary line of business application will lose the business one thousand dollars per hour of downtime.</p>
			<p>While simple, almost no real world workload actually loses revenue evenly hour by hour. In the real world it is most common to see a complex curve. For example, in the first minutes, and <a id="_idIndexMarker432"/>possibly even hours, we might <a id="_idIndexMarker433"/>see almost zero revenue loss. But then commonly we see a spike as outages go long enough to first be perceived by customers, then to cause customer concern. Lack of confidence and lack of operations spikes tend to hit a peak relatively quickly. Then long term revenue loss tends to start to kick in after days or weeks as customers leave. But this curve is different for every business. Of course, making an entire curve graph of all downtime scenarios is difficult and probably impractical, but the business should be able to predict significant inflection points along a timeline that represent major changes in impact behavior.</p>
			<p>It is tempting to look at outages as being all or nothing. Basically, ignoring workloads and seeing the entire company as completely down, as if the zombie apocalypse is happening and all of the staff have been infected. It is a rare workload that is going to impact any business in that manner. For example, if a company loses an email workload there will likely be an impact, but as email is not real time, it might take hours or even days before there is an actual loss of revenue (but if email is used to win real time bids, losing even a few minutes might be very impactful - it just all depends.) But assuming email could not be restored for hours, or days, a normal business would immediately begin mitigating the loss of the email workload through other channels. Maybe employees talk to each other in person, or use the company's instant messaging products. Perhaps sales teams begin to call customers on the phone rather than emailing. Working around a lost workload is often far more possible and effective that one realizes until <a id="_idIndexMarker434"/>a triage process is performed <a id="_idIndexMarker435"/>to see what a real world recovery might really look like.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor125"/>Workload interplay</h2>
			<p>Something else that<a id="_idIndexMarker436"/> we need to understand is how workloads interact with each other. As systems administrators we might have excellent insight into technical dependencies such as that a key ERP or CRM system depends on another application, such as email or financial, to function and if one is down, the other is down, too. That is an important aspect of workload dependency, but one that is well known and understood just by mentioning it. What is much harder to understand is the human workflow interdependence of systems.</p>
			<p>Some workloads may be exceptionally stand alone. Some may depend significantly on others. Others may overlap and provide risk mitigation unofficially.</p>
			<p>Lets look at the third case first. In many an organization today there might be traditional telephones, email, a few types of video conferencing solutions, and a handful of instant messaging solutions. Even in a tiny organization it is easy to casually end up with several overlapping solutions simply because so many things come bundled with so much functionality. In a situation like this, losing email might matter little for a very long time as internal communications may move to instant messaging and customer communications to telephone or video conference. Most organizations have the ability to work around a system that is down by using other tools at their disposal.</p>
			<p>But the converse is also true. Two technically unrelated systems, again lets say CRM and email, might not connect together but the human workflow may require that both be used at the same time and the loss of either one of them might be functionally equivalent to losing them both. So we have to consider all use cases, and all mitigation possibilities.</p>
			<p>This interplay knowledge will help us to determine how it makes sense to deploy some workloads. For example, if email and instant messaging work to overlap during a crisis, it likely makes sense to decouple them as much as possible so that if the hardware or software for one was to fail that it would not take down the other.</p>
			<p>If we have systems, like our email and CRM example, where one is useless without the other, then combining the two workloads to share a failure domain might make total sense. Meaning, as an example, if we had two independent servers one running the CRM and one running email, then each individual server would carry its own risk of failing with near certainty that those failures would not happen at an overlapping time. Each workload has an equal amount of expected annual downtime of X. The total downtime expected for the combined workloads is 2X. Easy math. Combine the two workloads onto a single server as each retains an equal amount of annual downtime risk, still X, and the combined is still 2X. But the effective downtime in the first case is 2X (or 1.99999X as there is some tiny chance of the outages overlapping) but in the second case is just 1X. How did we do that? Not by reducing any individual risk, but by reducing the effective risk - that is the risk impacting the business as a final result. Under the hood, we did reduce risk physically as a single server has half the risk of downtime of two equal servers simply because there is half as many devices to fail.</p>
			<p>Even a complete company shutdown is not necessary a total loss. Sending staff home for a surprise holiday might lower insurance costs and raise morale. Given a day or two to spend at home might reinvigorate workers who may be happy to return after systems have been restored <a id="_idIndexMarker437"/>and work more efficiently or maybe put in a little extra time to attempt to recoup lost business. We have to consider mitigation strategies when looking at losses from failed workloads. Some businesses may simply lose some efficiency, while others may lose customers.</p>
			<p>Of course we have to consider the possibility of the opposite. What if you are a business that depends heavily on customer perception of high uptime and even a tiny outage has a sprawling impact? Maybe your entire business generates only one thousand dollars per hour, but loss of customer confidence from even a two hour outage (which we might assume could only, at maximum, lose us two thousand dollars in this case) resulted in the loss of customers resulting in tens or hundreds of thousands of dollars of losses!</p>
			<p>All of these losses are just estimates. Even if an outage actually happens, there is no guaranteed way to know what revenue would have been without an outage having occurred. So if we cannot know this number for certain after something has happened then obviously we cannot know it with any certainty before the event that only might happen, has happened. Bottom line... estimating risk is very hard.</p>
			<p>In a large organization, consider playing <em class="italic">what if</em> games on a weekend with some staff from different departments. Run through scenarios of <em class="italic">X or Y has failed</em> and attempt to mitigate in a nearly real-world simulation on a small scale. Can you keep working with one tool or another? Which departments become dysfunctional, which keep humming along, how do your customers see the situation? This kind of <em class="italic">game</em> is best played with a combination of strong planners who are thinking about risk strategies and writing procedures as well <a id="_idIndexMarker438"/>as with a group of perceivers (for example: triage experts) who do not plan, but work tactically on the ground figuring out how to keep working with the tools at their disposal.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor126"/>Defining high availability</h2>
			<p>One of my <a id="_idIndexMarker439"/>favorite <a id="_idIndexMarker440"/>quotes in all of IT comes from John Nicholson who said <em class="italic">High availability isn't something that you buy, it is something that you do</em>. It is so tempting to see high availability as an intrinsic need in IT, and then to see it as so complicated that we cannot know how to approach it, and so fall prey to vendors who slap the unverified name <em class="italic">high availability</em> onto products, or even just slip it into a product name, and act as if buying a product can deliver high availability when logically, this is impossible. Imagine buying a high availability airplane, as an example. While you can make one airplane must more reliable than another, almost all of your overall reliability and safety comes from the pilot, not the plane. The same is true in IT. The best made product does little if operated poorly. A million-dollar cluster without backups is likely not as safe as a desktop with good backups!</p>
			<p>So first we need to establish a baseline for measurement. In our last section we said that stand alone server infrastructure serves as our baseline. This baseline has to represent what we will call <em class="italic">standard availability</em>. We <a id="_idIndexMarker441"/>now have two ways that we would hope that we can look at this availability. One is in absolute terms by giving a number such as a <em class="italic">nines</em> number and through industry evidence, it appears that well maintained, well-made stand alone servers can approach five nines of availability which roughly means six minutes, or less, of unplanned downtime per year (planned downtime for maintenance can represent a potential problem, but is not itself included in a reliability figure such as this.)</p>
			<p>Now keep in mind, when we are talking about a server or a system design architecture, we are not including the final workload, only the platform providing an underlying system onto which a hypervisor will be installed. So essentially hardware availability. Any software running on top may have its own reliability concerns and no amount of platform stability will fix instability from bad code in the final workload, for example.</p>
			<p>The other, and generally more useful, way to look at reliability of system architecture is not in unmeasurable absolute terms, but in relative terms comparing different designs to one another. No one really knows what system reliability really is. It is not a big secret that server vendors are keeping from us, they simply do not know. Every little system configuration difference produces very different reliability numbers and, like we said about planes, the users operating the systems create the largest impact in terms of reliability. A company with a pristine datacenter and continuous onsite support that responds to alerts immediately and spare parts on hand or nearby might be able to squeeze very different reliability numbers out of the same server stuck in a closet without air conditioning, lots of dust, and generally ignored. There are simply too many factors involved. And even if we could somehow account for all of the potential variation, in order to get meaningful statistics on systems so complex with failure rates so low, we would need to operate thousands or tens of thousands of servers for more than a decade <a id="_idIndexMarker442"/>to<a id="_idIndexMarker443"/> collect useful numbers and then all of the data would be outdated by more than a decade. So, for all intents and purposes, it cannot be measured.</p>
			<p>So, our most important tool is not talking in terms of <em class="italic">nines</em>, that is a great marketing tool and something that managers steeped in big heavy processes like Six Sigma like to repeat, but it means nothing in this context, but rather looking at orders of magnitude of systems deviating from our baseline. A system that is significantly more available than our baseline can be classified as <em class="italic">high availability</em> and a system that is significantly less available than our baseline can be classified as <em class="italic">low availability</em> and any system that is roughly the same as baseline remains <em class="italic">standard availability</em>. Beyond these general terms it becomes all but impossible to discuss.</p>
			<p>A system design like hyperconvergence would be generally classified as <em class="italic">high availability</em> as it is the most reliable design approach. And an IPOD would generally be classified as <em class="italic">low availability</em> as it is closer to the least reliable design approach, which is the network system design. Layered clustering is generally considered high availability, but not <em class="italic">as high</em> as hyperconvergence. Of course, in this case we are only considering the availability of the system design and ignoring individual components. If we use extremely highly available individual components at every layer of an IPOD, we can theoretically get it back up to standard availability, but likely at great cost.</p>
			<p>It is far more valuable to think of reliability in relative terms, rather than absolute ones. It is almost trivial to look at a standalone server, an IPOD, and hyperconvergence and see how there is a clear <em class="italic">high</em>, <em class="italic">medium</em>, and <em class="italic">low</em> availability based on nothing but common sense and the location of risk, risk mitigation, and risk accumulation in the design. It requires no special training or math to see how dramatically each is separated from the next and how improving the overall quality of components moves the absolute reliability number, but the relative does not change. And at the end of the day, this is all that we can know.</p>
			<p>By knowing what downtime impact will look like financially for our business, even if it is only a very rough estimate, we have something to work with when attempting to decide on how to invest in risk mitigation. We should never invest more in risk mitigation than what calculated risk shows as our potential losses. This sounds obvious but is a common stumbling point for assessment in many firms. For example, if a possible outage might cost us one thousand dollars and protecting effectively against that outage would cost two thousand<a id="_idIndexMarker444"/> dollars, we should not at all entertain<a id="_idIndexMarker445"/> paying to mitigate that risk. </p>
			<p>We should think of risk mitigation as a form of outage itself, for mathematical reasons. This makes calculations easier to understand. With good risk mitigation we would incur a minimal financial penalty now (say spending one thousand dollars) to protect against a large potential outage (that might cost us one hundred thousand dollars.) The upfront cost is guaranteed, the future risk is only a possibility. So any risk mitigation must therefore be much smaller than the potential damage that it is meant to protect against.</p>
			<p>An analogy of my own that I have been using for years to describe paying more for risk mitigation than the potential damage of the outage itself is: <em class="italic">That's like shooting yourself in the face today to avoid maybe getting a headache next year</em>.</p>
			<p>When comparing standard availability systems and high availability systems we might be talking about a difference of only several minutes per year, on average, of downtime. High availability, therefore, has to justify its cost and complexity very quickly. A massive public website where just a minute or two of being unavailable could cost millions in purchases or worse, erode customer confidence, therefore could easily justify a large expenditure in high availability systems even if the time saved seems trivial. But an internal system servicing employees where customer confidence is not a factor, and downtime does not lead users to turn to competitors (for example: email system, financial, CRM, and others) the lose of even several minutes a day, let alone a year, is likely to have no real financial impact whatsoever and investing heavily to protect those systems would be wasteful.</p>
			<p>So, how do we apply all of this to best practices? The hard answer is that risk assessments and resulting system design are very hard things to do. Determining risk is a long process involving a lot of math, logic, and to some degree, guessing. It requires that we understand our businesses and our technology stacks deeply. It demands that we engage the business at all levels, and from all departments, and consolidate information that is generally siloed. It forces us to evaluate other risk assessments against logic and expected emotional reactions. </p>
			<p>Rules of thumb tell us that the majority of systems that we deploy should be standalone servers, and nearly all remaining systems should be hyperconverged. These two standard patterns <a id="_idIndexMarker446"/>represent the near totality of what<a id="_idIndexMarker447"/> proper design will look like in the real world. All other designs are realistically relegated to extremely niche use cases with the IPOD being the ultimate <em class="italic">anti-pattern</em> of what not to do except for the most extreme of special cases.</p>
			<p>We have covered a lot of material in this chapter. But now we have an idea of how we make risk determinations, how we design our architectures based on that assessment. We understand how and why we use different kinds of virtualization, and why we always virtualize. And we know how to evaluate the use of cloud and locality for our deployments. Now to put all of this together! We use all of these tools in deciding the deployment of<a id="_idIndexMarker448"/> every workload! So many options, but that is what makes our careers challenging and fulfilling (and what makes us worth our salaries.)</p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor127"/>Summary</h1>
			<p>In summary, system architecture is complex and requires us to really dig into business needs, how operations works, talk to key roles throughout the organization and elicit input, and take a broad view of technological building blocks to construct solutions that deliver the performance and reliability that our workloads need at the minimum cost.</p>
			<p>We looked at fundamental components with virtual machines and containers and should now be able to defend our use of them and choose properly between them, as well as be able to use traditional containers without becoming confused with more recent application containers. And we learned about locality. You should be able to navigate the complicated linguistic minefield that is managers attempting to talk about the placement and ownership of server resources, analyze costs and risks and find the right option for your organization. Colocation, cloud, traditional virtualization, on premises are all options that you understand.</p>
			<p>And finally, the big piece, system design and architecture. Taking the physical and logical components of our system and building a full functional platform that empowers our workloads rather than crippling them. This has been a long chapter and touches on a lot of topics that are very rarely taught individual, let alone together. These are some really hard topics, and it is probably worth covering a lot of this material again before moving on. </p>
			<p>For many of us in systems administration we might use the material in this chapter almost never. For others it might be nearly everyday skills. These topics are often ones that allow you to completely elevate your career by demonstrating a concrete ability to take seemingly mundane technical minutia and applying background system design decisions to key organizational needs. Of all of our topics in this book, this one is probably the one that should empower you more than any other to stand out among your peers and cross organizational boundaries.</p>
			<p>I hope that with the information presented here that you can filter through sales and marketing misinformation, apply solid logic and reasoning, and build on concepts that will remain timeless. Taking the time to really understand failure domains, additive risk, false redundancy, and more will make you better at nearly every aspect of your information technology journey whether your goals are purely technical, or you dream of sitting in the board room chairs.</p>
			<p>In our next chapter, we are going to return to the seemingly more pedestrian topic of system patching, and move from the high level system strategies to in the trenches security and stability warfare.</p>
		</div>
	</div></body></html>