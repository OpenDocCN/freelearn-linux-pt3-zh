- en: '*Chapter 8*: Improving Administration Maturation with Automation through Scripting
    and DevOps'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I think that it is safe to say that for most of us in system administration
    that scripting and automation and where we naturally gravitate towards for thinking
    of what creates the best opportunities for overall system improvement. This might
    be treated, and automation is very important, without question, but it is not
    the end all of system administration either. It is safe to say that the more that
    we learn to script and automate, the more that we have free time to focus our
    energies on tasks that only humans can do while also developing a deeper appreciation
    for what developers do which is always be helpful for those of us in IT.
  prefs: []
  type: TYPE_NORMAL
- en: System automation is an area where it becomes much easier to obtain bragging
    rights as to what our daily task list looks like. When sitting around having beers
    at the proverbial system administrators cocktail lounge, we get little satisfaction
    over telling our compatriots how we wrote some really clean and easy to read documentation.
    But when we explain how we wrote a long, complicated script that takes hours of
    our weekly workload and turns it into a task that is run magically by the computer's
    scheduler we get kudos, attention, streamers, a ticker tape parade, fellow administrators
    buying us rounds of our favorite beverage and, if we are truly lucky, a pi√±ata.
  prefs: []
  type: TYPE_NORMAL
- en: Automation is typically the area of administration that most of us find to be
    both the most exciting, and the scariest. There are more building blocks, more
    concepts to understand, than in other areas of system administration. For the
    most part, system administration is a lot like taking a history class where yes,
    there are real benefits to knowing more pieces of history so that you have a larger
    context when learning something new, but generally you can learn about any specific
    event without a large understanding of all of the events related to it and that
    led up to it and still come away having learned something valuable and essentially
    understanding it. You will not be lost when learning Roman history just because
    you did not grok Greek history first. But scripting and automation is a lot more
    like math class where if you fail to learn addition, then learning how to find
    the square root is going to be completely impossible. Scripting is a skill that
    builds on top of itself and to be really useful you will want to learn a bit of
    it.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to start by looking at unscripted command line administration in
    comparison to working with a graphical user interface and use that as a foundation
    to move into automation itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we are going to learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The GUI and the CLI: Administration best practices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automation maturity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Infrastructure as Code** (**IaC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Documentation First Administration** (**DFA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modern tools of automation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The GUI and the CLI: Administration best practices'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are coming to Linux from the Windows world, you may be excused from realizing
    that nearly everything should be done from the command line, not from the graphical
    user interface. But really, even on Windows, Microsoft has been very clear, for
    a very long time, that the desktop experience is really for end users and not
    for system administrators and that they recommend either using PowerShell as the
    administration tool of choice when working on a local system directly or any number
    of remove management tools that connect via an API. Microsoft pushes quite hard
    to encourage those installing their systems for the past few generations to install
    their operating systems and hypervisors without graphical user interfaces at all.
  prefs: []
  type: TYPE_NORMAL
- en: Graphical User Interfaces, or GUIs as we will call them now to keep things short,
    present a lot of problems for system administrators.
  prefs: []
  type: TYPE_NORMAL
- en: The first issue with GUIs is bloat. During the installation of an enterprise
    operating system, when a GUI is available it is often more than half of all of
    the code that will be deployed in a system. Every additional line of code means
    more data that we have to store, the more we store the more we have to back up;
    more code to worry about having bugs or flaws or intentional backdoors; more code
    to patch and maintain and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Next is performance. GUIs require much more compute power and memory consumption
    while running than do non-GUI systems. It is not uncommon for a GUI to require
    2GB or more of additional system memory above and beyond what is needed for the
    system workloads. This might sound trivial, but especially if we are dealing with
    many systems consolidated onto a single piece of hardware it can add up very quickly.
    If we have twenty server virtual machines running on a single physical server
    it might not be uncommon in the Linux world for the average workload to only be
    between two and four gigabytes of memory. Adding two more gigabytes to each system
    would mean not only a nearly fifty percent increase, but forty gigabytes across
    the machines.
  prefs: []
  type: TYPE_NORMAL
- en: Consolidation and the age of squeezing systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the 1990s and 2000s, before the prevalence of virtualization, we were in
    an era where servers were gaining performance rapidly, but each individual system
    would only run a single operating system no matter how small the workloads on
    that system were. As systems became more powerful, much faster than software used
    more resources, there was a strong trend towards allowing system bloat because,
    at least when it came to hardware, it did not matter very much.
  prefs: []
  type: TYPE_NORMAL
- en: CPU and memory resources would normally come in discrete chunks and to have
    enough we would normally have to overbuy. It was rare to run a system close to
    its limits because it was so difficult to expand systems in those days. So a system
    would typically have a lot of spare resources by design to allow for a large margin
    of error and, of course, growth. Because of these factors, running a GUI on a
    server was more or less trivial.
  prefs: []
  type: TYPE_NORMAL
- en: Many factors have changed since those days. We could probably write an entire
    book just discussing why the industry temporarily moved from command line after
    decades of using nothing else and for a small blip from the mid-1990s to the early
    2000s had GUIs seemingly taking over as the dominant approach in server management
    only to go right back to the command line by around 2005\. Ignoring social trends
    driving changes we are concerned here with capacity concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Once virtualization became mainstream, and even more so as cloud computing began
    to become a major trend, the availability of spare resources for operating systems
    ceased to be a common thing, almost overnight. This might seem counterintuitive,
    given that virtualization inherently gives us more flexibility and power. But
    it also gives us the ability to scale down very effectively, and this is something
    that we did not have before. With virtualization we are rarely in a position of
    having dramatically excess system resources, especially predictably excess resources,
    and so there is a huge advantage to keeping individual virtual machines as lean
    as possible, and that means not running an enormous and largely useless GUI process.
    Very small businesses that still cannot combine their workloads to reach the effective
    lower bounds of a single server remain an exception to this rule and still have
    plenty of overhead to implement GUIs unless they would be good candidates for
    cloud computing.
  prefs: []
  type: TYPE_NORMAL
- en: In a traditional business, where there are multiple servers, a major advantage
    of virtualization is consolidation and avoiding the installation of GUIs may mean
    that fifty or sixty workloads can be installed on a single physical server instead
    of thirty or forty on the exact same hardware. This equates to the need to buy
    fewer servers and that means cost savings not just from lowering purchase costs,
    but also lowering power consumption, cooling costs, datacenter real estate costs,
    software licenses, and even IT staff.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at an example on public cloud computing, we can see the advantages
    of not having a GUI even more easily. Small workloads, which could include email
    servers, web servers, proxies, telephone PBXs, and on and on might only cost between
    five and ten dollars per month to run on their own. Adding a GUI will easily cause
    the cost of a cloud hosted virtual machine to double from five to ten or ten to
    twenty and so forth as the GUI creates a need for more CPU power, more storage,
    and most importantly, much more memory. It does not much effort to see how moving
    from ten dollars per month for a workload to twenty dollars will add up exceptionally
    quickly. As most cloud-based workloads are quite small adding a GUI to each one
    could have staggering capacity consequences as much as doubling the compute cost
    of a company's infrastructure!
  prefs: []
  type: TYPE_NORMAL
- en: The lack of appropriateness for a GUI is so dramatic that many vendors have
    traditionally not even provided a mechanism for attaching to a GUI in the cloud
    space. Amazon famously did not make GUIs possible on their standard cloud instances
    effectively forcing organizations to learn command line and even more advanced
    techniques involving management without a login. But nearly all cloud users opt
    for remote logins via a technology such as SSH. The cloud did more than anything
    else to demonstrate the risks and costs of the GUI.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to ubiquitous virtualization and cloud computing system administrators,
    especially those in the Windows world, would argue that GUIs just did not add
    that much overhead and that if they did anything to make someone's job easier
    that they were worth it. That myth has been exposed and no one can honestly make
    this claim today. GUIs are clearly nonsensical in any broad sense.
  prefs: []
  type: TYPE_NORMAL
- en: Often the biggest selling point for command line management at a managerial
    level is about security. A GUI presents a much larger attack surface for a malicious
    actor to use to attempt to breach a system. All of that extra code alone makes
    things much easier for a would-be attacker. And, of course, the functionality
    of a GUI has to make for very enticing attack surfaces just by the nature of needing
    to have more means of being accessible. More lines of code, more access methods,
    more management paths, lower performance and more all come together for overall
    increased security risk. Taken all together it may not be incredibly major, but
    the increase in risk is real and is measurable or, at least, estimable.
  prefs: []
  type: TYPE_NORMAL
- en: The final major point as to why command line management has become the *de facto*
    standard is efficiency. Yes, the very reason that so many point to as to why they
    chose to keep a GUI. The reality is that system administration is not a casual
    task, nor one where you can effectively just poke around and guess about what
    settings should be wear. To do the job well, or even safely, you must have a pretty
    solid understanding of a large number of items from operating specifics to general
    computing and networking understandings.
  prefs: []
  type: TYPE_NORMAL
- en: The GUI in management was traditionally promoted as a tool for those that were
    not used to an environment to be able to be effective quickly with less training
    and knowledge. A great concept if you are talking about a janitor. In system administration
    the last thing that we want is someone without deep knowledge and experience being
    able to act like they know what they are doing. This is dangerous on many levels.
    GUIs, sadly, actually make it much harder for many organizations to evaluate which
    candidates are even minimally qualified for a technical position.
  prefs: []
  type: TYPE_NORMAL
- en: Not only does a GUI pose a risk that someone without proper knowledge will start
    poking around, but for someone who knows what they are attempting to do the command
    line is vastly faster. It is faster for performing simple tasks, for performing
    most complex tasks, and it is far easier to script or automate. Command line management
    flows so easily directly into scripting that people often fail to be able to tell
    the two apart. If you ever truly compare tasks, it is not unheard of for command
    line work to take less than ten percent the amount of time that it takes to do
    the same task with a GUI!
  prefs: []
  type: TYPE_NORMAL
- en: Command line is not just more efficient for a system, it also makes multi-system
    management much easier as commands can be duplicated across systems in ways that
    GUI actions cannot be. Command line management can also easily be recorded, catalogued,
    searched, and so forth. It is plausible to do the same with a GUI but it requires
    long video recording which results in large amounts of storage needs and no simple
    way to parse or turn into documentation, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In the more modern era, we have also begun to face the problem of needing to
    perform most or all system administration remotely. This inadvertently played
    right into the command line's hand. The amount of data that needs to be sent,
    and the sensitivity to network lag for the command line are far smaller than for
    a GUI. A remote GUI session to a server generally uses a noticeable amount of
    network traffic. Remote GUI sessions are video streams, generally in pretty high
    resolution. In some cases, even a single user can cause network issues, especially
    if the server exists in a location with bad Internet access. The standard method
    for remote command line management is SSH.
  prefs: []
  type: TYPE_NORMAL
- en: SSH remote sessions will work just fine even over an archaic dial up Internet
    connection. And even the slowest modern Internet service is enough to handle scores,
    if not hundreds, of simultaneous SSH users. This is something that you generally
    cannot do with remote GUI sessions. Command line is nearly as effective over a
    tiny Internet connection from the other side of the globe while GUI remote management
    suffers noticeably from any network blips, limitations, or distance.
  prefs: []
  type: TYPE_NORMAL
- en: Command line is here to stay, but it is important to really understand why.
    It can be easy to forget that it is far more than just one or two small factors.
    There are really good reasons why you should be using command line whenever possible.
    Moving back and forth is not conducive to learning to be more efficient in either
    approach, as well. From a personal level it is expected that you would want to
    avoid the use of GUIs as much as possible so that you can focus on learning command
    line skills. Using the command line consistently is needed to really become efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Now we must acknowledge that there are a number of different command line options
    for Linux. We can use `BASH`, `Fish`, `zsh`, `tcsh`, PowerShell and more. Linux
    is, as we know, all about options and flexibility. This is a situation where less
    is probably more. Some of these shells are very nice and useful but, we must remember,
    that we are system administrators, and we need to make sure that we are totally
    familiar with the tools that we are likely to have access to in an emergency.
    Moving between shells is not particularly hard, especially in the Linux use case,
    but we should still be wary of spending time learning the nice keyboard shortcuts
    and auto-completion and other perks of a shell-like Fish or `zsh` because we may
    not be able to use those skills in the next job and that always has to be a consideration.
    And, in the case of an emergency if you were to get called to work on a system
    that you have not had a chance to set up previously you may be stuck with no option
    except for BASH. For me, this means that BASH is the only tool that I want to
    be learning.
  prefs: []
  type: TYPE_NORMAL
- en: And there you have it. All of the logic and reasoning so that you can go back
    to management and explain why you need to be working from the command line, why
    you need staff that works from the command line, and why your systems should rarely
    get installed with any GUI environment at all. In our next section we are going
    to talk about maturity levels in automation for systems.
  prefs: []
  type: TYPE_NORMAL
- en: Automation maturity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While there is no formal system for measuring automation maturity levels, there
    are some basic concepts of automation maturity that we can discuss. The idea here
    is that organizations sit, more or less, along a continuum from having no automation
    to being fully automated with most organizations sitting somewhere in the middle,
    but more likely to be towards no automation than towards being fully automated.
  prefs: []
  type: TYPE_NORMAL
- en: Not every organization needs to be, or even should be, completely automated.
    But in general, more automation is better when the cost to implement the automation
    is low enough to do so. Automation is not free, and it is quite possible to find
    organizations investing more in automating a process than it would cost to perform
    the duty manually over the lifespan of a system. We do not want to automate blindly
    only for the sake of automating.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, however, what we find is organizations skipping automation in nearly
    all cases and uses manual labour with all of its costs and risks instead. There
    is a natural tendency towards this because in the moment, any task will be easier
    if done manually. If we do not look ahead and invest, we would simply never automate,
    and this is often how companies view IT needs. If a task takes one hour to do
    manually and three hours to automate, that's the time of three tasks and hard
    to justify ignoring the fact that the same task will happen monthly and in four
    months would not only have been less effort to have automated, but the automation
    would make the task more reliable and consistent.
  prefs: []
  type: TYPE_NORMAL
- en: Nearly any organization will benefit from automating more than they do today.
    There is no need to look at automation and feel that it is an all or nothing proposition.
    Automate what you can, skip what you cannot do. Be practical. Hit the low hanging
    fruit first. The more than you automate the more time you have to automate other
    things in the future. You will improve your automation skills as you practice,
    as well, making each new automation something easier than the last. Automation
    is a great example of that kind of thing that is really hard to do the first time
    but gets progressively easier and easier until it is just the obvious way to approach
    things and becomes second nature.
  prefs: []
  type: TYPE_NORMAL
- en: Automation maturity is not exactly a direct continuum with each step *more mature*
    than the last. For example, if we look at scheduling tasks and scripting tasks,
    each of these can be done independent of the other. Both are useful on their own.
    We can script complex operations and run them manually. Or we can schedule simple,
    discrete commands to trigger things without human intervention. We can then put
    the two together to automatically kick off complex scripts that do many things
    at once. Which one do we consider first and which second is just arbitrary.
  prefs: []
  type: TYPE_NORMAL
- en: Local and remote automation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In case it is not overly obvious, we have the choice of implementing automation
    either locally with tasks scheduled or triggered to run on the server in question,
    or we have the ability to push our automation from an outside source which can
    give us a sort of centralization of automation. There is a sort of hybrid approach
    where a local scheduler or agent reaches out to a central server to request automation
    information which is technically still local, just with a centralized store to
    mimic control.
  prefs: []
  type: TYPE_NORMAL
- en: Often overlooked is the advantage of local automation being able to run even
    if remote systems become unavailable even to the point of the local system completely
    losing networking. My favorite task to keep local regardless of other automation
    decisions is system reboots. While it would be convenient to centralize reboots
    and I have seen organizations opt to do so I very much appreciate having a local,
    forced reboot that happens at least weekly and sometimes even daily. This gives
    me great peace of mind that even if something completely crashes on a server if
    it is still functional in any way that eventually a reboot process is going to
    make an attempt at restarting the machine and hopefully bringing it back online.
    A very niche need and one that may never be important to you, but I have witnessed
    systems become inaccessible for remote management while still providing their
    workloads and an automated, locally scheduled reboot brought them back online
    and making them accessible again.
  prefs: []
  type: TYPE_NORMAL
- en: An increasingly popular happy medium approach is to have a central control repository
    that contains all of the desired automation which is then pulled by an agent on
    the end points being automated. This repository contains both the automation itself,
    such as scripts, as well as the scheduling information or triggers. Then the information
    is actualized by a local script that is able to keep functioning independently
    even if the remote repository fails or becomes unavailable. In this way you only
    really risk losing access to update the list of scheduled tasks to make changes
    to them or to the schedule. As long as you do not need to send out new updates
    to the automation you do not have to worry about your repository being offline.
  prefs: []
  type: TYPE_NORMAL
- en: Command line
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Not so much a legitimate maturity level but more of a basic starting point,
    so we could think of this as a level zero, is moving to the command line and the
    use of a proper shell environment for interactive (that is: non-automated.) As
    we just discussed, being on the command line and learning command line syntax
    and tools is the fundamental building block on which all subsequent automation
    is going to be based. Understanding how to do tasks at the command line, how to
    manipulate text files, how to filter logs, and other common command line tasks
    will build quickly into obvious automation capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Scheduled tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best and easiest place to begin with automation is the scheduling of tasks.
    This may sound like the simplest and most obvious step, but surprisingly many
    organizations never even get this far. Linux has long been a stronghold of reliable,
    easy to manage local scheduling with `cron` having been built into not only Linux,
    but essentially all UNIX systems for almost half a century as it was released
    in 1975\. Cron is fast and efficient, ubiquitous and well known. Any experienced
    UNIX admin should be able to at least schedule a basic task when needed. Cron
    even handles tasks happening at boot time.
  prefs: []
  type: TYPE_NORMAL
- en: Simple tasks of all sorts can be run through `cron`. Common tasks used in most
    environments could include system updates, data collection, reboots, file cleanups,
    system replications, and backups. You can schedule anything, of course, but these
    are some good ideas for first time automaters looking for obviously recurring
    system needs.
  prefs: []
  type: TYPE_NORMAL
- en: Another common area for simple scheduled tasks are code updates via a repository
    like when we pull new code via `git`. Tasks such as code updates and subsequent
    database migrations can all be easily scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: Scripting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we say automation everyone always immediately thinks about scripting. At
    the end of the day, nearly everything in automation is scripting either directly,
    or under the hood somewhere. Scripting delivers the power when we want to move
    beyond the simplest of tasks or just calling scripts that someone else has made.
  prefs: []
  type: TYPE_NORMAL
- en: We cannot possibly teach scripting itself here, that is an entire topic in and
    of itself. Scripting is the closest that IT comes to crossing paths with the software
    development world. Where does combining IT command line tasks together turn into
    programming? Technically it is all programming, but an incredibly simplistic form
    of programming focused purely on system management tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, on Linux we write scripts in the BASH shell. BASH is a very simple
    language designed to be primarily used interactively as a live shell, BASH is
    how we assume all command line interactions with Linux will be performed. BASH
    is relatively powerful and capable and nearly any script can be written in it.
    At least when starting out, most Linux admins will turn to the BASH shell that
    they are already using in their command line environment and add scripting elements
    organically to move from a single command, a few basic commands strung together,
    and into full scripting just a little at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Any shell, such as `tcsh`, `ksh`, `fish`, and `zsh`, will allow you to script
    and in many cases with more power and flexibility than you can with BASH. Traditional
    shells, like `tcsh`, `ksh`, and BASH, can be very limiting and cumbersome to attempt
    to use for advanced scripting. Apple for its macOS UNIX operating system has recently
    moved to `zsh` to modernize it compared to other UNIX systems. Typically, a Linux
    system is not going to have a more modern, advanced shell installed by default,
    even though they are easily available on essentially any Linux based operating
    system.
  prefs: []
  type: TYPE_NORMAL
- en: You may work in an environment where an alternative shell is consistently provided
    or offered, or you may have the option of adding it yourself. If so, and especially
    if you will be doing cross platform scripting with macOS you might consider using
    `zsh` instead of BASH, or if you are doing a lot of Windows scripting its native
    shell PowerShell is also available on Linux.
  prefs: []
  type: TYPE_NORMAL
- en: PowerShell on Linux
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the weirdest things that you may ever encounter is the idea of running
    Microsoft's PowerShell on Linux. Many people are confused and believe that it
    does not even work. PowerShell does actually run just fine on Linux. The problem
    with PowerShell on Linux is that PowerShell users on Windows actually spend essentially
    no time at all learning PowerShell and nearly all of their time learning a range
    of CommandLets or small programs that can be called by PowerShell and easily combined
    with other little programs to give power to the system.
  prefs: []
  type: TYPE_NORMAL
- en: On Linux, of course, the same thing happens. If you are scripting on Linux you
    will surely be using tools like sed, awk, cut, head, tail, grep and so forth.
    These tools are a lot like CommandLets, but are actually just every day system
    executables. If you were to port `BASH` or `zsh` over to Windows you would find
    that the tools that you are accustomed to using on Linux were still not available.
    That is because they are tiny programs that you are calling from BASH, not part
    of BASH itself. BASH is just the programming language.
  prefs: []
  type: TYPE_NORMAL
- en: The reverse is also true. If you run PowerShell on Linux you still use sed,
    awk, cut, grep, head, tail and on and on. It is the language that has changed,
    not the operating system's suite of tooling and components.
  prefs: []
  type: TYPE_NORMAL
- en: So, while there can be value in learning one scripting language and attempting
    to use it over and over again between different operating systems, there is not
    the value that one might assume. You will likely spend far more time tripping
    over integration quirks, misunderstandings, and poor documentation than you could
    ever recuperate from language learning efficiency. Books, online guides, example
    code and so on will never work for you if you try to use PowerShell on Linux.
    There will assume that you are trying to do Windows tasks with access to Windows
    tools, always. PowerShell is, at its core, designed to be a truly modern shell
    that uses operating system objects to do its heavy lifting. BASH instead is focused
    on text processing and manipulation as Linux is traditionally built on text files
    and needs a scripting engine that will easily accommodate that.
  prefs: []
  type: TYPE_NORMAL
- en: Using something so foreign as PowerShell on Linux is a great tool for exposing
    where different components that we often see simply as *the command line* or *the
    shell* for what they are. If we use `zsh` on Linux, nearly everything that BASH
    has built in is replicated in `zsh`, and they both conventionally use the same
    operating system tools. PowerShell has few, if any, replicated built in commands
    and no shared conventions making it painfully obvious what was coming from the
    shell and what is part of the operating system outside of the shell.
  prefs: []
  type: TYPE_NORMAL
- en: In general, however, it is most advised to do all scripting in languages that
    are well supported in your environment. Just like we have said in other areas
    of system administration, it is important to use tools that are ubiquitous, well
    understood, and appropriate for the environment. For most that means BASH exclusively.
    BASH is the only scripting environment that is going to be absolutely available
    on every Linux system that you ever encounter. Other shells or scripting languages
    might be common, but none other are so universal.
  prefs: []
  type: TYPE_NORMAL
- en: When BASH proves to be too limiting for more advanced scripting it is uncommon
    to turn to another shell, such as `zsh`, as other shells remain very uncommon
    and generally lack in the extensive power that you are likely looking for once
    you are abandoning BASH. Traditionally it has been non-shell scripting languages
    that are used as BASH alternatives for advanced scripting such as **Python**,
    **Perl**, **Tcl**, **PHP**, and **Ruby**. Ruby has never gained much favor. PHP,
    while very common for certain tasks is pretty rare as a general system automation
    language. Perl and Tcl have fallen out of favor dramatically, but at one time
    Perl was the clear leader in system automation languages. That leaves Python as
    a very clear front runner for advanced scripting needs.
  prefs: []
  type: TYPE_NORMAL
- en: Python has many advantages overall. It is decently fast. It is available on
    nearly any platform or operating system (including all Linux, alternative UNIX,
    macOS, and Windows.) It is quite easy to learn (often used as a first language
    for new programmers to learn.) It is very often already installed because a great
    many applications and tools on Linux depend on Python so you will regularly find
    it already installed even when it is not intentionally installed as a standard.
    Because it is used so commonly for these tasks, it has become increasingly well
    suited to the role as documentation in how to use Python in this way has grown
    and other tools written around it have sprung up.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this time, nearly all system scripting for Linux is done in BASH when possible
    and in Python when more power or flexible is really needed. All other languages
    are really niche use cases. This means that BASH and Python also have additional
    reasons that we should be strongly considering them when choosing languages for
    our own scripting: standardization.'
  prefs: []
  type: TYPE_NORMAL
- en: System automation is different than general programming. With broader programming
    developers spend years learning multiple languages, language families, constructs,
    and spend all of their time in their programming environments. Moving between
    languages, learning a new one, adapting to language changes and so on are all
    part of the daily life of a developer and the overhead to move between languages
    is very low. For a system administrator this is a bit different. In theory we
    spend very little time learning programming and rarely are exposed to any real
    variety of languages. So for administrators, having just one or two languages
    that you use is important for being able to find resources, examples, peer review,
    and others to provide support for our automation in the future. These are certainly
    not the only acceptable languages, but they do hold rather sizable advantages
    over most other options.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, scripting is a very general topic and scripts can run from being
    just a few lines of simple commands run sequentially to giant programs full of
    complex code. Growing your skill in scripting is a topic all to itself and well
    worth investing significant time into. Good scripts will generally include their
    own logging mechanisms, error detection, functions, reusable components, and more.
    You can essentially invest indefinitely in greater and greater programming skills
    to apply to system automation scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Developer tooling for script writing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether you are just writing a very simple script or working on a masterpiece
    of automation to be passed down from generation to generation of system administrators
    in your organization, it may be worth taking an additional step to learn something
    about tooling used in software development to potentially aid in the script writing
    process.
  prefs: []
  type: TYPE_NORMAL
- en: On the simplest side are tools like integrated development environments or IDEs
    that can make writing code faster and easier and help you to avoid errors. Developers
    nearly always use these tools, but system administrators will often overlook them
    as they feel that they write scripts so little of the time that learning another
    tool may not be worth it. And perhaps it is not, but the more tooling you learn
    the more likely you are to use it and to write more scripts. A good IDE can be
    free and quite easy to use, so is a good starting point as you can integrate one
    into your process without really spending any money and only a few minutes to
    download and install one.
  prefs: []
  type: TYPE_NORMAL
- en: The other truly enormous toolset that developers almost universally use and
    system administrators rarely do are code repositories and version control tools
    like Git and Mercurial. With tools like these, and a central hosting of your code
    which is often associated with these tools, we can really leap forward in our
    script writing and management. These tools are also really useful for the management
    of other forms of textual data in our environments. Linux especially uses text-based
    configuration files which can be treated just like scripts and kept under version
    control and stored in version control systems. An excellent use of cross-domain
    skill sharing.
  prefs: []
  type: TYPE_NORMAL
- en: Version control is certainly the most must have technique from the software
    development world for use in our own scripting. Version control allows us to track
    our changes over time, to test code and have a simple ability to roll back, it
    allows for integrating multiple team members into the management of the same scripts,
    it tracks changes by user, empowers code review and auditing, simplifies data
    protection and deployment and so much more. If you use only one major development
    technique, this is the one to use. At first it will feel cumbersome, but quickly
    it will become second nature and make so many things that you do so much easier.
  prefs: []
  type: TYPE_NORMAL
- en: The development world has many other tools that we might potentially use like
    continuous integration, automated deployments, and code testing that all might
    provide to be useful depending on the scripting that we do, but nearly all of
    those are niche and completely optional even in a very heavily automated environment.
    Learning about these tools can expose you to options that may or may not make
    sense for your workflow, and will also give you great insight into how your potential
    development teams may be working.
  prefs: []
  type: TYPE_NORMAL
- en: Look to software engineering as a source of ideas about how to better approach
    your own script writing, but do not feel that you need to or even should adopt
    every tool and technique. Scripting for automation and product development do
    have overlap, but are ultimately different activities.
  prefs: []
  type: TYPE_NORMAL
- en: There is no secret to scripting other than just doing it. There are many good
    books available and resources online. Start with the simplest possible projects,
    look for opportunities to do scripting where you might have done work manually
    before. System tasks such as deployment or system setup checklists can be a great
    place to start. Or scripts to deploy a set of standard tools. Or perhaps a script
    to collect a specific set of data from many machines. I often find myself scripting
    data processing tasks. Once you start looking you will likely find many places
    where some new scripting skills can be put to work.
  prefs: []
  type: TYPE_NORMAL
- en: One of the best places to start using unscheduled scripting is for basic build
    and installation tasks. Using scripts to perform initial system setup and configuration
    including installing packages, adding users, downloading files, setting up monitoring,
    and so on. These tasks generally offer large benefits at relatively little effort
    and can serve as being essentially a form of documentation listing any packages
    and configuration changes needed for a system. Documentation done in this way
    is highly definitive because it truly documents the actual process used rather
    than one that is intended or expected.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation first
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In software engineering circles there is a concept of writing tests to verify
    code. While not perfect, running tests makes it far less likely for software to
    have bugs because there are tests that look for expected behavior and ensure that
    it is happening. We can still have bugs, this is anything but a guarantee, but
    it is a great step. After decades of writing tests for code, the idea that it
    would be feasible to write tests before writing code was floated and in research
    it is sometimes found that in doing so not only are bugs reduced but code writing
    efficiency can actually improve simply because test writing encourages thinking
    about problem solving in good ways. Test-first coding was considered a breakthrough
    in approaching software development.
  prefs: []
  type: TYPE_NORMAL
- en: This concept can carry over to the system administration world, in a manner
    of speaking, with the use of what I call documentation-first engineering. In this
    concept we start by writing documentation and then using that documentation to
    build the system. If it is not documented, we do not build it. Like test-driven
    coding, this approach forces us to think about how we want a system to work ahead
    of time which gives us another opportunity to make sure that what we are doing
    is well planned and sensible. And it allows us a chance to verify that our documentation
    is complete and sensible. When we write documentation after the fact it is far
    easier to make documentation that cannot really be followed for completing a task.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, such as those with low automation levels, this could mean simply
    documenting what we can in a wiki or word processor document and working from
    that as we deploy a system. If we have higher levels of automation then we may
    actually write code as documentation that builds our systems for us. Since the
    code itself functions as the documentation it is not just documentation-first,
    but the documentation actually does the work which absolutely guarantees that
    the documentation is complete and correct!
  prefs: []
  type: TYPE_NORMAL
- en: It is an intrinsic nature of automation to encourage better documentation and
    to move from documenting after the fact to before the fact and on to using the
    documentation itself as the build mechanism. This also means that we can potentially
    see double gains in efficiency as version control and backups and other mechanisms
    that we want to use for both documentation and scripting can be automatically
    applied to both.
  prefs: []
  type: TYPE_NORMAL
- en: Using advanced tools for our scripting may also be considered a higher step
    of automation maturity in a way.
  prefs: []
  type: TYPE_NORMAL
- en: Scripting combined with task scheduling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The hopefully obvious next step is to take task scheduling and our new found
    scripting knowledge and combine the two for even more power. Making complex tasks
    and making them able to run automatically without any human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Common tasks to automate in this manner will often include software updates.
    Having a script that looks for the latest updates, downloads them, prepares the
    environment, and deploys them all automatically on a schedule is very handy. Nearly
    any complex set of tasks that should be performed together can be scheduled in
    this way whether it is every minute or just on the third Tuesday of the month.
    Scripts are also very good for dealing with conditional situations where actions
    should only be performed under certain conditions such as only if storage is beyond
    a certain level or if certain people are logged in.
  prefs: []
  type: TYPE_NORMAL
- en: Almost a special case, and therefore well worth mentioning I believe, is using
    scheduled scripts to manage backups or replication.
  prefs: []
  type: TYPE_NORMAL
- en: State management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most amazing changes that we have experienced in system automation
    is the introduction of state machines and state management for systems. State
    can be a difficult concept to explain as this falls far outside of the normal
    thought processes in IT and system administration. State is often seen as the
    future of systems, however.
  prefs: []
  type: TYPE_NORMAL
- en: 'In traditional systems administration and engineering we talk and think about
    tasks: how do we make a system get from point A to point B. In state theory, we
    do not talk about the *how* of managing systems. Instead, we focus only on the
    intended results or *resultant state.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*To think of it in another way: we start focusing on ends, instead of focusing
    on means. We move from process oriented to being goal oriented.*'
  prefs: []
  type: TYPE_NORMAL
- en: This approach forces us to really change nearly everything that we think about
    and know about systems. It is a game changer in every sense and frees us, as humans,
    to focus far better on what we are good at while allowing the computer to do far
    better what it is good at.
  prefs: []
  type: TYPE_NORMAL
- en: All of this magic is done by what is called a *state machine.* In the context
    of system administration, a state machine is an agent or portion of code that
    is given a document or series of documents that dictate the desired state of the
    system. State can refer to nearly anything about a system such as what packages
    are installed, what the current patch level is, the contents of configuration
    files, the ports open in the firewall, and the list of which services should be
    running.
  prefs: []
  type: TYPE_NORMAL
- en: A state machine will take this documentation as to the intended state of the
    machine and guarantee (or at least attempt) to ensure that the system is in the
    state desired. If a package is missing, it will install it. If a service is not
    running, it will start it. If a configuration file is incorrect, it will correct
    it. The opposite is also true, if a program appears that is not supposed to be
    installed it will be removed. If a service starts that is not supposed to run,
    it will be shut down.
  prefs: []
  type: TYPE_NORMAL
- en: The state machine typically runs every so many minutes or seconds and scans
    its state file and determines how the system should be, then scans the system
    to verify that everything it knows about how it is, and how it should be, match.
    It then takes whatever corrective action is required. Of course, under the hood,
    this is all done by complex scripts and system tools, that when assembled provide
    the power to enforce state. The degree to which corrective action can be taken
    by the state machine is determined by the power of the scripts that it has access
    to use. It is not unlimited, but generally on Linux a state machine will have
    enough power to do whatever is effectively needed in a real world, non-attack
    scenario and will remain highly effective at slowly or thwarting many less intensive
    attacks as well.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, in theory, a state machine keeps a system in a nearly constant
    state that we desire. With state machines we spend our time writing the documentation
    of how we want a system to be and we let the state machine itself worry about
    how to make the system behave in the desired way. This includes the initial setup
    of a machine to take a basic, vanilla operating system install and turn it into
    a functional component of a specific workload. State machines work at the hypervisor
    and cloud levels as well, allowing us to maintain a standard conceptual approach
    not just inside of an individual system, but at the platform level providing the
    systems in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: The end of the login
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The nature of state management is to encourage, if not enforce, the end of the
    concept of logging into a server for administration purposes altogether. Before
    we think of eliminating logins, however, state management systems serve to improve
    traditional remote management through state.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally the biggest risks in remote administration are the needs to open
    ports and to have those ports open flexibly to allow for management from whatever
    location is necessary at the time. The idea of opening very few ports and locking
    them to a single IP address sounds like great security in theory, but is all but
    useless in real world practice. Having the flexibility for an administrator to
    log in quickly, from wherever they are at the time of an emergency, either requires
    too much exposure or far too many steps to limit access.
  prefs: []
  type: TYPE_NORMAL
- en: Enter state management. With state management a system can be instructed via
    the state definition file stored centrally in a repository to enable the SSH service,
    open a random port that gets used for that SSH service, and to lock it to the
    current IP address of an administrator or group of administrators. And the version
    control system will easily track that the change was requested, when it was requested,
    and by whom. In theory there could trivially be an approval step included as well
    as part of the mechanism. Once recorded the system would authorize access for
    the specified administrator(s). And once they were done, or on a set schedule,
    the state management system will revert the changes, after it has all been documented,
    and completely close off all avenues of access. The potential for enhanced security
    is incredible.
  prefs: []
  type: TYPE_NORMAL
- en: But that is only an interim step. With full state management we should, in theory,
    never need to log into a system at all. We should be able to do perform any management
    steps necessary via the state system itself or, even more appropriately, those
    steps should be being performed automatically by the state engine to ensure that
    proper state is maintained.
  prefs: []
  type: TYPE_NORMAL
- en: To fully enable a login-less mechanism of this nature we have to combine something
    like state management with concepts that we talked about in the last chapter such
    as having remote log collection and alerting so that even for tasks like capacity
    planning that there is no need to be physically logged into an individual system.
    To traditional system administrators this often sounds like blasphemy and all
    but impossible, but this is how many companies operate today and it is entirely
    possible with the right work being done up front.
  prefs: []
  type: TYPE_NORMAL
- en: For systems running on physical hardware inside of the office this might sound
    like overkill, and perhaps it is. For systems running on cloud servers this is
    highly practical in many cases. Human intervention should not be needed for a
    properly tested, documented, and running system. Manual management is difficult
    to document, very difficult to repeat, and highly error prone. Of course, human
    intervention can always be saved as a last resort, but the ability to remove it
    completely and depend on redeployment as a last resort is very obtainable today.
  prefs: []
  type: TYPE_NORMAL
- en: Automation maturity models give us a sort of roadmap of how to get from where
    we are to where we hope that we could be. Certainly not every organization has
    to get to the point where state management is handling all of their needs. Not
    every environment even needs to be scripted! Most organizations will continue
    to benefit no matter how far our maturity level is taken.
  prefs: []
  type: TYPE_NORMAL
- en: The final level of the maturity model we are saving for its own section. Taking
    what we have learned and applying the techniques we arrive at...
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Taking concepts that we have discussed here and looking at them another way,
    we discover the concept of *infrastructure as code.* Meaning that we can write
    code or configuration files that represent the entirety of our infrastructure.
    This is powerful and liberating.
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to confuse infrastructure as code concepts with state machine concepts
    because they will, in many cases, overlap quite extensively. There are critical
    differences, however.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as code can go hand in hand with state machines, but state machines
    do not allow for imperative system definitions. Infrastructure as code can be
    used to define state, also known as a declarative approach to infrastructure as
    code, or an imperative approach by which operations are defined rather than final
    state, making it feel much more like traditional systems administration where
    we focus on the means rather than the ends or the *how* rather than the *goal*.
  prefs: []
  type: TYPE_NORMAL
- en: Platforms and systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Infrastructure refers to both the systems that we run, that is the operating
    system containers, as well as the platforms, that is hypervisors and physical
    machines, on which they run. For most of what we are looking at in this section
    and certainly concepts like infrastructure as code, the applicability is equal
    to both aspects of infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: The tools and techniques that we apply at the system level will work with our
    platform level and vice versa. That means that we do not just get to rely on these
    awesome tools and techniques for configuring our operating systems and applications,
    but they can be used to actually deploy and build the virtual machine containers
    (both full virtualization and containerization) in both cloud and traditional
    non-cloud environments.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the same or similar tooling across these two domains means greater
    overall power through conceptual integration and a more complete picture of our
    infrastructure as a whole. A key benefit in much of modern computing is moving
    from purely seeing operating systems as the building blocks for workloads to also
    seeing the hypervisor as playing a direct role in workloads as well. Instead of
    the hypervisor simply providing a space for an operating system to contain a workload,
    the hypervisor or hypervisor cluster can serve as a platform that is workload
    aware and act as a tool in dividing up resources to provide as a workload component
    level.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the hypervisor or even hypervisor cluster level management will
    be aware that it is providing workload containers for application servers, proxies,
    processing nodes, storage, backups, databases, and so forth. Because the platform
    level is workload aware, it can then make intelligent provisioning decisions as
    to not only what kinds of resources will be needed, but also onto which nodes
    a workload should be deployed. If we have a three node hypervisor cluster and
    we provision three application virtual machines our provisioning should know to
    spread these out with only one virtual machine per node to increase redundancy;
    and it should know to do the same with the accompanying database that feeds those
    application servers. But it should also know to keep one application server on
    the same node as one database and to configure those to talk to the local database
    instance rather than a randomly non-local one. Bringing workload level awareness
    all the way down from the application, through the operating system, and down
    to the platform means better performance, with less effort, and more data protection.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, as we move into infrastructure as code, we naturally begin to merge
    systems and platform administration groups because it just makes sense to see
    these as two parts of a larger, holistic infrastructure vision.
  prefs: []
  type: TYPE_NORMAL
- en: Imperative infrastructure as code design gives us many of the upfront benefits
    that we also saw with state machines, but require less work to set up initially
    and much more work to maintain over time. Because of this, imperative systems
    were considered normal when infrastructure as code was first introduced but as
    the market matured and declarative (stateful) tool sets and pre-built imperative
    structures were made available, the shift to declarative infrastructure as code
    was inevitable.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, of course, all systems like this are going to be imperative.
    Any declarative system ultimately uses pre-defined imperative steps to arrive
    at the desired state. So in order to have a declarative system for us to use,
    either we or someone else has to wrote imperative scripts and tools that get us
    from many different starting points and result in the same ending point. It takes
    a lot of time and testing to build these components even once a base state engine
    has been designed.
  prefs: []
  type: TYPE_NORMAL
- en: These scripts have to exist for every task. If you want to define that a file
    must exist then we need tools under the hood that check for the existence of the
    file, logic to determine what to do if the file does not exist, how to find it,
    how to copy it, where to put it, what to do if a file copy fails, and so forth.
    This is probably the simplest use case and you can easily imagine how much more
    complicated it is to deal with any other task. Even a simplistic declarative system
    is going to be made from a myriad of imperative scripts that the administrator
    may never know about. Or it could be built by the administrator for very specific
    needs unique to the systems in question. The concept is flexible, but complex.
  prefs: []
  type: TYPE_NORMAL
- en: In theory we can have declarative state management without going as far as to
    have infrastructure as code. As odd as it may sound, it is actually somewhat common
    for those starting out with state systems to do so almost manually, attempting
    to issue stateful commands in a nearly imperative way to inform the system of
    desired state without documenting all aspects in code before doing so.
  prefs: []
  type: TYPE_NORMAL
- en: It is also common for the tools for these techniques to be deployed, and even
    used, but in a very incomplete way. This can be due to frustration, a lack of
    planning, internal corporate politics, you name it. Because working completely
    in this mode is so intensive and requires great planning up front it can be very
    difficult to obtain adequate time and buy-in from the powers that be to allow
    for complete implementations. Because of this we often see these tools being used
    to roll out basic, well-known functions using pre-built third party scripts, but
    complex configurations often unique to an organization still being done in a traditional
    way. The rush to get systems deployed will often drive this behavior.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to what is really the key best practice around infrastructure
    as code. What matters most is getting broad organizational buy-in and investing
    in proper documentation and code completeness prior to workloads being put into
    production.
  prefs: []
  type: TYPE_NORMAL
- en: Like any code that we would talk about in software engineering circles, our
    infrastructure as code requires testing before being used. Testing system administration
    code is often far easier than other types of applications, however. Creating our
    code for a new workload then attempting to deploy that workload from scratch is
    quite straightforward and can be attempting over and over again until results
    are perfect. Then system modifications, breaks, and other potential scenarios
    can be tested to ensure that the system will respond well under potential real-world
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: One of the great benefits of this type of testing is that the function of our
    code is to create infrastructure out of, roughly, nothing. So all we have to do
    is start with a blank slate and, if things are working correctly, our infrastructure
    will create itself and we can test from there.
  prefs: []
  type: TYPE_NORMAL
- en: It is true that even if we cannot go as far as we would like by creating an
    entire infrastructure that is self-creating, healing, configuring, and destroying
    we can at least use these tools and techniques to go part of the way. Starting
    with simply building the infrastructure from scratch and not maintaining it or
    decommissioning it is an excellent step.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as code as disaster recovery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have touched on this concept elsewhere and will really dig into it in the
    next chapter on Backups and Disaster Recovery, but it is so important that we
    have to discuss it whenever we talk about a constituent component of disaster
    recovery in modern systems. As we have talked about infrastructure as code we
    keep talking about the automated creation of systems where none existed previously.
  prefs: []
  type: TYPE_NORMAL
- en: This is exactly what is needed in a disaster recovery scenario. Our systems
    are gone and we need to bring them back. Having our systems, their standard files,
    their configurations, and more all stored as code in a place where it can easily
    be recovered or even better, not lost at all during a normal disaster, means that
    we are ready to build a new system, anywhere that we want, at the drop of a proverbial
    hat.
  prefs: []
  type: TYPE_NORMAL
- en: Because these types of build systems spend most of their time building workloads
    for testing and production deployment, they are easily justified for being fast,
    efficient, easy to use, well understood, and heavily tested. A system built in
    this way will be built identically during testing, the same in production, and
    the same during an emergency disaster recovery. We do all the hard work to make
    the system fast and repeatable up front so when something terrible happens we
    do not have to deviate from the established process.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional system restores after a disaster require building systems through
    a process that is unique and nothing like the process through which the systems
    were built initially. This is highly error prone for so many reasons. It is a
    process that rarely gets good testing or documentation. It is typically done without
    proper planning and under a high degree of stress. This is the worst possible
    time to be experiencing these conditions. There is so much to go wrong at a time
    when there is the highest pressure to get everything right.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding those problems by creating a system that builds itself the first time,
    the second time, every time, almost instantly and completely identically is truly
    a big deal. If anything justifies the benefits of automation and infrastructure
    as code, it is this. Having the confidence that your systems are going to come
    back fast and correctly is something that most companies do not have today. The
    fear that backups are not going to work, that the knowledge of how to get a system
    up and running correctly configured for a workload is missing, that licenses or
    system details are not documented, or that necessary packages are not readily
    available is dramatic. Instead of overlooking those problems and hoping for the
    best we have a modern approach that makes these issues simply go away.
  prefs: []
  type: TYPE_NORMAL
- en: Getting our organization to the level of truly implementing infrastructure as
    code is a tremendous step, but this is the future. Companies that do this have
    faster builds, faster recoveries from disaster, better security, are more agile,
    scale faster, and all of that means have a better chance of making more money.
    At the end of the day, our only job as system administrators is to do our job
    in such a way that we can increase the bottom line of the organization through
    our efforts, no matter how indirect and impossible to measure that they may be.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what the techniques are it is time to talk about the actual,
    already existing, ready to be tested tools that make things like state machines
    and infrastructure as code possible.
  prefs: []
  type: TYPE_NORMAL
- en: Modern tools of automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of this power comes primarily from modern tools that have been being introduced
    into the realm of system administration over the last fifteen to twenty years.
    The Linux world has been very fortunate to have been at the forefront of this
    movement since the very beginning. This comes naturally both because the Linux
    community tends to be one that thrives on and focuses on innovation, but also
    because the intrinsic nature of a system built around command line interfaces
    and simple text files for configuration and software repositories all make for
    vastly simpler automation. The design of Linux may not have been intentional to
    encourage automation, but nearly every major aspect of both the system implementation
    and the behavior of the ecosystem have led to it having the most ideal combination
    of factors to nearly always make it the leader in new automation tools and strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration management systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: New tools are always arising and techniques do vary over time so making a definitive
    list here is not possible, but there are some important tools that have managed
    to make a name for themselves to a point that they are worth mentioning as starting
    points for an investigation into the tooling that will likely make sense for your
    environment. The biggest tools for this type of automation, for infrastructure
    as code, at the time of this writing include **Chef**, **Puppet**, **SaltStack**,
    **CFEngine**, **Ansible**, and **Terraform**. The oldest of these, **CFEngine**,
    is so old that it was first introduced in its earliest form less than two years
    after the first Linux kernel was introduced!
  prefs: []
  type: TYPE_NORMAL
- en: All of these tools, often referred to as configuration management systems, share
    the common approach of allowing you to write code and configuration files to define
    your infrastructure environment and they manage the automation of the infrastructure
    based on that code. Most of them offer multiple functional behaviors such as the
    option to function either imperatively or declaratively. Most also offer the option
    to either pull configuration from the systems being managed or to push configuration
    from a central management location. So you get the range of functional options
    plus a range of product options.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest differences between these products really come down to the style
    of coding that is used to document your infrastructure and the cost, support,
    or licensing of the products. Most of the products in this space either started
    as or moved to being open source. Open source makes a lot of sense as the space
    has matured quickly and management tooling somewhat naturally tends towards open
    source as it is mostly made by end users or those that came from open source communities
    and is used only in technical circles. Closed source products naturally lend themselves
    towards more customer-visible products where managers, rather than engineers,
    are choosing them. A key strength to most infrastructure as code tools is that
    they are free, open source, and often included in the Linux distribution so engineering
    and administration teams can generally choose to test and deploy them even into
    production without management needing to approve or even be aware that they are
    doing so. Because of the licensing and use cases it is little different than needing
    to deploy OpenSSH or any other standard component used in day-to-day system administration.
  prefs: []
  type: TYPE_NORMAL
- en: Of course the easiest approach here is to simply read about a few tools, download
    and install a few, and see what you like. Working with more than one is not a
    bad thing and most of the concepts will carry from one to another, even if the
    style of scripting and documentation vary. Many of these tools are cross platform
    and while they are generally designed with Linux as the primary platform both
    for deployment and to be managed, it is not uncommon for other platforms, especially
    BSD but also Windows, macOS, and others, may be able to be managed as well. Consider
    the possibility of choosing a platform that will be expandable to meet all of
    your organization's needs well into the future. If you are a heterogeneous shop
    you may want to invest your technical know-how into a platform that could be managing
    everything across the domains rather than only managing Linux, even if Linux is
    where you start.
  prefs: []
  type: TYPE_NORMAL
- en: Desktops are servers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thinking of desktops as a form of server can be a bit confusing, but in a way,
    they are. They are simply one to one end user GUI servers that are generally deployed
    to desktops or homes rather than in the datacenter. This is a trivial bit of semantics
    until we start trying to understand how desktops may or may not fit into our greater
    support strategies.
  prefs: []
  type: TYPE_NORMAL
- en: But when we consider that desktops are truly just a special class of servers
    it quickly becomes apparent that it makes sense to potentially group them in for
    common administration as well. Of course if we are using a Linux distribution
    for a desktop this is much more obvious than if we are using Windows, for example,
    but the truth remains the same. If we are going to potentially support Linux and
    Windows servers both using the same tooling, there is really no barrier to doing
    the same for desktops regardless of the operating system(s) deployed there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditionally, but for no reason of which I am aware, servers and desktops
    have been managed using very different toolsets. I can hypothesize many reasons
    why this might be. Typically, two separate teams provide this management and each
    chooses its own tooling independently. Servers grew up in one world while desktop
    support grew up in another. Vendors want to sell more tools and catering to hubris
    always makes sales easy. And the most likely factor: servers are typically administered
    from the command line and most desktop support teams expect to work purely from
    a GUI.'
  prefs: []
  type: TYPE_NORMAL
- en: When we step back and look at desktops (and other end user devices) as if they
    are servers it is easy to see that the same tools to manage, document, and monitor
    servers work equally well with desktops. There is no real reason to treat them
    differently. Of course desktop support teams will always require the ability to
    remotely see, and probably interact with, the end user's graphical desktop to
    be able to assist the end user's issues directly, but that is a different need
    entirely from administration duties.
  prefs: []
  type: TYPE_NORMAL
- en: Desktops can be, or even should be, managed using the same advanced tools and
    techniques like infrastructure as code and state machines as we would with any
    other server. That is correct, when we understand that they are actually a server,
    then rules that apply to all servers also apply to desktops. Good semantics make
    everything easier to understand.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, there is even a good argument that end user devices are among the most
    valuable to apply these techniques to because they are the most likely to need
    to be rebuilt, modified, share profiles, undergo changes, be compromised, or need
    to be managed while not in communication with central services. This last point
    is especially valuable because state machines that keep functioning with their
    current state will continue to provide security and self-healing characteristics
    for machines that are off of the network and can enforce policies independently.
    This is possible to do in more traditional ways but is harder and less likely
    to be done.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the nature of how infrastructure as code systems tend to be deployed
    it is far more likely, as well, for these systems to keep functioning when a desktop
    or laptop is off of the corporate LAN compared to traditional management tooling
    that is generally built entirely around the LAN concept. Because end user devices
    traditionally have a high frequency of either moving on and off of or simply never
    existing on the LAN having tools that are not LAN-centric is typically much more
    important in the end user space than in the datacenter space already.
  prefs: []
  type: TYPE_NORMAL
- en: In some ways, mobile device management, or MDM as it is generally known, is
    an attempt to make tools that work more likely infrastructure as code and state
    machines, but that are presented more like traditional tools and sold through
    more traditional channels with a focus solely on end user management. These tools
    have been successful, I feel, because they are copying much of the technology
    and common approaches from this space and once we work with these tools we typically
    find that mobile device management tools do not make sense unless we are lacking
    these capabilities in our organizations.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the benefits of any new system, of course, come from convention rather
    than strict definition. One of the largest conventions in the infrastructure as
    code space is the move from LAN-centric to network agnostic system management
    deployments. It is common, and nearly expected, that the tooling for infrastructure
    as code systems will be hosted in some public form whether on cloud, VPS, or colocation,
    and kept external to any LAN(s) that may exist for your organization.
  prefs: []
  type: TYPE_NORMAL
- en: Hosting management infrastructure outside of the LAN means that any LAN-centric
    ties that we might otherwise make accidentally or casually are no longer possible
    unless we deploy a LAN extension technology like a VPN. This convention naturally
    moves us away from deploying technology that only works inside of a LAN or that
    uses the LAN boundaries as security features. Eliminating the LAN boundaries frees
    us to manage multiple sites, mobile users, even multiple organizations transparently
    from a single platform, as long as those systems use the Internet. Traditional
    systems break under so many circumstances, while also having common security vulnerabilities
    both because of the tendency to break easily or have gaps in utilization, and
    because LAN-centric thinking is not good security practice.
  prefs: []
  type: TYPE_NORMAL
- en: Version control systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The other category of tools that we should really address within the topic of
    automation is code repositories and version control systems. These are technically
    two separate things, but almost always go hand in hand.
  prefs: []
  type: TYPE_NORMAL
- en: At their core, version control systems simply keep track of the changes made
    to documents that we have so that we can track essential data such as who made
    a change, when it was made, and what the document looked like before, at the time
    of, and at a later date than the change. This alone is quite powerful, but most
    any system that does this today also serves to distribute the code and the version
    control so that it can be used by multiple people, in multiple places. And in
    being able to do that, can also be used to populate a central repository which
    can be treated as a master location for storage so that no single person's endpoint
    needs to be considered vital for the purposes of data protection. That central
    location becomes a location for backups and recovery, as well!
  prefs: []
  type: TYPE_NORMAL
- en: Version control systems influence many modern document systems and we covered
    both in our earlier chapter on documentation, but in this context we could look
    at real world products such as Google Docs, Microsoft Office Online, and Zoho
    Docs, all of which present traditional document file types or interfaces, but
    all provide version control of those documents. These are very clunky to use for
    the purpose of coding and code management, but all will serve in a pinch if you
    are looking to just get started quickly using what you already have deployed.
    These systems are essentially copying the mechanisms of traditional code version
    control systems and applying them to spreadsheets and word processing.
  prefs: []
  type: TYPE_NORMAL
- en: Since these office document types are so well known, it is almost easier to
    picture these as the standard (they are not) and thinking of code version control
    systems as being a code-specific modification of those document tools (they are
    not, they came first.) These systems generally (and by generally, I mean every
    situation that I am aware of) work with standard text files and so can be used
    with any text editing tools whether you work with something basic like *vi* or
    *nano* directly on the Linux command line or you work with robust tools like *Atom*
    or *MS Visual Studio Code* that provide fully graphical coding environments with
    deep editing awareness and features, you can use version control systems. Some
    advanced environments will actually have version control integrated directly into
    the applications so that you can automate the entire process from a single place
    and make it look and feel much closer to the office style tools!
  prefs: []
  type: TYPE_NORMAL
- en: 'In a practical sense, at this time, two protocols for version control have
    risen so far to the top that it almost feels like there are only two choices really
    left on the market: *git* and *mercurial.* In reality, there are many, but only
    these two require mentioning. Feel free to research other tools and protocols,
    but make sure that these two are included in any research short list that you
    might have. Both are free and work similarly and allow for all of the features
    that you often expect today including central repositories, copies on end user
    machines, automated deployments, version meta data, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the protocols that are used, much of the power stemming from version
    control systems today come from the online repository services that power them.
    Of these there are more and you can run your own in house as well. The two key
    players are Microsoft's GitHub and the open source GitLab. Both are hosted services
    with extensive free offerings, and GitLab also offers their software for free
    that you can host yourself if this is a business or technical requirement for
    your environment. These two services, and others like them, provide central git
    and Mercurial repository locations, a centralized location for backups, a simple
    web GUI for code manipulation and management, and a raft of processes, tools,
    and services around code automation. Much of which is likely overkill or useless
    in a system administration environment, but much of it does have a potential use.
    You can certainly get the benefits that you need without these types of services,
    but it is far harder to do so and nearly all successful environments have been
    relying on them for years. They are almost always free for the needs of system
    administration, do not avoid them. As hosted services, their use is yet another
    means of *breaking free* from LAN thinking, as well.
  prefs: []
  type: TYPE_NORMAL
- en: There is not much of best practices or even rules of thumb to discuss when talking
    about tools. Testing multiple tools, keeping up with the market as to what is
    available and what is nearly developed, evaluating the tools that make sense for
    your organization, and learning your chosen tools inside and out are all standard
    good approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rules of Thumb:'
  prefs: []
  type: TYPE_NORMAL
- en: Management tools should almost always be open source. This is an area where
    security is of the absolute utmost importance and where licensing limitations
    create security risks on their own. So open-source matters more here than in most
    areas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy management tools in a network-agnostic way. This means deploying them
    on the Internet in a place that is accessible to reasonably any machine located
    anywhere. Avoid any semblance of requiring or relying upon traditional LAN networking
    for connectivity or security unless absolutely necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Best Practice**: Keep code of all types under version control and in a repository.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we have covered the techniques and talked briefly about a handful of real
    world tools that you can use to initiate your investigations into tools that you
    want to try to deploy and learn for your own environment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we have looked at why automation is important. We investigated
    how we should approach automation and where to look to get started. We discussed
    maturity modeling. We delved into the rather complex topics of state machines
    and infrastructure as code. And finally we tackled actual tools that you can download
    and learn today to take your coding to a totally different level, entirely.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our next chapter we are going to be going into one of the absolutely most
    important, and most commonly avoided, topics in system administration: backups
    and disaster recovery. Do not try to skip over this coming chapter, if there is
    one thing that we need to get right in administration, it is our ability to avoid
    or recovery from a disaster.'
  prefs: []
  type: TYPE_NORMAL
