- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Exploring the Actual Filesystems Under the VFS
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 VFS 下的实际文件系统
- en: “Not all roots are buried down in the ground, some are at the top of a tree.”
    — Jinvirle
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “不是所有的根都埋在地下，有些根在树顶。” — Jinvirle
- en: 'The kernel’s I/O stack can be broken down into three major sections: the **virtual
    filesystem** (**VFS**), the **block layer**, and the **physical layer**. The different
    flavors of filesystems supported by Linux can be thought of as the tail end of
    the VFS layer. The first two chapters gave us a decent understanding of the role
    of VFS, the major structures used by VFS, and how it aids the end user processes
    to interact with the different filesystems through a common file model. This means
    that we’ll now be able to use the word *filesystem* in its commonly accepted context.
    Finally.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 内核的 I/O 栈可以分为三个主要部分：**虚拟文件系统**（**VFS**）、**块层**和**物理层**。Linux 支持的不同类型的文件系统可以视为
    VFS 层的尾端。前两章使我们对 VFS 的角色、VFS 使用的主要结构以及它如何帮助终端用户进程通过通用文件模型与不同的文件系统交互有了一个不错的理解。这意味着，我们现在将能够以通常接受的方式使用
    *文件系统* 这个词。终于。
- en: In [*Chapter 2*](B19430_02.xhtml#_idTextAnchor028), we defined and explained
    some important data structures used by the VFS to define a generic framework for
    different filesystems. In order for a particular filesystem to be supported by
    the kernel, it should operate within the boundaries defined in this framework.
    But it is not mandatory that all the methods defined by the VFS are used by a
    filesystem. The filesystems should stick to the structures defined in the VFS
    and build upon them to ensure commonality between them, but as each filesystem
    follows a different approach for organizing data, there might be a ton of methods
    and fields in these structures that are not applicable to a particular filesystem.
    In such cases, filesystems define the relevant fields as per their design and
    leave out the non-essential information.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第2章*](B19430_02.xhtml#_idTextAnchor028) 中，我们定义并解释了 VFS 使用的一些重要数据结构，这些数据结构为不同的文件系统定义了一个通用框架。为了使某个特定的文件系统能够被内核支持，它应该在这个框架定义的边界内运行。但是，并非所有由
    VFS 定义的方法都必须被文件系统使用。文件系统应遵循 VFS 中定义的结构并在此基础上进行扩展，以确保它们之间的通用性，但由于每个文件系统在组织数据方面的方式不同，这些结构中可能会有许多方法和字段对于某个特定文件系统并不适用。在这种情况下，文件系统根据其设计定义相关字段，省略不必要的信息。
- en: As we’ve seen, the VFS is sandwiched between user-space programs and *actual*
    filesystems and implements a common file model so that applications can use uniform
    access methods to perform their operations, regardless of the underlying filesystem
    in use. We’re now going to shift our focus to one particular side of this *sandwich*,
    which is the filesystems that contain user data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，VFS 位于用户空间程序与*实际*文件系统之间，并实现了一个通用文件模型，以便应用程序可以使用统一的访问方法来执行操作，而不管底层使用的是哪种文件系统。现在，我们将重点关注这个*三明治*的一个方面，即包含用户数据的文件系统。
- en: This chapter will introduce you to some of the more common and popular filesystems
    used in Linux. We’ll cover the working of the extended filesystem in great detail
    as it is most commonly used. We’ll also shed some light on **network filesystems**,
    and cover a few important concepts related to filesystems such as journaling,
    filesystems in user-space, and **copy-on-write** (**CoW**) mechanisms.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将向你介绍一些在 Linux 中使用的更常见和流行的文件系统。我们将详细讨论扩展文件系统的工作原理，因为它是最常用的。我们还将介绍一些**网络文件系统**，并探讨与文件系统相关的一些重要概念，如日志记录、用户空间中的文件系统和**写时复制**（**CoW**）机制。
- en: 'We’re going to cover the following main topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将覆盖以下主要主题：
- en: The concept of journaling
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录的概念
- en: CoW mechanisms
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoW 机制
- en: The extended filesystem family
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展文件系统家族
- en: Network filesystems
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络文件系统
- en: Filesystems in user space
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户空间中的文件系统
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter focuses entirely on filesystems and associated concepts. If you
    have experience with storage administration tasks in Linux but haven’t delved
    into the inner workings of filesystems, this chapter will serve as a valuable
    exercise. Having prior knowledge of filesystem concepts will enhance your understanding
    of the content covered in this chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章完全聚焦于文件系统及相关概念。如果你有 Linux 存储管理经验，但尚未深入了解文件系统的内部工作原理，那么本章将成为一个宝贵的练习。了解文件系统概念的前置知识将有助于你更好地理解本章所涉及的内容。
- en: The commands and examples presented in this chapter are distribution-agnostic
    and can be run on any Linux operating system, such as Debian, Ubuntu, Red Hat,
    Fedora, and so on. There are a few references to the kernel source code. If you
    want to download the kernel source, you can download it from [https://www.kernel.org](https://www.kernel.org).
    The code segments referred to in this book are from kernel `5.19.9`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中展示的命令和示例与发行版无关，可以在任何 Linux 操作系统上运行，如 Debian、Ubuntu、Red Hat、Fedora 等等。文中有一些关于内核源代码的参考。如果你想下载内核源代码，可以从[https://www.kernel.org](https://www.kernel.org)下载。本书中提到的代码片段来自内核
    `5.19.9`。
- en: The Linux filesystem gallery
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Linux 文件系统画廊
- en: As said earlier, one of the major benefits of using Linux is the wide range
    of supported filesystems. The kernel contains out-of-the-box support for some
    of these, such as XFS, Btrfs, and extended filesystem versions 2, 3, and 4\. These
    are considered **native filesystems** as they were designed keeping in mind the
    Linux principles and philosophies. On the other side of the aisle are filesystems
    such as NTFS and FAT. These can be considered **non-native filesystems**. This
    is because, although the Linux kernel is capable of understanding these filesystems,
    supporting them usually requires additional configuration as they do not fall
    in line with the conventions adopted by native filesystems. We’re going to keep
    our focus on the native filesystems and explain the key concepts associated with
    them.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，使用 Linux 的一个主要好处是支持的文件系统种类繁多。内核对其中一些文件系统提供开箱即用的支持，如 XFS、Btrfs 和扩展文件系统 2、3
    和 4 版本。这些被认为是**本地文件系统**，因为它们在设计时就考虑到了 Linux 的原则和理念。另一方面，像 NTFS 和 FAT 这样的文件系统可以被视为**非本地文件系统**。这是因为，尽管
    Linux 内核能够理解这些文件系统，但它们通常需要额外的配置支持，因为它们不符合本地文件系统采用的约定。我们将重点讨论本地文件系统，并解释与它们相关的关键概念。
- en: 'Although each filesystem claims to be better, faster, and more reliable and
    secure than all others, it is important to note that no filesystem can be the
    best fit for all kinds of applications. Every filesystem comes with its strengths
    and limitations. From a functional standpoint, filesystems can be classified as
    follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个文件系统都声称比其他文件系统更好、更快、更可靠和更安全，但需要注意的是，没有任何一个文件系统能够适用于所有类型的应用程序。每个文件系统都有其优点和局限性。从功能的角度来看，文件系统可以分类如下：
- en: '![Figure 3.1 – Linux filesystem gallery](img/B19430_03_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – Linux 文件系统画廊](img/B19430_03_01.jpg)'
- en: Figure 3.1 – Linux filesystem gallery
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – Linux 文件系统画廊
- en: '*Figure 3**.1* gives a glimpse of some of the supported filesystems and their
    respective categories. Given the plethora of filesystems supported by Linux, covering
    all of them will make us run out of space (filesystem pun!). Although the implementation
    details vary, filesystems usually make use of some common techniques for their
    internal operations. Some core concepts, such as journaling, are more common among
    filesystems. Similarly, some filesystems make use of the popular CoW technique,
    due to which they do not need journaling.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.1* 展示了一些支持的文件系统及其各自的类别。由于 Linux 支持大量的文件系统，覆盖所有文件系统将占用我们太多空间（这是一个文件系统的双关语！）。尽管实现细节有所不同，但文件系统通常会利用一些常见的技术来进行内部操作。一些核心概念，如日志记录，在文件系统中更为常见。类似地，一些文件系统使用了流行的
    CoW 技术，因此它们不需要日志记录。'
- en: Let’s explain the concept of journaling in filesystems.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来解释文件系统中的日志记录概念。
- en: The diary of a filesystem – the concept of journaling
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件系统的日志记录 – 日志记录的概念
- en: A filesystem uses complex structures to organize data on the physical disk.
    In the case of a system crash or abrupt failure, a filesystem is unable to finish
    off its operations in a graceful manner, which can corrupt its organizational
    structures. When the system is powered up the next time, the user will need to
    run a consistency or integrity check of some sort against the filesystem to detect
    and repair those damaged structures.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统使用复杂的结构来组织物理磁盘上的数据。在系统崩溃或突然故障的情况下，文件系统无法以优雅的方式完成其操作，这可能会损坏其组织结构。当系统下次启动时，用户需要对文件系统进行一致性或完整性检查，以检测和修复那些损坏的结构。
- en: 'When explaining VFS data structures in [*Chapter 2*](B19430_02.xhtml#_idTextAnchor028),
    we discussed that one of the fundamental principles followed in Linux is the separation
    of metadata from actual data. The metadata of a file is defined in an independent
    structure, called an **inode**. We also saw how a directory is treated as a special
    file and it contains the mapping of filenames to their inode numbers. Keeping
    this in mind, let’s say we’re creating a simple file to add some text to it. To
    go through with this, the kernel will need to perform the following operations:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 2 章*](B19430_02.xhtml#_idTextAnchor028) 中解释 VFS 数据结构时，我们讨论了 Linux 遵循的一个基本原则——将元数据与实际数据分离。文件的元数据在一个独立的结构中定义，称为
    **inode**。我们还看到，目录作为特殊文件来处理，并包含文件名到其 inode 编号的映射。记住这一点，假设我们正在创建一个简单的文件并向其中添加一些文本。为了实现这一点，内核需要执行以下操作：
- en: Create and initialize a new inode for the file to be created. An inode should
    be unique within a filesystem.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为要创建的文件创建并初始化一个新的 inode。一个 inode 在文件系统中应该是唯一的。
- en: Update the timestamps for the directory in which the file is being created.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新文件所在目录的时间戳。
- en: Update the inode for the directory. This is required so that the filenames-to-inode
    mapping is updated.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新目录的 inode。这是必需的，以便更新文件名到 inode 的映射关系。
- en: Even for an operation as simple as text file creation, the kernel needs to perform
    several I/O operations to update multiple structures. Let’s say that while performing
    one of these operations, there is a hardware or power failure due to which the
    system shuts down abruptly. All the operations required for creating a new file
    will not have completed successfully, which will render the filesystem structurally
    incomplete. If an inode for the file was initialized and not linked to the directory
    containing the file, the inode will be considered *orphaned*. Once the system
    is back online, a consistency check will be run on the filesystem, which will
    remove any such inodes that are not linked to any directory. After a crash, the
    filesystem itself might remain intact, but individual files could be impacted.
    In a worst-case scenario, the filesystem itself can also become permanently damaged.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是像文本文件创建这样简单的操作，内核也需要执行多个 I/O 操作以更新多个结构。假设在执行这些操作时，由于硬件或电力故障导致系统突然关闭。此时，创建新文件所需的所有操作都没有成功完成，这将导致文件系统结构不完整。如果文件的
    inode 已初始化但未链接到包含该文件的目录，则该 inode 将被视为*孤立*的。一旦系统重新上线，文件系统将进行一致性检查，删除任何未链接到任何目录的
    inode。系统崩溃后，文件系统本身可能保持完整，但个别文件可能会受到影响。在最坏的情况下，文件系统本身也可能会永久损坏。
- en: To improve filesystem reliability in case of outages and system crashes, the
    feature of journaling was introduced in filesystems. The first filesystem to support
    this feature was IBM’s **JFS**, also known as **Journaled Filesystem**. Over the
    last few years, journaling has become an essential ingredient in the design of
    filesystems.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高文件系统在发生中断和系统崩溃时的可靠性，文件系统引入了日志功能。第一个支持此功能的文件系统是 IBM 的 **JFS**，即 **日志文件系统**。近年来，日志记录已成为文件系统设计中的重要组成部分。
- en: The concept of filesystem journaling finds its roots in the design of database
    systems. In most databases, journaling guarantees data consistency and integrity
    in case a transaction fails due to external events, such as hardware failures.
    A database journal will keep track of uncommitted changes by recording such operations
    in a journal. When the system comes back online, the database will perform a recovery
    using the journal. Journaling in filesystems follows the same route.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统日志功能的概念起源于数据库系统的设计。在大多数数据库中，日志记录保证了数据的一致性和完整性，以防事务因外部事件（如硬件故障）而失败。数据库日志会通过记录操作来跟踪未提交的更改。当系统重新上线时，数据库将使用日志进行恢复。文件系统的日志功能也遵循相同的方式。
- en: Any changes that need to be performed on the filesystem are first written sequentially
    to a journal. These changes or modifications are referred to as transactions.
    Once a transaction has been written to a journal, it is then written to the appropriate
    location on the disk. In the case of a system crash, the filesystem replays the
    journal to see whether any transaction is incomplete. When the transaction has
    been written to its on-disk location, it is then removed from the journal.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 任何需要在文件系统上执行的更改，首先会顺序地写入日志。这些更改或修改被称为事务。一旦事务被写入日志，它会被写入磁盘上的相应位置。如果发生系统崩溃，文件系统会回放日志，查看是否有任何事务未完成。当事务已写入磁盘上的位置后，它就会从日志中删除。
- en: 'Depending on the journaling approach, either metadata or actual data (or both)
    is first written to the journal. Once data has been written to the filesystem,
    the transaction is removed from the journal:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 根据日志记录的方式，首先会将元数据或实际数据（或两者）写入日志。一旦数据被写入文件系统，事务就会从日志中删除：
- en: '![Figure 3.2 – Journaling in filesystems](img/B19430_03_02.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 文件系统中的日志记录](img/B19430_03_02.jpg)'
- en: Figure 3.2 – Journaling in filesystems
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 文件系统中的日志记录
- en: It’s important to note that, by default, the filesystem journal is also stored
    on the same filesystem, albeit in a segregated section. Some filesystems also
    allow storing the journal on a separate disk. The size of the journal is typically
    just a few megabytes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，默认情况下，文件系统日志也存储在同一文件系统中，尽管它被存储在一个隔离的区域。有些文件系统还允许将日志存储在独立的磁盘上。日志的大小通常只有几兆字节。
- en: The burning question – doesn’t journaling adversely affect performance?
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那个令人迫切想知道的问题——日志记录不会对性能产生负面影响吗？
- en: The entire point of journaling is to make the filesystem more reliable and preserve
    its structures in case of system crashes and hardware failures. With a journaling
    filesystem, data is first written to a journal and then to its specified disk
    location. It’s not difficult to see that we’re adding an extra hop to reach our
    destination as we’ll need to write the same data twice. Surely this is going to
    backfire and undermine the filesystem performance?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录的整个意义在于提高文件系统的可靠性，并在系统崩溃和硬件故障的情况下保护其结构。在启用日志记录的文件系统中，数据首先写入日志，然后再写入其指定的磁盘位置。显而易见，我们在到达目的地时增加了额外的步骤，因为我们需要将相同的数据写两次。这肯定会适得其反，破坏文件系统的性能吧？
- en: It’s one of those questions whose answer seems obvious but it isn’t. The filesystem
    performance doesn’t necessarily deteriorate when using journaling. In fact, in
    most cases, it’s the exact opposite. There could be workloads where the difference
    in both cases is negligible, but in most scenarios, especially in metadata-intensive
    workloads, filesystem journaling actually boosts performance. The degree to which
    the performance is enhanced can vary.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个看似答案显而易见，但实际上并非如此的问题。使用日志记录并不一定会导致文件系统性能下降。事实上，在大多数情况下，情况恰恰相反。某些工作负载下，两者之间的差异可能微不足道，但在大多数场景中，尤其是在元数据密集型的工作负载下，文件系统日志记录实际上可以提高性能。性能提升的程度可能有所不同。
- en: Consider a filesystem without journaling. Every time a file is modified, the
    natural course of action is to perform the relevant modifications on the disk.
    For metadata-intensive operations, this could negatively impact performance. For
    instance, modifications in file contents also require that the corresponding timestamps
    of the file are also updated. This means that every time a file is processed and
    modified, the filesystem has to go and update not only the actual file data but
    also the metadata. When journaling is enabled, **fewer seeks** to the physical
    disk are required as data is written to disk only when a transaction has been
    committed to the journal or when the journal fills up. Another benefit is the
    use of sequential writes in a journal. When using a journal, random write operations
    are converted into sequential writes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个没有日志记录的文件系统。每次修改文件时，采取的自然行动是直接在磁盘上执行相关的修改。对于元数据密集型操作，这可能会对性能产生负面影响。例如，文件内容的修改还需要相应地更新文件的时间戳。这意味着每次处理和修改文件时，文件系统不仅需要更新实际的文件数据，还需要更新元数据。启用日志记录后，**对物理磁盘的查找次数较少**，因为数据仅在事务已提交到日志或日志已满时才会写入磁盘。另一个好处是日志中使用了顺序写入。在使用日志时，随机写操作会转化为顺序写操作。
- en: In most cases, performance improvement occurs as a result of the cancellation
    of metadata operations. When metadata updates are required in a swift manner,
    such as recursively performing operations on a directory and its contents, the
    use of journaling can improve performance by reducing frequent trips to disks
    and performing multiple updates in an atomic operation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，性能的提升是通过取消元数据操作来实现的。当需要快速更新元数据时，比如递归地对目录及其内容进行操作，使用日志记录可以通过减少频繁的磁盘访问并在原子操作中执行多个更新来提高性能。
- en: Of course, how a filesystem implements journaling also plays a major role in
    this. Filesystems offer different approaches when it comes to journaling. For
    instance, some filesystems only journal the metadata of a file, while others write
    both metadata and actual data in a journal. Some filesystems also offer flexibility
    in their approach and allow end users to decide the journaling mode.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，文件系统如何实现日志记录在其中也起着重要作用。不同的文件系统在日志记录方面提供了不同的处理方式。例如，一些文件系统只记录文件的元数据，而另一些则在日志中同时记录元数据和实际数据。一些文件系统还提供灵活的处理方式，允许最终用户自行决定日志记录模式。
- en: To summarize, journaling is an important constituent of modern filesystems as
    it makes sure that the filesystem remains structurally sound, even in the case
    of a system crash.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，日志记录是现代文件系统的重要组成部分，因为它确保即使在系统崩溃的情况下，文件系统仍然保持结构的完整性。
- en: The curious case of CoW filesystems
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CoW文件系统的奇特情况
- en: '`fork ()` system call. The `fork ()` system call creates a new process by duplicating
    the calling process. When a new process is created using a `fork ()` system call,
    memory pages are shared between the parent and child processes. As long as the
    pages are being shared, they cannot be modified. When either the parent or child
    process attempts to modify a page, the kernel duplicates the page and marks it
    writable.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`fork()`系统调用。`fork()`系统调用通过复制调用进程来创建一个新进程。当使用`fork()`系统调用创建新进程时，父子进程之间会共享内存页面。只要页面被共享，它们就不能被修改。当父进程或子进程尝试修改页面时，内核会复制该页面并将其标记为可写。'
- en: Most filesystems in Linux that have existed for a long time use a very conventional
    approach when it comes to the core design principles. Over the past several years,
    two major changes in the extended filesystem have been the use of journaling and
    extents. Although efforts have been made to scale the filesystems for modern use,
    some major areas such as error detection, snapshots, and deduplication have been
    left out. These features are the need of today’s enterprise storage environments.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux中，长期存在的大多数文件系统在核心设计原则上采用了非常传统的方法。在过去的几年里，扩展文件系统的两个主要变化是使用日志记录和扩展（extents）。尽管已经采取了一些措施来扩展文件系统以适应现代应用，但一些关键领域如错误检测、快照和去重等却被忽略了。这些功能是当今企业存储环境中的需求。
- en: 'Filesystems that use the CoW approach for writing data differ from other filesystems
    in a notable way. When overwriting data on an Ext4 or XFS filesystem, the new
    data is written on top of the existing data. This means that the original data
    will be destroyed. Filesystems that use the CoW approach copy the old data to
    some other location on disk. The new incoming data is written to this new location.
    Hence, the phrase *Copy on Write*. As the old data or its snapshot is still there,
    the space utilization on the filesystem will be a lot more than the user would
    expect to see. This is often confusing to newer users and it might take some time
    to get used to this. Some Linux folks have a rather funny take on this: *CoWs
    ate my data*. As shown in *Figure 3**.3*, filesystems using the CoW approach write
    incoming data to a new block:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CoW（写时复制）方法的文件系统与其他文件系统有一个显著的不同。当在Ext4或XFS文件系统上覆盖数据时，新数据会写到现有数据上方。这意味着原始数据会被销毁。而使用CoW方法的文件系统则将旧数据复制到磁盘的其他位置，新的数据会写入这个新位置。因此，才有了*写时复制*这一术语。由于旧数据或其快照仍然存在，文件系统上的空间利用率会比用户预期的要高得多。这常常让新手用户感到困惑，可能需要一段时间才能适应。一些Linux用户对此有一种相当幽默的看法：*写时复制吃掉了我的数据*。如*图3.3*所示，使用CoW方法的文件系统会将新数据写入新的块：
- en: '![Figure 3.3 – The CoW approach in filesystems](img/B19430_03_03.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3 – 文件系统中的CoW方法](img/B19430_03_03.jpg)'
- en: Figure 3.3 – The CoW approach in filesystems
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 文件系统中的CoW方法
- en: As an analogy, we can loosely relate this to the concept of time travel in movies.
    When someone travels back in time and makes changes to the past, a parallel timeline
    is created. This creates a separate copy of the timeline that diverges from the
    original. CoW filesystems operate similarly. When a modification is requested
    on a file, instead of directly modifying the original data, a separate copy of
    the data is created. The original data remains intact while the modified version
    is stored separately.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作为类比，我们可以将其与电影中的时间旅行概念进行粗略比较。当有人回到过去并对过去做出更改时，会创建一条平行时间线。这会产生与原始时间线不同的副本。CoW文件系统的操作方式类似。当请求修改文件时，系统不会直接修改原始数据，而是创建数据的一个单独副本。原始数据保持不变，而修改后的版本则存储在另一个位置。
- en: Since the original data is preserved in the process, this opens up some interesting
    avenues. Because of this approach, filesystem recovery in the case of a system
    crash is simplified. The previous state of data is saved on an alternate location
    on disk. Hence, if there's an outage, the filesystem can easily revert to its
    former state. This makes the need for maintaining any journal obsolete. This also
    allows for the implementation of snapshots at the filesystem level. Only modified
    data blocks are copied to a new location. When a filesystem needs to be restored
    using a particular snapshot, the data can be easily reconstructed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在此过程中保留了原始数据，这为我们开辟了一些有趣的方向。正因如此，在系统崩溃的情况下，文件系统恢复变得更加简化。数据的先前状态被保存在磁盘上的另一个位置。因此，如果发生故障，文件系统可以轻松恢复到先前的状态。这使得维护任何日志文件的需求变得多余。它还允许在文件系统级别实现快照。只有被修改的数据块才会被复制到新位置。当需要通过特定的快照恢复文件系统时，数据可以轻松地重建。
- en: '*Table 3.1* highlights some major differences between journaling and CoW-based
    filesystems. Please note that the implementation and availability of some of these
    features may vary depending on the type of filesystem:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*表3.1* 突出了日志文件系统和基于CoW文件系统之间的一些主要区别。请注意，这些功能的实现和可用性可能会根据文件系统类型的不同而有所变化：'
- en: '|  | **Journaling** | **Copy-on-Write** |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | **日志文件系统** | **写时复制（CoW）** |'
- en: '| **Write handling** | Changes are recorded in a journal before applying them
    to the actual filesystem | A separate copy of data is created to make modifications
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **写入处理** | 在应用更改之前，修改内容会先记录在日志中 | 创建数据的单独副本并进行修改 |'
- en: '| **Original data** | Original data gets overwritten | Original data remains
    intact |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **原始数据** | 原始数据会被覆盖 | 原始数据保持不变 |'
- en: '| **Data consistency** | Ensures consistency by recording metadata changes
    and replaying them if needed | Ensures consistency by never modifying the original
    data |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| **数据一致性** | 通过记录元数据更改并在需要时重放日志来确保一致性 | 通过从不修改原始数据来确保一致性 |'
- en: '| **Performance** | Minimal overhead depending on the type of journaling mode
    | Some performance gains because of faster writes |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| **性能** | 取决于日志模式的类型，通常只有最小的开销 | 由于写入速度更快，性能有所提升 |'
- en: '| **Space utilization** | Journal size is typically in MB, so no additional
    space is required | More space is required due to separate copies of data |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| **空间利用率** | 日志大小通常为MB，因此不需要额外的空间 | 由于需要为数据创建单独的副本，因此需要更多的空间 |'
- en: '| **Recovery times** | Fast recovery times as the journal can be replayed instantly
    | Slower recovery times as data needs to be reconstructed using recent copies
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| **恢复时间** | 恢复时间较快，因为日志可以立即重放 | 恢复时间较慢，因为需要使用最近的副本重建数据 |'
- en: '| **Features** | No built-in support for features such as compression or deduplication
    | Built-in support for compression and deduplication |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| **功能** | 不支持如压缩或去重等功能 | 内建支持压缩和去重功能 |'
- en: Table 3.1 – Differences between CoW and journaling filesystems
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 – CoW与日志文件系统之间的区别
- en: Filesystems that use the CoW-based approach for organizing data include **Zettabyte
    Filesystem** (**ZFS**), **B-Tree Filesystem** (**Btrfs**), and Bcachefs. ZFS was
    initially used on Solaris and quickly gained popularity because of its powerful
    features. Although not included in the kernel because of licensing issues, it
    has been ported to Linux through the *ZFS on Linux* project. The Bcachefs filesystem
    was developed from the kernel’s block cache code and is quickly gaining popularity.
    It might become a part of future kernel releases. Btrfs, also fondly known as
    ButterFS, is directly inspired by ZFS. Unfortunately, because of a few bugs in
    early releases, its adoption slowed down in the Linux community. Nevertheless,
    it has been under active development and has been a part of the Linux kernel for
    over a decade.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于CoW（写时复制）方法来组织数据的文件系统包括**Zettabyte文件系统**（**ZFS**）、**B树文件系统**（**Btrfs**）和Bcachefs。ZFS最初在Solaris上使用，并因其强大的功能迅速获得了广泛的应用。尽管由于许可问题未能纳入内核，但它已经通过*ZFS
    on Linux*项目移植到了Linux上。Bcachefs文件系统是从内核的块缓存代码开发而来的，并且正迅速获得流行。它可能会成为未来内核发布的一部分。Btrfs，也被亲切地称为ButterFS，直接受ZFS启发。不幸的是，由于早期版本中的一些bug，它在Linux社区的采用进程放缓。然而，它一直在积极开发，并且已经成为Linux内核的一部分超过十年。
- en: Despite a few issues, Btrfs is the most advanced filesystem present in the kernel
    because of its rich feature set. As mentioned previously, Btrfs draws a lot of
    inspiration from ZFS and tries to offer almost identical features. Like ZFS, Btrfs
    is not just a simple disk filesystem, it also offers the functionality of a logical
    volume manager and software, **Redundant Array of Independent Disks** (**RAID**).
    Some of its features include snapshots, checksums, encryption, deduplication,
    and compression, which are usually not available in regular block filesystems.
    All these characteristics greatly simplify storage management.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在一些问题，Btrfs仍然是内核中最先进的文件系统，因其丰富的功能集。如前所述，Btrfs受到了ZFS的深刻影响，并力图提供几乎相同的功能。像ZFS一样，Btrfs不仅仅是一个简单的磁盘文件系统，它还提供了逻辑卷管理器和软件**独立磁盘冗余阵列**（**RAID**）的功能。它的一些功能包括快照、校验和、加密、去重和压缩，这些功能通常在常规块文件系统中无法使用。所有这些特点极大简化了存储管理。
- en: To summarize, the CoW approach of filesystems such as Btrfs and ZFS ensures
    that existing data is never overwritten. Hence, even in the case of a sudden system
    crash, existing data will not be in an inconsistent state.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，像Btrfs和ZFS这样的文件系统的CoW方法确保现有数据永远不会被覆盖。因此，即使在系统突然崩溃的情况下，现有数据也不会处于不一致的状态。
- en: Extended filesystem
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展文件系统
- en: 'The **extended filesystem**, shortened as **Ext**, has been a trusted aide
    of the Linux kernel since its inception and is almost as old as the Linux kernel
    itself. It was first introduced in the kernel 0.96c. Over the years, the extended
    filesystem has gone through some major changes that have resulted in multiple
    versions of the filesystem. These versions are briefly explained as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩展文件系统**，简称**Ext**，自Linux内核诞生以来一直是其可信赖的助手，几乎与Linux内核本身一样古老。它最早在内核版本0.96c中被引入。多年来，扩展文件系统经历了一些重大变化，导致了多个版本的文件系统。这些版本简要介绍如下：'
- en: '**The First Extended Filesystem**: The first filesystem to run Linux was Minix
    and it supported a maximum filesystem size of 64 MB. The extended filesystem was
    designed to overcome the shortcomings in Minix and was generally considered an
    extension of the Minix filesystem. The extended filesystem supported a maximum
    filesystem size of 2 GB. It was also the first filesystem to make use of the VFS.
    The first Ext filesystem only allowed one timestamp per file, as compared to the
    three timestamps used today.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第一个扩展文件系统**：第一个运行Linux的文件系统是Minix，支持的最大文件系统大小为64 MB。扩展文件系统的设计旨在克服Minix中的不足，通常被认为是Minix文件系统的扩展。扩展文件系统支持最大2
    GB的文件系统大小。它也是第一个使用VFS（虚拟文件系统）的文件系统。第一个Ext文件系统每个文件只能有一个时间戳，而与今天的三个时间戳相比，它有些简陋。'
- en: '**The Second Extended Filesystem**: Almost a year after the release of the
    first extended filesystem, its second version, Ext2, was released. The Ext2 filesystem
    addressed the limitations of its predecessor, such as partition sizes, fragmentation,
    filename lengths, timestamps, and maximum file size. It also introduced several
    new features including the concept of filesystem blocks. The design of Ext2 was
    inspired by BSD’s Berkeley Fast File System. The Ext2 filesystem supported much
    larger filesystem sizes, up to a few terabytes.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二扩展文件系统**：在第一个扩展文件系统发布约一年后，第二版 Ext2 文件系统发布了。Ext2 文件系统解决了前一个版本的限制，如分区大小、碎片化、文件名长度、时间戳和最大文件大小等。它还引入了多个新特性，包括文件系统块的概念。Ext2
    的设计灵感来自 BSD 的 Berkeley Fast File System。Ext2 文件系统支持更大的文件系统大小，最多可达到几 TB。'
- en: '**The Third Extended Filesystem**: The Ext2 filesystem was widely adopted but
    fragmentation and filesystem corruption in the case of a crash remained big concerns.
    The third extended filesystem, Ext3, was designed keeping this in mind. The most
    important feature introduced in this release was **journaling**. Through journaling,
    the Ext3 filesystem kept track of uncommitted changes. This reduced the risk of
    data loss if the system crashed because of a hardware or power failure.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第三扩展文件系统**：Ext2 文件系统得到了广泛应用，但在系统崩溃时仍然存在碎片化和文件系统损坏的巨大问题。第三扩展文件系统 Ext3 在设计时考虑到了这一点。该版本引入的最重要特性是**日志功能**。通过日志功能，Ext3
    文件系统可以跟踪未提交的更改。这在系统因硬件或电力故障崩溃时，减少了数据丢失的风险。'
- en: '**The Fourth Extended Filesystem**: Ext4 is currently the latest version of
    the extended filesystem family. The Ext4 filesystem offers several improvements
    over Ext2 and Ext3 in terms of performance, fragmentation, and scalability, while
    also keeping backward compatibility with Ext2 and Ext3\. When it comes to Linux
    distributions, Ext4 is probably the most frequently deployed filesystem.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第四扩展文件系统**：Ext4 是扩展文件系统家族中目前最新的版本。Ext4 文件系统在性能、碎片化和可扩展性方面相较于 Ext2 和 Ext3
    提供了若干改进，同时保持了与 Ext2 和 Ext3 的向后兼容性。在 Linux 发行版中，Ext4 可能是最常部署的文件系统。'
- en: We’re going to mainly focus on the design and structure of the most recent version
    of the extended filesystem, Ext4.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主要关注最新版本的扩展文件系统 Ext4 的设计和结构。
- en: Blocks – the lingua franca of filesystems
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 块 —— 文件系统的通用语言
- en: At the lowest level, a hard drive is addressed in units of sectors. Sectors
    are the physical property of a disk drive and are normally 512 bytes in size.
    Although, these days, it’s not uncommon to see drives using a sector size of 4
    KB. The sector size is something that we cannot tinker with as it is decided by
    the drive manufacturer. As a sector is the smallest addressable unit on the drive,
    any operation performed on the physical drive is always going to be larger than
    or equal to the sector size.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在最低层次上，硬盘是以扇区为单位进行寻址的。扇区是磁盘驱动器的物理属性，通常大小为 512 字节。尽管如今，使用 4 KB 扇区大小的驱动器并不罕见。扇区大小是我们无法更改的，因为它是由驱动器制造商决定的。由于扇区是驱动器上最小的可寻址单位，任何对物理驱动器执行的操作，都会大于或等于扇区大小。
- en: 'A filesystem is created on top of the physical drive and does not address the
    drive in terms of sectors. All filesystems (and the extended filesystem family
    is no exception to this) address a physical drive in terms of blocks. A block
    is a group of physical sectors and is the fundamental unit of a filesystem. An
    Ext4 filesystem performs all operations in terms of blocks. On x86 systems, the
    filesystem block size is set to 4 KB by default. Although it can be set to a lower
    or higher value, the block size should always satisfy the following two constraints:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统是建立在物理驱动器之上的，并且不以扇区为单位来访问驱动器。所有文件系统（扩展文件系统系列也不例外）都是以块为单位来访问物理驱动器。块是物理扇区的集合，是文件系统的基本单位。Ext4
    文件系统在进行所有操作时，都是以块为单位。在 x86 系统上，文件系统的块大小默认设置为 4 KB。虽然可以设置为更小或更大的值，但块大小应始终满足以下两个约束条件：
- en: The block size should always be a power-of-two multiple of the disk sector size
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块大小应始终是磁盘扇区大小的二次幂倍数。
- en: The block size should always be less than or equal to the memory page size
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块大小应始终小于或等于内存页大小。
- en: 'The maximum filesystem block size is the page size of the architecture. On
    most x86-based systems, the default page size of the kernel is 4 KB. So, the filesystem
    block size cannot exceed 4 KB. The page size of the VFS cache also amounts to
    4 KB. The restriction of the block size to be less than or equal to the kernel’s
    page size is not limited to the extended filesystem only. The page size is defined
    during kernel compilation and is 4 KB for x86_64 systems. As shown ahead, the
    `mkfs` program for Ext4 will throw a warning if a block size greater than the
    page size is specified. Even if a filesystem is created with a block size greater
    than the page size, it cannot be mounted:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统的最大块大小是架构的页面大小。在大多数基于x86的系统上，内核的默认页面大小为4 KB。因此，文件系统块大小不能超过4 KB。VFS缓存的页面大小也为4
    KB。块大小限制不仅限于扩展文件系统。页面大小在内核编译时定义，对于x86_64系统为4 KB。如下所示，对于Ext4的`mkfs`程序，如果指定的块大小大于页面大小，将会发出警告。即使使用大于页面大小的块大小创建文件系统，也无法挂载：
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once a filesystem has been created, the block size cannot be changed. The Ext4
    filesystem divides the available storage into logical blocks of 4 KB by default.
    The selection of block size has a significant impact on the space efficiency and
    performance of the filesystem. The block size dictates the minimum on-disk size
    of a file, even if its actual size is less than the block size. Let’s say that
    our filesystem uses a block size of 4 KB and we save a simple text file of 10
    bytes on it. This 10-byte file, when stored on the physical disk, will use 4 KB
    of space. A block can only hold a single file. This means that for a 10-byte file,
    the remaining space in a block (4 KB – 10 bytes) is wasted. As shown next, a simple
    text file containing the `"hello"` string will occupy a full filesystem block:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文件系统创建完成，块大小就无法更改。默认情况下，Ext4文件系统将可用存储空间分割为4 KB的逻辑块。选择块大小对文件系统的空间利用效率和性能有重大影响。块大小决定了文件的最小磁盘大小，即使实际大小小于块大小。假设我们的文件系统使用4
    KB的块大小，我们在其上保存一个包含10字节的简单文本文件。这个10字节的文件，在物理磁盘上存储时将占用4 KB的空间。一个块只能容纳一个文件。这意味着对于一个10字节的文件，块中剩余的空间（4
    KB - 10字节）将被浪费。如下所示，一个包含字符串`"hello"`的简单文本文件将占用整个文件系统块：
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `stat` command gives us a block count of `8`, which is a bit misleading,
    as it is actually the sector count. This is because the `stat` system call assumes
    that 512 bytes of disk space are allocated per block. The block count here indicates
    that `4096` bytes (8 x 512) are physically allocated on the disk. The file size
    is `6` bytes only, but it occupies one full block on the disk. As shown next,
    when we add another line of text in the file, the file size increases from `6`
    to `19` bytes, but the numbers of used sectors and blocks remain the same:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`stat`命令给出了一个块计数为`8`，但这有点误导，因为实际上这是扇区计数。这是因为`stat`系统调用假设每个块分配了512字节的磁盘空间。这里的块计数表示在磁盘上实际分配了`4096`字节（8
    x 512）。文件大小只有`6`字节，但它占据了一个完整的块。如下所示，当我们在文件中添加另一行文本时，文件大小从`6`增加到`19`字节，但使用的扇区和块的数量保持不变：'
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Is there a more efficient approach to organizing data?
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有没有更有效的数据组织方法？
- en: Given that a small text file occupies a full block, it’s not difficult to see
    the impact of filesystem block size. Having a lot of small files on a filesystem
    of a large block size can result in a waste of disk space, and a filesystem can
    quickly run out of blocks. Let’s see a visual representation for a clearer understanding.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一个小文本文件占据一个完整的块，可以看出文件系统块大小的影响。在大块大小的文件系统上有很多小文件可能导致磁盘空间的浪费，并且文件系统很快可能用尽块。我们来看一个更清晰理解的可视化表示。
- en: 'Let’s say we have four files of varied sizes as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有四个不同大小的文件如下：
- en: File A -> 5 KB
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件A -> 5 KB
- en: File B -> 1 KB
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件B -> 1 KB
- en: File C -> 7 KB
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件C -> 7 KB
- en: File D -> 2 KB
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件D -> 2 KB
- en: 'Going with the approach of allocating a whole block (4 KB) to a single file,
    the files will be stored on disk as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 按照分配一个完整块（4 KB）给单个文件的方法，文件将存储在磁盘上如下：
- en: '![Figure 3.4 – Even the smallest of files occupy a full block](img/B19430_03_04.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4 - 即使是最小的文件也占据一个完整的块](img/B19430_03_04.jpg)'
- en: Figure 3.4 – Even the smallest of files occupy a full block
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 - 即使是最小的文件也占据一个完整的块
- en: As is apparent from *Figure 3**.4*, we’re wasting 3 KB of space in blocks 2
    and 3, and 1 KB and 2 KB in blocks 5 and 6, respectively. It’s pretty clear that
    too many small files spoil the blocks!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如*图 3.4*所示，我们在第 2 和第 3 块中浪费了 3 KB 的空间，在第 5 和第 6 块中分别浪费了 1 KB 和 2 KB 的空间。显然，太多的小文件会浪费块空间！
- en: 'Let’s try an alternative approach and try to store the files in a more condensed
    format to avoid wasting space:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一种替代方法，并尝试以更紧凑的格式存储文件，以避免浪费空间：
- en: '![Figure 3.5 – An alternate method for storing files](img/B19430_03_05.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 存储文件的另一种方法](img/B19430_03_05.jpg)'
- en: Figure 3.5 – An alternate method for storing files
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 存储文件的另一种方法
- en: It’s not difficult to see that the second approach is more compact and efficient.
    We’re now able to store the same four files in only four blocks as compared to
    the six in the first approach. We are even able to save 1 KB of filesystem space.
    Apparently, allocating a whole filesystem block for a single file seems like an
    inefficient method for managing space, but in reality, that is a necessary evil.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 不难看出，第二种方法更紧凑且高效。现在，我们能够仅用四个块存储相同的四个文件，而不是第一个方法中的六个块。我们甚至能够节省 1 KB 的文件系统空间。显然，为单个文件分配一个完整的文件系统块似乎是一种低效的空间管理方法，但实际上，这是必要的权宜之计。
- en: On the first look, the second approach seems far better, but do you see the
    design flaw? A filesystem following this approach would have major pitfalls. If
    filesystems were designed to accommodate multiple files in a single block, they
    would need to devise a mechanism that would keep track of individual file boundaries
    within a single block. This would increase the design complexity by a fair margin.
    Additionally, this would lead to massive fragmentation, which would degrade the
    filesystem performance. If the size of a file increases, the incoming data would
    have to be adjusted in a separate block. Files would be stored in random blocks
    and there would be no sequential access. All of this would result in poor filesystem
    performance and neutralize any advantage gained from this condensed approach.
    Therefore, every file occupies a full block, even if its size is less than the
    filesystem block size.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从第一眼看，第二种方法似乎好得多，但你看出设计缺陷了吗？如果文件系统采用这种方法，可能会遇到重大的问题。如果文件系统设计为在单一块中容纳多个文件，它们需要设计一种机制来跟踪单个块内每个文件的边界。这会大大增加设计的复杂性。此外，这还会导致严重的碎片化，从而降低文件系统的性能。如果文件的大小增加，新增的数据将不得不调整到一个单独的块中。文件将存储在随机块中，没有顺序访问。所有这些都会导致文件系统性能差，并使这种紧凑方法的任何优势都变得毫无意义。因此，每个文件都占据一个完整的块，即使它的大小小于文件系统块的大小。
- en: The structural layout of an Ext4 filesystem
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ext4 文件系统的结构布局
- en: 'The individual blocks in Ext4 are arranged into another unit called block groups.
    A **block group** is a collection of contiguous blocks. When it comes to the organization
    of the block group, there are two cases. For the first block group, the first
    1,024 bytes are not used. These are reserved for the installation of boot sectors.
    For the first block group, the layout is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Ext4 中的各个块被组织成另一种单元，称为块组。**块组**是一个连续块的集合。关于块组的组织，有两种情况。对于第一个块组，第一个 1,024 字节不使用。这些字节保留用于安装启动扇区。第一个块组的布局如下：
- en: '![Figure 3.6 – Layout for block group 0](img/B19430_03_06.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 块组 0 的布局](img/B19430_03_06.jpg)'
- en: Figure 3.6 – Layout for block group 0
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 块组 0 的布局
- en: 'If the filesystem is created with a block size of 1 KB, the superblock will
    be kept in the next block. For all other block groups, the layout becomes the
    following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文件系统是以 1 KB 的块大小创建的，则超级块将保存在下一个块中。对于所有其他块组，布局变为如下所示：
- en: '![Figure 3.7 – Layout for block group 1 and onward](img/B19430_03_07.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – 块组 1 及之后的布局](img/B19430_03_07.jpg)'
- en: Figure 3.7 – Layout for block group 1 and onward
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 块组 1 及之后的布局
- en: Let’s discuss the constituents of an Ext4 block group.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下 Ext4 块组的组成部分。
- en: Superblock
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超级块
- en: 'As explained in [*Chapter 2*](B19430_02.xhtml#_idTextAnchor028), the `fs/ext4/ext4.h`
    and, as shown next, it contains dozens of fields defining the different attributes
    of the filesystem:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第 2 章*](B19430_02.xhtml#_idTextAnchor028)所述，`fs/ext4/ext4.h`，如下面所示，它包含定义文件系统不同属性的多个字段：
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `__le32` data types indicate that the representation is in little-endian
    order. As evident from its definition in the kernel source, the Ext4 superblock
    defines a number of properties to characterize the filesystem. This contains information
    such as the total number of blocks and block groups in the filesystem, the total
    number of used and unused blocks, the block size, the total number of used and
    unused inodes, the filesystem state, and a lot more. The information contained
    in a superblock is of utmost importance as it is the first thing that is read
    when mounting a filesystem. Given its critical nature, multiple copies of the
    superblock are kept at different locations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`__le32` 数据类型表示数据采用小端序（little-endian）表示。正如其在内核源代码中的定义所示，Ext4 超级块定义了多个属性来描述文件系统。它包含的信息包括文件系统中的块总数和块组总数、已用和未用块的总数、块大小、已用和未用的
    inode 总数、文件系统状态等。超级块中包含的信息至关重要，因为它是挂载文件系统时第一个被读取的内容。鉴于其关键性质，超级块的多个副本会保存在不同位置。'
- en: 'Most fields in the superblock definition are easily understood. Some interesting
    fields are explained here:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 超级块定义中的大多数字段都容易理解。这里解释了一些有趣的字段：
- en: '**Block size calculation**: The block size of the Ext4 filesystem is computed
    using this 32-bit value. The block size is calculated as follows:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**块大小计算**：Ext4 文件系统的块大小是通过此 32 位值计算的。块大小计算方法如下：'
- en: '*Ext4 block size = 2 ^ (10 +* *s_log_block_size)*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*Ext4 块大小 = 2 ^ (10 +* *s_log_block_size)*'
- en: The minimum block size of an Ext4 filesystem can be 1 KB when `s_log_block_size`
    is zero. The Ext4 filesystem supports a maximum block size of 64 KB.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `s_log_block_size` 为零时，Ext4 文件系统的最小块大小为 1 KB。Ext4 文件系统支持最大 64 KB 的块大小。
- en: '`s_log_cluster_size`.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s_log_cluster_size`。'
- en: '`s_mnt_count` field indicates the number of times the filesystem has been mounted
    since the last consistency check was run. The `s_max_mnt_count` field imposes
    a hard limit on the number of mounts, beyond which a consistency check is mandatory.
    The filesystem state is saved in `s_state`. It can be one of the following:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s_mnt_count` 字段表示自上次一致性检查以来，文件系统已挂载的次数。`s_max_mnt_count` 字段对挂载次数施加了硬性限制，超过该限制将强制执行一致性检查。文件系统状态保存在
    `s_state` 中，可能的状态有以下几种：'
- en: '`cleanly unmounted`'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`已干净卸载`'
- en: '`errors detected`'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`检测到错误`'
- en: '`orphans` `being recovered`'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`孤儿` `正在恢复中`'
- en: If the filesystem state in `s_state` is not clean, the check is enforced automatically.
    The date of the last consistency check is saved in `s_lastcheck`. If the time
    specified in the `s_checkinterval` field has passed since the last check, the
    consistency check is enforced on the filesystem.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `s_state` 中的文件系统状态不是干净的，将自动强制执行检查。上次一致性检查的日期保存在 `s_lastcheck` 中。如果自上次检查以来，`s_checkinterval`
    字段指定的时间已过去，文件系统将强制执行一致性检查。
- en: '`s_magic` field in the superblock contains this magic number. For Ext4, its
    value is `0xEF53`. The `s_rev_level` and `s_minor_rev_level` fields are used to
    differentiate between filesystem versions.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超级块中的 `s_magic` 字段包含此魔数。对于 Ext4，其值为 `0xEF53`。`s_rev_level` 和 `s_minor_rev_level`
    字段用于区分文件系统版本。
- en: '`0` (root user). An Ext4 filesystem reserves 5% of filesystem blocks for the
    super or root user. This is done so that the root user processes continue to run,
    even if the non-root processes cannot write to the filesystem.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`（根用户）。Ext4 文件系统为超级用户或根用户保留了 5% 的文件系统块。这是为了确保即使非根用户进程无法写入文件系统，根用户进程仍然可以继续运行。'
- en: '`11`, which belongs to the `lost+found` directory on an Ext4 filesystem.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`11`，属于 Ext4 文件系统中的 `lost+found` 目录。'
- en: '`sda` and `sdb`) can often change, resulting in confusion and incorrect mount
    points. The UUID is a unique identifier for a filesystem and can be used in `/etc/fstab`
    to mount filesystems.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sda` 和 `sdb`）通常会发生变化，从而导致混淆和不正确的挂载点。UUID 是文件系统的唯一标识符，可以在 `/etc/fstab` 中用于挂载文件系统。'
- en: '`s_feature_compat` field contains a 32-bit bitmask of compatible features.
    The filesystem is free to support the features defined in this field. On the other
    hand, if any feature defined in `s_feature_incompat` is not understood by the
    kernel, the filesystem mount operation will not succeed.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s_feature_compat` 字段包含一个 32 位的兼容特性位掩码。文件系统可以自由支持此字段中定义的特性。另一方面，如果 `s_feature_incompat`
    中定义的任何特性内核无法理解，文件系统挂载操作将无法成功。'
- en: Data block and inode bitmaps
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据块和 inode 位图
- en: The Ext4 filesystem uses a negligible amount of space to organize some internal
    structures. Most of the space in a filesystem is used for storing user data. The
    Ext4 filesystem stores user data in data blocks. As we learned in [*Chapter 2*](B19430_02.xhtml#_idTextAnchor028),
    the metadata of each file is stored in a separate structure, called an inode.
    The inodes are also stored on disk, albeit in a reserved space. Inodes are unique
    in a filesystem. Every filesystem uses a technique to keep track of allocated
    and available inodes. Similarly, there has to be a method through which the number
    of allocated and free blocks can be tracked.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Ext4 uses a bitmap as an allocation structure. A bitmap is a sequence of bits.
    Separate bitmaps are used to track the number of inodes and data blocks. The data
    block bitmap tracks the usage of data blocks within the block group. Similarly,
    the inode bitmap keeps track of entries in the inode table. A bit value of `0`
    indicates that the block or inode is available for use. A value of `1` indicates
    that the block or inode is occupied.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: The bitmaps for both the inode and data block are of one block each. As a byte
    is composed of 8 bits, that means, for the default block size of 4 KB, the block
    bitmap can represent a maximum of 8 x 4 KB = 32,768 blocks per group. This can
    be verified in the output of `mkfs` or through the `tune2fs` program.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Inode tables
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to inode bitmaps, a block group also contains an inode table. The
    inode table spans a series of consecutive blocks. The definition of an Ext4 inode
    is present in the `fs/ext4/ext4.h` file:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The Ext4 inode has a size of 256 bytes. Some fields of particular interest
    are as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '`i_uid` and `i_gid` fields serve as the user and group identifiers.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`i_atime`, `i_ctime`, and `i_mtime`. These describe the last access time, inode
    change time, and data modification time, respectively. The file deletion time
    is saved in `i_dtime`. These 4 fields are 32-bit signed integers that represent
    the elapsed seconds since the Unix epoch time, January 1, 1970, 00:00:00 UTC.
    For calculating time with subsecond accuracy, the `i_atime_extra`, `i_mtime_extra`,
    and `i_ctime_extra` fields are used.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`i_links_count` field. This is a 16-bit value, which means that Ext4 allows
    for a maximum of 65K hard links for a file.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`i_block`, which is an array of length `EXT4_N_BLOCKS`. The value of `EXT4_N_BLOCKS`
    is `15`. As discussed in [*Chapter 2*](B19430_02.xhtml#_idTextAnchor028), an inode
    structure uses pointers for block addressing. First, 12 pointers point directly
    to block addresses and are called **direct pointers**. The next three pointers
    are indirect pointers. An **indirect pointer** points to a block of pointers.
    The 13th, 14th, and 15th pointers provide single-, double-, and triple-level indirection.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Group descriptors
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Group descriptors are stored just after the superblock in the filesystem layout.
    Every block group has a group descriptor associated with it, so there are as many
    group descriptors as the number of block groups. It’s important to understand
    that the block group descriptors describe the contents of each block group in
    the filesystem. This means they include information about the local, as well as
    all the other block groups in the filesystem. The group descriptor structure is
    defined in `fs/ext4/ext4.h`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 分组描述符存储在超级块之后的文件系统布局中。每个块组都有一个与之关联的分组描述符，因此块组的数量与分组描述符的数量相同。理解块组描述符是描述文件系统中每个块组的内容这一点非常重要。这意味着它们包含了有关本地块组以及文件系统中所有其他块组的信息。分组描述符结构定义在`fs/ext4/ext4.h`中：
- en: '[PRE5]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Some important fields are described further:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些重要字段的进一步描述：
- en: '`bg_block_bitmap_lo`, `bg_inode_bitmap_lo`, and `bg_inode_table_lo`, whereas
    the most significant bits are stored in `bg_block_bitmap_hi`, `bg_inode_bitmap_hi`,
    and `bg_inode_table_hi`.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bg_block_bitmap_lo`、`bg_inode_bitmap_lo` 和 `bg_inode_table_lo`，而最重要的位则存储在
    `bg_block_bitmap_hi`、`bg_inode_bitmap_hi` 和 `bg_inode_table_hi` 中。'
- en: '`bg_free_blocks_count_lo`, `bg_free_blocks_count_hi`, `bg_free_inodes_count_lo`,
    `bg_free_inodes_count_hi`, `bg_used_dirs_count_lo`, and `bg_used_dirs_count_hi`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bg_free_blocks_count_lo`、`bg_free_blocks_count_hi`、`bg_free_inodes_count_lo`、`bg_free_inodes_count_hi`、`bg_used_dirs_count_lo`
    和 `bg_used_dirs_count_hi`。'
- en: 'As each block group descriptor includes information about both local and non-local
    block groups, it contains a descriptor for each block group in the filesystem.
    Because of this, the following information can be determined from any single block
    group:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个块组描述符包含关于本地和非本地块组的信息，因此它包含文件系统中每个块组的描述符。因此，可以从任何单一块组中确定以下信息：
- en: The number of free blocks and inodes
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空闲块和inode的数量
- en: The location of the inode table in the filesystem
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件系统中inode表的位置
- en: The location of block and inode bitmaps
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块和inode位图的位置
- en: Reserved GDT blocks
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保留的GDT块
- en: One of the most useful features of an Ext4 filesystem is its on-the-fly expansion.
    The size of an Ext4 filesystem can be increased on the fly without any disruption.
    The reserved **group descriptor table** (**GDT**) blocks are put aside at the
    time of filesystem creation. This is done to aid in the process of filesystem
    expansion. Increasing the size of the filesystem involves the addition of physical
    disk space and the creation of filesystem blocks in the newly added disk space.
    This also means that to accommodate the newly added space, more block groups and
    group descriptors will be required. These reserved GDT blocks are used when an
    Ext4 filesystem is to be extended.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Ext4文件系统最有用的功能之一是其动态扩展功能。Ext4文件系统的大小可以在不中断的情况下动态增加。保留的**组描述符表**（**GDT**）块在文件系统创建时就被预留出来。这是为了帮助文件系统扩展过程。增加文件系统的大小涉及到增加物理磁盘空间并在新增加的磁盘空间中创建文件系统块。这也意味着，为了容纳新增的空间，将需要更多的块组和组描述符。当要扩展一个Ext4文件系统时，这些预留的GDT块就会被使用。
- en: Journaling modes
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志记录模式
- en: Like most filesystems, Ext4 also implements the concept of journaling to prevent
    data corruption and inconsistencies in the case of a system crash. The default
    journal size is typically just a few megabytes. The journaling in Ext4 uses the
    generic journaling layer in the kernel, known as the `jbd2` process in that list.
    This is the kernel thread responsible for updating the Ext4 journal.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 像大多数文件系统一样，Ext4也实现了日志记录的概念，以防止系统崩溃时数据损坏和不一致。默认的日志大小通常只有几兆字节。Ext4中的日志记录使用内核中的通用日志记录层，称为`jbd2`进程。这是负责更新Ext4日志的内核线程。
- en: 'Ext4 offers a great deal of flexibility when it comes to journaling. The Ext4
    filesystem supports three journaling modes. Depending upon the requirements, the
    journaling mode can be changed if required. By default, journaling is enabled
    at the time of filesystem creation. If desired, it can be disabled later. The
    different journaling modes are explained here:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Ext4在日志记录方面提供了极大的灵活性。Ext4文件系统支持三种日志记录模式。根据需求，如果需要，日志记录模式可以更改。默认情况下，在创建文件系统时启用日志记录。如果需要，可以稍后禁用它。不同的日志记录模式在这里进行了解释：
- en: '**Ordered**: In ordered mode, only metadata is journaled. The actual data is
    directly written to disk. The order of the operations is strictly followed. First,
    the metadata is written to the journal; second, the actual data is written to
    disk; and last, the metadata is written to disk. If there is a crash, filesystem
    structures are preserved. However, the data being written at the time of the crash
    may be lost.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ordered**（有序模式）：在有序模式下，只有元数据被日志记录。实际数据直接写入磁盘。操作的顺序严格按照规定执行。首先，元数据被写入日志；其次，实际数据被写入磁盘；最后，元数据被写入磁盘。如果发生崩溃，文件系统结构会被保留。然而，在崩溃时正在写入的数据可能会丢失。'
- en: '**Writeback**: The writeback mode also only journals metadata. The difference
    is that actual data and metadata can be written in any order. This is a slightly
    more risky approach than ordered mode but offers much better performance.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Writeback**（回写模式）：回写模式也只记录元数据的日志。不同之处在于，实际数据和元数据可以按任何顺序写入。这种方法比有序模式稍微有风险，但性能要好得多。'
- en: '**Journal**: In journal mode, both data and metadata are written to the journal
    first, before being committed to the disk. This offers the highest level of security
    and consistency but can adversely affect performance, as all write operations
    have to be performed twice.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Journal**（日志模式）：在日志模式下，数据和元数据都首先被写入日志，然后才提交到磁盘。这提供了最高级别的安全性和一致性，但可能会影响性能，因为所有写操作必须执行两次。'
- en: 'The default journaling mode is *ordered*. If you want to change the journal
    mode, you’ll need to unmount the filesystem and add the desired mode in the corresponding
    `fstab` entry. For instance, to change the journaling mode to *writeback*, add
    `data=writeback` against the relevant filesystem entry in the `/etc/fstab` file.
    Once done, you can verify the journaling mode as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的日志模式是*ordered*（有序模式）。如果你想更改日志模式，你需要卸载文件系统，并在相应的`fstab`条目中添加所需的模式。例如，要将日志模式更改为*writeback*（回写模式），在`/etc/fstab`文件中相应的文件系统条目中添加`data=writeback`。完成后，你可以通过以下方式验证日志模式：
- en: '[PRE6]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can also display information about the filesystem journal using the `logdump`
    command from `debugfs`. For instance, you can check the journal for the `sdc`
    device as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用`debugfs`中的`logdump`命令显示文件系统日志的信息。例如，你可以通过如下方式查看`sdc`设备的日志：
- en: '[PRE7]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Filesystem extents
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文件系统扩展
- en: We’ve covered the use of indirect pointers to address large files. Through the
    use of indirect pointers, an inode can keep track of data blocks that contain
    the file contents. For large files, this approach becomes a bit inefficient. The
    higher the number of blocks occupied by a file, the higher the number of pointers
    required to keep track of each block. This creates a complex mapping scheme and
    increases the metadata usage per file. As a result, some operations on large files
    are performed rather slowly.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了使用间接指针来处理大文件的问题。通过使用间接指针，一个inode可以跟踪包含文件内容的数据块。对于大文件，这种方法变得有些低效。文件占用的块数越多，跟踪每个块所需的指针数量就越多。这就创建了一个复杂的映射方案，并增加了每个文件的元数据使用量。因此，对于大文件的一些操作执行起来会相对较慢。
- en: Ext4 makes use of extents to address this problem and reduce the metadata required
    to keep track of data blocks. An **extent** is a pointer plus a length of blocks
    – basically, a bunch of contiguous physical blocks. When using extents, we only
    need to know the address of the first and last block of this contiguous range.
    For instance, let’s say that we’re using an extent size of 4 MB. To store a 100
    MB file, we can allocate 25 contiguous blocks. Since the blocks are contiguous,
    we only need to remember the address of the first and last blocks. Assuming a
    block size of 4 KB, while using pointers, we would need to create an indirect
    mapping of 25,600 blocks to store a 100 MB file.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Ext4通过使用扩展来解决这个问题，并减少跟踪数据块所需的元数据。**扩展**是一个指针加上块的长度——基本上是一组连续的物理块。在使用扩展时，我们只需要知道这个连续范围的第一个和最后一个块的地址。例如，假设我们使用4
    MB的扩展大小。要存储一个100 MB的文件，我们可以分配25个连续的块。由于这些块是连续的，我们只需要记住第一个和最后一个块的地址。假设块大小为4 KB，在使用指针时，我们需要创建一个间接映射来存储一个100
    MB的文件，这需要25,600个块。
- en: Block allocation policies
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 块分配策略
- en: When it comes to filesystem performance, fragmentation is a silent killer. The
    Ext4 filesystem uses several techniques to improve the overall performance and
    reduce fragmentation. The block allocation policies in Ext4 ensure that related
    information exists within the same filesystem block group.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件系统性能方面，碎片化是一个隐形的杀手。Ext4文件系统使用多种技术来提高整体性能并减少碎片化。Ext4中的块分配策略确保相关信息存在于同一文件系统块组内。
- en: 'When a new file is to be created and saved, the filesystem will need to initialize
    an inode for that file. Ext4 will then select an appropriate block group for that
    file. The design of Ext4 makes sure that maximum effort is made to do the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当要创建并保存新文件时，文件系统需要为该文件初始化一个inode。然后，Ext4会为该文件选择一个合适的块组。Ext4的设计确保最大程度地做以下操作：
- en: Allocate the inode in the block group that contains the parent directory of
    the file
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在包含文件父目录的块组中分配inode
- en: Allocate a file to the block group that contains the file’s inode
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文件分配到包含文件inode的块组
- en: Once a file has been saved on disk, after some time, the user wants to add new
    data to the file. Ext4 will start a search for free blocks, from the block that
    was most recently allocated to the file.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文件已保存在磁盘上，过了一段时间，用户可能想向该文件添加新数据。Ext4将开始从最近分配给该文件的块开始，寻找空闲块。
- en: When writing data to an Ext3 filesystem, the block allocator only allocated
    a single 4 KB block at a time. Assuming a block size of 4KB, for a single 100
    MB file, the block allocator would need to be called 25,600 times. Similarly,
    when a file is extended and new blocks are allocated from the block group, they
    can be in random order. This random allocation can result in excessive disk seeking.
    This approach does not scale well and causes fragmentation and performance issues.
    The Ext4 filesystem offers a significant improvement on this through the use of
    a multi-block allocator. When a new file is created, the multi-block allocator
    in Ext4 allocates multiple blocks in a single call. This reduces the overhead
    and increases performance. If the file uses those blocks, the data is written
    in a single multi-block extent. If the file does not use the extra allocated blocks,
    they are freed.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在写入数据到Ext3文件系统时，块分配器每次只分配一个4KB的块。假设块大小为4KB，对于一个单独的100MB文件，块分配器需要调用25600次。同样，当文件被扩展并从块组中分配新的块时，这些块可能是随机顺序的。这样的随机分配可能导致过度的磁盘寻道。该方法扩展性差，且会导致碎片化和性能问题。Ext4文件系统通过使用多块分配器显著改进了这一点。当创建新文件时，Ext4中的多块分配器会在一次调用中分配多个块。这减少了开销并提高了性能。如果文件使用了这些块，数据将写入一个单一的多块范围。如果文件没有使用额外分配的块，这些块将被释放。
- en: The Ext4 filesystem uses delayed allocation and does not allocate the blocks
    immediately upon a write operation. This is done because the kernel makes heavy
    use of the page cache. All operations are first performed in the kernel’s page
    cache and then flushed to disk after some time. Using delayed allocation, the
    blocks are only allocated when data is actually being written to disk. This is
    extremely useful as the filesystem can then allocate contiguous extents for saving
    the file.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Ext4文件系统使用延迟分配，并不会在写入操作时立即分配块。这是因为内核大量使用页面缓存。所有操作首先在内核的页面缓存中执行，然后在一段时间后刷新到磁盘。通过使用延迟分配，只有在数据实际写入磁盘时才会分配块。这非常有用，因为文件系统可以为保存文件分配连续的块。
- en: Ext4 tries to keep the data blocks of a file in the same block group as its
    inode. Similarly, all inodes in a directory are placed in the same block group
    as the directory.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Ext4尽量将文件的数据块保存在与其inode相同的块组中。同样，目录中的所有inode也被放置在与目录相同的块组中。
- en: Examining the result of an mkfs operation
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查mkfs操作的结果
- en: 'Let’s summarize our discussion about Ext4 and see what happens when we create
    an Ext4 filesystem using `mkfs`. The following command was run on a disk of only
    1 GB:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下关于Ext4的讨论，看看当我们使用`mkfs`创建Ext4文件系统时会发生什么。以下命令在一个只有1GB的磁盘上运行：
- en: '[PRE8]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let’s examine the output.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下输出结果。
- en: As the man page of `mkfs.ext4` will tell you, the discarding device blocks feature
    is especially useful for solid-state drives. By default, the `mkfs` command will
    issue a `TRIM` command to inform the underlying drive to erase unused blocks.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如`mkfs.ext4`的手册页所述，丢弃设备块功能对固态硬盘特别有用。默认情况下，`mkfs`命令会发出`TRIM`命令，通知底层驱动器擦除未使用的块。
- en: The filesystem consists of 262,144 blocks of 4 KB each. The total number of
    inodes in the filesystem is 65,536\. The UUID can be used in `fstab` to mount
    the filesystem.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件系统由262,144个4 KB的块组成。文件系统中的总inode数量为65,536。UUID可以在`fstab`中用于挂载文件系统。
- en: The stride and stripe widths are used when the underlying storage is a RAID
    volume.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当底层存储是RAID卷时，会使用跨步（stride）和条带宽度（stripe width）。
- en: An Ext4 filesystem will by default reserve 5% space for the superuser.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Ext4文件系统会为超级用户保留5%的空间。
- en: We see that the filesystem has divided the 262,144 blocks into 8 block groups.
    There are a total of 32,768 blocks per group. Each block has 8,192 inodes. This
    is in line with the total number of inodes mentioned earlier – that is, 8 x 8,192
    = 65,536.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到文件系统将262,144个块划分为8个块组。每个块组共有32,768个块。每个块有8,192个inode。这与前面提到的inode总数一致——即8
    x 8,192 = 65,536。
- en: The copies of the superblock structure are stored on multiple blocks. The filesystem
    will always be mounted using the primary superblock. But if the primary superblock
    gets corrupted for some reason, the filesystem can be mounted using the backups
    saved on different block locations. The filesystem journal occupies 8,192 blocks,
    which gives us a journal size of 8,192 x 4 KB = 32 MB.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 超级块结构的副本存储在多个块上。文件系统将始终使用主超级块进行挂载。但如果主超级块由于某种原因损坏，可以使用存储在不同块位置的备份来挂载文件系统。文件系统日志占用8,192个块，这使得日志的大小为8,192
    x 4 KB = 32 MB。
- en: The extended filesystem is one of the oldest Linux-specific software projects.
    Over the years, it has gone through several enhancements in terms of reliability,
    scalability, and performance. Most of the concepts associated with Ext4, such
    as journaling, the use of extents, and delayed allocation, also apply to XFS,
    although XFS uses different techniques to implement these features. Like all block-based
    filesystems, Ext4 divides the available disk space into fixed-size blocks. Being
    a native filesystem, it makes extensive use of the structures defined in VFS and
    implements them as per its own design. Because of its proven track record of stability,
    it is the most used filesystem across Linux distributions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展文件系统是最古老的Linux特定软件项目之一。多年来，它在可靠性、可扩展性和性能方面经历了几次增强。大多数与Ext4相关的概念，如日志记录、使用扩展区和延迟分配，也适用于XFS，尽管XFS使用不同的技术来实现这些功能。与所有基于块的文件系统一样，Ext4将可用磁盘空间划分为固定大小的块。作为本地文件系统，它广泛使用在VFS中定义的结构，并根据自己的设计实现它们。由于其经过验证的稳定性，它是Linux发行版中最常用的文件系统。
- en: Network filesystem
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络文件系统
- en: The evolution of computer networks and network protocols made remote file sharing
    possible. This gave rise to the concept of distributed computing and client-server
    architectures, which can be referred to as distributed filesystems. The idea was
    to store data on a central location on one or more servers. There are multiple
    clients that request access to this data through different programs and protocols.
    This includes protocols such as **File Transfer Protocol** (**FTP**) and **Secure
    File Transfer Protocol** (**SFTP**). The use of these programs makes it possible
    to transfer data between two machines.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机网络和网络协议的发展使得远程文件共享成为可能。这催生了分布式计算和客户端-服务器架构的概念，这些可以称为分布式文件系统。其思想是将数据存储在一个或多个服务器上的中央位置。多个客户端通过不同的程序和协议请求访问这些数据。包括**文件传输协议**（**FTP**）和**安全文件传输协议**（**SFTP**）等协议。使用这些程序使得在两台计算机之间传输数据成为可能。
- en: As compared to any traditional filesystem, a filesystem that uses the distributed
    approach will require some additional elements for its functioning. We’ve seen
    that processes make use of the generic system call layer to issue read or write
    requests. In the case of conventional filesystems, both the process (which issues
    the request) and the storage (which serves that request) are part of the same
    system. In distributed systems, there can be a dedicated client-side application
    that is used for accessing the filesystem. In response to a generic system call
    such as `read ()`, the client side will send a message to the server requesting
    read access to a particular resource.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何传统文件系统相比，使用分布式方法的文件系统在运行时需要一些额外的元素。我们已经看到进程通过通用系统调用层发出读写请求。在传统文件系统的情况下，发出请求的进程和提供该请求的存储都属于同一系统。在分布式系统中，可能会有一个专门的客户端应用程序用于访问文件系统。响应如`read
    ()`等通用系统调用时，客户端会向服务器发送消息，请求访问特定资源的读取权限。
- en: One of the oldest filesystems to follow this approach is the **Network Filesystem**,
    shortened as **NFS**. The NFS protocol was created by Sun Microsystems in 1984\.
    NFS is a distributed filesystem, which allows accessing files stored in a remote
    location. NFS version 4 is the most recent version of the protocol. Since the
    communication between the client and server is over a network, any request by
    the clients will traverse all the layers in the **Open Systems Interconnection**
    (**OSI**) model.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: NFS architecture
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From an architecture standpoint, NFS has three major components:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '**Remote Procedure Calls** (**RPCs**): To allow processes to send and receive
    messages to and from each other, the kernel offers different **inter-process communication**
    (**IPC**) mechanisms. The NFS uses RPCs as a method of communication between an
    NFS client and server. RPC is an extension of the IPC mechanism. As the name suggests,
    in RPC, the procedure called by the client doesn’t need to be in the same address
    space as the client. It can be in a remote address space. The RPC service is implemented
    at the session layer.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**External Data Representation** (**XDR**): NFS uses XDR as the standard for
    encoding binary data at the presentation layer of the OSI model. The use of XDR
    ensures that all stakeholders use the same language when communicating. The use
    of a standardized method for transferring data is necessary as data representation
    may differ between the two systems. For instance, it is possible that the NFS
    participants may be architecturally different and have different endian-ness.
    For instance, if data is being transmitted from a system using big-endian architecture
    to a system using little-endian architecture, the bytes will be received in the
    reverse order. XDR uses a canonical method of data representation. When an NFS
    client needs to write data on an NFS server, it will convert the local representation
    of relevant data into its equivalent XDR encoding. Similarly, when this XDR-encoded
    data is received by the server, it will decode and convert it back into its local
    representation.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NFS procedures**: All the NFS operations function at the application layer
    of the OSI model. The procedures defined at this layer specify the different tasks
    that can be performed on files residing on the NFS server. These procedures include
    file operations, directory operations, and filesystem operations.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 3**.8* depicts the route taken by an I/O request when using NFS:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Flow of an I/O request in NFS](img/B19430_03_08.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Flow of an I/O request in NFS
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: In version 2, NFS used UDP as the underlying transport protocol and as a result,
    NFS v2 and v3 were stateless. An advantage of this approach was the slightly better
    performance because of the lower overhead when using UDP. Since version 4, the
    default protocol has been changed to TCP. Mounting NFS shares using TCP is a more
    reliable option. NFS version 4 is stateful, which means that both the NFS client
    and server maintain information about open files and file locks. In the case of
    a server crash, both the client and server side work to recover the state prior
    to the failure. NFS version 4 also introduced the **compound** request format.
    By using the compound request format, the NFS client can combine several operations
    into a single request. The compound procedure acts as a wrapper to coalesce one
    or more operations into a single RPC request.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Like any regular filesystem, the NFS also needs to be mounted to establish a
    logical connection between the client and server. This mount operation is a bit
    different from a local filesystem. While mounting an NFS filesystem, we do not
    need to create a filesystem since the filesystem already exists on the remote
    side. The `mount` command will include the name of the remote directory to be
    mounted. In NFS terms, this is called an **export**. The NFS server keeps a list
    of filesystems that can be exported and a list of hosts that are allowed to access
    these exports.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: The NFS server uses a special structure to uniquely identify a file. This structure
    is known as a **file handle**. This handle makes use of an inode number, a filesystem
    identifier, and a generation number. The generation number plays a critical role
    in this identification process. Let’s say that file A had an inode number of 100
    and was deleted by the user. A new file, say B, was created and was assigned the
    recently freed inode number, 100\. When trying to access a file using its file
    handle, this can cause confusion, as now, file B uses the inode number that was
    previously assigned to file A. Because of this, the file handle structure also
    uses a generation number. This generation number is incremented every time an
    inode is reused by the server.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Comparing NFS with regular block filesystems
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network filesystems are also referred to as *file-level storage*. As such, I/O
    operations performed on an NFS are called file-level I/O operations. Unlike block
    filesystems, file-level I/O doesn’t specify the block address of a file when requesting
    an operation. Keeping track of the exact location of the file on the disk is the
    job of the NFS server. Upon receiving the request from the NFS client, the NFS
    server will convert it into a block-level request and perform the requested operation.
    This does introduce additional overhead and is one of the major reasons that the
    performance of an NFS pales in comparison to a regular block filesystem. In the
    case of block filesystems and storage, applications have the freedom to decide
    how filesystem blocks will be accessed or modified. For NFS, the management of
    filesystem structures is entirely the responsibility of the NFS server.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the differences between an NFS and a block filesystem:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – NFS versus block filesystem](img/B19430_03_09.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – NFS versus block filesystem
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, NFS is one of the most popular protocols for remote file sharing.
    It is distributive in nature and follows a client-server architecture. The request
    from an NFS client ends up on the NFS server after traversing the entire network
    stack. To standardize data representation between the client and server, NFS uses
    XDR for data encoding at the presentation layer of the OSI model. Although it
    lags in performance when compared to regular block storage, it is still used in
    most enterprise infrastructures, mainly for backups and archiving.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: FUSE – a unique method for creating filesystems
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ve talked about how the kernel divides the system into two portions: user
    space and kernel space. All the privileged system resources reside in kernel space.
    The kernel code, including that of filesystems, also exists in kernel space in
    a separate area of memory. It is not possible for normal user-space application
    programs to access it. The distinction between user space and kernel space programs
    restricts any regular process from modifying the kernel code.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Although this approach is essential to the kernel’s design, it does create a
    few problems in the development process. Consider the example of any filesystem.
    As all the filesystem code exists in the kernel space, in the case of a bug in
    the filesystem code, it is extremely difficult to perform any troubleshooting
    or debugging because of this segregation. Any operations on the filesystem also
    need to be performed by the root user.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: The **filesystem in user space** (**FUSE**) framework was designed to address
    some of these limitations. Through the use of the FUSE interface, filesystems
    can be created without tinkering with the kernel code. As such, the code for such
    filesystems only exists in user space. Both the actual data and metadata on the
    filesystem are managed by user-space processes. This is extremely flexible as
    it allows non-privileged users to mount the filesystem. It’s important to note
    that FUSE-based filesystems can be stackable, meaning that they can be deployed
    on top of existing filesystems such as Ext4 and XFS. One of the most widely used
    FUSE-based solutions that makes use of this approach is **GlusterFS**. GlusterFS
    operates as a user-space filesystem and can be stacked on top of any existing
    block-based filesystem such as Ext4 or XFS.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'The functionality provided by FUSE is achieved using a kernel module (`fuse.ko`)
    and a user-space daemon using the `libfuse` library. The FUSE kernel module is
    responsible for registering the filesystem with VFS. The interaction between the
    user-space daemon and the kernel is achieved using a character device, `/dev/fuse`.
    This device plays the role of a bridge between the user-space daemon and the kernel
    module. The user-space daemon will read from and write requests to this device:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – The FUSE approach](img/B19430_03_10.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – The FUSE approach
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: When a process in user space performs any operation on a FUSE filesystem, the
    relevant system call is sent to the VFS layer. Upon checking that this corresponds
    to a FUSE-based filesystem, VFS will forward this request to the FUSE kernel module.
    The FUSE driver will create a request structure and put it in the FUSE queue in
    `/dev/fuse`. The communication between the kernel module and `libfuse` library
    is achieved using a special file descriptor. The user-space daemon will open the
    `/dev/fuse` device to process the result. If the FUSE filesystem is stacked on
    top of an existing filesystem, then the request will again be routed to the kernel
    space so that it can be passed to the filesystem underneath.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: FUSE filesystems are not as robust as traditional filesystems but they offer
    a great deal of flexibility. They are easy to deploy and can be mounted by non-privileged
    users. Since the filesystem code is in user space, it is easier to troubleshoot
    and make changes. Even in the case of a bug in the code, the functionality of
    the kernel will not be impacted.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having covered the workings of VFS in the first two chapters, this chapter gave
    you an introduction to common filesystems and their concepts. The Linux kernel
    is capable of supporting around 50 filesystems and covering each one of them is
    an impossible task. We maintained our focus on the native filesystems in Linux,
    as the kernel is capable of supporting them out of the box. We explained some
    features that are common among a group of filesystems, such as journaling, CoW
    mechanisms, and FUSE. The major focus of this chapter was the working and internal
    design of the extended filesystem. The extended filesystem has been around since
    kernel version 0.96 and is the most widely deployed filesystem on computing platforms.
    We also shed some light on the architecture of network filesystems and explained
    the differences between file and block storage. At the end, we discussed FUSE,
    which offers an interface for user space programs to export a filesystem to the
    Linux kernel.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter, we’ve now completed our exploration of VFS and filesystem
    layers in the kernel. This brings the curtains down on *Part 1* of this book.
    I would like to think that this has been a good learning journey so far and I
    hope that it stays that way. The second part of this book, starting from [*Chapter
    4*](B19430_04.xhtml#_idTextAnchor072), will focus on the block layer in the kernel,
    which provides an upstream interface to the filesystems.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Navigating Through the Block Layer'
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part introduces the role of the block layer in the Linux kernel. The block
    layer is a key part of the kernel’s storage stack because the interfaces implemented
    in the block layer are used by the user space applications to access the available
    storage devices. This part will explain the block layer and its major components,
    such as the device mapper framework, block devices, block layer data structures,
    the multi-queue framework, and the different I/O schedulers.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapters:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B19430_04.xhtml#_idTextAnchor072), *Understanding the Block Layer,
    Block Devices, and Data Structures*'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B19430_05.xhtml#_idTextAnchor090), *Understanding the Block Layer,
    Multi-Queue, and Device Mapper*'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B19430_06.xhtml#_idTextAnchor101), *Understanding I/O Handling
    and Scheduling in the Block Layer*'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
