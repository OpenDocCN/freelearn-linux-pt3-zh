- en: '*Chapter 3*: System Storage Best Practices'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Probably the most complicated and least understood components of **System Administration**
    involve the area of **storage**. Storage tends to be poorly covered, rarely taught,
    and often treated as myth rather than science. Storage also involves the most
    fear because it is in storage that our mistakes risk losing data, and nothing
    tends to be a bigger failure than data loss.
  prefs: []
  type: TYPE_NORMAL
- en: Storage decisions impact performance, capacity, longevity, and most importantly,
    *durability*. Storage is where we have the smallest margin of error as well as
    where we can make the biggest impact. In other areas of planning and design we
    often get the benefit of quite a bit of *fudge factor*, mistakes are often graceful
    such as a system that is not quite as fast as it needs to be or is somewhat more
    costly than necessary, but in storage overbuilding might double total costs and
    mistakes will quite easily result in non-functional systems. Failure tends to
    be anything but graceful.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to address how we look at and understand storage in Linux systems
    and demystify storage so that you can approach it methodically and empirically.
    By the end of this chapter, you should be prepared to decide on the best storage
    products and designs for your workload taking into account all of the needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will look at the following key topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring key factors in storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Understanding block storage: Local and SAN'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Surveying filesystems and network filesystems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting to know **logical volume management** (**LVM**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing RAID and RAIN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about replicated local storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing storage architectures and risk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring key factors in storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When thinking about storage for systems administration we are concerned with
    **cost**, **durability**, **availability**, **performance**, **scalability**,
    **accessibility**, and **capacity**. It is easy to get overwhelmed with so many
    moving parts when it comes to storage and that makes it a risk that we may lose
    track of what it is that we want to accomplish. In every storage decision, we
    need to remain focused on these factors. Most importantly, on all of these factors.
    It is extremely tempting to focus on just a few causing us to lose our grasp of
    the complete picture.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, if you study postmortems of storage systems that have failed
    to meet business needs, you will almost always find that one or more of these
    factors was forgotten during the design phase. It is very tempting to become focused
    on one or two key factors and ignore the others, but we really have to maintain
    a focus on all of them to ensure storage success.
  prefs: []
  type: TYPE_NORMAL
- en: We should begin by breaking down each factor individually.
  prefs: []
  type: TYPE_NORMAL
- en: Cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It might seem that no one could forget about cost as a factor in storage but
    believe me it happens, and it happens often. As a rule, IT is a business function
    and all businesses, by definition, are about making money, and the cost of providing
    an infrastructure need always must factor in profits. So, because of this, no
    decision in IT (or anywhere in a business) should happen without cost as a consideration.
    We should never allow cost to be forgotten, or just as bad allowing someone to
    state that *cost is no object* because that can never be true and makes absolutely
    no sense. Cost may not be the primary concern, and the budgetary limits may be
    flexibility, but cost always matters.
  prefs: []
  type: TYPE_NORMAL
- en: Storage is generally one of the costliest components of a production system
    so we tend to see costs be more sensitive when dealing with storage than when
    dealing with other parts of physical system design such as CPU and RAM. Storage
    is also often easiest to solve by simply throwing more money at it and so many
    people when planning for hardware err on the side of overbuilding because it is
    easy. Of course, we can always do this and as long as we understand enough of
    our storage needs and how storage works it will *work* outside of being overly
    expensive. But, of course, it is difficult to be effective system administrators
    if we are not cost effective - the two things go together.
  prefs: []
  type: TYPE_NORMAL
- en: Durability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Nothing is more important when it comes to storage than durability: the ability
    of the storage mechanism to resist data loss. Durability is one of two aspects
    of reliability. For most workloads and most system scenarios, durability is what
    trumps all else. It is very rare that we want to store something that we cannot
    reliably retrieve even if that retrieval is slow, delayed, or expensive. Concepts
    such as data availability or performance mean nothing if the data is lost.'
  prefs: []
  type: TYPE_NORMAL
- en: Durability also refers to data that resists corruption or decay. In storage
    we have to worry about the potential of a portion of our data set losing integrity
    which may or may not be something that we can detect. Just because we can retrieve
    data alone does not tell us that the data that we are retrieving is exactly what
    it is supposed to be. Data corruption can mean a file that we can no longer read,
    a database that we can no longer access, an operating system that no longer boots,
    or worse, it can even mean a number in an accounting application changing to a
    different, but valid, number which is all but impossible to detect.
  prefs: []
  type: TYPE_NORMAL
- en: Availability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditionally, we thought about data reliability mostly in terms of how available
    our data was when it came time to retrieve it. Availability is often referred
    to as *uptime* and if your storage is not available, neither is your workload.
    So, while availability generally takes a back seat to durability, it is still
    extremely important and one of the two key aspects of overall storage reliability.
  prefs: []
  type: TYPE_NORMAL
- en: There are times when availability and performance become intertwined. There
    can be situations where storage performance drops so significantly that data becomes
    effectively unavailable. Consider a shower that just drips every few seconds,
    technically there is still water, but it is not coming through the pipes fast
    enough to be able to use it.
  prefs: []
  type: TYPE_NORMAL
- en: We will be talking about RAID in depth in just a little bit, but availability
    and performance are good real-world examples. A famous situation can arise with
    large RAID 6 arrays when a drive or two have failed and have been replaced and
    the array is online and in the process of actively rebuilding (a process by which
    missing data is recalculated from metadata.) It is quite common for the RAID system
    to be overwhelmed due to the amount of data being processed and written that the
    resulting array, while technically online and available, is so slow that it cannot
    be used in any meaningful way and operating systems or applications attempting
    to use it will not just be useless from the extreme slowness but may even error
    out reporting that the storage is offline due to the overly long response times.
    *Available* can become a murky concept if we are not careful.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to computers in the twenty first century storage is almost always
    the most significant performance bottleneck in our systems. CPU and RAM almost
    always have to wait on storage rather than the other way around. Modern storage
    using solid state technologies has done much to close the performance gap between
    storage systems and other components, but the gap remains rather large.
  prefs: []
  type: TYPE_NORMAL
- en: Performance can be difficult to measure as there are many ways of looking at
    it, and different types of storage media tend to have very different performance
    characteristics. There are concepts such as latency (time before data retrieval
    begins), throughput (also known as *bandwidth*, measuring the rate at which data
    can be streamed), and input/output operations per second or IOPS (the number of
    storage related activities that can be performed in each amount of time.) Most
    people think of storage only in terms of throughput, but traditionally IOPS have
    been the most useful measurement of performance for most workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is always tempting to reduce factors to something simple to understand and
    compare. But if we think about cars, we could compare three vehicles: one with
    a fast acceleration but a low top speed, one with slow acceleration and a high-top
    speed, and a tractor trailer that is slow to accelerate and has a low top speed
    but can haul a lot of stuff at once. The first car would shine if we only cared
    about latency: the time for the first packet to arrive. The second car would shine
    if we cared about how quickly a small workload could be taken from place to place.
    This is most like measuring IOPS. The tractor trailer will be unbeatable if our
    concern is how much total data can be hauled between systems over the duration
    of the system. That''s our throughput or bandwidth. With cars, most people think
    of a *fast car* as the one with the best top speed, but with storage most people
    think about the tractor trailer example as what they want to measure, but not
    what *feels* fast when they use it. In reality, performance is a matter of perspective.
    Different workloads perceive performance differently.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, a backup deals in steady, linear data and will benefit most from
    storage systems designed around throughput. Therefore, tape works so well for
    backup performance and why old optical media such as CD and DVD were acceptable.
    But other workloads, like databases, depend heavily on IOPS and low latency and
    benefit little from total throughput and so really benefit from solid state storage.
    Other workloads like file servers often need a blend of performance and work just
    fine with spinning hard drives. You have to know your workload in order to design
    a proper storage system to support it.
  prefs: []
  type: TYPE_NORMAL
- en: Performance is even more complex when we start thinking in terms of burstable
    versus sustainable rates. There is just a lot to consider, and you cannot short
    circuit this process.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A typical physical system deployment is expected to see four to eight years
    in production today and it is not uncommon to hear of systems staying in use far
    longer. Spend any amount of time working in IT and you are likely to encounter
    systems still powered on and completely critical to a company's success that have
    been in continuous use for twenty years or even more! Because a storage system
    is expected to have such a long lifespan, we have to consider how that system
    might be able to grow or change over that potential time period.
  prefs: []
  type: TYPE_NORMAL
- en: Most workloads experience capacity growth needs over time and a storage design
    that can expand capacity as needed can be beneficial both for just protecting
    against the unknown but also by allowing us to invest minimally up front and spending
    more only *if* and *when* additional capacity becomes needed. Some storage systems
    may also be able to scale in terms of performance as well. This is less common
    and less commonly considered critical, yet even if a workload only increases capacity
    needs and not performance needs *per se*, larger capacity alone can warrant a
    need for increases performance just to handle tasks such as backups since large
    capacities mean larger time to backup and restore.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, you could also have a situation where the needs for reliability (durability,
    availability, or both) may need to increase over time. This, too, can be possible,
    but is likely to be much more complex.
  prefs: []
  type: TYPE_NORMAL
- en: Storage is an area in which flexibility to adjust configuration over time is
    often the hardest, but also the most important. We cannot always foresee what
    future needs will be. We need to plan our best to allow for flexibility to adjust
    whenever possible.
  prefs: []
  type: TYPE_NORMAL
- en: Capacity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we look at capacity, the amount of data that can be storage on a system.
    Capacity might seem straightforward, but it can be confusing. Even in simple disk-based
    arrays we have to think in terms of raw capacity (the sum of the capacities of
    all devices) and in terms of the resultant capacity (the usable capacity of the
    system that can be accessed for storage purposes. Many storage systems have redundancies
    to provide for reliability and performance and this comes at the cost of consumed
    raw capacity. So, we have to be aware of how our configuration of our storage
    will affect the final outcome. Storage admins will talk in terms of both raw and
    usable capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good handle on the aspects of storage that we need to keep
    in mind we can dive into learning more about how storage components are put together
    to build **enterprise storage subsystems**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding block storage: Local and SAN'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the root of any standard storage mechanism that we will encounter today is
    the concept of **block devices**. Block devices are storage devices that allow
    for non-volatile data storage that can be stored and retrieved in arbitrary order.
    In a practical sense, think of the *standard* block device as being the hard drive.
    Hard drives are the prototypical block device, and we can think of any other block
    device as behaving like a hard drive. We can also refer to this as implementing
    a drive interface or *appearance*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many things are block devices. Traditional spinning hard drives, solid state
    drives (SSD), floppy disks, CD-ROM, DVD-ROM, tape drives, RAM disks, RAID arrays
    and more are all block devices. As far as a computer is concerned, all of these
    devices are the same. This makes things simple as a system administrator: everything
    is built on block devices.'
  prefs: []
  type: TYPE_NORMAL
- en: From a system administrator perspective, we often simple refer to block devices
    as *disks* because from the perspective of the operating system we cannot tell
    much about the devices and only know that we are getting block storage. That block
    storage might be a physical disk, a logical device built on top of multiple disks,
    an abstraction built on top of memory, a tape drive, or remote system, you name
    it. We cannot really tell. To us it is just a block device and since block devices
    generally represent disks, we call them disks. It is not necessarily accurate,
    but it is useful.
  prefs: []
  type: TYPE_NORMAL
- en: Locally attached block storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simplest type of block storage devices is those that are physically attached
    to our system. We are familiar with this in the form of standard internal hard
    drives, for example. Local block devices commonly attach by way of SAS, SATA,
    and NVMe connections today. In the recent past, **Parallel SCSI** (just called
    **SCSI** at the time), and **Parallel ATA** (aka **PATA**) just called **ATA**
    or **IDE** at the time, were standards. All of these technologies, as well as
    some more obscure, allow physical block devices to attach directly to a computer
    system.
  prefs: []
  type: TYPE_NORMAL
- en: It is locally attached storage that we will work with most of the time. And
    all block devices have to be locally attached somewhere in order to be used. So
    this technology is always relevant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Locally attached block devices come with a lot of inherent advantages over
    alternatives. Being locally attached there is a natural performance and reliability
    advantage: the system is as simple as it gets and that means that there is less
    to go wrong. All other things being equal, simple trumps complex. Storage is a
    great example of this. Fewer moving parts and shorter connection paths means we
    get the lowest possible latency, highest possible throughput, and highest reliability
    at the lowest cost!'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, locally attached storage comes with caveats or else no one would
    even make another option. The negative of locally attached storage is flexibility.
    There are simply some scenarios that locally attached storage cannot accommodate
    and so we must sometimes opt for alternative approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Storage Area Networks (SAN)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The logical alternative to a locally attached device is a remotely attached
    device and while one would think that we would simply refer to these types of
    block devices in this manner, we do not. A remote attached device uses a network
    protocol to implement the concept of *remoteness* into the storage and the network
    on which a remote device is communicating is called a **Storage Area Network**
    and because of this, common vernacular simply refers to all remote block storage
    as being a **SAN**.
  prefs: []
  type: TYPE_NORMAL
- en: The terrible terminology of SAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Technically speaking, a Storage Area Network should be a reference to a dedicated
    network that is used to carry block device traffic and in very technical circles
    this is how the term is used. Devices on a SAN can be direct block devices, disk
    arrays, and other similar *block over network* devices. The SAN is the network,
    not a *thing* that you can buy.
  prefs: []
  type: TYPE_NORMAL
- en: In the common parlance, however, it is standard to refer to any device that
    provides storage, implements a block device interface, and connects to a network
    rather than directly to a computer as a SAN. You hear this every day in phrases
    like *did you buy a SAN?*, *we need a SAN engineer*, *I spoke to our SAN vendor*,
    *should we upgrade the SAN?*, and *where is our SAN?* Go to your nearest IT hardware
    vendor and ask them to sell you a SAN and they will without hesitation, the terminology
    is so standard that dollars to donuts says that they will be completely confused
    if you try to act like a SAN is anything but a hardware device into which you
    place hard drives and is connected to a network via some sort of cable.
  prefs: []
  type: TYPE_NORMAL
- en: Because storage is complex, confusing, and scary and because storage area networks
    add additional layers of complexity on top of the basics this entire arena became
    treated as black boxes full of magic and terminology quickly deteriorated and
    most beliefs around SAN became based on misconceptions and myth. Common myths
    include impossible ideas such that SANs cannot fail, that SANs are faster than
    the same technology without the networking layer, that SANs are a requirement
    of other technologies, and others.
  prefs: []
  type: TYPE_NORMAL
- en: We can only be effective system administrators if we understand how the technology
    works and avoid giving in to myths (and marketing.) For example, we cannot make
    meaningful risk analysis or performance decisions if we believe that a device
    is magic and do not consider its actual risk profile.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, a SAN is an extremely simple concept. We take any block device whether
    it is a physical device like an actual hard drive, or some more complicated concept
    like an array of drives, and encapsulate the standard block device protocol (such
    as SCSI or ATA) and send that over a network protocol (such as TCP/IP, Ethernet,
    or *FiberChannel*.) The network protocol acts as a simple tunnel, in a way, to
    get the block protocol over a long distance. That's all that there is to it. At
    the end of the day, it is still just a SCSI or ATA based device, but now able
    to be used over a long distance.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, what we just added is a bit of complexity, so SANs are automatically
    more fragile than local storage. Any and all risk and complication of local storage
    remains plus any complications and risk of the networking layer and equipment.
    The risk is cumulative. Plus, the extra networking layer, processing, and distance
    all must add additional latency to the storage transaction.
  prefs: []
  type: TYPE_NORMAL
- en: Because of these factors, SAN based storage is always slower and more fragile
    than otherwise identical local storage. The very factors that most myths have
    used to promote SAN are exactly their weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SAN approach does have its strengths, of course, or else it would serve
    no purpose. A SAN allows for three critical features: distance, consolidation,
    and shared connections.'
  prefs: []
  type: TYPE_NORMAL
- en: Distance can mean anything from a few extra feet to across the world. Of course,
    with longer distances come higher latencies, and typically storage is very sensitive
    to latency, so it is pretty rare that remote block storage is useful from outside
    of the range of local connection technologies. If you have to pull block storage
    data over the WAN, you will likely experience at very least latencies that cause
    severe performance issues and will typically see untenable bandwidth constraints.
    Typical production block storage is assumed to be many GB/s (that big B, not little
    b) of throughput and sub-millisecond latency, but WAN connections rarely hit even
    a single Gb/s and even the best latencies are normally a few milliseconds if not
    scores or more!
  prefs: []
  type: TYPE_NORMAL
- en: Consolidation was traditionally the driving value of a SAN. Because many systems
    can physically connect to a single storage array over a single network it became
    easy, for the first time, to invest in a single, expensive storage system that
    could be used by many physically separate computer systems at once. The storage
    on the device would be *sliced* and every device that attaches to it sees its
    own unique portion of the storage.
  prefs: []
  type: TYPE_NORMAL
- en: When local storage isn't local
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With all of the interfaces, abstractions, and incorrect terminology that often
    exists in IT, it can be really easy to lose track of exactly what is happening
    much of the time. SANs are one of these places were getting confused is par for
    the course. It is the nature of a SAN to take a block device that is far away
    and make it seem, to the computer using it, as if it were local. But it can also
    take something that is local, and make it seem local, when it is really remote.
    Did I just say that?
  prefs: []
  type: TYPE_NORMAL
- en: The best example is that of the external USB hard drive. We all use them; they
    are super common. Go to any local department store and pick one up. Order one
    online. You probably have five on a shelf that you have forgotten about. A USB
    drive, while external, is obviously still local, right?
  prefs: []
  type: TYPE_NORMAL
- en: Well, it isn't all that easy to say. Sure, it is physically close. But in technology
    terms remote means that something is *over a network* and local is *not over a
    network*. It does not matter how far away something is, it is the network aspect
    that determines local and remote devices. Otherwise, my desktop in Texas is physically
    attached to my dad's desktop in New York because there is a series of cables the
    entire way in between them.
  prefs: []
  type: TYPE_NORMAL
- en: This presents an interesting challenge because, you see, USB is actually a very
    simple networking protocol, as are IEEE 1394 and Thunderbolt. If you physically
    dissect an external drive you can see this at work, to some degree. They are made
    from standard hard drives, generally with SATA interfaces, and a tiny network
    adapter that encapsulates the SATA protocol into the USB network protocol to be
    sent over the network (often just two feet total distance.)
  prefs: []
  type: TYPE_NORMAL
- en: USB and its ilk might not feel like a network protocol, but it really is. It
    is a layer two network protocol that competes with Ethernet and can attach multiple
    devices, to multiple computers, and can even use things similar to switches. It
    is a real networking platform and that means that external hard drives attached
    via USB are, in fact, tiny SANs! Hard to believe, but it is true. Consider your
    mind blown.
  prefs: []
  type: TYPE_NORMAL
- en: Storage, being the largest cost of most systems, being able to be shared and
    sliced more efficiently lowered cost to deploy new physical computer systems.
    Hard drives, for example, might come in 1TB sizes, but a single system might need
    only 80GB or 300GB or whatever and with a shared SAN hundreds of computers systems
    might share a single storage array and each use only what they need. Today we
    gain most of this efficiency through local storage with virtualization, but before
    virtualization was broadly available only systems like a SAN were able to address
    this cost savings. So, in the early days of SAN, the focus was cost savings. Other
    features really came later. This value has mostly inverted today and so is generally
    more expensive than having overprovisioned local storage but can still exist in
    some cases.
  prefs: []
  type: TYPE_NORMAL
- en: The last value is shared connections. This is where two or more computers access
    the same portion of the storage on the same device - seeing the same data. This
    might sound a bit like traditional file sharing, but it is anything but that.
  prefs: []
  type: TYPE_NORMAL
- en: In file sharing we are used to computers having a *smart* gatekeeping device
    that arbitrates access to files. With a SAN, we must remember that this is a *dumb*
    block device that has no logic of its own. Using a SAN to attach two or more computer
    systems to a single logical block device means that each computer thinks of the
    storage as being its own, private, fully isolated system and has no knowledge
    of other systems that might be also attached to it. This can lead to all kinds
    of problems from lost changes to corrupt files, to destroyed file systems. Of
    course, there are mechanisms that can be used to make shared storage spaces possible,
    but by definition they are not implemented by the SAN and have to be provided
    at a higher level on the computer systems themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Shared SCSI connections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the days before SAN, or before SANs were popular and widely available, there
    was another technique allowing two computers to share a single pool of hard drives:
    shared SCSI.'
  prefs: []
  type: TYPE_NORMAL
- en: With this technique, a single SCSI ribbon cable (typically able to connect to
    eight, sixteen, or even thirty-two devices. One device would need to be a controller,
    presumably on the motherboard of a computer. The other connections were open for
    connecting hard drives. But another connector could be connected to another controller
    on a separate computer and the two computers could each see and access the same
    drives.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the limitations of needing to share a single ribbon cable between two
    physical computers made this technique outrageously limited and awkward, but feasible.
    The primary value to a setup of this nature was allowing one computer system to
    fail and the other to take over, or to double the CPU and RAM resources assigned
    to a single data set beyond what could fit in a single server chassis. But the
    reliability and performance limits of the storage component left the system generally
    less than practical and so this technique was rarely implemented in the real world.
    But historically it is very important because it is the foundation of modern shared
    block storage, it was standard knowledge expected in late 1990s systems training,
    and it helps to visualize how SAN works today - more elegant, more flexible, but
    fundamentally the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: Today, the biggest use cases for shared block storage connections is for clustered
    systems that are designed to use this kind of storage as shared backing for virtualization.
    This was the height of fashion around 2010 but has since given way to other approaches
    to tackle this kind of need. This would now be a rather special case system design.
    But the technologies that are used here will be co-opted for other storage models
    as we will soon see.
  prefs: []
  type: TYPE_NORMAL
- en: The world of SAN has many popular connection technologies. There are super simple
    SAN transports that are so simple that no one recognizes them as being such including
    USB, Thunderbolt, and IEEE1394/Firewire. Then there are a range of common enterprise
    class SAN protocols such as iSCSI (SCSI over IP), FibreChannel, FCoE (Fibre Channel
    over Ethernet), *FC-NVMe* (NVMe over Fiber Channel), and so on. Each SAN protocol
    presents its own advantages and challenges, and typically vendors only offer a
    small selection from their own equipment so choosing a vendor will typically limit
    your SAN options and picking a SAN option will limit your vendor selection choices.
    Understanding all of these protocols moves us from the systems world into the
    networking one. It is rare that as a system administrator you will be in a position
    to choose or even influence the choices in SAN design, typically this will be
    chosen for you by the storage, networking, and/or platform teams. If you do get
    to have influence in this area then significant study of these technologies, their
    benefits, and their applicability to your workload(s) will be necessary but is
    far outside of the scope of this tome.
  prefs: []
  type: TYPE_NORMAL
- en: Block storage is not going anywhere. As much as we get excited about new storage
    technologies, such as object storage, block storage remains the underpinning of
    all other storage types. We have to understand block devices both physically and
    logically as we will use them in myriad ways as the building blocks of our storage
    platforms. Block storage is powerful and ubiquitous. It represents the majority
    of storage that we interact with during our engineering phases and is expected
    to remain at the core of everything that we do for decades to come.
  prefs: []
  type: TYPE_NORMAL
- en: 'When deciding between local and remote block storage there is a useful rule
    of thumb: *You always want to use local storage until you have a need that local
    storage cannot fulfill. Or you never want to use remote storage until you have
    no other choice.*'
  prefs: []
  type: TYPE_NORMAL
- en: Surveying filesystems and network filesystems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sitting on top of block storage we typically find a **filesystem**. Filesystems
    are the primary (and by primary, I mean like they make up something like 99.999%
    or more of use cases) manner of final data storage on computer systems. Filesystems
    are what hold files, as we know them, on our computer storage.
  prefs: []
  type: TYPE_NORMAL
- en: A filesystem is a data organization format that sits on top of block storage
    and provides a mechanism for organizing, identifying, storing, and retrieving
    data using the file analogy. You use filesystems every day on everything. They
    are used even when you cannot see them whether it is on your desktop, cell phone,
    or even on your VoIP phone, or microwave oven! Filesystems are everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: Filesystems are really databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to get a little geeky with me for a moment and be honest you are
    reading a book on system administration best practices so we both know you are
    loving getting into some serious details, we can look at what a filesystem really
    is. At its core a filesystem is a NoSQL database, specifically a file database
    (essentially a specialized document database), that uses a raw block device as
    its storage mechanism and is only able to store and retrieve files.
  prefs: []
  type: TYPE_NORMAL
- en: There are other specialty databases that use block devices directly (often called
    raw storage when dealing with database lingo), but they are rare. Filesystems
    are a database type that is so common, so much more common than all other database
    types combined, that no one ever talks about or thinks about them being databases
    at all. But under the hood, they are truly a database in every sense.
  prefs: []
  type: TYPE_NORMAL
- en: To show direct comparisons, a standard database regardless of type has a standardized
    storage format, a retrieval format, a database engine (driver), and in some cases
    a database management layer (that can often allow for the use of multiple database
    engineers within a single system interface), and a query interface for accessing
    the data. Whether you compare MongoDB or MS SQL Server you will find that filesystems
    behave identically. The chosen filesystem on disk format is the storage format,
    the retrieval format is the *file*, the database engine is the filesystem drive,
    the database management system in Linux is the Virtual File System (which we will
    discuss later), and the query language is a list of underlying POSIX commands
    implemented in C (with simple shell-based abstractions that we can use for convenience.)
    Compare to standard databases and there is no way to tell them apart! Very cool
    stuff.
  prefs: []
  type: TYPE_NORMAL
- en: After a computer system is deployed, nearly everything that we do with it from
    a storage perspective involves working on the filesystem. We tend to focus heavily
    on block storage during engineering phases, and filesystems during administration
    phases. But certainly, we have to plan our filesystems properly prior to deploying
    a system to production. Proper filesystem planning is actually something that
    is heavily overlooked with most people simply accepting defaults and rarely thinking
    filesystem design at all.
  prefs: []
  type: TYPE_NORMAL
- en: Most operating systems have native support for a few different filesystems.
    In most cases an operating system has one obviously standard and primary filesystem
    and a handful of special case filesystems that are relegated to use on niche hardware
    devices or for compatibility with other systems. For example, Apple macOS uses
    APFS (Apple File System) for all normal functions but can use ISO 9660 when working
    with optical disks or FAT32 and **exFAT** used for compatibility with Windows
    storage devices (such as USB memory sticks or external hard drives.) Windows is
    similar but with NTFS instead of APFS. Windows recently has added one alternative
    filesystem, ReFS, for special needs, but it is not commonly used or understood.
  prefs: []
  type: TYPE_NORMAL
- en: In Linux, however, we have several primary filesystem options and scores of
    specialty filesystem options. We have no way to go through them all here, but
    we will talk about several of the most important as understanding why we have
    them, and when to choose them is very important. Thankfully in production systems
    we really only have to concern ourselves with a few key products. If you find
    filesystems interesting, you can research the many Linux filesystem options to
    learn more about filesystem design and history and you might even find one that
    you want to use somewhere special!
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the key Linux filesystems today with which we need to be concerned
    for everyday purposes: XFS, EXT4, ZFS, BtrFS. Nearly everything that we do will
    involve one of those four. There are loads of less popular filesystems that are
    well integrated and work perfectly well like JFS and ReiserFS but are almost never
    seen in production. There are older formats like EXT2 and EXT3 that have been
    superseded by more recent updates. There are loads and loads of filesystems that
    are standard on other systems that can be used on Linux like NTFS from Windows
    or UFS from the BSD family. There are the standard niche filesystems like ISO
    9660 and FAT32 that we mentioned earlier. Linux gives you options at every turn
    and filesystem selection is a great example of just how extreme it can get.'
  prefs: []
  type: TYPE_NORMAL
- en: 'EXT: The Linux filesystem family'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Nearly every operating system has its own, special-sauce filesystem that it
    uses natively or by default and is tightly associated with it and Linux is no
    exception.... just kidding, Linux is absolutely the exception which is amazing
    considering how much more robust Linux is in its filesystem options than any other
    operating system. Illumos has ZFS, FreeBSD has UFS, Windows has NTFS, macOS has
    APFS, AIX has JFS, IRIX had XFS and on, and on. Linux truly has no filesystem
    of its own, yet it has nearly everyone elses.
  prefs: []
  type: TYPE_NORMAL
- en: Most people talk about the EXT filesystem family as being the Linux native filesystem
    and certainly nothing else comes close to matching that description. When Linux
    was first being developed, long before anyone had actually run it, the MINIX filesystem
    was ported to it and became the default filesystem as the new operating system
    began to take off. But as the name suggests, the MINIX Filesystem was native to
    MINIX and predated Linux altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Just one year after Linux was first announced, the EXT filesystem (or MINIX
    Extended File System) was created taking the MINIX Filesystem and, you guessed
    it, extending it with new features mostly around timestamping.
  prefs: []
  type: TYPE_NORMAL
- en: As Linux began to grow, EXT grew with it and just one year after EXT was first
    released its successor EXT2 was released as a dramatic upgrade taking the Linux
    filesystem ecosystem from a hobby system to a serious enterprise system. EXT2
    ruled the Linux ecosystem almost exclusively from its introduction in 1993 until
    2001 when Linux went through a bit of a filesystem revolution. EXT2 was such a
    major leap forward that it was backported to MINIX itself and had drivers appear
    on other operating systems like Windows and macOS. Possibly no filesystem is more
    *iconically* identified with Linux than EXT2.
  prefs: []
  type: TYPE_NORMAL
- en: By 2001 many operating systems were looking to more advanced filesystem technologies
    to give them a competitive advantage against the market and Linux did so both
    by introducing more filesystem options and by adding journaling functionality
    to EXT2 to increment its version to EXT3\. This gave the EXT family some much
    needed stability.
  prefs: []
  type: TYPE_NORMAL
- en: Seven more years and we received one additional major upgrade to Linux' quasi-native
    filesystem with EXT4\. Surprisingly, the primary developer on EXT3 and EXT4 stated
    that while EXT4 was a large step forward that it was essentially a stopgap measure
    of adding improvements to what is very much a 1980s technology. Filesystem design
    principals leaped forward especially in the early 2000s and the EXT family is
    likely at the end of the road, but still has a lot of useful life left in it.
  prefs: []
  type: TYPE_NORMAL
- en: I am going to delve into a little bit of detail for each of the main filesystem
    options, but to be clear this is a cursory look. Filesystem details can change
    quickly and can vary between versions or implementations so for really specific
    details such as maximum file size, file count, filesystem size and so forth please
    look to Wikipedia or filesystem documentation. You will not need to memorize these
    details and rarely will you even need to know them. In the 1990s filesystem limitations
    were so dramatic that you had to be acutely aware of them and work around them
    at every turn. Today any filesystem we are going to use is able to handle almost
    anything that we throw at it, so we really want to understand where different
    products shine or falter and when to consider which ones at a high level.
  prefs: []
  type: TYPE_NORMAL
- en: EXT4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linux, as a category, has no default filesystem in the way that other operating
    systems do, but if you were to attempt to make the claim that any filesystem deserves
    this title today that honour would have to go to **EXT4**. More deployed Linux-based
    operating systems today choose EXT4 as their default filesystem than any other.
    But this is beginning to change so it seems unlikely that EXT4 will remain dominant
    for more than a couple years yet.
  prefs: []
  type: TYPE_NORMAL
- en: EXT4 is reasonably fast and robust, quite flexible, well known, and meets the
    needs of nearly any deployment. It is the jack of all trades of Linux filesystems.
    For a typical deployment, it is going to work quite well.
  prefs: []
  type: TYPE_NORMAL
- en: EXT4 is what we call a *pure filesystem*, which means it is just a filesystem
    and does not do anything else. This makes it easier to understand and use, but
    also makes it more limited.
  prefs: []
  type: TYPE_NORMAL
- en: XFS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like the EXT family, **XFS** dates back to the early 1990s, and comes from a
    Linux competitor, in this case SGI's IRIX UNIX system. It is venerable and robust
    and was ported to Linux in 2001, the same year that EXT3 released. For twenty
    years now EXT3/4 and XFS have competed for the hearts and souls of Linux Administrators
    (and Linux Distro creators to choose them as the default filesystem.)
  prefs: []
  type: TYPE_NORMAL
- en: XFS is also a *pure filesystem* and is very commonly used. XFS is famous for
    its extremely high performance and reliability. XFS is sometimes specifically
    recommended by high performance applications like databases to keep them running
    at their peak.
  prefs: []
  type: TYPE_NORMAL
- en: XFS is probably the most deployed filesystem when the system administrator is
    deliberately choosing the filesystem rather than simply taking the default, is
    probably the most recommended by application vendors, and is my own personal choice
    for most workloads where storage needs are non-trivial.
  prefs: []
  type: TYPE_NORMAL
- en: Over the years, EXT4 and XFS have gone back and forth in popularity. My own
    observations say that XFS has slowly been edging ahead over the years.
  prefs: []
  type: TYPE_NORMAL
- en: The one commonly cited caveat of XFS compared to EXT4 is that EXT4 is able to
    either shrink *or* grow a volume once it has been deployed. XFS can grow, but
    cannot shrink, a volume that has been deployed. However, it is almost unheard
    of for a properly deployed production system to shrink a filesystem, so this is
    generally seen as trivia and not relevant to filesystem decision making (especially
    with the advent of thin provisioned block storage.)
  prefs: []
  type: TYPE_NORMAL
- en: ZFS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Releasing to Solaris in 2006, **ZFS** is generally considered to be the foundation
    of truly modern filesystem design. By the time that work on ZFS had begun in 2001
    the industry was already beginning to take filesystem design very seriously and
    many new concepts were being introduced regularly, but ZFS really took these design
    paradigms to a new level and ZFS remains a significant leader in many areas still
    today.
  prefs: []
  type: TYPE_NORMAL
- en: 'ZFS really has three high level areas in which it attempted to totally disrupt
    the filesystem industry. First in size: ZFS was able to address multiple orders
    of magnitude more storage capacity than any filesystem before it. Second in reliability:
    ZFS introduced more robust data protection mechanisms than other filesystems had
    allowing it to protect against data loss in significant ways. And third, in integration:
    ZFS was the first real *non-pure* filesystem where ZFS represented a filesystem,
    a RAID system, and a logical volume manager all built into a single filesystem
    driver. We will go into depth about RAID and LVMs later in this chapter. This
    integration was significant as it allows the storage layers to communicate and
    coordinate like never before. Pure filesystems like EXT4 and XFS can and do use
    these technologies but do so through external components rather than integrated
    ones.'
  prefs: []
  type: TYPE_NORMAL
- en: While ZFS is not new, having been in production systems for at least fifteen
    years, it is quite new for release on Linux. It took many years before a port
    of ZFS was made available for Linux, and then there were many years during which
    licensing concerns kept it from being released in a consumable format for Linux.
    Today the only major Linux distribution that officially supports and packages
    ZFS is Ubuntu, but Ubuntu's dominant market position makes ZFS automatically widely
    available. At this time, it is less than two years since ZFS was able to be used
    for the bootable root filesystem on Ubuntu. So ZFS is quite new in the production
    Linux space in any widely accessible way. Its use appears to be growing rapidly
    now that it is available.
  prefs: []
  type: TYPE_NORMAL
- en: ZFS represents probably the most advanced, reliable, and scalable filesystem
    available on Linux as of the time of this writing. It should be noted that from
    a purely filesystem-based performance perspective that ZFS is not known to shine.
    It is rare that storage performance tweaking at the filesystem level is considered
    valuable but when it is modern filesystems with all of their extra reliability
    typically cannot compete with older, more basic filesystems. This has to be noted
    as it is so often simply assumed that more modern systems are going to also be
    automatically faster, as well. This is not the case here.
  prefs: []
  type: TYPE_NORMAL
- en: BtrFS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pronounced *Butter-F-S*, **BtrFS** is the current significant attempt to make
    a Linux native filesystem (there was a previous attempt called ReiserFS in the
    early 2000s that got some traction but ended badly for non-technical reasons.)
    BtrFS is intended to mimic the work of ZFS, but native to Linux and with a compatible
    license.
  prefs: []
  type: TYPE_NORMAL
- en: BtrFS trails ZFS significantly with many features still not implemented, but
    with work ongoing. BtrFS is very much alive and increasingly more Linux distributions
    are supporting it and even choosing it as a default filesystem. BtrFS feels as
    if it might be the most likely long-term future for Linux.
  prefs: []
  type: TYPE_NORMAL
- en: Like ZFS, BtrFS is a modern, heavily integrated filesystem that is beginning
    to include functionality from RAID and LVM layers of the storage stack. Performance
    is the weakest point for BtrFS today.
  prefs: []
  type: TYPE_NORMAL
- en: Stratis
  prefs: []
  type: TYPE_NORMAL
- en: Given its industry support, we need to make mention of Stratis. Stratis is not
    a filesystem itself *per se* but act much like one. Stratis is an attempt to build
    the functionality of integrated (or *volume-managing file systems*) like ZFS and
    BtrFS using the existing components of XFS and the standard Linux LVM layer.
  prefs: []
  type: TYPE_NORMAL
- en: In its early days on IRIX, XFS was designed to be used with IRIX's native LVM
    and the two integrated naturally providing something not entirely unlike ZFS or
    BtrFS today. When XFS was ported to Linux its associated LVM layer was not ported,
    but the native Linux LVM was made to work with it instead. XFS + LVM has long
    been an industry standard approach and Stratis mearly is attempting to provide
    a more accessible means of doing so while integrating best practices and simplified
    management.
  prefs: []
  type: TYPE_NORMAL
- en: This sums up the four current production filesystem options that you will likely
    encounter or be responsible for choosing between. Remember that you can mix and
    match filesystems on a single system. It is very common, in fact, to use EXT4
    as a boot filesystem for basic operating system functionality while then relying
    on XFS for a high-performance database storage filesystem or BtrFS for a large
    file server filesystem. Use what makes sense for the workload at an individual
    filesystem layer. Do not feel that you have to be stuck using only one filesystem
    across all systems, let alone within a single system!
  prefs: []
  type: TYPE_NORMAL
- en: Most of the real intense technical aspects of filesystems are in the algorithms
    that deal with searching for and storing the bits onto the block devices. The
    details of these algorithms are way beyond the scope of not only this book but
    systems administration in general. If you are interested in filesystems, learning
    how data is stored, protected, and retrieved from the disk can be truly fascinating.
    For systems administration tasks it is enough to understand filesystems at a high
    level.
  prefs: []
  type: TYPE_NORMAL
- en: Sadly, there is no way to provide a real best practice around filesystem selection.
    It is unlikely that you are going to have reason to seriously consider the use
    of any rare filesystem not mentioned here in a production setting, but all four
    that are listed here have valuable use cases and all should be considered. Often
    the choice of filesystem is not made in isolation unless you are working with
    a very specific product that requires or recommends a specific one for the purpose
    of some feature. Instead, the choice of filesystem is normally going to be dependent
    on many other storage decisions including RAID, LVM, physical support needs, drive
    media, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: Clustered file systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All of the filesystems that we have discussed thus far, regardless of how modern
    they are, are *standard* filesystems or *non-shared* filesystems. They are only
    viable when access is guaranteed to be from only a single operating system. In
    almost all cases, this is just fine.
  prefs: []
  type: TYPE_NORMAL
- en: If you recall from our discussion on SANs, however, we mentioned that there
    are use cases where we may want multiple computer systems to be able to read and
    write for the same storage area at the same time. Clustered or *shared storage*
    filesystems are the mechanism that can allow that to work.
  prefs: []
  type: TYPE_NORMAL
- en: Clustered filesystems work just like traditional filesystems do, but with the
    added features by which they will write locking and sharing information to the
    filesystem so that multiple computer systems are able to coordinate their use
    of the filesystem between attached nodes. In a standard filesystem there is only
    one computer accessing the filesystem at a time so knowing what file is open,
    when a file has been updated, when a write is cached and so forth are all handled
    in memory. If two or more computers try to share data from a traditional filesystem,
    they cannot share this data in memory and so will inevitably create data corruption
    as they overwrite each other's changes, fail to detect updated files, and do all
    sorts of nasty things from outdated write caches!
  prefs: []
  type: TYPE_NORMAL
- en: Since the only shared component of these systems is the filesystem, all communications
    between nodes accessing a file system have to happen in the filesystem itself.
    There is literally no other possible way without going to a mechanism that is
    no longer shared storage but shared compute which is much more complicated and
    expensive.
  prefs: []
  type: TYPE_NORMAL
- en: To describe how clustered filesystems work in the simplest terms we can think
    of each computer knowing, from the filesystem, that a specific section of the
    block device (disks) is set aside in an extremely rigid format and size to be
    an area where the nodes read and write their current status of interaction with
    the filesystem. If node A needs to open File X, it will put in a note that it
    is holding that file open. If node B deletes a file, it will put in a note that
    it is going to delete and update it once the file is deleted. node C can tell
    what activity is going on just by reading this one small piece of the filesystem.
    All of the nodes connected know not to cache the data in this area, to state any
    action that they plan to take, and to log anything that they have done. If any
    node misbehaves, the whole system corrupts, and data is lost.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, as you can tell, this creates a lot of performance overhead at a
    minimum. And this system necessarily requires absolute trust between all connected
    nodes as the access and data integrity controls are left up to the individual
    nodes. There is not and there cannot be any mechanism to force the nodes to behave
    correctly. The nodes have to do so voluntarily. This means that any bug in the
    code, any failure of memory, any admin with root access, any malware that gains
    access to a single node, and others. can bypass *any* and *all* controls and read,
    modify, destroy, encrypt, and others, to any degree that it wishes and all of
    the kinds of security and controls that we normally assume protect us do not exist.
    Shared storage is extremely simple, but we are so used to storage abstractions
    that it becomes complex to try to think about how any storage system could be
    so simple.
  prefs: []
  type: TYPE_NORMAL
- en: Like with regular filesystems, Linux has multiple clustered filesystems that
    we will commonly see in use. The most common one is GFS2 followed by OCFS2\.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with SANs in general, the same rule will apply to clustered file systems:
    you do not want to use them, until you have to.'
  prefs: []
  type: TYPE_NORMAL
- en: Network filesystems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Network filesystems** are always a little bit hard to describe but benefit
    well from the *you will know it when you see it* phenomenon. Unlike regular filesystems
    that sit on top of a block device and provide a way to access storage in the form
    of files, network filesystems take a filesystem and extend it over a network.
    This might sound a lot like a SAN, but it is very different. A SAN shares a set
    of block devices over a network. Network filesystems share filesystems over a
    network.'
  prefs: []
  type: TYPE_NORMAL
- en: Network filesystems are quite common, and you probably see them every day, but
    we often do not think about them being what they really are. We often refer to
    network filesystems as *shares* or *mapped drives* and the standard protocols
    used are NFS and SMB (sometimes called CIFS, which is not really accurate.) Servers
    that implement network filesystems are called file servers and if you make a file
    server into an appliance, it is called a NAS (for Network Attached Storage.) Network
    filesystems are also often thought of as *NAS protocols* for this reason, just
    as block over network protocols are thought of as *SAN protocols*.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike shared block protocols, network filesystems are *smart* with the machine
    that shares out the storage having a local filesystem that it understands and
    intelligence about the files involved so concepts like file locking, caching,
    file updates and so forth can be handled through a single gatekeeper that can
    enforce security and integrity and there is no need to trust the accessing nodes.
    The key difference is that a SAN is just storage blindly attached to a network,
    it can be as simple as a network adapter bolted onto a hard drive (and it often
    is, actually.) A device implementing a network filesystem, on the other hand,
    is a server and required a CPU, RAM, and an operating system to function. Shared
    block storage is almost exclusively used in very limited deployments with carefully
    controlled servers. Network filesystems can be used almost anywhere that a SAN
    can be used but are also commonly used to share storage directly to end user devices
    as their robust security, ease of use and lack of needed end point trust make
    them highly useful where SANs would be impossible to deploy.
  prefs: []
  type: TYPE_NORMAL
- en: Network filesystems run as an additional network-enabled layer on top of traditional
    filesystems and do not replace the *on disk* filesystems that we already have.
    In speaking of interfaces, we would describe network filesystems as *consuming
    a filesystem interface* and also *presenting a filesystem interface*. Basically,
    it is filesystem in, filesystem out.
  prefs: []
  type: TYPE_NORMAL
- en: Like with traditional filesystems, Linux actually offers a large range of network
    filesystem options, many of which are historical in nature or extremely niche.
    A common example is the **Apple Filing Protcol** or **AFP** (aka *AppleTalk*)
    which Linux offers, but is not used on any production operating system today.
    Today only NFS and SMB really see any real work usage in any way.
  prefs: []
  type: TYPE_NORMAL
- en: NFS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The original *network file system* in wide use and literally the source of the
    name, **NFS** dates back to 1984! NFS cannot be native to Linux as it predates
    Linux by seven years, but NFS has been the default network file system across
    all UNIX-based or inspired operating systems since its inception and represents
    a rather significant standard because of this. Because Linux is so prominent today,
    most people think of NFS as being *Linux' protocol*.
  prefs: []
  type: TYPE_NORMAL
- en: NFS is available on essentially any system. Any UNIX system, even macOS, offers
    NFS as does Windows Server! NFS is an open standard and all but universal. NFS
    maintains popularity by being simple to use, robust on the network and generally
    performs well. NFS remains heavily used on servers wherever direct filesharing
    between systems is required, especially on backup systems.
  prefs: []
  type: TYPE_NORMAL
- en: SMB
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Server Message Block** (**SMB**) protocol predates NFS and was originally
    available in 1983\. These are very old protocols indeed. SMB did not really find
    much widespread usage until Microsoft really began to promote it around 1990 and
    with the rise of the Windows NT platform throughout the 1990s SMB began to become
    quite popular along with it.
  prefs: []
  type: TYPE_NORMAL
- en: SMB really benefited from Microsoft's heavy use of mapped drives between their
    servers and workstations which made the SMB protocol very visible to many users
    both traditional and technical.
  prefs: []
  type: TYPE_NORMAL
- en: In Linux, support for the SMB protocol is provided by the Samba package (Samba
    is a joke on the SMB letters.) Linux has good support for SMB, but using it is
    more complex than working with NFS.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing between NFS and SMB for file sharing needs on Linux generally comes
    down to the use case. If working with predominantly UNIX systems, generally NFS
    makes the most sense. If working predominantly with Windows systems, then SMB
    generally makes the most sense. Both are powerful and robust and can service a
    wide variety of needs.
  prefs: []
  type: TYPE_NORMAL
- en: Where decision making can get extremely hard is in cases where we can provide
    for the same need it totally different ways. For example, if you need to provide
    shared storage backends for virtualization you might have the option of a network
    filesystem like NFS or a clustered filesystem on a SAN like GFS2\. These two approaches
    cannot be easily compared as every aspect of the two systems is likely to be different
    including the vendors and hardware and so comparisons typically must be done at
    a full stack level and not at the network technology level.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have explored file systems technologies and seen a broad scope of real-world
    file system options for Linux systems and looked at how filesystems can be local
    or remote, single access, or clustered to allow for multiple access vectors. At
    the same time, we have a good idea of how to approach decisions involving filesystem
    selection and configuration. We know when choose different filesystem technologies
    as well as what to look for in new or alternative systems that we may not have
    looked at specifically here. Filesystems need not be scary or confusing but can
    be valuable tools in our bag of tricks that we can use to fine tune our systems
    for safety, scalability, access, or performance. Next, we will look at one of
    the least understood areas of storage, the *logical volume*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know logical volume management (LVM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hate to apply terms like *new* to technology that was in use by the late 1980s
    but compared to most concepts in computer storage **logical volume management**
    (**LVM**) is pretty new and is far less known than most other standard storage
    technologies to the majority of system administrators. LVMs were relegated to
    extremely high-end server systems prior to Linux introducing the first widely
    available product in 1998 and Microsoft following suit in 2000\. Today LVMs are
    ubiquitous and available, often natively and by default, on most operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: An LVM is the primary storage virtualization technology in use today. An LVM
    allows us to take an arbitrary number of block devices (meaning one or more, generally
    called *physical volumes*) and combine, split, or otherwise modify them and present
    them as an arbitrary number of block devices (generally called *logical volumes*)
    to the system. This might sound complex, but it really is not. A practical example
    can make it seem quite easy.
  prefs: []
  type: TYPE_NORMAL
- en: An example is when we have a computer system that has three hard drives attached
    to it. They can be all the same, or they can be different. In fact, one could
    be a traditional spinning hard drive, one a modern SSD, and one an external USB
    drive (or a RAID array, SAN, you name it). We can add all three to our LVM as
    physical volumes. An LVM will allow us to treat this as a single storage pool
    and turn it into any configuration that we want. We might turn it all into a single
    logical volume so that we simply get the combined storage of the three. Or maybe
    we will create a dozen logical volumes and use each for a different purpose. We
    can have as many physical volumes as we want and create as many logical volumes
    as we want. Logical volumes can be any size that we want (in some cases even bigger
    than the total physical size!) We are not limited to traditional disk sizes. With
    logical volumes we often find it useful to make more, smaller volumes so that
    we can have more management and isolation.
  prefs: []
  type: TYPE_NORMAL
- en: With an LVM, we can think of the system as consuming a block device and presenting
    a block device. Because LVMs use and provide block devices (aka disk appearances)
    they are *stackable*, meaning if you wanted to, you could have a block device
    on which there is an LVM which makes a logical volume, that is used by another
    LVM, that makes another logical volume, that is used by another LVM, and so forth.
    This is in no way practical, but it helps to visualize how an LVM sits in a *storage
    stack*. It is always in the middle, somewhere, but other than being in the middle
    it is very flexible.
  prefs: []
  type: TYPE_NORMAL
- en: LVMs only need to provide this basic *block in, block out* functionality to
    be an LVM, but there are other features that are commonly added to LVMs to really
    make them incredibly useful. Some of the most standard features that we tend to
    expect to be found in an LVM include live resizing of logical volumes, *hot plugging*
    of physical devices, snapshot functionality, cache options, and thin provisioning.
  prefs: []
  type: TYPE_NORMAL
- en: On Linux, as with most things, we have not one, but multiple logical volume
    managers! This is becoming more common as creating integrated filesystems with
    their own LVMs has become the trend in recent years. In production on Linux today
    we have LVM2, ZFS, and BtrFS. You will, of course, recognize the latter two as
    being filesystems that we mentioned earlier. When most people are talking about
    a logical volume manager on Linux, they mean LVM2, generally just called LVM.
    But ZFS and BtrFS' integrated logical volume managers are becoming increasingly
    popular approaches as well.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the *stackable* nature of an LVM, that is consuming and providing
    block devices, we are able to use LVM2 in conjunction with ZFS or BtrFS if we
    so choose and can either disable their integrated LVM layers as being unnecessary,
    or we can use them if they have features that we want to take advantage of! Talk
    about flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever happen to partitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you recall working in IT in the 1990s, we used to talk about disk partitioning
    quite incessantly. It was a regular topic. How do you set the partitions, how
    many do you make, basic and extended partitions, what partitioning software to
    use, and so on and so forth. To be sure, partitions still exist, we just have
    not needed them for a very long time now (not since Windows 2000 or Linux 2.4,
    for example.)
  prefs: []
  type: TYPE_NORMAL
- en: Partitions are a very rigid *on disk* system for slicing a physical disk into
    separate areas that can each be presented to the system as an individual block
    device (aka drive.) In this way, partitions are like a super basic LVM without
    the flexibility. Partitions are limited to existing as a part of a single block
    device and the mapping of which areas of the block device belong to which partition
    are kept in a simple partition table at the start of the device.
  prefs: []
  type: TYPE_NORMAL
- en: Partitions were the forerunner of logical volumes and some people still use
    them (but only because they are not familiar with logical volumes.) Partitions
    are not flexible and lack important options like thin provisioning and snapshots,
    that logical volumes can offer, and while resizing a partition is technically
    possible it is inflexible, hard, and extremely risky.
  prefs: []
  type: TYPE_NORMAL
- en: Everything that partitions offered LVMs offer too, plus lots more, without giving
    anything up. The need for partitioning (the act of creating multiple filesystems
    out of a block device) has decreased significantly over the years. In the late
    1990s it was standard and all but required for even the simplest server and often
    even a desktop to have good reason to be divided up into different filesystems.
    Today it is far more common to merge many block devices into a single filesystem.
    Mostly this is because filesystem performance and reliability have totally changed
    and the driving factors for partitioning have eroded. There are good reasons to
    still divide filesystems today. We simply need to do so far less of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Many mechanisms today, such as backup utilities, leverage the power of the LVM
    layer to do tasks like freezing the state of the block device so that a complete
    backup can be taken. Because an LVM operates beneath the final filesystem layer
    it has certain capabilities lacking at other layers. LVM is the storage layer
    where we get the least critical features, but it tends to be where all of the
    magic happens. LVMs give us flexibility to modify storage layouts after initial
    deployment and to interact with those storage systems at a block level. An LVM
    is a core technology component in providing the feel of a modern twenty-first
    century operating system.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, any new technology layer will have caveats. An LVM adds another layer
    of complication and more pieces for you, as the system administrator to understand.
    Learning to manage the LVM is hardly a huge undertaking, but it is quite a bit
    more to have to learn than if you do not have one. LVMs also introduce a small
    amount of performance overhead as they translate between the physical devices
    and the logical ones. Typically, this is a truly tiny amount of overhead, but
    it is overhead nonetheless.
  prefs: []
  type: TYPE_NORMAL
- en: In general the benefits of an LVM dramatically outweigh the cost and more and
    more systems are starting to simply deploy an LVM layer without asking the end
    user whether or not they want it because increasingly functionality that customers
    simply expect an operating system to have depend on the LVM layer and allowing
    systems to be deployed without one often leaves customers stranded unclear why
    their systems do not live up to expectations and often in a way that they do not
    realize for months or years after initial deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Like other forms of virtualization, storage virtualization and LVMs are most
    important for *protecting against the unknown*. If we knew everything about how
    a system would be used for its entire lifespan, things like resizing, backups,
    consolidation and so forth would have little value, but this is not how the real-world
    works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practices when it comes to an LVM is generally accepted to be: Unless
    you can positively provide solid technical reasons why the overhead of the LVM
    is going to be impactful and that that impact outweighs the protections that an
    LVM provides, always deploy an LVM.'
  prefs: []
  type: TYPE_NORMAL
- en: Logical volume managers provide a critical building block to robust storage
    solutions and are, in many ways, what separates modern storage from classical
    computing systems. Understanding how logical volumes abstract storage concepts
    and let us manipulate and build storage to act like we want gives us many options
    and lead us to additional concepts such as RAID and RAIN, which we will discuss
    next that use LVM to construct data protection, expansion, and performance capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing RAID and RAIN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have looked at so many ways of interfacing with our storage. But probably
    the most exciting is when we start to deal with **RAID** (**Redundant Array of
    Inexpensive Disks**) and by extension its descendant, **RAIN** (**Redundant Array
    of Independent Nodes**). Before we go too far, it must be noted that RAID is a
    huge topic that would require a book of its own to truly address in a meaningful
    way. Understanding how RAID works and all of the calculations necessary to understand
    the nuances of its performance and risk is a major subject all on its own. My
    goal here is to introduce the concept, explain how it fits into a design, expose
    the best practices around it, and prepare you for further research.
  prefs: []
  type: TYPE_NORMAL
- en: RAID and RAIN are mechanisms for taking many *storage devices* (block devices)
    and using the natural device multiplicity (often misstated as redundancy) to provide
    some combination of improved performance, reliability, or scalability over what
    possibility with only an individual drive is. Like an LVM, RAID and RAIN are *mid-stack*
    technologies that consumer block device interfaces and provide block device interfaces
    and therefore can be *stacked* on top of an LVM, below an LVM, on top of another
    RAID, on top of a mixture of hardware devices, and so forth. Very flexible.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, RAID and RAIN are actually each a specialized form of LVM! No one,
    ever, talks about these technologies in this way and you will get some strange
    looks at the company Christmas party if you start discussing RAID as a specialized
    LVM, but it actually is. RAID and RAIN are extreme subsets of LVM functionality
    with a very tight focus. It is not actually uncommon for general purpose LVMs
    to have RAID functionality built into them and the trend with the integrated filesystems
    is to have the LVM and RAID layers both integrated with the filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: RAID
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAID standard for *Redundant Array of Inexpensive Disks* and was originally
    introduced as a set of technologies that work at the block device level to turn
    multiple devices into one. RAID-like technologies go way back to even the 1960s
    and the term and modern definitions are from 1988, which means that RAID actually
    pre-dates more general purpose LVM.
  prefs: []
  type: TYPE_NORMAL
- en: RAID essentially takes an array of block devices and puts them into lockstep
    with each other under one of many different data storage regimes, called *levels*.
    Each RAID level acts different and uses a different mechanism to merge the underlying
    physical disks into a single virtual disk. By making multiple drives act as if
    they were a single drive, we can extend different aspects of the storage as needed
    but gaining in one area normally comes at a cost in another so understanding the
    way that RAID works is important.
  prefs: []
  type: TYPE_NORMAL
- en: RAID is an area where there is a very high importance on the system administrator
    having a deep understanding of the inner workings of the storage subsystem. And
    surprisingly it is an area where very few system administrators truly know how
    their system works.
  prefs: []
  type: TYPE_NORMAL
- en: While RAID comes defined as a series of *levels*, but do not be fooled. The
    levels are simply different types of storage that share the underlying RAID basics.
    RAID levels do not really build on one another and a higher number does not represent
    some intrinsically superior product.
  prefs: []
  type: TYPE_NORMAL
- en: Because RAID is really a form of an LVM, it can sit anywhere in the storage
    stack and in the wild can be found almost anywhere. Some very popular RAID levels
    are actually *stacked RAID* leveraging this innate artefact of its design, most
    notably RAID 10.
  prefs: []
  type: TYPE_NORMAL
- en: RAID also comes in both hardware and software variants. Hardware RAID is much
    like a graphics card that connects directly to a monitor and offloads work from
    the main computer systems and talks to the hardware directly. A hardware RAID
    card does exactly this reducing load on the main computer system, interfacing
    to hardware storage devices directly, and potentially offering special features
    (like a cache) in the hardware. Software RAID, instead, leverages the generally
    much more powerful system CPU and RAM and has more flexible configurations. Both
    approaches are completely viable.
  prefs: []
  type: TYPE_NORMAL
- en: Each RAID level has a unique set of properties and makes sense to be used at
    a different time. RAID is a complex topic deserving of its own tome to tackle
    properly. RAID is not a topic that we can look at too quickly, which has been
    a danger with storage in the past. RAID risk factors are often distilled into
    meaningless statements such as stating numerically *how many drives can a RAID
    array of level X recover from?* This means nothing and is meant as a way of simplifying
    something very complex into something that can simply be memorized or thrown onto
    a chart. RAID does not work this way. Each RAID level has a complex story around
    performance, reliability, cost, scalability, real world implementations, and so
    forth.
  prefs: []
  type: TYPE_NORMAL
- en: RAIN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over time as systems became larger and more complex, the limitations with the
    RAID approach began to become apparent. RAID is simple and easy to implement.
    But RAID is inflexible and there are some key features like simple resizing, automated
    rebalancing, and flexible node sizing that it handles poorly. A new family of
    technologies were needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAIN eschews the *full block device* approach to arrays that RAID was based
    on and instead breaks up storage by smaller chunks, often by blocks, and handles
    replication at that level. In order to do this effectively, RAIN must not just
    understand the concept of these blocks, but also the block devices (or *disks*)
    that they are on, but also the nodes in which they exist. This nodal awareness
    lends RAIN its moniker: *Redundant Array of Independent Nodes*.'
  prefs: []
  type: TYPE_NORMAL
- en: Oddly, in RAIN it is not really the nodes that are necessarily redundant but
    really the blocks and you can actually implement RAIN on a single physical device
    to directly compete with traditional RAID in its simplest form, but this is rarely
    done.
  prefs: []
  type: TYPE_NORMAL
- en: Because RAIN handles block level replication it can have many advantages over
    RAID. For example, it can use different sizes devices rather fluidly. Drives of
    varying sizes can be thrown *willy nilly* into servers and tied together with
    great efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Under RAID if a drive fails, we need a replacement drive that is capable of
    taking over its place in the array. This is often problematic, especially with
    older arrays. RAIN can avoid this issue by allowing any combination of available
    capacity across the array absorb the lost capacity of a failed hard drive.
  prefs: []
  type: TYPE_NORMAL
- en: RAIN comes in such a variety of implementations, each one essentially unique,
    that we really cannot talk about it in any standard way. Most solutions are proprietary
    today and while a few well known open products have been made and have made their
    way into being standard components of the Linux ecosystem generally they are external
    to the distributions they behave much like proprietary products in how we must
    approach them.
  prefs: []
  type: TYPE_NORMAL
- en: In the future we may see significant consolidation or at least standardization
    within the RAIN market as these technologies become more available and well understood.
    Until then we need to approach RAIN with the understanding of how block replication
    *could work* and know that each implementation may make drastically different
    design choices. RAIN might be built into the kernel or might exist as an application
    running higher in the stack, in some cases it could even run in a virtual machine
    virtualized on top of a hypervisor! How RAIN will react to a lost drive, to load
    balancing, locational affinity, rebalancing during loss, rebuild after repair,
    and so forth are not defined by any standards. To use RAIN, you must research
    and learn in depth about any solution that you will be considering and think critically
    about how its artefacts will impact your workloads over time.
  prefs: []
  type: TYPE_NORMAL
- en: RAIN is almost guaranteed to be the future of system storage. As we move more
    and more towards clusters, hyperconvergence, cloud and cloud-like designs RAIN
    feels more and more natural. And adoption of RAIN will only increase as understanding
    increases. This simply takes time even though the technology itself is not new.
  prefs: []
  type: TYPE_NORMAL
- en: Nearly every production system that we will ever design or support, will use
    some form of RAID or RAIN whether locally or remotely. By now, we are prepared
    to think about how the decision of what RAID level or configuration or what RAIN
    implementation is chosen will impact our systems. Taking the time to deeply understand
    storage factors in these multi-device aggregation frameworks interact is one of
    the most valuable high level knowledge areas for system administrators across
    the board. In our next section, we will build on these technologies to see how
    local storage can be made redundant with external systems, or nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about replicated local storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Possibly the most critical storage type, and the least understood, is **replicated
    local storage** or **RLS**. RLS is not a difficult concept, it is quite simple.
    But there are many myths surrounding other concepts, such as SAN, that they have
    clouded the functionality of RLS. For example, many people have started using
    the term *shared storage* as a proxy for *external storage* or possible for *SAN*.
    But external storage does not mean that it is or can be shared, and local storage
    does not mean that it is not or cannot be shared.
  prefs: []
  type: TYPE_NORMAL
- en: The term replicated local storage refers to two or more computer systems which
    have local storage that is replicated between them. From the perspective of each
    computer system, the storage is locally attached, just like normal. But there
    is a process that replicates the data from one system to another, so that changes
    made on one appear on the other.
  prefs: []
  type: TYPE_NORMAL
- en: Replicated local storage can be achieved in multiple ways. The simplest, and
    earliest for was to use **Network RAID**, that is RAID technology simply used
    over a network. **Mirrored RAID** (aka **RAID 1**) is the simplest technology
    for this and makes for the best example.
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways to handle this scenario, one is a hot/cold pair where one
    node is *hot* and has access to write to the storage and the other node(s) can
    read from it and, presumably, take over as the hot writeable node should the original
    hot node fail or relinquish control. This model is easy and is similar to many
    traditional models for shared storage on a SAN as well. This approach allows the
    use of regular (non-clustered) filesystems such as XFS or ZFS.
  prefs: []
  type: TYPE_NORMAL
- en: The other approach is a live/live system where all nodes replicating the storage
    can read and write at any time. This requires the same clustered filesystem support
    that we would need with any shared block storage. Just like with a SAN being used
    at the same time by two nodes, the nodes in an RLS cluster will need to communicate
    by storing their activity data in a special area of the clustered file system.
  prefs: []
  type: TYPE_NORMAL
- en: Replicated local storage can give us many of the benefits typically associated
    with a SAN or other external storage, namely the ability for multiple nodes to
    access data at once. While also having the benefits of locality including improved
    performance and resilience because there are fewer dependencies. Of course, the
    replication traffic has its own overhead, and this has to be considered. There
    are many ways that replication can be configured, some with very little overhead,
    some with a great deal.
  prefs: []
  type: TYPE_NORMAL
- en: It is common to feel that replicated local storage is new or novel or in some
    way unusual. Nothing could be further from reality. In fact, what is rarely understood,
    is that for high reliability storage systems RLS is always used. Whether it is
    used locally (that is, directly attached to the compute systems) or if it is used
    remotely (meaning that the remote storage uses RLS to make itself more reliable),
    RLS is at the core of nearly any true high availability storage layer.
  prefs: []
  type: TYPE_NORMAL
- en: RLS comes in multiple flavours, primarily Network RAID and RAIN. We could call
    it Network RAIN when used in this situation, but we do not. Unlike RAID, which
    is nearly always local only, RAIN is nearly always used in an RLS situation and
    so the network nature of it is nearly assumed, or at least the network option
    of it.
  prefs: []
  type: TYPE_NORMAL
- en: RLS in so many forms on Linux that we cannot really talk about all of the options.
    We will have to focus on a few more common ones. RLS is an area where there are
    many open sources as well as commercial and proprietary solutions with a wide
    variety of performance, reliability, and features; and implemented in often very
    different ways. RLS can add rather a new level of complexity to any storage situation
    because you have to consider the local storage communication, the replication
    communication, any potential network communication between nodes and remote storage
    (that is local to another node), and how the algorithms and protocols interact
    with each other.
  prefs: []
  type: TYPE_NORMAL
- en: DRBD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first and simplest RLS technology on Linux is **DRBD** or the **Distributed
    Replicated Storage System**, which is a Network RAID layer baked right into the
    Linux kernel. Wikipedia states that DRBD is not *Network RAID*, but then describes
    it as exactly Network RAID. Whether it is or not might be little more than semantics,
    in practice it is indistinguishable from Network RAID in use, in practice, by
    description, and even under the hood. Like all RAID, DRBD consumes block devices
    and appears as a block device allowing it to be stacked anywhere in the middle
    of the storage stack just like regular RAID and LVMs.
  prefs: []
  type: TYPE_NORMAL
- en: DRBD is built on a RAID 1 (mirrored RAID) mechanism and so allows for two to
    many nodes with each node getting a copy of the data.
  prefs: []
  type: TYPE_NORMAL
- en: DRBD is very flexible and reliable, and because of its simplicity it is far
    easier for most system administrators to understand clearly as to how it works
    and fits into the storage infrastructure. But because DRBD is limited to full
    block device replication by way of mirroring, as is RAID 1, the ability to scale
    is quite limited. On its own, DRBD is very much focused on classic two node clusters
    or very niche use cases with a large number of compute nodes needing to share
    a small amount of identical data.
  prefs: []
  type: TYPE_NORMAL
- en: Making DRBD flexible
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because DRBD is just a software RAID tool, in effect, and because you have complete
    management of it, and because RAID acts as an LVM with total flexibility to sit
    anywhere in a stack, you can take DRBD and turn it into something far more scalable
    than it might first appear. But currently this process is all manual, although
    in theory you could script it or create tools to otherwise automate these kinds
    of procedures.
  prefs: []
  type: TYPE_NORMAL
- en: One powerful technique that we can use is the concept of *staggering* our RAID
    1 with extra logical block devices to mimic RAID 1E which operates essentially
    like RAID 1 but is scalable. This technique works by taking the physical storage
    space on an individual node and logically breaking it into two (or theoretically
    more) sections with an LVM technology. In a standard Network RAID setup, the entire
    space of the storage on node 1 is mirrored to the entire storage space on node
    2\. But now that we have split storage on each node, we mirror the first portion
    of node 1 to a portion of node 2; and node 2 does the same but with node 3; and
    node 3 does the same but with node 4; and this same process carries on indefinitely
    until whatever the terminal node number is does this with node 1 completing the
    *circle* and every machine has RAID 1 for its data, split between two other nodes
    as its mirrored pair. In this way, we can make a Network RAID 1 ring indefinitely
    large.
  prefs: []
  type: TYPE_NORMAL
- en: This technique is powerful, to be sure. But it is extremely cumbersome to document
    and maintain. If you have a static cluster that never changes, it can work very
    well. But if you regularly grow or modify the cluster, it can be quite problematic.
  prefs: []
  type: TYPE_NORMAL
- en: DRBD, and most Network RAID technologies, are typically blessed with good overall
    performance and, perhaps more importantly, rather predictable performance. DRBD,
    by its nature of presenting a final block device, is inherently local. In order
    to access DRBD resources remotely it would be necessary to use DRBD as a building
    block to a SAN device which would then be shared remotely. This is, of course,
    semantics only. DRBD is always local because to DRBD the SAN is the local compute
    node, the SAN interface is another layer higher up the proverbial stack and so
    while the SAN would be remote, DRBD would be local!
  prefs: []
  type: TYPE_NORMAL
- en: Gluster and CEPH
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While two different technologies entirely, **Gluster** and **CEPH** are both
    free, open source, modern RAIN solutions designed for Linux that allow for high
    levels of reliability and high degrees of scalability. Both of these solutions
    at least offer the option of having storage be local to the compute node in question.
    Both are very complex solutions with many deployment options. We cannot simply
    assume that the use of either technology tells us that storage is going to be
    local or remote. Local is the more common application, by far, but both have options
    to directly build a remotely accessible and separate storage tier that is accessed
    over a network if designed to do so.
  prefs: []
  type: TYPE_NORMAL
- en: These technologies are far too complex and full of options for us to dig into
    here. We will necessarily have to treat them at a very high level only, although
    this should be more than adequate for our needs.
  prefs: []
  type: TYPE_NORMAL
- en: RAIN storage of this nature is the most common approach to handling large pools
    of servers (compute nodes) that will share a pool of storage. This technique gives
    the storage the opportunity to be local, to rebalance itself automatically in
    the event of a failure, but rarely will guarantee data locality. The storage across
    the group is a pool, only. So there can be an affinity for locality with data,
    but there is not the strict enforcement of locality as there is with DRBD. This
    gives more control to DRBD, but far more flexibility and better utilization with
    Gluster or CEPH.
  prefs: []
  type: TYPE_NORMAL
- en: Proprietary and third-party open-source solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to what comes baked in or potentially included with a Linux distribution
    are many third party components that you can install. Nearly all of these products
    will fall into the RAIN category and very in price, support, and capabilities.
    A few worth knowing the names of include *LizardFS*, *MooseFS*, and *Lustre*.
  prefs: []
  type: TYPE_NORMAL
- en: It is impossible to cover the potential range of commercial products that may
    be or may become available. RAIN storage is an area of current development and
    still many vendors make products in this space but do not make them widely available.
    In some cases, you can find commercial RAID or RAIN systems that are only available
    when included with appliances of one type or another or when included in some
    other project. But all of these storage systems follow the same basic concepts
    and once you know what those are and how they can work, you can make good decisions
    about your storage systems even if you have not necessarily worked with a specific
    implementation previously.
  prefs: []
  type: TYPE_NORMAL
- en: Virtualization abstraction of storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is all too easy to get lost when talking about storage and forget that most
    of the time, storage is not even something that we have to worry about as system
    administrators! At least not in the way that we have been approaching it.
  prefs: []
  type: TYPE_NORMAL
- en: The storage administrator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is not uncommon in larger organizations to decide to separate storage and
    systems duties as there are so many complexities and nuances to storage that having
    a team that is dedicated to understanding them can make sense. If you have worked
    in a Fortune 500 environment you have probably witnessed this.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the biggest problems with this come from separating the people who deeply
    understand the workloads from some of the most important factors that determine
    the performance and reliability of those workloads. Separation often also requires
    that core architectural decisions be made politically rather than technically.
    If you use local storage, you cannot separate the storage and systems teams in
    any realistic way. Because of this, many organizations have used often terrible
    technical design decisions to create skill silos within their organizations without
    considering how this would impact workloads. The deployment of SAN technology
    is quite often done for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Irrespective of the efficacy of this approach, when in use this generally means
    that storage is taken out of the hands of systems administrators. This simplifies
    our role dramatically while simultaneously cutting us off at the knees when it
    comes to being able to provide ultimate value. We can request certain levels of
    performance or reliability and must trust that our needs will be met or that we,
    at the very least, will not be held accountable for their failures.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, it is common to separate systems and platform teams. In this case
    we see the same effect. The platform team, which manages the hypervisor underneath
    the systems, will provide storage capacity to the systems team and systems must
    simple consume what is made available to them.
  prefs: []
  type: TYPE_NORMAL
- en: In both of these cases storage is abstracted from the system and provided simple
    as a *blind block device(s)* to us on the systems team. When this happens, we
    still have to understand how underlying components might work, which questions
    to ask, and at the end of the day still have to manage file systems on top of
    the provided block devices. The block device interface remains the universal *hand
    off* interface from a storage or platform team to the systems team.
  prefs: []
  type: TYPE_NORMAL
- en: 'An additional aside: the same thing will often happen to the platform team.
    They might have to take blind storage from a storage team, apply it to the hypervisor
    layer, then carve up that block device into smaller block devices to give to the
    systems team!'
  prefs: []
  type: TYPE_NORMAL
- en: In most cases today, our Linux systems will be virtualized in some manner. We
    have to understand storage all of the way down the stack because Linux itself
    may be the hypervisor (such as in the case of KVM) or be used to control the hypervisor
    (as is the case with Xen) or provide storage to a higher-level hypervisor (like
    VirtualBox) and in all these cases it is Linux managing every aspect of the storage
    experience potentially. Linux may also be being used to create a SAN device or
    storage layer in some other form. We have to understand storage inside and out,
    but the majority use cases will be that our Linux systems will be getting their
    storage from their hypervisor even if we are the managers of that hypervisor.
  prefs: []
  type: TYPE_NORMAL
- en: While they can choose to behave in many different ways, most people set up hypervisors
    to act like an LVM layer for storage. They are a bit of a special case because
    they convert from block to filesystem, then back to block for the handoff to the
    virtual machine, but the concept remains the same. Some hypervisor setups will
    simply pass through a direct block connection to underlying storage whether a
    local disk, a SAN, or a logical volume from an LVM. These are all valid approaches
    and leave more options for the virtual machine to dictate how it will interact
    with the storage. But by and large having the block layer of storage terminate
    with the hypervisor, be turned into a filesystem, creating *block device containers*
    on top of the filesystem and allowing the virtual machines to consume those devices
    as regular block devices is what is expected from virtualization that many people
    actually refer to this artefact of virtualization approaches as being intrinsic
    to virtualization itself, which it is not.
  prefs: []
  type: TYPE_NORMAL
- en: You can use this technique inside of a system as well. Examples of this include
    mounting file system file types like *qcow2*, *vhdx*, or *iso* files! Something
    that we do every day, but rarely think about or realize what we are actually doing.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously when getting our storage from the hypervisor, concerns about standard
    (non-replicated) local storage, replicated local storage, standard (non-replicated)
    remote storage, or replicated remote storage are all made at a different layer
    than the system, but the decisions are still made, and those decisions completely
    impact how our systems will ultimately run.
  prefs: []
  type: TYPE_NORMAL
- en: We have learned about a lot of storage abstraction approaches and paradigms
    now with LVMs, RAID, and RAIN. Now we need to start to think about how we will
    put these technologies together to build our own storage solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing storage architectures and risk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nothing creates more risk for our systems than our storage. That should go without
    saying, but it has to be said. Storage is where we, as system administrators,
    have our greatest opportunity to make a difference, and it is the place where
    we are mostly likely to fail and fail spectacularly.
  prefs: []
  type: TYPE_NORMAL
- en: In order to address risks and opportunities in regard to storage, we must understand
    our entire storage stack and how every layer and component interact with each
    other. Storage can feel overwhelming, there are so many moving pieces and optional
    components.
  prefs: []
  type: TYPE_NORMAL
- en: We can mitigate some of the overwhelming feelings by providing design patterns
    for success and understanding when different patterns should be considered.
  prefs: []
  type: TYPE_NORMAL
- en: General storage architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two truly high-level axis in **storage architecture**: *local* versus
    *remote*, and *standard* versus *replicated*.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the natural assumption for most people is to jump immediately to
    believing that replicated and remote are the obvious starting point. This is actually
    not true. This is probably the least sensible starting point for storage as it
    has the least likely to be useful combination of factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simple local storage: The brick'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Believe it or not, the most commonly appropriate storage design for companies
    of all sizes is local, **non-replicated storage**! Bear in mind that *replicated*
    when we speak of storage architectures does *not* reference a lack of backups
    nor a local of local replication (such as RAID mirroring) but only refers to whether
    or not storage is replicated, in real or near-real time, to a second totally separate
    system.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover this again, and slightly differently, when we look at total system
    design rather than looking at storage in isolation. Like most things in life,
    keeping things simple generally makes the most sense. Replication sounds amazing,
    a must have feature, but replication costs money and often a lot of it and to
    do replication well often impacts performance, potentially dramatically.
  prefs: []
  type: TYPE_NORMAL
- en: Replicating disaster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A common mistake made in storage design is getting an emotional feeling that
    the more replication that we have, the more that we are shielded from disaster.
    To some degree this is true, replicated some files locally with RAID 1 does a
    lot to protect against an individual hard drive failing and remote replication
    can protect against an entire node failing, but neither does anything to protect
    against much more common problems like accidental file deletion, malicious file
    destruction, file corruption, or ransomware.
  prefs: []
  type: TYPE_NORMAL
- en: If we do something simple, like delete a file that we shouldn't, then instantly
    our high-power replication mechanism will ensure that our deletion ripples through
    the entire system in a millisecond or two. Instead of protecting us, it might
    be the mechanism that replicates our mistake faster than we can react. Overbuilding
    replication mechanisms typically protects only against hardware failure and can
    quickly turn into a situation of diminishing returns.
  prefs: []
  type: TYPE_NORMAL
- en: That first level of RAID might be highly valuable because hard drive failure
    remains a very real risk and even the tiniest drive hiccup can cause significant
    data loss. But replicating between nodes will only protect against entire system
    loss which is quite a bit less common. RAID protection is relatively cheap, often
    costing only a few hundred dollars to implement. Nodal replication, however, will
    require dramatically more hardware to achieve replication generally costing thousands
    or tens of thousands of dollars for a fraction of the protection that RAID is
    already providing.
  prefs: []
  type: TYPE_NORMAL
- en: Mechanisms like RAID, especially RAID 1 (mirroring) are also extremely simple
    to implement and very straightforward. It is quite uncommon to encounter data
    loss caused by human error in mirrored RAID. The same cannot be said for replicated
    storage between nodes. There is far more to go wrong, and the chances of human
    error causes data loss is much higher. We do not simply mitigate risks by choosing
    to go this expensive route, we introduce other risks that we have to mitigate
    for as well.
  prefs: []
  type: TYPE_NORMAL
- en: Many system administrators feel that they cannot use simple, local, non-replicated
    storage, and problems with company politics cannot be overlooked. If your company
    is going to *play politics* and blame you, as the system administrator, even when
    the mistake is not yours and your decision was the best one for the business,
    then you are forced to make dangerous decisions that are not in the interest of
    the profitability of the business. That is not something that a system administrator
    can control.
  prefs: []
  type: TYPE_NORMAL
- en: As a system administrator, we can manage this political problem *in some cases*
    by presenting (and documenting well) risk and financial decisions to demonstrate
    why a decision that may ultimately have led to data loss to have still been the
    correct decision. No decision can every eliminate all risks, as IT professionals
    and especially as system administrators we are always making the decision as to
    how much risk to attempt to mitigate and at what financial cost should we do so.
  prefs: []
  type: TYPE_NORMAL
- en: Risk assessments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the hardest, yet most important, aspects of IT and especially systems
    administration is doing risk assessments to allow for proper planning. Risk is
    rarely taught either formally or organically. This is an area where nearly all
    businesses fail spectacularly and IT, where risk is absolutely key to everything
    that we do, is generally left with no training, no resources, and no support.
  prefs: []
  type: TYPE_NORMAL
- en: Teaching risk is a career in and of itself, but a few techniques that we should
    be using all of the time can be covered here. At its core, risk is about assigning
    a cost that we can apply against projected profits.
  prefs: []
  type: TYPE_NORMAL
- en: We have two key aspects of risk. One is the chance that something bad will happen,
    the second is the impact of that event. The first we can express as a matter of
    *happens X times per year* if you find that handy. The second can be expressed
    in monetary terms such as *it will cost $5,000*. If something will happen once
    a decade then you could say it is .1x per year. So, something that impacts us
    for five grands would have an annual cost of $500\. This is ridiculously simplistic
    and not really how risk works. But it's an unbelievable useful tool in expressing
    risk decisions to management where they want millions of factors distilled to
    a single bottom line number.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have to take our numbers that show the cost of a risk mitigation strategy.
    For example, if we are going to implement a replication technology that costs
    $300/year in licensing and requires ten hours of system administration time per
    year at $120/hour as can project a cost of mitigation to be $1500/year.
  prefs: []
  type: TYPE_NORMAL
- en: Next weeks need a mitigation effectiveness. Nothing is really one hundred percent.
    But a good replication strategy might be 95% or a typically one might be around
    65% effective. With these numbers we can do some hard math.
  prefs: []
  type: TYPE_NORMAL
- en: We know that we are at risk of losing roughly $500 per year. If we spend $1500
    per year, we can 95% surely stop the $500 loss. $500 * .95 = $475\. So, take $1500-$475=$1025
    of loss per year caused by the risk mitigation strategy. These are numbers you
    can take to a CFO. Do this math, you should be able to show savings or protection,
    not a loss. If you show a loss then you really, really need to avoid that plan.
    It means that the risk protection mechanism is, for all intents and purposes,
    representative of a *disaster* simply by implementing it.
  prefs: []
  type: TYPE_NORMAL
- en: Math, it might sound trite to say, but the average system administration and
    even the average CFO will often run from using basic math to show if an idea is
    good or bad and go purely on emotion. Using math will protect you. It means you
    can go to the CFO and CEO and stand your ground. You cannot argue with math. Show
    them the math, if they decide that the math is wrong, have them work the numbers.
    If they decide to ignore the math, well, you know what kind of organization you
    work for and you should really think long and hard about what kind of future there
    is at a company that thinks that profits are not their driving factor. And if
    something goes wrong in the future and you get blamed, you can pull out the math
    and ask, *if this was not the right decision, why did we not see it in the math
    when we made the decision?*
  prefs: []
  type: TYPE_NORMAL
- en: Nothing feels better that defending successfully a seemingly crazy decision
    that has been backed by math. Show that you are doing the best job possible. Do
    not just say it, do not make unsubstantiated claims. Use math and prove why you
    are making decisions. Elevate the state of decision making from guesswork to science.
  prefs: []
  type: TYPE_NORMAL
- en: Not all workloads can simple be treated this simply. But vastly more than are
    normally assumed can. This should be your standard assumption unless you have
    solid math to show otherwise. Or you are dealing with a situation that goes beyond
    math, such as life support systems where uptime and reliability are more important
    than money can every describe. Otherwise, use math.
  prefs: []
  type: TYPE_NORMAL
- en: This simplest of all storage approaches is easily thought of as *the brick*.
    It is simple, it is stable, it is reliable, it is easy to backup and restore,
    it is easy to understand and support. This solution is so effective today, especially
    with modern storage technologies and hardware, that I will state that it is appropriate
    for 85-90% of all production workloads, regardless of company size, and at least
    99% of small business workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'RLS: The ultra-high reliability solution'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What workloads and situations do not make sense for the simple architecture
    above almost always fall into this category: **replicated local storage** (**RLS**).
    RLS allows for highly available storage with great performance at reasonable cost.
    Nothing will match the performance and cost of straight local storage that we
    mentioned first, but if you need higher availability and better reliability than
    that solution can provide, this is almost certainly the solution for you.'
  prefs: []
  type: TYPE_NORMAL
- en: RLS provides the highest level of reliability available because it has fewer
    moving parts. A remote high availability solution must necessarily deal with distance,
    cabling, and network protocols as additional components at a minimum, and typically
    with networking equipment (like switches) additionally, all above and beyond the
    risks of RLS. Remember that remote storage solutions wanting to accomplish true
    high availability are going to have to do so by using RLS locally in their own
    cluster, and then making that cluster of storage available remotely over a network
    so you have all of the complications and any potential problems with RLS, plus
    the overhead and risks of the remote access technology.
  prefs: []
  type: TYPE_NORMAL
- en: If straight local storage with no remote replication takes ninety percent of
    all workloads, standard RLS much take ninety percent of what remains (the two
    should take about ninety nine percent together.) When doing proper planning, these
    two options are so simple, cost effective, and safe that they are just impossible
    to beat under normal circumstances. These are your break and butter options.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative storage reliability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While RLS might feel like the absolute end all, be all of storage protection,
    it really is not. It is great when you have to rely on the storage layer to handle
    reliability. But in that regards, it is a kludge or a band aid, not the ultimate
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: In a perfect world we have storage mechanisms that are a layer higher than our
    actual storage layer with things like databases. A database management system
    is able to do a much better job at maintaining availability and coordinating data
    replication between nodes than a blind block replication mechanism can ever do.
    Putting replication where the application intelligence is just makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, applications would use their databases as their layer for storage and
    state reliability and let intelligence systems that know how the data is used
    replicate what matters. Databases are one of the most ideal mechanisms for replication
    because they know the data that they have and are able to make good decisions
    about it.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, many enterprise applications do not use any form of storage
    or even systems replication whatsoever. Using *unreliable* systems and highly
    reliable *applications* is a solid strategy and offers benefits that you can get
    in no other way. Because of this, we can sometimes ignore high availability needs
    at the raw storage layer and just focus on performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The lab environment: Remote shared standard storage'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This architecture is very popular because it checks all of the boxes for salespeople:
    *expensive*, *risky*, and *confusing*. It is true, salespeople push this design
    more than any other because it generates so many ways to bill the customer for
    additional services while passing all responsibility on to the customer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All architectures have a legitimate place, more or less, in an ecosystem and
    this one is not an exception. But before we state a use case we should state where
    its benefits lie: non-replicated remote storage is *slower*, *riskier*, and more
    *expensive* (on the surface) than local storage mechanisms. Where do we find value
    for such a design?'
  prefs: []
  type: TYPE_NORMAL
- en: Primary the benefits are in lab, testing, and other environments where the value
    to data durability is negligible, but we can benefit from large environments where
    storage can be carved out judiciously to create maximum cost savings at a large
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: Nearly every large environment has a need for this kind of storage at some point
    in their collection of workloads. The key is to identify where this mix of needs
    is sensible and not to attempt to apply it where it is inappropriate. Because
    this kind of storage is cheap at large scale (but outrageous expensive at small
    scale) and because managers so often mistake remote shared storage for the *magic
    box that cannot fail* it is often used where it is least applicable. There is
    no magic, it is a poor technology for most production workloads and should be
    chosen only with extreme caution.
  prefs: []
  type: TYPE_NORMAL
- en: The rule of thumb here is that you should only use this type of storage if you
    can decisively prove that it makes sense - that even standard reliability and
    performance are not valuable. If you have any doubts, or if the organization has
    any doubts, then choose a different storage architecture before making a mistake
    with this one means maximum risk. Performance is a *soft failure*, it is easy
    to correct after the fact and it normally has marginal impact if you get it wrong.
    Data loss is a *hard failure* where getting it wrong is not a graceful failure
    but a catastrophic failure and there is little ability to correct it later.
  prefs: []
  type: TYPE_NORMAL
- en: Fail gracefully
  prefs: []
  type: TYPE_NORMAL
- en: We cannot always avoid failure. In fact, much of the time skirting failure is
    a critical part of our job. In order to make this work we have to understand how
    to fail well and a key component of that is the idea of *failing gracefully*.
    And in storage, this is an area where this concept is more pronounced than in
    other areas.
  prefs: []
  type: TYPE_NORMAL
- en: The idea with failing gracefully is that if we fail, we want it to be in a small,
    incremental way rather than in a tragic, total disaster kind of way. Many storage
    decisions that we make are designed around this. We know that we might get things
    wrong. So, we want to make sure that we are as close to correct as possible, but
    also with taking into consideration *what if* we are wrong.
  prefs: []
  type: TYPE_NORMAL
- en: In this way we tend heavily towards RAID 10 in RAID and towards local storage
    in architecture. We want solutions that, if we are wrong, result in us being too
    safe rather than losing the data because we thought that it would not matter.
  prefs: []
  type: TYPE_NORMAL
- en: While it takes a lot more than just bad storage decisions to determine an entire
    architecture, remote non-replicated storage is the foundation point of the popular
    design used by vendors and resellers which they typically call a **3-2-1 architecture**
    and what IT practitioners called the **Inverted Pyramid of Doom**.
  prefs: []
  type: TYPE_NORMAL
- en: This architecture is traditionally deployed broadly but is by no small margin
    the least likely architecture to ever me appropriate in a production environment.
    It is slow, it is complex, it is costly, and it carries the highest risks. It
    makes sense primarily in lab environments were recreating the data being stored
    is, at most, time consuming. This is an architecture truly designed around the
    needs of typically non-production workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'The giant scale: Remote replicated storage'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our last main architecture to consider is the *biggest* of them all. Remote
    replicated storage. It might see like this would be the storage architecture that
    you would see in every enterprise, and while not exactly rare, it is seen far
    less commonly than you would guess.
  prefs: []
  type: TYPE_NORMAL
- en: Remote replicated storage is the costliest to implement at small scale but can
    become quite affordable at large scale. It suffers from extra complexity over
    RLS (which means less reliable) and lower performance than either RLS or straight
    local storage and so only makes sense when cost savings is at a premium, but a
    certain degree of reliability is still warranted. A bit of a niche.
  prefs: []
  type: TYPE_NORMAL
- en: Considering that the two local storage architectures were granted ninety nine
    percent of workload deployments (by me, of course) between them, this architecture
    gets most of the last percentage that is left, at least in production.
  prefs: []
  type: TYPE_NORMAL
- en: The safest system is only so safe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of my more dramatic stories from my decades as a system administrator comes
    from a time that I was working at a university library as a consultant. I was
    brought in to work on some large-scale databases in this two-admin shop. The senior
    admin was out on vacation and only the junior admin was still around. The environment
    used a highly redundant and high availability SAN system on which all systems
    depended. There was an extreme amount of protection of this system from UPS to
    generators to hardware redundancy all at great expense.
  prefs: []
  type: TYPE_NORMAL
- en: While I was there, the junior admin decided to do some maintenance on the SAN
    and, for whatever reason, accidentally clicked to delete all of the volumes on
    the SAN. This was, of course, an accident but a very careless one by someone who
    was assuming that every possible failure scenario was carefully guarded against.
    But all of the high availability, all of the redundancy, all of the special technology
    did nothing to address simple human error.
  prefs: []
  type: TYPE_NORMAL
- en: With one click of her mouse, the entire library's computer systems were gone.
    The applications, the databases, the users, the logs. All of it. Everything was
    dependent on a single storage system and that system could be turned off or, in
    this case, deleted with essentially no effort by someone with access. All of the
    eggs were in a single backet that, while made very sturdily, had a big opening
    and could easily be turned upside down.
  prefs: []
  type: TYPE_NORMAL
- en: To make matters far, far worse the junior system administrator was not emotionally
    prepared for the event and was so terrified of losing her job that she had to
    be hospitalized and all of the potential IT staff that might have been able to
    have stepped in to assist were instead engaged in getting her an ambulance. Through
    poor technological planning, and through poor human planning, an easily avoidable
    disaster that should have only turned into a minor recovery disaster turned into
    a huge outage. Luckily there were good backups, and the system was able to be
    restored relatively quickly. But it highlighted well just how much we often invest
    in protecting against mechanical failure and how little we address human frailty
    and, in reality, it is human error that is far more likely to be the cause of
    a disaster than machines failing.
  prefs: []
  type: TYPE_NORMAL
- en: '*Storage architectures* and *risk* is the hardest part of storage design. Drilling
    down into filesystem details is generally fun and carries extremely little risk
    to us as system administrators. If we pick EXT4 when BtrFS would have been best,
    the penalty is probably nominal, so much so that we would never expect anyone
    to ever find out that we did not make the perfect decision. But choosing the wrong
    storage architecture could result in massive cost, large downtime, or big time
    data loss.'
  prefs: []
  type: TYPE_NORMAL
- en: We really have to take the time to understand the needs of our business and
    workloads. How much risk is okay, what performance do we need, how do we meet
    all needs at the optimum cost. If we do not know the answers then we need to find
    out.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice is, as always, to take the time to learn all business needs, learn
    all of the available factors and apply. But that is very hard to do in practice.
    Some rules of thumb to assist us will come in very handy.
  prefs: []
  type: TYPE_NORMAL
- en: Storage best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Attempting to distill storage into **best practices** is rather hard. At the
    highest level, the fundamental rule to storage is that there are no shortcuts,
    you need to understand all aspects of the storage infrastructure, understand the
    workloads, and apply that combined knowledge allow with an understanding of institutional
    risk aversion to determine workload ideals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going further, best practices include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'RAID: If data is worth storing, it is worth having on RAID (or RAIN.) If you
    are questioning the value of having RAID (at a minimum) on your servers, then
    reconsider storing the data at all.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RAID Physicality: Both hardware and software RAID implementations are equally
    viable. Determine what factors apply best to your needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LVM: Like general virtualization which we will touch on in future chapters,
    storage virtualization is not about providing a concrete feature set that we need
    on day one. It is about providing mechanisms to protect against the unknown and
    to be flexible for whatever happens in the future. Unless you can present an incredible
    strong argument for what LVM is not needed, use it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Filesystem: Do not be caught up in hype or trends. Research the features that
    matter to you today and that are likely to protect you against the unknown in
    the future and use a filesystem that is reliable, robust, and tested where you
    can feel confident that your filesystem choice is not going to hamper you long
    term.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Storage Architecture: Unless you can prove that you require or financially
    benefit significantly from remote storage, keep your storage access local. And
    unless you can demonstrate clear benefit from nodal replication, do not replicate
    between nodes. Simple trumps complex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a system administrator you might deal with storage design decisions infrequently.
    But no matter how infrequent, storage design decisions have some of the most dramatic
    impacts on our long-term system compared to any other decisions that we make.
    Take your time to really determine what is needed for every workload.
  prefs: []
  type: TYPE_NORMAL
- en: Storage example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should step back and put together an example of how these pieces might fit
    together in a real life scenario. We cannot reasonably make examples for every
    common, let alone plausible, storage scenario but hopefully we can give a taste
    of what we are talking about in this chapter to make it all come together for
    you.
  prefs: []
  type: TYPE_NORMAL
- en: To keep things reasonably simple, I am going to work as generically as possible
    with the absolutely most common setup found in small and medium businesses. Or
    at least what probably should be the most common setup for them.
  prefs: []
  type: TYPE_NORMAL
- en: Smaller businesses generally benefit from keeping their designs quite simple.
    Lacking large, experienced staff and often at high risk from turnover, small businesses
    need systems that require less maintenance and those that can easily be maintained
    by consultants or staff that may not possess tribal knowledge of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: For these kinds of environments, and also for many larger ones, hardware RAID
    with hot and blind swappable drives are important. They allow hardware maintenance
    to be done by bench professionals without a necessity to engage systems administration
    tasks. This becomes extremely critical when dealing with colocation or other distant
    facilities. In these cases, someone from the IT department may have no way to
    be physically involved at all.
  prefs: []
  type: TYPE_NORMAL
- en: So, we start with some general case assumptions. At the physical layer we have
    eight hard drives. These can be spinning drives, SSD, NVMe, any block device.
    It does not really matter. What matters is that we have multiple, but we want
    them to all act as a single unit.
  prefs: []
  type: TYPE_NORMAL
- en: So, we add to this a hardware RAID controller. This RAID controller we use to
    attach all of the drives and set it to put them into an appropriate RAID level.
    This could be any number of options, but for this example we will say that they
    are in RAID 10.
  prefs: []
  type: TYPE_NORMAL
- en: From the very beginning of our example, without having even installed Linux
    or anything of the sort, we have used the hardware of our system to implement
    our first two layers of storage! The physical devices, and the first abstraction
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: We will not show actually inserting the drives here, that is purely a manual
    process and chances are your server vendor has done this already for you, anyway.
  prefs: []
  type: TYPE_NORMAL
- en: As for setting up the RAID itself, every controller and vendor is slightly different,
    but the basics are the same and the task is always extremely simple. That is much
    of the point of the RAID controller - to reduce the amount of knowledge and planning
    necessary around RAID operations to the bare minimum both up front and during
    operational phases. For our example here, to make things easier to demonstrate,
    we are going to assume that we are dealing with a single array that is *not* the
    array from which our operating system is running so that we can show some of the
    steps more easily from the command line. But this is just an example.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that hardware RAID is different for every vendor and potentially every
    product by a vendor. So you always need to be aware of exactly how your specific
    product works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we should note that to the RAID controller, each drive attached to it
    is a block device. This is the unique case where the block device interface, pretending
    to be a physical hard drive, is actually and truly a hard drive! In every subsequent
    case we will be using software to implement the interface of a hard drive but
    the drive that we represent will be logical, not physical:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is a real syntax for a real-world RAID controller. Typically, you will
    do this task graphically from a GUI. But sometimes you will want to use a command
    line utility. When possible, I work from the command line, it is more repeatable
    and far easier to document.
  prefs: []
  type: TYPE_NORMAL
- en: Once we are past this phase of initial hardware configuration then we can proceed
    with the Linux specific portions of the example.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware RAID controllers typically create their own naming conventions in the
    /dev filesystem. In our example case, the controller uses the `cciss` syntax and
    created device `c0d1` under that system. All of these will vary depending on the
    control and configuration that you use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we are going to create a logical volume layer on top of the RAID layer
    to allow us more flexibility in our storage system. To do so we must start by
    adding the newly created device to Linux'' LVM system as *a physical device*.
    We do this with the `pcvreate` command and the path to our new device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Very fast and easy. Now the LVM subsystem is aware of our new RAID array device.
    But of course, all that LVM knows and cares about is that it is a block device.
    That it is a RAID array specifically is actually not something that LVM can detect,
    nor does it matter. The point here is that it is abstracted and can be utilized
    the same no matter what it is. The speed, capacity, and safety characteristics
    are encapsulated in the RAID layer and now we can think of it purely as a hard
    drive as we move forward.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting point here is that when using a hardware RAID controller
    this abstract and virtualized hard drive representation is only a logical hard
    drive, but as a block device it is actually physical! Mind blowing, I know. It's
    really hardware, it just is not a hardware hard drive. Ponder on that for a moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'How that our RAID array is under LVM''s management we can add the drive to
    a volume group. In this example we will be adding it to a new volume group that
    we are creating just for the purposes of this example. We name this new group
    `vg1`. Here is an example command doing just this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Okay, now we are getting somewhere. The capacity of the individual physical
    hard drives being combined by the RAID controller into a single logical drive
    under LVM control is now in a capacity pool or *volume group* where we can start
    to carve out actually useful subsets of that capacity to use on our server.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the volume group created, all that is left is to make the actual *logical
    volumes*. Remember that logical volumes have replaced *partitions* as the primary
    means of dividing a block device into consumable portions that we can use. For
    our example we are going to do the absolute simplest thing and tell the LVM system
    to make just one logical volume that is as large as possible; that is, using 100%
    of the available capacity of the volume group (which is currently at 0% utilization
    as this is the very first thing that we will have done with it.):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This command tells LVM to create a new logical volume, using 100% of the free
    space that is available, in volume group `vg1`, and name the new logical volume
    `lv_data`. That is it. We now have a logical volume that we can use! It should
    be obvious that we could have made a smaller logical volume, say of 50% of the
    available space, and then made a second one, also of 100% of what was remaining
    after that to give us two equal sized logical volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that an LVM system like the one found here in Linux, gives us flexibility
    that we would often lack if we were to apply a filesystem directly to the physical
    drives or even to the virtual drive presented by the RAID controller hardware.
    The LVM system lets us add more physical devices to the volume group, for example,
    which gives us more capacity for making logical volumes. It will let us resize
    the individual logical volumes, both growing or shrinking them. LVM will also
    allow us to snapshot a logical volume which is very useful for building a backup
    mechanism or preparing to do a risky system modification so that we can revert
    quickly. LVM does very important things.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that `lv_data` has been created we will need, in most cases, to format
    it with a filesystem in order to make it truly useful. We will format with XFS.
    In the real world today, XFS would be the most likely to be recommended filesystem
    for general purpose needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Very simple. In a few seconds we should have a fully formatted logical volume.
    In applying the filesystem format we stop the chain of block device interfaces
    and now present a filesystem interface which is the change that allows applications
    to use the storage in a standard way instead of using block devices as are used
    by storage system components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, one last step is necessary, we have to mount the new filesystem
    to a folder to make it usable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: That is it! We just implemented a multi-layer abstraction based storage system
    for one of the most common system scenarios. We have built an XFS file system
    on top of LVM logical volume management on top of a hardware RAID controller on
    top of multiple individual physical hard drives.
  prefs: []
  type: TYPE_NORMAL
- en: Because each layer uses the block device interface, we could have mixed and
    matched so many more additional features. Like using two RAID controllers and
    merging their capacity with the volume group. Or making multiple volume groups.
    We could have made many logical volumes. We could have used software RAID (called
    MD RAID in Linux) to create RAID using the output of the two RAID controllers!
    The sky is really the limit, but practicality keeps us grounded.
  prefs: []
  type: TYPE_NORMAL
- en: At this point if you *cd /data* you can use the new filesystem just as if it
    has always been there. That it is a new filesystem, that it is built on all these
    abstraction layers, that there are multiple physical devices making all of this
    magic happen is completely hidden from you at this point. It just works.
  prefs: []
  type: TYPE_NORMAL
- en: Now in the past, if this was 2004, we would generally stop here and say that
    we have described what a real world server is likely going to look like if it
    is implemented well. But this is not 2004 by a long shot and we really need to
    talk more about how we are likely going to see our storage used in the most common
    scenarios. So today we need to think about how our virtualization layers are going
    to use this storage, because things get even more interesting here.
  prefs: []
  type: TYPE_NORMAL
- en: We will assume that our /data filesystem that we just created will be used to
    store the drive images of a few virtual machines. Of course, these drive images
    are just individual files that we store in the filesystem. Very simple. No different
    than creating and storing a text file in /data (except VM drive images tend to
    be just a tad larger.)
  prefs: []
  type: TYPE_NORMAL
- en: What is neat about a drive imagine (this could be a QCOW, VHD, ISO, or other)
    is that they sit on top of a filesystem but, when opened by a special driver that
    is able to read them, they present a block interface again! That is right. We
    have gone from block to block to block to block to filesystem to block again!
    In some unique cases we might not even use a hypervisor but might use this new
    block device file somewhere in our regular server. Windows does this commonly
    with VHD files as a way to pass data around in certain circumstances. MacOS does
    this as their standard means of creating installer packages. On Linux it is far
    less common but just as available.
  prefs: []
  type: TYPE_NORMAL
- en: But assuming that we are doing something normal, we will assume that we are
    running a hypervisor, KVM almost certainly, and that the virtual machines that
    are going to run on KVM are going to use disk image files storage on our newly
    minted file system. In this case, much of what we have done here is likely to
    happen yet again inside of that virtual machine.
  prefs: []
  type: TYPE_NORMAL
- en: Some portions would not be very sensible to recreate. The physical drives are
    already managed by a physical RAID controller. The speed, capacity, and reliability
    of the storage is already established by that system and does not need to be duplicated
    here. The standard approach is for a single drive image file to be presented to
    the operating system running in a virtual machine as a single block device. No
    different than how our operating system was presented with the block device from
    the hardware RAID controller.
  prefs: []
  type: TYPE_NORMAL
- en: Now inside of the virtual machine we will often run through the same exercise.
    We add the presented block device as an LVM physical volume. Then we add that
    physical volume to a volume group. Then we carve out one or more logical volumes
    from that volume group. We then format that logical volume with our filesystem
    of choice and mount it. Of course, typically much of that is not done by hand
    as we have done here but rather than the installation process automating much
    of it.
  prefs: []
  type: TYPE_NORMAL
- en: We can add more steps such as using MD RAID. Or we can use fewer, such as by
    skipping LVM entirely. We could do all of the same steps with just a single hard
    drive and no RAID controller. This would be far less powerful physically, but
    all of the examples would work the same at a technical level. We could use VLM
    on the physical machine but not in the virtual machines, or vice versa! The flexibility
    is there to do what is needed. It is all about understanding how block devices
    can be layered, how a filesystem goes on a block device and how block files can
    turn a filesystem back into block devices!
  prefs: []
  type: TYPE_NORMAL
- en: Abstraction and encapsulation are amazingly powerful tools in our IT arsenal
    and rarely are they so tangible.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have survived to the end of this chapter and are still hanging in with
    me, congrats, we made it! Storage is a big deal when it comes to systems administration
    and likely no other area that you manage will you be able to bring as much value
    to your organization.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered storage basics building on the concepts of block device interfaces,
    abstraction techniques, filesystems and their interfaces, and used these concepts
    to investigate multi-device redundancy and how it can be used to build complex
    and robust data storage, and how storage access across devices can be handled
    to meet any potential need. My goal here has been to give you the knowledge necessary
    to think carefully on your own about your storage needs for any given workload,
    and an understanding of availability technologies and how you can apply them to
    meet those goals most effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Never again should you see storage as a magic black box or a daunting task that
    you dread to tackle. Instead, you can see storage as an opportunity to shine and
    to demonstrate how proper system administration best practices can be applied
    to maximize whatever storage factors matter most for your workload without simply
    throwing money at the challenge or worse, simply ignoring it and hoping that you
    can find another job before things fall apart.
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we are going to look at system architecture at an even
    higher level. Many of the most interesting concepts from this chapter will be
    recurring there. System architecture relies on storage architecture very heavily
    and many redundancy and system protection paradigms are shared. It can be quite
    excited to see how good storage design elements can lead to a truly high performance,
    highly available, and cost-effective final solution.
  prefs: []
  type: TYPE_NORMAL
