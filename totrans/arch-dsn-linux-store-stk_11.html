<html><head></head><body>
<div id="_idContainer114">
<h1 class="chapter-number" id="_idParaDest-176"><a id="_idTextAnchor199"/><span class="koboSpan" id="kobo.1.1">11</span></h1>
<h1 id="_idParaDest-177"><a id="_idTextAnchor200"/><span class="koboSpan" id="kobo.2.1">Tuning the I/O Stack</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Well, here we are at the end of our journey. </span><span class="koboSpan" id="kobo.3.2">Just because you are reading the introduction of the final chapter does not mean that you’ve read through the entire book, but I’ll take my chances. </span><span class="koboSpan" id="kobo.3.3">If you’ve indeed followed us along, then I hope your journey was worth it and has left you yearning </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">for more.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Getting back to brass tacks, the previous two chapters centered on the performance analysis of the I/O stack. </span><a href="B19430_09.xhtml#_idTextAnchor160"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.7.1"> focused on the most common disk metrics and the tools that can help us to identify performance bottlenecks in physical disks. </span><span class="koboSpan" id="kobo.7.2">In any performance analysis, the physical disks come under far more scrutiny than any other layer, which can sometimes be misleading. </span><span class="koboSpan" id="kobo.7.3">Therefore, in </span><a href="B19430_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.8.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.9.1">, we saw how we can investigate the higher layers in the I/O stack, such as filesystems and the </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">block layer.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">This brings us to the next logical step in our quest. </span><span class="koboSpan" id="kobo.11.2">Once we’ve identified the elements plaguing our environment, what steps can we take to mitigate those limitations? </span><span class="koboSpan" id="kobo.11.3">It is important to have specific goals for the desired tuning outcomes because, at the end of the day, performance tuning is a trade-off between choices. </span><span class="koboSpan" id="kobo.11.4">For instance, tuning the system for low latency may reduce its overall throughput. </span><span class="koboSpan" id="kobo.11.5">A performance baseline should first be determined, and any tweaks or adjustments should be carried out in small sets. </span><span class="koboSpan" id="kobo.11.6">This chapter will deal with the different tweaks that can be applied to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">I/O performance.</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">Here’s an outline of </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">what follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.15.1">How memory usage </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">affects I/O</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Tuning the </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">memory subsystem</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Tuning </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">the filesystem</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">Choosing the </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">right scheduler</span></span></li>
</ul>
<h1 id="_idParaDest-178"><a id="_idTextAnchor201"/><span class="koboSpan" id="kobo.23.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.24.1">The material presented in this chapter builds on the concepts discussed in preceding chapters. </span><span class="koboSpan" id="kobo.24.2">If you’ve followed along and have become familiar with the functions of each layer in the disk I/O hierarchy, you’ll find this chapter much easier to follow. </span><span class="koboSpan" id="kobo.24.3">If you have a prior understanding of memory management in Linux, that will be a </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">huge plus.</span></span></p>
<p><span class="koboSpan" id="kobo.26.1">The commands and examples presented in this chapter are distribution-agnostic and can be run on any Linux operating system, such as Debian, Ubuntu, Red Hat, or Fedora. </span><span class="koboSpan" id="kobo.26.2">There are quite a few references to the kernel source code. </span><span class="koboSpan" id="kobo.26.3">If you want to download the kernel source, you can download it </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">from </span></span><a href="https://www.kernel.org"><span class="No-Break"><span class="koboSpan" id="kobo.28.1">https://www.kernel.org</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.29.1">.</span></span></p>
<h1 id="_idParaDest-179"><a id="_idTextAnchor202"/><span class="koboSpan" id="kobo.30.1">How memory usage affects I/O</span></h1>
<p><span class="koboSpan" id="kobo.31.1">As we’ve seen, VFS serves as an entry point for our I/O requests and includes different types of caches, the most important of which is the page cache. </span><span class="koboSpan" id="kobo.31.2">The purpose of page cache is to improve I/O performance and minimize the expense of I/O as generated by swapping and file system operations, thus avoiding unnecessary trips to the underlying physical disks. </span><span class="koboSpan" id="kobo.31.3">Although we haven’t explored it in these pages, it is important to have an idea about how the kernel goes about managing its memory management subsystem. </span><span class="koboSpan" id="kobo.31.4">The memory management subsystem is also referred to as the </span><strong class="bold"><span class="koboSpan" id="kobo.32.1">virtual memory manager</span></strong><span class="koboSpan" id="kobo.33.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.34.1">VMM</span></strong><span class="koboSpan" id="kobo.35.1">). </span><span class="koboSpan" id="kobo.35.2">Some of the responsibilities of the virtual memory manager include </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.37.1">Managing the allocation of physical memory for all the user space and kernel </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">space applications</span></span></li>
<li><span class="koboSpan" id="kobo.39.1">Implementation of virtual memory and </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">demand paging</span></span></li>
<li><span class="koboSpan" id="kobo.41.1">The mapping of files into processes </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">address space</span></span></li>
<li><span class="koboSpan" id="kobo.43.1">Freeing up memory in case of shortage, either by pruning or </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">swapping caches</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.45.1">As it’s often said, </span><em class="italic"><span class="koboSpan" id="kobo.46.1">the best I/O is the one that is avoided</span></em><span class="koboSpan" id="kobo.47.1">. </span><span class="koboSpan" id="kobo.47.2">The kernel follows this approach and makes generous allocation of the free memory, filling it up with the different types of caches. </span><span class="koboSpan" id="kobo.47.3">The greater the amount of free memory available, the more effective the caching mechanism. </span><span class="koboSpan" id="kobo.47.4">This all works well for general use cases, where applications perform small-scale requests and there is a relative amount of page </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">caches available:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer111">
<span class="koboSpan" id="kobo.49.1"><img alt="Figure 11.1 – A page cache can speed up I/O performance" src="image/B19430_11_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.50.1">Figure 11.1 – A page cache can speed up I/O performance</span></p>
<p><span class="koboSpan" id="kobo.51.1">Conversely, if memory is scarcely available, not only will the caches be pruned regularly but the data might also get swapped out to disk, which will ultimately hurt performance. </span><span class="koboSpan" id="kobo.51.2">The kernel works under the </span><strong class="bold"><span class="koboSpan" id="kobo.52.1">temporal locality principle</span></strong><span class="koboSpan" id="kobo.53.1">, meaning that the recently accessed blocks of data are more likely to be accessed again. </span><span class="koboSpan" id="kobo.53.2">This is generally good for most cases. </span><span class="koboSpan" id="kobo.53.3">It could take a few milliseconds to read data from a random part of the disk, whereas accessing that same data from memory if it is cached only takes a few nanoseconds. </span><span class="koboSpan" id="kobo.53.4">Therefore, any request that can be readily served from the page cache minimizes the cost of an </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">I/O operation.</span></span></p>
<h1 id="_idParaDest-180"><a id="_idTextAnchor203"/><span class="koboSpan" id="kobo.55.1">Tuning the memory subsystem</span></h1>
<p><span class="koboSpan" id="kobo.56.1">It’s a bit strange that how Linux deals with memory can have a major say in disk performance memory. </span><span class="koboSpan" id="kobo.56.2">As already explained, the default behavior of the kernel works well in most cases. </span><span class="koboSpan" id="kobo.56.3">However, as they say, an excess of everything is bad. </span><span class="koboSpan" id="kobo.56.4">Frequent caching can result in a few </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">problematic scenarios:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.58.1">When the kernel has accumulated a large amount of data in the page cache and eventually starts to flush that data onto disk, the disk will remain busy for quite some time because of the excessive write operations. </span><span class="koboSpan" id="kobo.58.2">This can adversely affect the overall I/O performance and increase disk </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">response times.</span></span></li>
<li><span class="koboSpan" id="kobo.60.1">The kernel does not have a sense of the criticality of the data in the page cache. </span><span class="koboSpan" id="kobo.60.2">Hence, it does not distinguish between </span><em class="italic"><span class="koboSpan" id="kobo.61.1">important</span></em><span class="koboSpan" id="kobo.62.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.63.1">unimportant</span></em><span class="koboSpan" id="kobo.64.1"> I/O. </span><span class="koboSpan" id="kobo.64.2">The kernel picks whichever block of data it deems appropriate and schedules it for a write or read operation. </span><span class="koboSpan" id="kobo.64.3">For instance, if an application performs both background and foreground I/O operations, then usually, the priority of the foreground operations should be higher. </span><span class="koboSpan" id="kobo.64.4">However, I/O belonging to background tasks can overwhelm </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">foreground tasks.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.66.1">The cache provided by the kernel usually enables applications to obtain better performance when reading and writing data, but the algorithms used by the page cache are not designed for a particular application; they are designed to be general-purpose. </span><span class="koboSpan" id="kobo.66.2">In most cases, this default behavior would work just fine, but in some cases, this can backfire. </span><span class="koboSpan" id="kobo.66.3">For some self-caching applications, such as database management systems, this approach might not offer the best results. </span><span class="koboSpan" id="kobo.66.4">Applications such as databases have a better understanding of the way data is organized internally. </span><span class="koboSpan" id="kobo.66.5">Hence, these systems prefer to have their own caching mechanism to improve read and </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">write performance.</span></span></p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor204"/><span class="koboSpan" id="kobo.68.1">Using direct I/O</span></h2>
<p><span class="koboSpan" id="kobo.69.1">If data is cached directly at the application level, then moving data from disk to the page cache and back to the application’s cache will constitute a significant overhead, resulting in more CPU and memory usage. </span><span class="koboSpan" id="kobo.69.2">In such scenarios, it might be desirable to bypass the kernel’s page cache altogether and leave the responsibility of caching to the application. </span><span class="koboSpan" id="kobo.69.3">This is known as </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.70.1">direct I/O</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.72.1">Using direct I/O, all the file reads and writes go directly from the application to the storage device, bypassing the kernel’s page cache. </span><span class="koboSpan" id="kobo.72.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.73.1">Unix filesystem</span></strong><span class="koboSpan" id="kobo.74.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.75.1">UFS</span></strong><span class="koboSpan" id="kobo.76.1">) filesystem (not supported in Linux) includes direct I/O as filesystem parameters, which can be specified while mounting the filesystem. </span><span class="koboSpan" id="kobo.76.2">In Linux, direct I/O is not a filesystem parameter, nor is there a command to enable it. </span><span class="koboSpan" id="kobo.76.3">Instead, it is the responsibility of an application to initiate direct I/O. </span><span class="koboSpan" id="kobo.76.4">The application can invoke direct I/O by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.77.1">O_DIRECT</span></strong><span class="koboSpan" id="kobo.78.1"> flag on a system call, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.79.1">open ()</span></strong><span class="koboSpan" id="kobo.80.1">. </span><span class="koboSpan" id="kobo.80.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.81.1">O_DIRECT</span></strong><span class="koboSpan" id="kobo.82.1"> flag is only a status flag (represented by DIR), which is passed by the application while opening or creating a file so that it can go around the page cache of </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">the kernel:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer112">
<span class="koboSpan" id="kobo.84.1"><img alt="Figure 11.2 – The different ways of performing I/O" src="image/B19430_11_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.85.1">Figure 11.2 – The different ways of performing I/O</span></p>
<p><span class="koboSpan" id="kobo.86.1">It doesn’t make sense to use direct I/O for regular applications, as it can cause performance deterioration. </span><span class="koboSpan" id="kobo.86.2">However, for self-caching applications, it can offer significant gains. </span><span class="koboSpan" id="kobo.86.3">The recommended method is to check the status of direct I/O via an application. </span><span class="koboSpan" id="kobo.86.4">However, if you want to check via the command line, use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.87.1">lsof</span></strong><span class="koboSpan" id="kobo.88.1"> command to check the flags through which a file </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">is opened.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer113">
<span class="koboSpan" id="kobo.90.1"><img alt="Figure 11.3 – Checking direct I/O" src="image/B19430_11_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.91.1">Figure 11.3 – Checking direct I/O</span></p>
<p><span class="koboSpan" id="kobo.92.1">For files opened by the application through the </span><strong class="source-inline"><span class="koboSpan" id="kobo.93.1">O_DIRECT</span></strong><span class="koboSpan" id="kobo.94.1"> flag, the </span><strong class="bold"><span class="koboSpan" id="kobo.95.1">FILE-FLAG</span></strong><span class="koboSpan" id="kobo.96.1"> column</span><a id="_idIndexMarker266"/><span class="koboSpan" id="kobo.97.1"> of the output will include the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.98.1">DIR</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.99.1"> flag.</span></span></p>
<p><span class="koboSpan" id="kobo.100.1">The performance gains from direct I/O come from avoiding the CPU cost of copying data from disk into the page cache, and from steering clear of the double buffering, once in the application and once in </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">the filesystem.</span></span></p>
<h2 id="_idParaDest-182"><a id="_idTextAnchor205"/><span class="koboSpan" id="kobo.102.1">Controlling the write-back frequency</span></h2>
<p><span class="koboSpan" id="kobo.103.1">As already explained, caching </span><a id="_idIndexMarker267"/><span class="koboSpan" id="kobo.104.1"> has its advantages, as it accelerates many accesses to files. </span><span class="koboSpan" id="kobo.104.2">Once most of the free memory has been occupied by the cache, the kernel has to make a decision on how to free memory in order to entertain incoming I/O operations. </span><span class="koboSpan" id="kobo.104.3">Using the </span><strong class="bold"><span class="koboSpan" id="kobo.105.1">Least Recently Used</span></strong><span class="koboSpan" id="kobo.106.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.107.1">LRU</span></strong><span class="koboSpan" id="kobo.108.1">) approach, the</span><a id="_idIndexMarker268"/><span class="koboSpan" id="kobo.109.1"> kernel does two things – it evicts old data from the page cache and even offloads some of it to the swap area, in order to make room for </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">incoming requests.</span></span></p>
<p><span class="koboSpan" id="kobo.111.1">Again, it all comes down to the specifics. </span><span class="koboSpan" id="kobo.111.2">The default approach is good enough, and that is exactly how the kernel should go about making room for incoming data. </span><span class="koboSpan" id="kobo.111.3">However, consider the </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">following scenarios:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.113.1">What if the data currently in the cache won’t be accessed again in the future? </span><span class="koboSpan" id="kobo.113.2">This is true for most backup operations. </span><span class="koboSpan" id="kobo.113.3">A backup operation will read and write a lot of data from the disk, which will be cached by the kernel. </span><span class="koboSpan" id="kobo.113.4">However, it is unlikely that this data, which is present in the cache, will be accessed in the near future. </span><span class="koboSpan" id="kobo.113.5">However, the kernel will keep this data in the cache and might evict the older pages, which had a greater probability of being </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">accessed again.</span></span></li>
<li><span class="koboSpan" id="kobo.115.1">Swapping data to disk will generate a lot of disk I/O, which won’t be good </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">for performance.</span></span></li>
<li><span class="koboSpan" id="kobo.117.1">When a large amount of data has been cached, a system crash can result in a major loss of data. </span><span class="koboSpan" id="kobo.117.2">This can be a significant concern if data is of a </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">sensitive nature.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.119.1">It’s not </span><a id="_idIndexMarker269"/><span class="koboSpan" id="kobo.120.1">possible to disable the page cache. </span><span class="koboSpan" id="kobo.120.2">Even if there was, it’s not something that should be done. </span><span class="koboSpan" id="kobo.120.3">There are, however, a number of parameters that can be tweaked to control its behavior. </span><span class="koboSpan" id="kobo.120.4">As shown here, there are several parameters that can be controlled through the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.121.1">sysctl</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.122.1"> interface:</span></span></p>
<pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.123.1">[root@linuxbox ~]# sysctl -a | grep dirty</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.124.1">vm.dirty_background_ratio = 10</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.125.1">vm.dirty_background_bytes = 0</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.126.1">vm.dirty_ratio = 20</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.127.1">vm.dirty_bytes = 0</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.128.1">vm.dirty_writeback_centisecs = 500</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.129.1">vm.dirty_expire_centisecs = 3000</span></strong></pre>
<p><span class="koboSpan" id="kobo.130.1">Let us look at them </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">in detail:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.132.1">vm.dirty_background_ratio</span></strong><span class="koboSpan" id="kobo.133.1">: The write-back flusher threads initiate the flushing of dirty pages to disk when the percentage of dirty pages in the cache surpasses a certain threshold. </span><span class="koboSpan" id="kobo.133.2">Prior to this threshold, no pages are written to the disk. </span><span class="koboSpan" id="kobo.133.3">Once flushing begins, it occurs in the background without causing any disturbance to the </span><span class="No-Break"><span class="koboSpan" id="kobo.134.1">foreground processes.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.135.1">vm.dirty_ratio</span></strong><span class="koboSpan" id="kobo.136.1">: This refers to the threshold of system memory utilization, beyond which the writing process gets blocked and dirty pages are written out to </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">the disk.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.138.1">For large memory systems, hundreds of GB of data can be flushed from the page cache to disk, which will cause noticeable delays and adversely affect not only the disk’s performance but also overall system performance. </span><span class="koboSpan" id="kobo.138.2">In such cases, lowering these values might be helpful, as data will be flushed to disk on a regular basis, avoiding the </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">write storm.</span></span></p>
<p><span class="koboSpan" id="kobo.140.1">You can check the current values of these parameters using </span><strong class="source-inline"><span class="koboSpan" id="kobo.141.1">sysctl</span></strong><span class="koboSpan" id="kobo.142.1"> – for instance, if the values are </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.144.1">
vm.dirty_background_ratio=10
vm.dirty_ratio=20</span></pre>
<p><span class="koboSpan" id="kobo.145.1">Think of </span><strong class="source-inline"><span class="koboSpan" id="kobo.146.1">vm.dirty_ratio</span></strong><span class="koboSpan" id="kobo.147.1"> as the upper limit. </span><span class="koboSpan" id="kobo.147.2">Using these previously mentioned values </span><a id="_idIndexMarker270"/><span class="koboSpan" id="kobo.148.1">means that when the percentage of dirty pages in the cache reaches 10%, the background threads are triggered to write them to the disk. </span><span class="koboSpan" id="kobo.148.2">However, when the total number of dirty pages in the cache exceeds 20%, all writes are blocked until a portion of the dirty pages are written to the disk. </span><span class="koboSpan" id="kobo.148.3">These two parameters have the following </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">two counterparts:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.150.1">vm.dirty_background_bytes</span></strong><span class="koboSpan" id="kobo.151.1">: This denotes the amount of dirty memory, expressed in bytes, that triggers the background flusher threads to initiate writing back to the disk. </span><span class="koboSpan" id="kobo.151.2">This is the counterpart of </span><strong class="source-inline"><span class="koboSpan" id="kobo.152.1">vm.dirty_background_ratio</span></strong><span class="koboSpan" id="kobo.153.1">, and only one of them can be configured. </span><span class="koboSpan" id="kobo.153.2">The value can be defined either as a percentage or a precise number </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">of bytes.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.155.1">vm.dirty_bytes</span></strong><span class="koboSpan" id="kobo.156.1">: This is the amount of dirty memory, expressed in bytes, that results in the writing process getting blocked and writing out the dirty pages to the disk. </span><span class="koboSpan" id="kobo.156.2">This controls the same tunable as </span><strong class="source-inline"><span class="koboSpan" id="kobo.157.1">vm.dirty_ratio</span></strong><span class="koboSpan" id="kobo.158.1">, and only one of them can </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">be set.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.160.1">vm.dirty_expire_centisecs</span></strong><span class="koboSpan" id="kobo.161.1">: This indicates how long something can be in the cache before it needs to be written. </span><span class="koboSpan" id="kobo.161.2">This tunable specifies the age at which the dirty data is deemed suitable for write-back by the flusher threads. </span><span class="koboSpan" id="kobo.161.3">The time duration is measured in hundredths of </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">a second.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.163.1">To summarize, the default behavior of the kernel’s page cache works well most of the time, and usually, it won’t require any tweaking. </span><span class="koboSpan" id="kobo.163.2">However, for intelligent applications such as large-scale databases, the frequent caching of data can become a hurdle. </span><span class="koboSpan" id="kobo.163.3">Fortunately, there are a few workarounds available. </span><span class="koboSpan" id="kobo.163.4">Such applications can be configured to use direct I/O, which will bypass the page cache. </span><span class="koboSpan" id="kobo.163.5">The kernel also offers several parameters that can be used to tweak the behavior of the page cache. </span><span class="koboSpan" id="kobo.163.6">However, it is important to note that changing these values can result in increased I/O traffic. </span><span class="koboSpan" id="kobo.163.7">Therefore, workload-specific testing should be conducted prior to </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">making changes.</span></span></p>
<h1 id="_idParaDest-183"><a id="_idTextAnchor206"/><span class="koboSpan" id="kobo.165.1">Tuning the filesystem</span></h1>
<p><span class="koboSpan" id="kobo.166.1">As we</span><a id="_idIndexMarker271"/><span class="koboSpan" id="kobo.167.1"> focus on tuning the different components that can impact I/O performance, we’ll try to steer our conversation away from the hardware side of things. </span><span class="koboSpan" id="kobo.167.2">Given the advancements in hardware, upgrading the memory, compute, network, and storage apparatus is bound to add at least some level of performance gains. </span><span class="koboSpan" id="kobo.167.3">However, most of the time, the magnitude of those gains will be limited. </span><span class="koboSpan" id="kobo.167.4">You need a well-designed and configured software stack to take advantage of that hardware. </span><span class="koboSpan" id="kobo.167.5">As we’re not focusing on a particular type of application, we’ll try to present some general tweaks that can be used to fine-tune your I/O. </span><span class="koboSpan" id="kobo.167.6">Again, note that the parameters that will be presented here or were discussed earlier require thorough testing and may not offer the same results in </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">different environments.</span></span></p>
<p><span class="koboSpan" id="kobo.169.1">Coming back to the topic of our discussion, filesystems are responsible for organizing data on disk and are the point of contact for an application to perform I/O. </span><span class="koboSpan" id="kobo.169.2">This makes them an ideal candidate for the tuning and troubleshooting process. </span><span class="koboSpan" id="kobo.169.3">Some applications explicitly mention the filesystem that should be used for optimal performance. </span><span class="koboSpan" id="kobo.169.4">As Linux supports different flavors of filesystems that use different techniques to store user data, some mount options might not be common </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">among filesystems.</span></span></p>
<h2 id="_idParaDest-184"><a id="_idTextAnchor207"/><span class="koboSpan" id="kobo.171.1">Block size</span></h2>
<p><span class="koboSpan" id="kobo.172.1">Filesystems </span><a id="_idIndexMarker272"/><span class="koboSpan" id="kobo.173.1">address physical storage in terms of blocks. </span><span class="koboSpan" id="kobo.173.2">A </span><strong class="bold"><span class="koboSpan" id="kobo.174.1">block</span></strong><span class="koboSpan" id="kobo.175.1"> is a </span><a id="_idIndexMarker273"/><span class="koboSpan" id="kobo.176.1">group of physical sectors and is the fundamental unit of I/O for a filesystem. </span><span class="koboSpan" id="kobo.176.2">Each file in the filesystem will occupy at least one block, even if the file contains nothing. </span><span class="koboSpan" id="kobo.176.3">By default, a block size of 4 KB is used for most filesystems. </span><span class="koboSpan" id="kobo.176.4">If the application mostly creates a large number of small-sized files in the filesystem, typically of a few bytes or less than a couple of KB, then it is best to use smaller block sizes than the default value of </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">4 KB.</span></span></p>
<p><span class="koboSpan" id="kobo.178.1">Filesystems perform better if applications use the same read and write size as the block size, or use a size that is a multiple of the block size. </span><span class="koboSpan" id="kobo.178.2">The block size for a filesystem can only be specified during its creation and cannot be changed afterward. </span><span class="koboSpan" id="kobo.178.3">Therefore, the block size needs to be decided before creating </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">the filesystem.</span></span></p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor208"/><span class="koboSpan" id="kobo.180.1">Filesystem I/O alignment</span></h2>
<p><span class="koboSpan" id="kobo.181.1">The </span><a id="_idIndexMarker274"/><span class="koboSpan" id="kobo.182.1">concept of I/O alignment is generally overlooked, but this can have a huge impact on the filesystem performance. </span><span class="koboSpan" id="kobo.182.2">This is especially true for the complex enterprise storage systems of today, which consist of flash drives that have different page sizes and some form of RAID configuration on top </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">of them.</span></span></p>
<p><span class="koboSpan" id="kobo.184.1">The I/O alignment for filesystems is concerned with how data is distributed and organized across the filesystem. </span><span class="koboSpan" id="kobo.184.2">That’s one side of the coin. </span><span class="koboSpan" id="kobo.184.3">If the underlying physical storage consists of a striped RAID configuration, the data should be aligned with the underlying storage geometry for optimal performance. </span><span class="koboSpan" id="kobo.184.4">For instance, for a RAID device with a 64 K per-disk stripe size and 10 data-bearing disks, the filesystem should be created </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.186.1">For XFS, it should be </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">the following:</span></span></li>
</ul>
<pre class="source-code"><span class="koboSpan" id="kobo.188.1">
mkfs.xfs -f -d su=64k,sw=10  /dev/sdX</span></pre>
<p><span class="koboSpan" id="kobo.189.1">XFS provides two sets of tunables in this regard. </span><span class="koboSpan" id="kobo.189.2">Depending on the specification units that you’ve set, you can </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">use these:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.191.1">Sunit</span></strong><span class="koboSpan" id="kobo.192.1">: A stripe unit, in </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">512-byte blocks</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.194.1">swidth</span></strong><span class="koboSpan" id="kobo.195.1">: A stripe width, in </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">512-byte blocks</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.197.1">Alternatively, you can </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">use these:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.199.1">su</span></strong><span class="koboSpan" id="kobo.200.1">: A per-disk stripe unit, in K if suffixed </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">with </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.202.1">k</span></em></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.203.1">sw</span></strong><span class="koboSpan" id="kobo.204.1">: A stripe width, by the number of </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">data disks</span></span></li>
</ul>
<ul>
<li><span class="koboSpan" id="kobo.206.1">For Ext4, the command would look </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">as follows:</span></span></li>
</ul>
<pre class="source-code"><span class="koboSpan" id="kobo.208.1">
mkfs.ext4 -E stride=16,stripe-width=160 /dev/sdX</span></pre>
<p><span class="koboSpan" id="kobo.209.1">Ext4 also provides a couple of tunables, which can be used </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">as follows:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.211.1">stride</span></strong><span class="koboSpan" id="kobo.212.1">: The number of filesystem blocks on each data-bearing disk in </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">that stripe</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.214.1">stripe-width</span></strong><span class="koboSpan" id="kobo.215.1">: The total stripe-width in filesystem blocks, equal to (stride) x (the number of </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">data-bearing disks)</span></span></li>
</ul>
<h2 id="_idParaDest-186"><a id="_idTextAnchor209"/><span class="koboSpan" id="kobo.217.1">LVM I/O alignment</span></h2>
<p><span class="koboSpan" id="kobo.218.1">Every abstraction layer</span><a id="_idIndexMarker275"/><span class="koboSpan" id="kobo.219.1"> created on top of a RAID device must be aligned to a multiple of </span><strong class="source-inline"><span class="koboSpan" id="kobo.220.1">Stripe Width</span></strong><span class="koboSpan" id="kobo.221.1">, plus any required initial alignment offset. </span><span class="koboSpan" id="kobo.221.2">This ensures that a read or write request of a single block at the filesystem will not span the RAID stripe boundaries and cause multiple stripes to be read and written at the disk level, adversely </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">affecting performance.</span></span></p>
<p><span class="koboSpan" id="kobo.223.1">The first physical extent allocated within the physical volume should be aligned to a multiple of the RAID </span><strong class="source-inline"><span class="koboSpan" id="kobo.224.1">Stripe Width</span></strong><span class="koboSpan" id="kobo.225.1">. </span><span class="koboSpan" id="kobo.225.2">If the physical volume is created directly on a raw disk, then it should also be offset by any required initial alignment offset. </span><span class="koboSpan" id="kobo.225.3">To check where the physical extents start, use </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.227.1">
pvs -o +pe_start</span></pre>
<p><span class="koboSpan" id="kobo.228.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.229.1">pe_start</span></strong><span class="koboSpan" id="kobo.230.1"> refers to the first </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">physical extent.</span></span></p>
<p><span class="koboSpan" id="kobo.232.1">The logical volumes are always allocated a contiguous range of physical extents when possible. </span><span class="koboSpan" id="kobo.232.2">If a contiguous range doesn’t exist, non-contiguous ranges might be allocated. </span><span class="koboSpan" id="kobo.232.3">Since a non-contiguous range of extents can impact performance, there is an option (</span><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">--contiguous</span></strong><span class="koboSpan" id="kobo.234.1">) while creating a logical volume to prevent the non-contiguous allocation </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">of extents.</span></span></p>
<h2 id="_idParaDest-187"><a id="_idTextAnchor210"/><span class="koboSpan" id="kobo.236.1">Journaling</span></h2>
<p><span class="koboSpan" id="kobo.237.1">As</span><a id="_idIndexMarker276"/><span class="koboSpan" id="kobo.238.1"> explained in </span><a href="B19430_03.xhtml#_idTextAnchor053"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.239.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.240.1">, the concept of journaling guarantees data consistency and integrity if I/O operations on a filesystem fail due to external events. </span><span class="koboSpan" id="kobo.240.2">Any changes that need to be performed on the filesystem are first written to a journal. </span><span class="koboSpan" id="kobo.240.3">Once data has been written to a journal, it is then written to the appropriate location on the disk. </span><span class="koboSpan" id="kobo.240.4">If there is a system crash, the filesystem replays the journal to see if any operation was left in an incomplete state. </span><span class="koboSpan" id="kobo.240.5">This reduces the likelihood that the filesystem will become corrupted if there are any </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">hardware failures.</span></span></p>
<p><span class="koboSpan" id="kobo.242.1">Apparently, the journaling approach adds extra overhead and can potentially affect filesystem performance. </span><span class="koboSpan" id="kobo.242.2">However, given the sequential nature of journal writes, the filesystem performance is not affected. </span><span class="koboSpan" id="kobo.242.3">So, it is recommended to keep the filesystem journal enabled to ensure </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">data integrity.</span></span></p>
<p><span class="koboSpan" id="kobo.244.1">It is, however, recommended to change the mode of the filesystem journal to suit your needs. </span><span class="koboSpan" id="kobo.244.2">Most filesystems don’t have multiple journaling modes but Ext4 offers a great deal of flexibility in this regard. </span><span class="koboSpan" id="kobo.244.3">The Ext4 offers three journaling modes. </span><span class="koboSpan" id="kobo.244.4">Among them, the </span><strong class="bold"><span class="koboSpan" id="kobo.245.1">write-back mode</span></strong><span class="koboSpan" id="kobo.246.1"> offers </span><a id="_idIndexMarker277"/><span class="koboSpan" id="kobo.247.1">considerably better performance than the ordered and data mode. </span><span class="koboSpan" id="kobo.247.2">The write-back mode only journals the metadata and does not follow any order when writing the data and metadata to disk. </span><span class="koboSpan" id="kobo.247.3">The </span><strong class="bold"><span class="koboSpan" id="kobo.248.1">ordered mode</span></strong><span class="koboSpan" id="kobo.249.1"> on </span><a id="_idIndexMarker278"/><span class="koboSpan" id="kobo.250.1">the other hand follows a strict order and first writes the actual data before the metadata. </span><span class="koboSpan" id="kobo.250.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.251.1">data mode</span></strong><span class="koboSpan" id="kobo.252.1"> offers</span><a id="_idIndexMarker279"/><span class="koboSpan" id="kobo.253.1"> the lowest performance, as it has to write both the data and metadata to a journal, resulting in twice the number </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">of operations.</span></span></p>
<p><span class="koboSpan" id="kobo.255.1">Another thing that</span><a id="_idIndexMarker280"/><span class="koboSpan" id="kobo.256.1"> can be done to improve journaling is to use an external journal. </span><span class="koboSpan" id="kobo.256.2">The default location for a filesystem journal is on the same block device as the data. </span><span class="koboSpan" id="kobo.256.3">If the I/O workload is metadata-intensive and the synchronous metadata writes to the journal must complete successfully before any associated data writes can start, this can result in I/O contention and may impact performance. </span><span class="koboSpan" id="kobo.256.4">In such cases, it can be a good idea to use an external device for filesystem journaling. </span><span class="koboSpan" id="kobo.256.5">The journal size is typically very small and requires very little storage space. </span><span class="koboSpan" id="kobo.256.6">The external journal should ideally be placed on fast physical media with a battery-backed </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">write-back cache.</span></span></p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor211"/><span class="koboSpan" id="kobo.258.1">Barriers</span></h2>
<p><span class="koboSpan" id="kobo.259.1">As </span><a id="_idIndexMarker281"/><span class="koboSpan" id="kobo.260.1">mentioned earlier, most filesystems make use of journaling to keep track of changes that have not yet been written to disk. </span><span class="koboSpan" id="kobo.260.2">A </span><strong class="bold"><span class="koboSpan" id="kobo.261.1">write barrier</span></strong><span class="koboSpan" id="kobo.262.1"> is a </span><a id="_idIndexMarker282"/><span class="koboSpan" id="kobo.263.1">kernel mechanism that guarantees the proper ordering and accurate writing of filesystem metadata onto persistent storage, even if storage devices with unstable write caches lose power. </span><span class="koboSpan" id="kobo.263.2">Write barriers enforce proper on-disk ordering of journal commits by forcing the storage device to flush its cache at certain intervals. </span><span class="koboSpan" id="kobo.263.3">This makes volatile write caches safe to use, but it can incur some performance deficit. </span><span class="koboSpan" id="kobo.263.4">If the storage device cache is battery-backed, disabling filesystem barriers may offer some </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">performance improvement.</span></span></p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor212"/><span class="koboSpan" id="kobo.265.1">Timestamps</span></h2>
<p><span class="koboSpan" id="kobo.266.1">The </span><a id="_idIndexMarker283"/><span class="koboSpan" id="kobo.267.1">kernel records information about when files were created (</span><strong class="source-inline"><span class="koboSpan" id="kobo.268.1">ctime</span></strong><span class="koboSpan" id="kobo.269.1">) and last modified (</span><strong class="source-inline"><span class="koboSpan" id="kobo.270.1">mtime</span></strong><span class="koboSpan" id="kobo.271.1">) ,as well as when they were last accessed (</span><strong class="source-inline"><span class="koboSpan" id="kobo.272.1">atime</span></strong><span class="koboSpan" id="kobo.273.1">). </span><span class="koboSpan" id="kobo.273.2">If an application frequently modifies a bunch of files, then their corresponding timestamps will need to be updated every time. </span><span class="koboSpan" id="kobo.273.3">Performing these modifications also requires I/O operations, and when there are too many of them, there is a cost associated </span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">with them.</span></span></p>
<p><span class="koboSpan" id="kobo.275.1">To mitigate </span><a id="_idIndexMarker284"/><span class="koboSpan" id="kobo.276.1">this, there is a special mount option for filesystems called </span><strong class="source-inline"><span class="koboSpan" id="kobo.277.1">noatime</span></strong><span class="koboSpan" id="kobo.278.1">. </span><span class="koboSpan" id="kobo.278.2">When a filesystem is mounted with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.279.1">noatime</span></strong><span class="koboSpan" id="kobo.280.1"> option, reading from the filesystem will not update the file’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.281.1">atime</span></strong><span class="koboSpan" id="kobo.282.1"> information. </span><span class="koboSpan" id="kobo.282.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.283.1">noatime</span></strong><span class="koboSpan" id="kobo.284.1"> setting is significant, as it removes the requirement for a system to perform writes to the filesystem for files that are only read. </span><span class="koboSpan" id="kobo.284.2">This can lead to noticeable performance improvements, since write operations can </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">be expensive.</span></span></p>
<h2 id="_idParaDest-190"><a id="_idTextAnchor213"/><span class="koboSpan" id="kobo.286.1">Read-ahead</span></h2>
<p><span class="koboSpan" id="kobo.287.1">The</span><a id="_idIndexMarker285"/><span class="koboSpan" id="kobo.288.1"> read-ahead functionality in filesystems can enhance file access performance by proactively fetching data that is expected to be required soon and storing it in the page cache, which allows for faster access compared to retrieving the data from the disk. </span><span class="koboSpan" id="kobo.288.2">A higher read-ahead value indicates that the system will prefetch data further ahead of the current read position. </span><span class="koboSpan" id="kobo.288.3">This is especially true for </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">sequential workloads.</span></span></p>
<h2 id="_idParaDest-191"><a id="_idTextAnchor214"/><span class="koboSpan" id="kobo.290.1">Discarding unused blocks</span></h2>
<p><span class="koboSpan" id="kobo.291.1">As we </span><a id="_idIndexMarker286"/><span class="koboSpan" id="kobo.292.1">explained in </span><a href="B19430_08.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.293.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.294.1">, in SSDs, a write operation can be done at the page level, but the erase operation always affects entire blocks. </span><span class="koboSpan" id="kobo.294.2">As a result, writing data to SSDs is very fast as long as empty pages can be used. </span><span class="koboSpan" id="kobo.294.3">However, once previously written pages need to be overwritten, the writes slow down considerably, impacting performance. </span><span class="koboSpan" id="kobo.294.4">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.295.1">trim</span></strong><span class="koboSpan" id="kobo.296.1"> command tells the SSD to discard the blocks that are no longer needed and can be deleted. </span><span class="koboSpan" id="kobo.296.2">The filesystem is the only component in the I/O stack that knows the parts of the SSD that should be trimmed. </span><span class="koboSpan" id="kobo.296.3">Most filesystems offer mount parameters that implement </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">this feature.</span></span></p>
<p><span class="koboSpan" id="kobo.298.1">To summarize, filesystems, to a certain extent, map logical addresses to physical addresses. </span><span class="koboSpan" id="kobo.298.2">When an application writes data, the filesystem decides how to distribute writes properly in order to make the best use of the underlying physical storage. </span><span class="koboSpan" id="kobo.298.3">This makes filesystems a very important layer when it comes to performance tuning. </span><span class="koboSpan" id="kobo.298.4">Most of the changes in filesystems cannot be done on the fly; they’re either performed during filesystem creation or require unmounting and remounting the filesystem. </span><span class="koboSpan" id="kobo.298.5">So, the decision regarding the choice of filesystem parameters should be made in advance, as changing things afterward can be a </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">disruptive activity.</span></span></p>
<h1 id="_idParaDest-192"><a id="_idTextAnchor215"/><span class="koboSpan" id="kobo.300.1">Choosing the right scheduler</span></h1>
<p><span class="koboSpan" id="kobo.301.1">The</span><a id="_idIndexMarker287"/><span class="koboSpan" id="kobo.302.1"> sole purpose of I/O schedulers is to optimize disk access requests. </span><span class="koboSpan" id="kobo.302.2">There are some common techniques used by schedulers, such as merging I/O requests that are adjacent on disk. </span><span class="koboSpan" id="kobo.302.3">The idea is to avoid frequent trips to the physical storage. </span><span class="koboSpan" id="kobo.302.4">Aggregating requests that are situated in close proximity on the disk reduces the frequency of the drive’s seeking operations, thereby enhancing the overall response time of disk operations. </span><span class="koboSpan" id="kobo.302.5">I/O schedulers aim to optimize throughput by rearranging access requests into sequential order. </span><span class="koboSpan" id="kobo.302.6">However, this strategy may cause some I/O requests to wait for an extended time, resulting in latency problems in certain situations. </span><span class="koboSpan" id="kobo.302.7">I/O schedulers strive to achieve a balance between maximizing throughput and distributing I/O requests equitably among all processes. </span><span class="koboSpan" id="kobo.302.8">As with all other things, Linux has a variety of I/</span><a id="_idTextAnchor216"/><span class="koboSpan" id="kobo.303.1">O schedulers available. </span><span class="koboSpan" id="kobo.303.2">Each has its own set </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">of strengths:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-9">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.305.1">Use case</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.306.1">Recommended </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.307.1">I/O scheduler</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.308.1">Desktop GUI, interactive applications, and soft real-time applications, such as audio and </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">video players</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.310.1">Budget Fair Queuing</span></strong><span class="koboSpan" id="kobo.311.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.312.1">BFQ</span></strong><span class="koboSpan" id="kobo.313.1">), as it guarantees good system responsiveness and a low latency for </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">time-sensitive applications</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.315.1">Traditional </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">mechanical drives</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.317.1">BFQ or </span><strong class="bold"><span class="koboSpan" id="kobo.318.1">Multiqueue</span></strong><span class="koboSpan" id="kobo.319.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.320.1">MQ</span></strong><span class="koboSpan" id="kobo.321.1">)-Deadline – both are considered suitable for slower drives. </span><span class="koboSpan" id="kobo.321.2">Kyber/none are biased in favor of </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">faster disks.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.323.1">High-performing SSDs and NVMe drives as </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">local storage</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.325.1">Preferable none, but Kyber might also be a good alternative in </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">some cases</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.327.1">Enterprise </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">storage arrays</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.329.1">None, as most storage arrays have built-in logic to schedule I/Os </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">more efficiently</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.331.1">Virtualized environments</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.332.1">MQ-Deadline is a good option. </span><span class="koboSpan" id="kobo.332.2">If the hypervisor layer does its own I/O scheduling, then using the none scheduler might </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">be beneficial,</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.334.1">Table 11.1 – Some use cases for I/O schedulers</span></p>
<p><span class="koboSpan" id="kobo.335.1">The </span><a id="_idIndexMarker288"/><span class="koboSpan" id="kobo.336.1">good thing is that an I/O scheduler can be changed on the fly. </span><span class="koboSpan" id="kobo.336.2">It is also possible to use a different I/O scheduler for every storage device on the system. </span><span class="koboSpan" id="kobo.336.3">A good starting point to select or fine-tune an I/O scheduler is to determine the system’s purpose or role. </span><span class="koboSpan" id="kobo.336.4">It is generally accepted that there is no single I/O scheduler that can meet all of a system’s diverse </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">I/O demands.</span></span></p>
<h1 id="_idParaDest-193"><a id="_idTextAnchor217"/><span class="koboSpan" id="kobo.338.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.339.1">After having spent two chapters trying to diagnose and analyze the performance of different layers in the I/O stack, this chapter focused on the performance-tuning aspect of the I/O stack. </span><span class="koboSpan" id="kobo.339.2">Throughout this book, we’ve familiarized ourselves with the multi-level hierarchy of the I/O stack and built an understanding of the components that can impact the overall </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">I/O performance.</span></span></p>
<p><span class="koboSpan" id="kobo.341.1">We started this chapter by briefly going through the functions of the memory subsystem and how it can impact the I/O performance of a system. </span><span class="koboSpan" id="kobo.341.2">As all write operations are, by default, first performed in the page cache, the way the page cache is configured to behave can have a major say in an application’s I/O performance. </span><span class="koboSpan" id="kobo.341.3">We also explained the concept of direct I/O and defined some of the different parameters that can be used to tweak the </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">write-back cache.</span></span></p>
<p><span class="koboSpan" id="kobo.343.1">We also looked at the different tuning options when it comes to filesystems. </span><span class="koboSpan" id="kobo.343.2">Filesystems offer different mount options that can be changed to reduce some I/O overhead. </span><span class="koboSpan" id="kobo.343.3">Additionally, the filesystem block size, its geometry, and I/O alignment in terms of the underlying RAID configuration can also impact performance. </span><span class="koboSpan" id="kobo.343.4">Finally, we explained some use cases of the different scheduling flavors </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">in Linux.</span></span></p>
<p><span class="koboSpan" id="kobo.345.1">I guess that’s a wrap! </span><span class="koboSpan" id="kobo.345.2">I sincerely hope that this book took you on an enlightening exploration of the intricate layers comprising the Linux kernel’s storage stack. </span><span class="koboSpan" id="kobo.345.3">Starting with an introduction to VFS in </span><a href="B19430_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.346.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.347.1">, we tried to navigate the complex terrain of storage architecture. </span><span class="koboSpan" id="kobo.347.2">Each chapter delves deeper into the intricacies of the Linux storage stack, exploring topics such as VFS data structures, filesystems, the role of the block layer, multi-queue and device-mapper frameworks, I/O scheduling, the SCSI subsystem, physical storage hardware, and its performance tuning and analysis. </span><span class="koboSpan" id="kobo.347.3">Our goal was to prioritize the conceptual side of things and examine the flow of disk I/O activity, which is why we’ve not dived too much into general storage </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">administration tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.349.1">As we bring our adventure to a close, I hope you’ve gained a comprehensive understanding of the Linux storage stack, its major components, and their interactions, and now possess the necessary knowledge and skills to make informed decisions, analyze, troubleshoot, and optimize storage performance in </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">Linux environments.</span></span></p>
</div>
</body></html>