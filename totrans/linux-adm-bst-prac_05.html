<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer012">
			<h1 id="_idParaDest-61"><em class="italic"><a id="_idTextAnchor061"/>Chapter 3</em>: System Storage Best Practices</h1>
			<p>Probably the most complicated and least understood components of <strong class="bold">System Administration</strong> involve the area of <strong class="bold">storage</strong>. Storage tends to be poorly covered, rarely taught, and often treated as myth rather than science. Storage also involves the most fear because it is in storage that our mistakes risk losing data, and nothing tends to be a bigger failure than data loss. </p>
			<p>Storage decisions impact performance, capacity, longevity, and most importantly, <em class="italic">durability</em>. Storage is where we have the smallest margin of error as well as where we can make the biggest impact. In other areas of planning and design we often get the benefit of quite a bit of <em class="italic">fudge factor</em>, mistakes are often graceful such as a system that is not quite as fast as it needs to be or is somewhat more costly than necessary, but in storage overbuilding might double total costs and mistakes will quite easily result in non-functional systems. Failure tends to be anything but graceful.</p>
			<p>We are going to address how we look at and understand storage in Linux systems and demystify storage so that you can approach it methodically and empirically. By the end of this chapter, you should be prepared to decide on the best storage products and designs for your workload taking into account all of the needs. </p>
			<p>In this chapter we will look at the following key topics:</p>
			<ul>
				<li>Exploring key factors in storage</li>
				<li>Understanding block storage: Local and SAN</li>
				<li>Surveying filesystems and network filesystems</li>
				<li>Getting to know <strong class="bold">logical volume management</strong> (<strong class="bold">LVM</strong>)</li>
				<li>Utilizing RAID and RAIN</li>
				<li>Learning about replicated local storage</li>
				<li>Analyzing storage architectures and risk</li>
			</ul>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor062"/>Exploring key factors in storage</h1>
			<p>When thinking <a id="_idIndexMarker179"/>about storage for systems administration we are concerned with <strong class="bold">cost</strong>, <strong class="bold">durability</strong>, <strong class="bold">availability</strong>, <strong class="bold">performance</strong>, <strong class="bold">scalability</strong>, <strong class="bold">accessibility</strong>, and <strong class="bold">capacity</strong>. It is easy to get overwhelmed with so many moving parts when it comes to storage and that makes it a risk that we may lose track of what it is that we want to accomplish. In every storage decision, we need to remain focused on these factors. Most importantly, on all of these factors. It is extremely tempting to focus on just a few causing us to lose our grasp of the complete picture.</p>
			<p>In most cases, if you study postmortems of storage systems that have failed to meet business needs, you will almost always find that one or more of these factors was forgotten during the design phase. It is very tempting to become focused on one or two key factors and ignore the others, but we really have to maintain a focus on all of them to ensure storage success.</p>
			<p>We should begin by breaking down each factor individually.</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor063"/>Cost</h2>
			<p>It might seem that<a id="_idIndexMarker180"/> no one could forget about cost as a factor in storage but believe me it happens, and it happens often. As a rule, IT is a business function and all businesses, by definition, are about making money, and the cost of providing an infrastructure need always must factor in profits. So, because of this, no decision in IT (or anywhere in a business) should happen without cost as a consideration. We should never allow cost to be forgotten, or just as bad allowing someone to state that <em class="italic">cost is no object</em> because that can never be true and makes absolutely no sense. Cost may not be the primary concern, and the budgetary limits may be flexibility, but cost always matters.</p>
			<p>Storage is generally one of the costliest components of a production system so we tend to see costs be more sensitive when dealing with storage than when dealing with other parts of physical system design such as CPU and RAM. Storage is also often easiest to solve by simply throwing more money at it and so many people when planning for hardware err on the side of overbuilding because it is easy. Of course, we can always do this and as long as we understand enough of our storage needs and how storage works it will <em class="italic">work</em> outside of being overly expensive. But, of course, it is difficult to be effective system administrators if we are not cost effective - the two things go together.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor064"/>Durability</h2>
			<p>Nothing is more <a id="_idIndexMarker181"/>important when it comes to storage than durability: the ability of the storage mechanism to resist data loss. Durability is one of two aspects of reliability. For most workloads and most system scenarios, durability is what trumps all else. It is very rare that we want to store something that we cannot reliably retrieve even if that retrieval is slow, delayed, or expensive. Concepts such as data availability or performance mean nothing if the data is lost.</p>
			<p>Durability also refers to data that resists corruption or decay. In storage we have to worry about the potential of a portion of our data set losing integrity which may or may not be something that we can detect. Just because we can retrieve data alone does not tell us that the data that we are retrieving is exactly what it is supposed to be. Data corruption can mean a file that we can no longer read, a database that we can no longer access, an operating system that no longer boots, or worse, it can even mean a number in an accounting application changing to a different, but valid, number which is all but impossible to detect.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor065"/>Availability</h2>
			<p>Traditionally, we <a id="_idIndexMarker182"/>thought about data reliability mostly in terms of how available our data was when it came time to retrieve it. Availability is often referred to as <em class="italic">uptime</em> and if your storage is not available, neither is your workload. So, while availability generally takes a back seat to durability, it is still extremely important and one of the two key aspects of overall storage reliability.</p>
			<p>There are times when availability and performance become intertwined. There can be situations where storage performance drops so significantly that data becomes effectively unavailable. Consider a shower that just drips every few seconds, technically there is still water, but it is not coming through the pipes fast enough to be able to use it.</p>
			<p>We will be talking about RAID in depth in just a little bit, but availability and performance are good real-world examples. A famous situation can arise with large RAID 6 arrays when a drive or two have failed and have been replaced and the array is online and in the process of actively rebuilding (a process by which missing data is recalculated from metadata.) It is quite common for the RAID system to be overwhelmed due to the amount of data being processed and written that the resulting array, while technically online and available, is so slow that it cannot be used in any meaningful way and operating systems or applications attempting to use it will not just be useless from the extreme slowness but may even error out reporting that the storage is offline due to<a id="_idIndexMarker183"/> the overly long response times. <em class="italic">Available</em> can become a murky concept if we are not careful.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/>Performance</h2>
			<p>When it comes to<a id="_idIndexMarker184"/> computers in the twenty first century storage is almost always the most significant performance bottleneck in our systems. CPU and RAM almost always have to wait on storage rather than the other way around. Modern storage using solid state technologies has done much to close the performance gap between storage systems and other components, but the gap remains rather large.</p>
			<p>Performance can be difficult to measure as there are many ways of looking at it, and different types of storage media tend to have very different performance characteristics. There are concepts such as latency (time before data retrieval begins), throughput (also<a id="_idIndexMarker185"/> known as <em class="italic">bandwidth</em>, measuring the rate at which data can be streamed), and input/output operations per second or IOPS (the number of storage related activities that <a id="_idIndexMarker186"/>can be performed in each amount of time.) Most people think of storage only in terms of throughput, but traditionally IOPS have been the most useful measurement of performance for most workloads.</p>
			<p>It is always tempting to reduce factors to something simple to understand and compare. But if we think about cars, we could compare three vehicles: one with a fast acceleration but a low top speed, one with slow acceleration and a high-top speed, and a tractor trailer that is slow to accelerate and has a low top speed but can haul a lot of stuff at once. The first car would shine if we only cared about latency: the time for the first packet to arrive. The second car would shine if we cared about how quickly a small workload could be taken from place to place. This is most like measuring IOPS. The tractor trailer will be unbeatable if our concern is how much total data can be hauled between systems over the duration of the system. That's our throughput or bandwidth. With cars, most people think of a <em class="italic">fast car</em> as the one with the best top speed, but with storage most people think about the tractor trailer example as what they want to measure, but not what <em class="italic">feels</em> fast when they use it. In reality, performance is a matter of perspective. Different workloads perceive performance differently.</p>
			<p>For example, a backup deals in steady, linear data and will benefit most from storage systems designed around throughput. Therefore, tape works so well for backup performance and why old optical media such as CD and DVD were acceptable. But other workloads, like databases, depend heavily on IOPS and low latency and benefit little from total throughput and so really benefit from solid state storage. Other workloads like file servers often need a blend of performance and work just fine with spinning hard drives. You have to know your workload in order to design a proper storage system to support it.</p>
			<p>Performance is even <a id="_idIndexMarker187"/>more complex when we start thinking in terms of burstable versus sustainable rates. There is just a lot to consider, and you cannot short circuit this process.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor067"/>Scalability</h2>
			<p>A typical physical <a id="_idIndexMarker188"/>system deployment is expected to see four to eight years in production today and it is not uncommon to hear of systems staying in use far longer. Spend any amount of time working in IT and you are likely to encounter systems still powered on and completely critical to a company's success that have been in continuous use for twenty years or even more! Because a storage system is expected to have such a long lifespan, we have to consider how that system might be able to grow or change over that potential time period.</p>
			<p>Most workloads experience capacity growth needs over time and a storage design that can expand capacity as needed can be beneficial both for just protecting against the unknown but also by allowing us to invest minimally up front and spending more only <em class="italic">if</em> and <em class="italic">when</em> additional capacity becomes needed. Some storage systems may also be able to scale in terms of performance as well. This is less common and less commonly considered critical, yet even if a workload only increases capacity needs and not performance needs <em class="italic">per se</em>, larger capacity alone can warrant a need for increases performance just to handle tasks such as backups since large capacities mean larger time to backup and restore.</p>
			<p>In theory, you could also have a situation where the needs for reliability (durability, availability, or both) may need to increase over time. This, too, can be possible, but is likely to be much more complex. </p>
			<p>Storage is an area in which flexibility to adjust configuration over time is often the hardest, but also the most important. We cannot always foresee what future needs will be. We need to plan<a id="_idIndexMarker189"/> our best to allow for flexibility to adjust whenever possible.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor068"/>Capacity</h2>
			<p>Finally, we look at <a id="_idIndexMarker190"/>capacity, the amount of data that can be storage on a system. Capacity might seem straightforward, but it can be confusing. Even in simple disk-based arrays we have to think in terms of raw capacity (the sum of the capacities of all devices) and in terms of the resultant capacity (the usable capacity of the system that can be accessed for storage purposes. Many storage systems have redundancies to provide for reliability and performance and this comes at the cost of consumed raw capacity. So, we have to be aware of how our configuration of our storage will affect the final outcome. Storage admins will talk in terms of both raw and usable capacity.</p>
			<p>Now that we have a good handle on the aspects of storage that we need to keep in mind we can dive into learning more about how storage components are put together to build <strong class="bold">enterprise storage subsystems</strong>.</p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor069"/>Understanding block storage: Local and SAN</h1>
			<p>At the root of any <a id="_idIndexMarker191"/>standard storage mechanism that we will encounter today is the <a id="_idIndexMarker192"/>concept of <strong class="bold">block devices</strong>. Block devices are storage devices that allow for non-volatile data storage that can be stored and retrieved in arbitrary order. In a practical sense, think of the <em class="italic">standard</em> block device as being the hard drive. Hard drives are the prototypical block device, and we can think of any other block device as behaving like a hard drive. We can also refer to this as implementing a drive interface or <em class="italic">appearance</em>.</p>
			<p>Many things are block devices. Traditional <a id="_idIndexMarker193"/>spinning hard drives, solid state drives (SSD), floppy disks, CD-ROM, DVD-ROM, tape drives, RAM disks, RAID arrays and more are all block devices. As far as a computer is concerned, all of these devices are the same. This makes things simple as a system administrator: everything is built on block devices. </p>
			<p>From a system administrator perspective, we often simple refer to block devices as <em class="italic">disks</em> because from the perspective of the operating system we cannot tell much about the devices and only know that we are getting block storage. That block storage might be a physical disk, a logical device built on top of multiple disks, an abstraction built on top of memory, a tape drive, or remote system, you name it. We cannot really tell. To us it is just a block device and since block devices generally represent disks, we call them disks. It is not<a id="_idIndexMarker194"/> necessarily accurate, but it is useful.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor070"/>Locally attached block storage</h2>
			<p>The simplest <a id="_idIndexMarker195"/>type of block storage devices is those that are physically attached to our system. We are familiar with this in the form of standard internal hard drives, for example. Local block devices commonly attach by way of SAS, SATA, and NVMe connections today. In the <a id="_idIndexMarker196"/>recent past, <strong class="bold">Parallel SCSI</strong> (just<a id="_idIndexMarker197"/> called <strong class="bold">SCSI</strong> at the<a id="_idIndexMarker198"/> time), and <strong class="bold">Parallel ATA</strong> (aka <strong class="bold">PATA</strong>) just<a id="_idIndexMarker199"/> called <strong class="bold">ATA</strong> or <strong class="bold">IDE</strong> at<a id="_idIndexMarker200"/> the time, were standards. All of these technologies, as well as some more obscure, allow physical block devices to attach directly to a computer system.</p>
			<p>It is locally attached storage that we will work with most of the time. And all block devices have to be locally attached somewhere in order to be used. So this technology is always relevant.</p>
			<p>Locally attached block devices come with a lot of inherent advantages over alternatives. Being locally attached there is a natural performance and reliability advantage: the system is as simple as it gets and that means that there is less to go wrong. All other things being equal, simple trumps complex. Storage is a great example of this. Fewer moving parts and shorter connection paths means we get the lowest possible latency, highest possible throughput, and highest reliability at the lowest cost!</p>
			<p>Of course, locally attached storage comes with caveats or else no one would even make another option. The negative of locally attached storage is flexibility. There are simply some scenarios that locally attached storage cannot accommodate and so we must sometimes opt for alternative approaches.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor071"/>Storage Area Networks (SAN)</h2>
			<p>The logical<a id="_idIndexMarker201"/> alternative to a locally attached<a id="_idIndexMarker202"/> device is a remotely attached device and while one would think that we would simply refer to these types of block devices in this manner, we do not. A remote attached device uses a network protocol to implement the concept of <em class="italic">remoteness</em> into the storage and the network on which a remote device is communicating is called a <strong class="bold">Storage Area Network</strong> and because of this, common <a id="_idIndexMarker203"/>vernacular simply refers to all <a id="_idIndexMarker204"/>remote block storage as being a <strong class="bold">SAN</strong>.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor072"/>The terrible terminology of SAN</h2>
			<p>Technically speaking, a <a id="_idIndexMarker205"/>Storage Area Network should be<a id="_idIndexMarker206"/> a reference to a dedicated network that is used to carry block device traffic and in very technical circles this is how the term is used. Devices on a SAN can be direct block devices, disk arrays, and other similar <em class="italic">block over network</em> devices. The SAN is the network, not a <em class="italic">thing</em> that you can buy.</p>
			<p>In the common parlance, however, it is standard to refer to any device that provides storage, implements a block device interface, and connects to a network rather than directly to a computer as a SAN. You hear this every day in phrases like <em class="italic">did you buy a SAN?</em>, <em class="italic">we need a SAN engineer</em>, <em class="italic">I spoke to our SAN vendor</em>, <em class="italic">should we upgrade the SAN?</em>, and <em class="italic">where is our SAN?</em> Go to your nearest IT hardware vendor and ask them to sell you a SAN and they will without hesitation, the terminology is so standard that dollars to donuts says that they will be completely confused if you try to act like a SAN is anything but a hardware device into which you place hard drives and is connected to a network via some sort of cable.</p>
			<p>Because storage is complex, confusing, and scary and because storage area networks add additional layers of complexity on top of the basics this entire arena became treated as black boxes full of magic and terminology quickly deteriorated and most beliefs around SAN became based on misconceptions and myth. Common myths include impossible ideas such that SANs cannot fail, that SANs are faster than the same technology without the networking layer, that SANs are a requirement of other technologies, and others.</p>
			<p>We can only be effective system administrators if we understand how the technology works and avoid giving in to myths (and marketing.) For example, we cannot make meaningful risk analysis or performance decisions if we believe that a device is magic and do not consider its actual risk profile.</p>
			<p>In theory, a SAN is an extremely simple concept. We take any block device whether it is a physical device like an actual hard drive, or some more complicated concept like an array of drives, and encapsulate the standard block device protocol (such as SCSI or ATA) and send that over a network protocol (such as TCP/IP, Ethernet, or <em class="italic">FiberChannel</em>.) The network protocol acts as a simple tunnel, in a way, to get the block protocol over a long distance. That's all that there is to it. At the end of the day, it is still just a SCSI or ATA based device, but now able to be used over a long distance.</p>
			<p>Of course, what we just added is a bit of complexity, so SANs are automatically more fragile than local storage. Any and all risk and complication of local storage remains plus any complications and risk of the networking layer and equipment. The risk is cumulative. Plus, the extra networking layer, processing, and distance all must add additional latency to the storage transaction.</p>
			<p>Because of these factors, SAN based storage is always slower and more fragile than otherwise identical local storage. The very factors that most myths have used to promote SAN are exactly their weaknesses. </p>
			<p>The SAN approach does have its strengths, of course, or else it would serve no purpose. A SAN allows for three critical features: distance, consolidation, and shared connections. </p>
			<p>Distance can mean anything from a few extra feet to across the world. Of course, with longer distances come higher latencies, and typically storage is very sensitive to latency, so it is pretty rare that remote block storage is useful from outside of the range of local connection technologies. If you have to pull block storage data over the WAN, you will likely experience at very least latencies that cause severe performance issues and will typically see untenable bandwidth constraints. Typical production block storage is assumed to be many GB/s (that big B, not little b) of throughput and sub-millisecond latency, but WAN connections rarely hit even a single Gb/s and even the best latencies are normally a few milliseconds if not scores or more!</p>
			<p>Consolidation was<a id="_idIndexMarker207"/> traditionally the driving value of a SAN. Because<a id="_idIndexMarker208"/> many systems can physically connect to a single storage array over a single network it became easy, for the first time, to invest in a single, expensive storage system that could be used by many physically separate computer systems at once. The storage on the device would be <em class="italic">sliced</em> and every device that attaches to it sees its own unique portion of the storage.</p>
			<h3>When local storage isn't local</h3>
			<p>With all of the<a id="_idIndexMarker209"/> interfaces, abstractions, and incorrect terminology that often exists in IT, it can be really easy to lose track of exactly what is happening much of the time. SANs are one of these places were getting confused is par for the course. It is the nature of a SAN to take a block device that is far away and make it seem, to the computer using it, as if it were local. But it can also take something that is local, and make it seem local, when it is really remote. Did I just say that? </p>
			<p>The best example is that of the external USB hard drive. We all use them; they are super common. Go to any local department store and pick one up. Order one online. You probably have five on a shelf that you have forgotten about. A USB drive, while external, is obviously still local, right?</p>
			<p>Well, it isn't all that easy to say. Sure, it is physically close. But in technology terms remote means that something is <em class="italic">over a network</em> and local is <em class="italic">not over a network</em>. It does not matter how far away something is, it is the network aspect that determines local and remote devices. Otherwise, my desktop in Texas is physically attached to my dad's desktop in New York because there is a series of cables the entire way in between them. </p>
			<p>This presents an interesting challenge because, you see, USB is actually a very simple networking protocol, as are IEEE 1394 and Thunderbolt. If you physically dissect an external drive you can see this at work, to some degree. They are made from standard hard drives, generally with SATA interfaces, and a tiny network adapter that encapsulates the SATA protocol into the USB network protocol to be sent over the network (often just two feet total distance.)</p>
			<p>USB and its ilk might not feel like a network protocol, but it really is. It is a layer two network protocol that competes with Ethernet and can attach multiple devices, to multiple computers, and can even use things similar to switches. It is a real networking platform and that means that external hard drives attached via USB are, in fact, tiny SANs! Hard to believe, but it is true. Consider your mind blown.</p>
			<p>Storage, being the largest cost of most systems, being able to be shared and sliced more efficiently lowered cost to deploy new physical computer systems. Hard drives, for example, might come in 1TB sizes, but a single system might need only 80GB or 300GB or whatever and with a shared SAN hundreds of computers systems might share a single storage array and each use only what they need. Today we gain most of this efficiency through local storage with virtualization, but before virtualization was broadly available only systems like a SAN were able to address this cost savings. So, in the early days of SAN, the focus was cost savings. Other features really came later. This value has mostly inverted today and so is generally more expensive than having overprovisioned local storage but can still exist in some cases.</p>
			<p>The last value is shared connections. This is where two or more computers access the same portion of the storage on the same device - seeing the same data. This might sound a bit like traditional file sharing, but it is anything but that. </p>
			<p>In file sharing we are used to computers having a <em class="italic">smart</em> gatekeeping device that arbitrates access to files. With a SAN, we must remember that this is a <em class="italic">dumb</em> block device that has no logic of its own. Using a SAN to attach two or more computer systems to a single logical block device means that each computer thinks of the storage as being its own, private, fully isolated system and has no knowledge of other systems that might be also attached to it. This can lead to all kinds of problems from lost changes to corrupt files, to destroyed file systems. Of course, there are mechanisms that can be used to make shared<a id="_idIndexMarker210"/> storage spaces possible, but by definition they are not implemented by the SAN and have to be provided at a higher level on the computer systems themselves. </p>
			<h3>Shared SCSI connections</h3>
			<p>In the days <a id="_idIndexMarker211"/>before <a id="_idIndexMarker212"/>SAN, or before SANs were popular and widely available, there was another technique allowing two computers to share a single pool of hard drives: shared SCSI. </p>
			<p>With this technique, a single SCSI ribbon cable (typically able to connect to eight, sixteen, or even thirty-two devices. One device would need to be a controller, presumably on the motherboard of a computer. The other connections were open for connecting hard drives. But another connector could be connected to another controller on a separate computer and the two computers could each see and access the same drives.</p>
			<p>Due to the limitations of needing to share a single ribbon cable between two physical computers made this technique outrageously limited and awkward, but feasible. The primary value to a setup of this nature was allowing one computer system to fail and the other to take over, or to double the CPU and RAM resources assigned to a single data set beyond what could fit in a single server chassis. But the reliability and performance limits of the storage component left the system generally less than practical and so this technique was rarely implemented in the real world. But historically it is very important because it is the foundation of modern shared block storage, it was standard knowledge expected in late 1990s systems training, and it helps to visualize how SAN works today - more elegant, more flexible, but fundamentally the same thing.</p>
			<p>Today, the biggest use cases for shared block storage connections is for clustered systems that are designed to use this kind of storage as shared backing for virtualization. This was the height of fashion around 2010 but has since given way to other approaches to tackle this kind of need. This would now be a rather special case system design. But the technologies that are used here will be co-opted for other storage models as we will soon see.</p>
			<p>The world of SAN has many popular connection technologies. There are super simple SAN transports that are so simple that no one recognizes them as being such including USB, Thunderbolt, and IEEE1394/Firewire. Then there are a range of common enterprise class SAN protocols such as iSCSI (SCSI over IP), FibreChannel, FCoE (Fibre Channel over Ethernet), <em class="italic">FC-NVMe</em> (NVMe over Fiber Channel), and so on. Each SAN protocol presents its own advantages and challenges, and typically vendors only offer a small selection from their own equipment so choosing a vendor will typically limit your SAN options and <a id="_idIndexMarker213"/>picking a SAN option will limit your vendor selection <a id="_idIndexMarker214"/>choices. Understanding all of these protocols moves us from the systems world into the networking one. It is rare that as a system administrator you will be in a position to choose or even influence the choices in SAN design, typically this will be chosen for you by the storage, networking, and/or platform teams. If you do get to have influence in this area then significant study of these technologies, their benefits, and their applicability to your workload(s) will be necessary but is far outside of the scope of this tome.</p>
			<p>Block storage is not going anywhere. As much as we get excited about new storage technologies, such as object storage, block storage remains the underpinning of all other storage types. We have to understand block devices both physically and logically as we will use them in myriad ways as the building blocks of our storage platforms. Block storage is powerful and ubiquitous. It represents the majority of storage that we interact with during our engineering phases and is expected to remain at the core of everything that we do for decades to come.</p>
			<p>When deciding<a id="_idIndexMarker215"/> between <a id="_idIndexMarker216"/>local and remote block storage there is a useful rule of thumb: <em class="italic">You always want to use local storage until you have a need that local storage cannot fulfill. Or you never want to use remote storage until you have no other choice.</em></p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor073"/>Surveying filesystems and network filesystems</h1>
			<p>Sitting on top of<a id="_idIndexMarker217"/> block <a id="_idIndexMarker218"/>storage we typically find a <strong class="bold">filesystem</strong>. Filesystems are the primary (and by primary, I mean like they make up something like 99.999% or more of use cases) manner of final data storage on computer systems. Filesystems are what hold files, as we know them, on our computer storage. </p>
			<p>A filesystem is a data organization format that sits on top of block storage and provides a mechanism for organizing, identifying, storing, and retrieving data using the file analogy. You use filesystems every day on everything. They are used even when you cannot see them whether it is on your desktop, cell phone, or even on your VoIP phone, or microwave oven! Filesystems are everywhere.</p>
			<h3>Filesystems are really databases</h3>
			<p>If you want to get a <a id="_idIndexMarker219"/>little geeky with me for a moment and be honest you are reading a book on system administration best practices so we both know you are loving getting into some serious details, we can look at what a filesystem really is. At its core a filesystem is a NoSQL database, specifically a file database (essentially a specialized document database), that uses a raw block device as its storage mechanism and is only able to store and retrieve files.</p>
			<p>There are other specialty databases that use block devices directly (often called raw storage when dealing with database lingo), but they are rare. Filesystems are a database type that is so common, so much more common than all other database types combined, that no one ever talks about or thinks about them being databases at all. But under the hood, they are truly a database in every sense. </p>
			<p>To show direct comparisons, a standard database regardless of type has a standardized storage format, a retrieval format, a database engine (driver), and in some cases a database management layer (that can often allow for the use of multiple database engineers within a single system interface), and a query interface for accessing the data. Whether you compare MongoDB or MS SQL Server you will find that filesystems behave identically. The chosen filesystem on disk format is the storage format, the retrieval format is the <em class="italic">file</em>, the database engine is the filesystem drive, the database management system in Linux is the Virtual File System (which we will discuss later), and the query language is a list of underlying POSIX commands implemented in C (with simple shell-based abstractions that we can use for convenience.) Compare to standard databases and there is no way to tell them apart! Very cool stuff.</p>
			<p>After a computer system is deployed, nearly everything that we do with it from a storage perspective involves working on the filesystem. We tend to focus heavily on block storage during engineering phases, and filesystems during administration phases. But certainly, we have to plan our filesystems properly prior to deploying a system to production. Proper filesystem planning is actually something that is heavily overlooked with most people simply accepting defaults and rarely thinking filesystem design at all.</p>
			<p>Most operating systems have native support for a few different filesystems. In most cases an operating system has one obviously standard and primary filesystem and a handful of special case filesystems that are relegated to use on niche hardware devices or for compatibility with other systems. For example, Apple macOS uses APFS (Apple File System) for all normal functions but can use ISO 9660 when working with optical disks or FAT32 and <strong class="bold">exFAT</strong> used for compatibility with Windows storage devices (such as USB memory sticks or<a id="_idIndexMarker220"/> external hard drives.) Windows is similar but with NTFS instead of APFS. Windows recently has added one alternative filesystem, ReFS, for special needs, but it is not commonly used or understood.</p>
			<p>In Linux, however, we have several primary filesystem options and scores of specialty filesystem options. We have no way to go through them all here, but we will talk about several of the most important as understanding why we have them, and when to choose them is very important. Thankfully in production systems we really only have to concern ourselves with a few key products. If you find filesystems interesting, you can research the many Linux filesystem options to learn more about filesystem design and history and you might even find one that you want to use somewhere special!</p>
			<p>These are the key Linux filesystems today with which we need to be concerned for everyday purposes: XFS, EXT4, ZFS, BtrFS. Nearly everything that we do will involve one of those four. There are loads of less popular filesystems that are well integrated and work perfectly well like JFS and ReiserFS but are almost never seen in production. There are older formats like EXT2 and EXT3 that have been superseded by more recent updates. There are loads and loads of filesystems that are standard on other systems that can be used on Linux like NTFS from Windows or UFS from the BSD family. There are the standard niche filesystems like ISO 9660 and FAT32 that we mentioned earlier. Linux gives you options at every turn and filesystem selection is a great example of just how extreme it can get.</p>
			<h3>EXT: The Linux filesystem family</h3>
			<p>Nearly every <a id="_idIndexMarker221"/>operating <a id="_idIndexMarker222"/>system has its own, special-sauce filesystem that it uses natively or by default and is tightly associated with it and Linux is no exception.... just kidding, Linux is absolutely the exception which is amazing considering how much more robust Linux is in its filesystem options than any other operating system. Illumos has ZFS, FreeBSD has UFS, Windows has NTFS, macOS has APFS, AIX has JFS, IRIX had XFS and on, and on. Linux truly has no filesystem of its own, yet it has nearly everyone elses.</p>
			<p>Most people talk about the EXT filesystem family as being the Linux native filesystem and certainly nothing else comes close to matching that description. When Linux was first being developed, long before anyone had actually run it, the MINIX filesystem was ported to it and became the default filesystem as the new operating system began to take off. But as the name suggests, the MINIX Filesystem was native to MINIX and predated Linux altogether.</p>
			<p>Just one year after Linux was first announced, the EXT filesystem (or MINIX Extended File System) was created taking the MINIX Filesystem and, you guessed it, extending it with new features mostly around timestamping.</p>
			<p>As Linux began to grow, EXT grew with it and just one year after EXT was first released its successor EXT2 was released as a dramatic upgrade taking the Linux filesystem ecosystem from a hobby system to a serious enterprise system. EXT2 ruled the Linux ecosystem almost exclusively from its introduction in 1993 until 2001 when Linux went through a bit of a filesystem revolution. EXT2 was such a major leap forward that it was backported to MINIX itself and had drivers appear on other operating systems like Windows and macOS. Possibly no filesystem is more <em class="italic">iconically</em> identified with Linux than EXT2.</p>
			<p>By 2001 many operating systems were looking to more advanced filesystem technologies to give them a competitive advantage against the market and Linux did so both by introducing more filesystem options and by adding journaling functionality to EXT2 to increment its version to EXT3. This gave the EXT family some much needed stability.</p>
			<p>Seven more years and we received one additional major upgrade to Linux' quasi-native filesystem with EXT4. Surprisingly, the primary developer on EXT3 and EXT4 stated that while EXT4 was a large step forward that it was essentially a stopgap measure of adding improvements to what is very much a 1980s technology. Filesystem design principals leaped forward especially in the early 2000s and the EXT family is likely at the end of the road, but still has a lot of useful life left in it.</p>
			<p>I am going to delve into a little bit of detail for each of the main filesystem options, but to be clear this is a cursory look. Filesystem details can change quickly and can vary between versions or implementations so for really specific details such as maximum file size, file count, filesystem size and so forth please look to Wikipedia or filesystem documentation. You will not need to memorize these details and rarely will you even need to know them. In the 1990s filesystem limitations were so dramatic that you had to be acutely aware of them and work around them at every turn. Today any filesystem we are going to <a id="_idIndexMarker223"/>use is <a id="_idIndexMarker224"/>able to handle almost anything that we throw at it, so we really want to understand where different products shine or falter and when to consider which ones at a high level.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor074"/>EXT4</h2>
			<p>Linux, as a <a id="_idIndexMarker225"/>category, has <a id="_idIndexMarker226"/>no default filesystem in the way that other operating systems do, but if you were to attempt to make the claim that any filesystem deserves this title today that honour would have to go to <strong class="bold">EXT4</strong>. More deployed Linux-based operating systems today choose EXT4 as their default filesystem than any other. But this is beginning to change so it seems unlikely that EXT4 will remain dominant for more than a couple years yet.</p>
			<p>EXT4 is reasonably fast and robust, quite flexible, well known, and meets the needs of nearly any deployment. It is the jack of all trades of Linux filesystems. For a typical deployment, it is going to work quite well.</p>
			<p>EXT4 is what we call a <em class="italic">pure filesystem</em>, which means it is just a filesystem and does not do anything else. This makes it easier to understand and use, but also makes it more limited.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor075"/>XFS</h2>
			<p>Like the <a id="_idIndexMarker227"/>EXT family, <strong class="bold">XFS</strong> dates <a id="_idIndexMarker228"/>back to the early 1990s, and comes from a Linux competitor, in this case SGI's IRIX UNIX system. It is venerable and robust and was ported to Linux in 2001, the same year that EXT3 released. For twenty years now EXT3/4 and XFS have competed for the hearts and souls of Linux Administrators (and Linux Distro creators to choose them as the default filesystem.)</p>
			<p>XFS is also a <em class="italic">pure filesystem</em> and is very commonly used. XFS is famous for its extremely high performance and reliability. XFS is sometimes specifically recommended by high performance applications like databases to keep them running at their peak.</p>
			<p>XFS is probably the most deployed filesystem when the system administrator is deliberately choosing the filesystem rather than simply taking the default, is probably the most recommended by application vendors, and is my own personal choice for most workloads where storage needs are non-trivial.</p>
			<p>Over the years, EXT4 and XFS have gone back and forth in popularity. My own observations say that XFS has slowly been edging ahead over the years. </p>
			<p>The one commonly cited caveat of XFS compared to EXT4 is that EXT4 is able to either shrink <em class="italic">or</em> grow a volume once it has been deployed. XFS can grow, but cannot shrink, a volume that has been deployed. However, it is almost unheard of for a properly deployed production system to shrink a filesystem, so this is generally seen as trivia and not relevant to<a id="_idIndexMarker229"/> filesystem <a id="_idIndexMarker230"/>decision making (especially with the advent of thin provisioned block storage.)</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor076"/>ZFS</h2>
			<p>Releasing to<a id="_idIndexMarker231"/> Solaris <a id="_idIndexMarker232"/>in 2006, <strong class="bold">ZFS</strong> is generally considered to be the foundation of truly modern filesystem design. By the time that work on ZFS had begun in 2001 the industry was already beginning to take filesystem design very seriously and many new concepts were being introduced regularly, but ZFS really took these design paradigms to a new level and ZFS remains a significant leader in many areas still today.</p>
			<p>ZFS really has three high level areas in which it attempted to totally disrupt the filesystem industry. First in size: ZFS was able to address multiple orders of magnitude more storage capacity than any filesystem before it. Second in reliability: ZFS introduced more robust data protection mechanisms than other filesystems had allowing it to protect against data loss in significant ways. And third, in integration: ZFS was the first real <em class="italic">non-pure</em> filesystem where ZFS represented a filesystem, a RAID system, and a logical volume manager all built into a single filesystem driver. We will go into depth about RAID and LVMs later in this chapter. This integration was significant as it allows the storage layers to communicate and coordinate like never before. Pure filesystems like EXT4 and XFS can and do use these technologies but do so through external components rather than integrated ones.</p>
			<p>While ZFS is not new, having been in production systems for at least fifteen years, it is quite new for release on Linux. It took many years before a port of ZFS was made available for Linux, and then there were many years during which licensing concerns kept it from being released in a consumable format for Linux. Today the only major Linux distribution that officially supports and packages ZFS is Ubuntu, but Ubuntu's dominant market position makes ZFS automatically widely available. At this time, it is less than two years since ZFS was able to be used for the bootable root filesystem on Ubuntu. So ZFS is quite new in the production Linux space in any widely accessible way. Its use appears to be growing rapidly now that it is available.</p>
			<p>ZFS represents probably the most advanced, reliable, and scalable filesystem available on Linux as of the time of this writing. It should be noted that from a purely filesystem-based performance perspective that ZFS is not known to shine. It is rare that storage performance tweaking at the filesystem level is considered valuable but when it is modern<a id="_idIndexMarker233"/> filesystems <a id="_idIndexMarker234"/>with all of their extra reliability typically cannot compete with older, more basic filesystems. This has to be noted as it is so often simply assumed that more modern systems are going to also be automatically faster, as well. This is not the case here.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor077"/>BtrFS</h2>
			<p>Pronounced <em class="italic">Butter-F-S</em>, <strong class="bold">BtrFS</strong> is the <a id="_idIndexMarker235"/>current significant attempt to make a<a id="_idIndexMarker236"/> Linux native filesystem (there was a previous attempt called ReiserFS in the early 2000s that got some traction but ended badly for non-technical reasons.) BtrFS is intended to mimic the work of ZFS, but native to Linux and with a compatible license.</p>
			<p>BtrFS trails ZFS significantly with many features still not implemented, but with work ongoing. BtrFS is very much alive and increasingly more Linux distributions are supporting it and even choosing it as a default filesystem. BtrFS feels as if it might be the most likely long-term future for Linux.</p>
			<p>Like ZFS, BtrFS is a modern, heavily integrated filesystem that is beginning to include functionality from RAID and LVM layers of the storage stack. Performance is the weakest point for BtrFS today.</p>
			<p class="callout-heading">Stratis</p>
			<p class="callout">Given its industry support, we need to make mention of Stratis. Stratis is not a filesystem itself <em class="italic">per se</em> but act much like one. Stratis is an attempt to build the functionality of integrated (or <em class="italic">volume-managing file systems</em>) like ZFS and BtrFS using the existing components of XFS and the standard Linux LVM layer.</p>
			<p class="callout">In its early days on IRIX, XFS was designed to be used with IRIX's native LVM and the two integrated naturally providing something not entirely unlike ZFS or BtrFS today. When XFS was ported to Linux its associated LVM layer was not ported, but the native Linux LVM was made to work with it instead. XFS + LVM has long been an industry standard approach and Stratis mearly is attempting to provide a more accessible means of doing so while integrating best practices and simplified management.</p>
			<p>This sums up the four current production filesystem options that you will likely encounter or be responsible for choosing between. Remember that you can mix and match filesystems on a single system. It is very common, in fact, to use EXT4 as a boot filesystem for basic operating system functionality while then relying on XFS for a high-performance database storage filesystem or BtrFS for a large file server filesystem. Use what makes sense for the workload at an individual filesystem layer. Do not feel that you have to be stuck using only one filesystem across all systems, let alone within a single system!</p>
			<p>Most of the real intense technical aspects of filesystems are in the algorithms that deal with searching<a id="_idIndexMarker237"/> for and storing the bits onto the block devices. The details of these <a id="_idIndexMarker238"/>algorithms are way beyond the scope of not only this book but systems administration in general. If you are interested in filesystems, learning how data is stored, protected, and retrieved from the disk can be truly fascinating. For systems administration tasks it is enough to understand filesystems at a high level.</p>
			<p>Sadly, there is no way to provide a real best practice around filesystem selection. It is unlikely that you are going to have reason to seriously consider the use of any rare filesystem not mentioned here in a production setting, but all four that are listed here have valuable use cases and all should be considered. Often the choice of filesystem is not made in isolation unless you are working with a very specific product that requires or recommends a specific one for the purpose of some feature. Instead, the choice of filesystem is normally going to be dependent on many other storage decisions including RAID, LVM, physical support needs, drive media, and so forth.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor078"/>Clustered file systems</h2>
			<p>All of the<a id="_idIndexMarker239"/> filesystems <a id="_idIndexMarker240"/>that we have discussed thus far, regardless of how modern they are, are <em class="italic">standard</em> filesystems or <em class="italic">non-shared</em> filesystems. They are only viable when access is guaranteed to be from only a single operating system. In almost all cases, this is just fine.</p>
			<p>If you recall from our discussion on SANs, however, we mentioned that there are use cases where we may want multiple computer systems to be able to read and write for the same storage area at the same time. Clustered or <em class="italic">shared storage</em> filesystems are the mechanism that can allow that to work.</p>
			<p>Clustered filesystems work just like traditional filesystems do, but with the added features by which they will write locking and sharing information to the filesystem so that multiple computer systems are able to coordinate their use of the filesystem between attached nodes. In a standard filesystem there is only one computer accessing the filesystem at a time so knowing what file is open, when a file has been updated, when a write is cached and so forth are all handled in memory. If two or more computers try to share data from a traditional filesystem, they cannot share this data in memory and so will inevitably create data corruption as they overwrite each other's changes, fail to detect updated files, and do all sorts of nasty things from outdated write caches!</p>
			<p>Since the only shared component of these systems is the filesystem, all communications between nodes accessing a file system have to happen in the filesystem itself. There is literally no other possible way without going to a mechanism that is no longer shared storage but shared compute which is much more complicated and expensive.</p>
			<p>To describe how clustered filesystems work in the simplest terms we can think of each computer knowing, from the filesystem, that a specific section of the block device (disks) is set aside in an extremely rigid format and size to be an area where the nodes read and write their current status of interaction with the filesystem. If node A needs to open File X, it will put in a note that it is holding that file open. If node B deletes a file, it will put in a note that it is going to delete and update it once the file is deleted. node C can tell what activity is going on just by reading this one small piece of the filesystem. All of the nodes connected know not to cache the data in this area, to state any action that they plan to take, and to log anything that they have done. If any node misbehaves, the whole system corrupts, and data is lost.</p>
			<p>Of course, as you can tell, this creates a lot of performance overhead at a minimum. And this system necessarily requires absolute trust between all connected nodes as the access and data integrity controls are left up to the individual nodes. There is not and there cannot be any mechanism to force the nodes to behave correctly. The nodes have to do so voluntarily. This means that any bug in the code, any failure of memory, any admin with root access, any malware that gains access to a single node, and others. can bypass <em class="italic">any</em> and <em class="italic">all</em> controls and read, modify, destroy, encrypt, and others, to any degree that it wishes and all of the kinds of security and controls that we normally assume protect us do not exist. Shared storage is extremely simple, but we are so used to storage abstractions that it becomes complex to try to think about how any storage system could be so simple.</p>
			<p>Like with regular filesystems, Linux has multiple clustered filesystems that we will commonly see in use. The most common one is GFS2 followed by OCFS2. </p>
			<p>As with SANs<a id="_idIndexMarker241"/> in general, the same rule will apply to clustered <a id="_idIndexMarker242"/>file systems: you do not want to use them, until you have to.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor079"/>Network filesystems</h2>
			<p><strong class="bold">Network filesystems</strong> are always a<a id="_idIndexMarker243"/> little bit hard to describe but benefit well from the <em class="italic">you will know it when you see it</em> phenomenon. Unlike regular filesystems that sit on top of a block device and provide a way to access storage in the form of files, network filesystems take a filesystem and extend it over a network. This might sound a lot like a SAN, but it is very different. A SAN shares a set of block devices over a network. Network filesystems share filesystems over a network.</p>
			<p>Network filesystems are quite common, and you probably see them every day, but we often do not think about them being what they really are. We often refer to network filesystems as <em class="italic">shares</em> or <em class="italic">mapped drives</em> and the standard protocols used are NFS and SMB (sometimes called CIFS, which is not really accurate.) Servers that implement network filesystems are called file servers and if you make a file server into an appliance, it is called a NAS (for Network Attached Storage.) Network filesystems are also often thought of as <em class="italic">NAS protocols</em> for this reason, just as block over network protocols are thought of as <em class="italic">SAN protocols</em>.</p>
			<p>Unlike shared block protocols, network filesystems are <em class="italic">smart</em> with the machine that shares out the storage having a local filesystem that it understands and intelligence about the files involved so concepts like file locking, caching, file updates and so forth can be handled through a single gatekeeper that can enforce security and integrity and there is no need to trust the accessing nodes. The key difference is that a SAN is just storage blindly attached to a network, it can be as simple as a network adapter bolted onto a hard drive (and it often is, actually.) A device implementing a network filesystem, on the other hand, is a server and required a CPU, RAM, and an operating system to function. Shared block storage is almost exclusively used in very limited deployments with carefully controlled servers. Network filesystems can be used almost anywhere that a SAN can be used but are also commonly used to share storage directly to end user devices as their robust security, ease of use and lack of needed end point trust make them highly useful where SANs would be impossible to deploy.</p>
			<p>Network filesystems run as an additional network-enabled layer on top of traditional filesystems and do not replace the <em class="italic">on disk</em> filesystems that we already have. In speaking of interfaces, we would describe network filesystems as <em class="italic">consuming a filesystem interface</em> and also <em class="italic">presenting a filesystem interface</em>. Basically, it is filesystem in, filesystem out.</p>
			<p>Like with traditional filesystems, Linux actually offers a large range of network filesystem options, many of <a id="_idIndexMarker244"/>which are historical in nature or extremely niche. A common example<a id="_idIndexMarker245"/> is the <strong class="bold">Apple Filing Protcol</strong> or <strong class="bold">AFP</strong> (aka <em class="italic">AppleTalk</em>) which Linux offers, but is not used on any production operating system today. Today only NFS and SMB really see any real work usage in any way.</p>
			<h3>NFS</h3>
			<p>The original <em class="italic">network file system</em> in wide<a id="_idIndexMarker246"/> use and literally <a id="_idIndexMarker247"/>the source of the name, <strong class="bold">NFS</strong> dates back to 1984! NFS cannot be native to Linux as it predates Linux by seven years, but NFS has been the default network file system across all UNIX-based or inspired operating systems since its inception and represents a rather significant standard because of this. Because Linux is so prominent today, most people think of NFS as being <em class="italic">Linux' protocol</em>. </p>
			<p>NFS is available on essentially any system. Any UNIX system, even macOS, offers NFS as does Windows Server! NFS is an open standard and all but universal. NFS maintains popularity by being simple to use, robust on the network and generally performs well. NFS remains heavily used on servers wherever direct filesharing between systems is required, especially on backup systems.</p>
			<h3>SMB</h3>
			<p>The <strong class="bold">Server Message Block</strong> (<strong class="bold">SMB</strong>) protocol <a id="_idIndexMarker248"/>predates <a id="_idIndexMarker249"/>NFS and was originally available in 1983. These are very old protocols indeed. SMB did not really find much widespread usage until Microsoft really began to promote it around 1990 and with the rise of the Windows NT platform throughout the 1990s SMB began to become quite popular along with it. </p>
			<p>SMB really benefited from Microsoft's heavy use of mapped drives between their servers and workstations which made the SMB protocol very visible to many users both traditional and technical. </p>
			<p>In Linux, support for the SMB protocol is provided by the Samba package (Samba is a joke on the SMB letters.) Linux has good support for SMB, but using it is more complex than working with NFS.</p>
			<p>Choosing between NFS and SMB for file sharing needs on Linux generally comes down to the use case. If working with predominantly UNIX systems, generally NFS makes the most sense. If working predominantly with Windows systems, then SMB generally makes the most sense. Both are powerful and robust and can service a wide variety of needs. </p>
			<p>Where decision making can get extremely hard is in cases where we can provide for the same need it totally different ways. For example, if you need to provide shared storage backends for virtualization you might have the option of a network filesystem like NFS or a clustered filesystem on a SAN like GFS2. These two approaches cannot be easily compared as every aspect of the two systems is likely to be different including the vendors and hardware and so comparisons typically must be done at a full stack level and not at the network technology level.</p>
			<p>Now we have explored file systems technologies and seen a broad scope of real-world file system options for Linux systems and looked at how filesystems can be local or remote, single access, or clustered to allow for multiple access vectors. At the same time, we have a good idea of how to approach decisions involving filesystem selection and configuration. We know when choose different filesystem technologies as well as what to look for in new or alternative systems that we may not have looked at specifically here. Filesystems need not be scary or confusing but can be valuable tools in our bag of tricks that we can use to fine tune our systems for safety, scalability, access, or performance. Next, we will look at one of the least understood areas of storage, the <em class="italic">logical volume</em>.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor080"/>Getting to know logical volume management (LVM)</h1>
			<p>I hate to apply terms <a id="_idIndexMarker250"/>like <em class="italic">new</em> to technology that was in use by the late 1980s but compared to most concepts in computer storage <strong class="bold">logical volume management</strong> (<strong class="bold">LVM</strong>) is pretty new and is far less known than most other standard storage technologies to the majority of system administrators. LVMs were relegated to extremely high-end server systems prior to Linux introducing the first widely available product in 1998 and Microsoft following suit in 2000. Today LVMs are ubiquitous and available, often natively and by default, on most operating systems.</p>
			<p>An LVM is the primary storage virtualization technology in use today. An LVM allows us to take an arbitrary number of block devices (meaning one or more, generally called <em class="italic">physical volumes</em>) and <a id="_idIndexMarker251"/>combine, split, or otherwise modify them and present them as an arbitrary number of block devices (generally called <em class="italic">logical volumes</em>) to the <a id="_idIndexMarker252"/>system. This might sound complex, but it really is not. A practical example can make it seem quite easy.</p>
			<p>An example is when we have a computer system that has three hard drives attached to it. They can be all the same, or they can be different. In fact, one could be a traditional spinning hard drive, one a modern SSD, and one an external USB drive (or a RAID array, SAN, you name it). We can add all three to our LVM as physical volumes. An LVM will allow us to treat this as a single storage pool and turn it into any configuration that we want. We might turn it all into a single logical volume so that we simply get the combined storage of the three. Or maybe we will create a dozen logical volumes and use each for a different purpose. We can have as many physical volumes as we want and create as many logical volumes as we want. Logical volumes can be any size that we want (in some cases even bigger than the total physical size!) We are not limited to traditional disk sizes. With logical volumes we often find it useful to make more, smaller volumes so that we can have more management and isolation.</p>
			<p>With an LVM, we can think of the system as consuming a block device and presenting a block device. Because LVMs use and provide block devices (aka disk appearances) they are <em class="italic">stackable</em>, meaning if you wanted to, you could have a block device on which there is an LVM which makes a logical volume, that is used by another LVM, that makes another logical volume, that is used by another LVM, and so forth. This is in no way practical, but it helps to visualize how an LVM sits in a <em class="italic">storage stack</em>. It is always in the middle, somewhere, but other than being in the middle it is very flexible.</p>
			<p>LVMs only need to provide this basic <em class="italic">block in, block out</em> functionality to be an LVM, but there are other features that are commonly added to LVMs to really make them incredibly useful. Some of the most standard features that we tend to expect to be found in an LVM include live resizing of logical volumes, <em class="italic">hot plugging</em> of physical devices, snapshot functionality, cache options, and thin provisioning.</p>
			<p>On Linux, as with most things, we have not one, but multiple logical volume managers! This is becoming more common as creating integrated filesystems with their own LVMs has become the trend in recent years. In production on Linux today we have LVM2, ZFS, and BtrFS. You will, of course, recognize the latter two as being filesystems that we mentioned earlier. When most people are talking about a logical volume manager on Linux, they mean LVM2, generally just called LVM. But ZFS and BtrFS' integrated logical volume managers are becoming increasingly popular approaches as well.</p>
			<p>Because of the <em class="italic">stackable</em> nature of an LVM, that is consuming and providing block devices, we are able <a id="_idIndexMarker253"/>to use LVM2 in conjunction with ZFS or BtrFS if we so choose and can either disable their integrated LVM layers as being unnecessary, or we can use them if they have features that we want to take advantage of! Talk about flexibility.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor081"/>Whatever happen to partitions</h2>
			<p>If you recall <a id="_idIndexMarker254"/>working in IT in the 1990s, we used to talk about disk partitioning quite incessantly. It was a regular topic. How do you set the partitions, how many do you make, basic and extended partitions, what partitioning software to use, and so on and so forth. To be sure, partitions still exist, we just have not needed them for a very long time now (not since Windows 2000 or Linux 2.4, for example.)</p>
			<p>Partitions are a very rigid <em class="italic">on disk</em> system for slicing a physical disk into separate areas that can each be presented to the system as an individual block device (aka drive.) In this way, partitions are like a super basic LVM without the flexibility. Partitions are limited to existing as a part of a single block device and the mapping of which areas of the block device belong to which partition are kept in a simple partition table at the start of the device. </p>
			<p>Partitions were the forerunner of logical volumes and some people still use them (but only because they are not familiar with logical volumes.) Partitions are not flexible and lack important options like thin provisioning and snapshots, that logical volumes can offer, and while resizing a partition is technically possible it is inflexible, hard, and extremely risky.</p>
			<p>Everything that partitions offered LVMs offer too, plus lots more, without giving anything up. The need for partitioning (the act of creating multiple filesystems out of a block device) has decreased significantly over the years. In the late 1990s it was standard and all but required for even the simplest server and often even a desktop to have good reason to be divided up into different filesystems. Today it is far more common to merge many block devices into a single filesystem. Mostly this is because filesystem performance and reliability have totally changed and the driving factors for partitioning have eroded. There are good reasons to still divide filesystems today. We simply need to do so far less of the time.</p>
			<p>Many mechanisms <a id="_idIndexMarker255"/>today, such as backup utilities, leverage the power of the LVM layer to do tasks like freezing the state of the block device so that a complete backup can be taken. Because an LVM operates beneath the final filesystem layer it has certain capabilities lacking at other layers. LVM is the storage layer where we get the least critical features, but it tends to be where all of the magic happens. LVMs give us flexibility to modify storage layouts after initial deployment and to interact with those storage systems at a block level. An LVM is a core technology component in providing the feel of a modern twenty-first century operating system.</p>
			<p>Of course, any new technology layer will have caveats. An LVM adds another layer of complication and more pieces for you, as the system administrator to understand. Learning to manage the LVM is hardly a huge undertaking, but it is quite a bit more to have to learn than if you do not have one. LVMs also introduce a small amount of performance overhead as they translate between the physical devices and the logical ones. Typically, this is a truly tiny amount of overhead, but it is overhead nonetheless.</p>
			<p>In general the benefits of an LVM dramatically outweigh the cost and more and more systems are starting to simply deploy an LVM layer without asking the end user whether or not they want it because increasingly functionality that customers simply expect an operating system to have depend on the LVM layer and allowing systems to be deployed without one often leaves customers stranded unclear why their systems do not live up to expectations and often in a way that they do not realize for months or years after initial deployment.</p>
			<p>Like other forms of virtualization, storage virtualization and LVMs are most important for <em class="italic">protecting against the unknown</em>. If we knew everything about how a system would be used for its entire lifespan, things like resizing, backups, consolidation and so forth would have little value, but this is not how the real-world works.</p>
			<p>Best practices when it comes to an LVM is generally accepted to be: Unless you can positively provide solid technical reasons why the overhead of the LVM is going to be impactful and that that impact outweighs the protections that an LVM provides, always deploy an LVM.</p>
			<p>Logical volume managers provide a critical building block to robust storage solutions and are, in many ways, what separates modern storage from classical computing systems. Understanding how logical volumes abstract storage concepts and let us manipulate and build storage to act<a id="_idIndexMarker256"/> like we want gives us many options and lead us to additional concepts such as RAID and RAIN, which we will discuss next that use LVM to construct data protection, expansion, and performance capabilities. </p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor082"/>Utilizing RAID and RAIN</h1>
			<p>We have<a id="_idIndexMarker257"/> looked at so many ways of interfacing<a id="_idIndexMarker258"/> with our storage. But probably the most exciting is when we start to deal with <strong class="bold">RAID</strong> (<strong class="bold">Redundant Array of Inexpensive Disks</strong>) and by extension its descendant, <strong class="bold">RAIN</strong> (<strong class="bold">Redundant Array of Independent Nodes</strong>). Before we go too far, it must be noted that RAID is a huge topic that would require a book of its own to truly address in a meaningful way. Understanding how RAID works and all of the calculations necessary to understand the nuances of its performance and risk is a major subject all on its own. My goal here is to introduce the concept, explain how it fits into a design, expose the best practices around it, and prepare you for further research.</p>
			<p>RAID and RAIN are mechanisms for taking many <em class="italic">storage devices</em> (block devices) and using the natural device multiplicity (often misstated as redundancy) to provide some combination of improved performance, reliability, or scalability over what possibility with only an individual drive is. Like an LVM, RAID and RAIN are <em class="italic">mid-stack</em> technologies that consumer block device interfaces and provide block device interfaces and therefore can be <em class="italic">stacked</em> on top of an LVM, below an LVM, on top of another RAID, on top of a mixture of hardware devices, and so forth. Very flexible.</p>
			<p>In fact, RAID and RAIN are actually each a specialized form of LVM! No one, ever, talks about these technologies in this way and you will get some strange looks at the company Christmas party if you start discussing RAID as a specialized LVM, but it actually is. RAID and RAIN are extreme subsets of LVM functionality with a very tight focus. It is not actually uncommon for general purpose LVMs to have RAID functionality built into them <a id="_idIndexMarker259"/>and the trend with the <a id="_idIndexMarker260"/>integrated filesystems is to have the LVM and RAID layers both integrated with the filesystem.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor083"/>RAID</h2>
			<p>RAID standard<a id="_idIndexMarker261"/> for <em class="italic">Redundant Array of Inexpensive Disks</em> and was originally introduced as a set of technologies that work at the block device level to turn multiple devices into one. RAID-like technologies go way back to even the 1960s and the term and modern definitions are from 1988, which means that RAID actually pre-dates more general purpose LVM.</p>
			<p>RAID essentially takes an array of block devices and puts them into lockstep with each other under one of many different data storage regimes, called <em class="italic">levels</em>. Each RAID level acts different and uses a different mechanism to merge the underlying physical disks into a single virtual disk. By making multiple drives act as if they were a single drive, we can extend different aspects of the storage as needed but gaining in one area normally comes at a cost in another so understanding the way that RAID works is important.</p>
			<p>RAID is an area where there is a very high importance on the system administrator having a deep understanding of the inner workings of the storage subsystem. And surprisingly it is an area where very few system administrators truly know how their system works.</p>
			<p>While RAID comes defined as a series of <em class="italic">levels</em>, but do not be fooled. The levels are simply different types of storage that share the underlying RAID basics. RAID levels do not really build on one another and a higher number does not represent some intrinsically superior product.</p>
			<p>Because RAID is really a form of an LVM, it can sit anywhere in the storage stack and in the wild can be found almost anywhere. Some very popular RAID levels are actually <em class="italic">stacked RAID</em> leveraging this innate artefact of its design, most notably RAID 10.</p>
			<p>RAID also comes in both hardware and software variants. Hardware RAID is much like a graphics card that connects directly to a monitor and offloads work from the main computer systems and talks to the hardware directly. A hardware RAID card does exactly this reducing load on the main computer system, interfacing to hardware storage devices directly, and potentially offering special features (like a cache) in the hardware. Software RAID, instead, leverages the generally much more powerful system CPU and RAM and has more flexible configurations. Both approaches are completely viable. </p>
			<p>Each RAID level has a unique set of properties and makes sense to be used at a different time. RAID is a complex topic deserving of its own tome to tackle properly. RAID is not a topic that we can look at too quickly, which has been a danger with storage in the past. RAID risk factors are often distilled into meaningless statements such as stating numerically <em class="italic">how many drives can a RAID array of level X recover from?</em> This means nothing and is meant as a way of simplifying something very complex into something that <a id="_idIndexMarker262"/>can simply be memorized or thrown onto a chart. RAID does not work this way. Each RAID level has a complex story around performance, reliability, cost, scalability, real world implementations, and so forth.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor084"/>RAIN</h2>
			<p>Over time as <a id="_idIndexMarker263"/>systems became larger and more complex, the limitations with the RAID approach began to become apparent. RAID is simple and easy to implement. But RAID is inflexible and there are some key features like simple resizing, automated rebalancing, and flexible node sizing that it handles poorly. A new family of technologies were needed.</p>
			<p>RAIN eschews the <em class="italic">full block device</em> approach to arrays that RAID was based on and instead breaks up storage by smaller chunks, often by blocks, and handles replication at that level. In order to do this effectively, RAIN must not just understand the concept of these blocks, but also the block devices (or <em class="italic">disks</em>) that they are on, but also the nodes in which they exist. This nodal awareness lends RAIN its moniker: <em class="italic">Redundant Array of Independent Nodes</em>.</p>
			<p>Oddly, in RAIN it is not really the nodes that are necessarily redundant but really the blocks and you can actually implement RAIN on a single physical device to directly compete with traditional RAID in its simplest form, but this is rarely done.</p>
			<p>Because RAIN handles block level replication it can have many advantages over RAID. For example, it can use different sizes devices rather fluidly. Drives of varying sizes can be thrown <em class="italic">willy nilly</em> into servers and tied together with great efficiency. </p>
			<p>Under RAID if a drive fails, we need a replacement drive that is capable of taking over its place in the array. This is often problematic, especially with older arrays. RAIN can avoid this issue by allowing any combination of available capacity across the array absorb the lost capacity of a failed hard drive. </p>
			<p>RAIN comes in such a variety of implementations, each one essentially unique, that we really cannot talk about it in any standard way. Most solutions are proprietary today and while a few well known open products have been made and have made their way into being standard components of the Linux ecosystem generally they are external to the distributions they behave much like proprietary products in how we must approach them.</p>
			<p>In the future we <a id="_idIndexMarker264"/>may see significant consolidation or at least standardization within the RAIN market as these technologies become more available and well understood. Until then we need to approach RAIN with the understanding of how block replication <em class="italic">could work</em> and know that each implementation may make drastically different design choices. RAIN might be built into the kernel or might exist as an application running higher in the stack, in some cases it could even run in a virtual machine virtualized on top of a hypervisor! How RAIN will react to a lost drive, to load balancing, locational affinity, rebalancing during loss, rebuild after repair, and so forth are not defined by any standards. To use RAIN, you must research and learn in depth about any solution that you will be considering and think critically about how its artefacts will impact your workloads over time.</p>
			<p>RAIN is almost guaranteed to be the future of system storage. As we move more and more towards clusters, hyperconvergence, cloud and cloud-like designs RAIN feels more and more natural. And adoption of RAIN will only increase as understanding increases. This simply takes time even though the technology itself is not new.</p>
			<p>Nearly every production system that we will ever design or support, will use some form of RAID or RAIN whether locally or remotely. By now, we are prepared to think about how the decision of what RAID level or configuration or what RAIN implementation is chosen will impact our systems. Taking the time to deeply understand storage factors in these multi-device aggregation frameworks interact is one of the most valuable high level knowledge areas for system administrators across the board. In our next section, we will build on<a id="_idIndexMarker265"/> these technologies to see how local storage can be made redundant with external systems, or nodes.</p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor085"/>Learning about replicated local storage</h1>
			<p>Possibly the most <a id="_idIndexMarker266"/>critical storage type, and the least understood, is <strong class="bold">replicated local storage</strong> or <strong class="bold">RLS</strong>. RLS is not a difficult concept, it is quite simple. But there are many myths surrounding other concepts, such as SAN, that they have clouded the functionality of RLS. For example, many people have started using the term <em class="italic">shared storage</em> as a proxy for <em class="italic">external storage</em> or possible for <em class="italic">SAN</em>. But external storage does not mean that it is or can be shared, and local storage does not mean that it is not or cannot be shared. </p>
			<p>The term replicated local storage refers to two or more computer systems which have local storage that is replicated between them. From the perspective of each computer system, the storage is locally attached, just like normal. But there is a process that replicates the data from one system to another, so that changes made on one appear on the other.</p>
			<p>Replicated local storage can be achieved in multiple ways. The simplest, and earliest for was to use <strong class="bold">Network RAID</strong>, that <a id="_idIndexMarker267"/>is RAID<a id="_idIndexMarker268"/> technology simply used over a network. <strong class="bold">Mirrored RAID</strong> (aka <strong class="bold">RAID 1</strong>) is the simplest technology for this and makes for the best example. </p>
			<p>There are two ways to handle this scenario, one is a hot/cold pair where one node is <em class="italic">hot</em> and has access to write to the storage and the other node(s) can read from it and, presumably, take over as the hot writeable node should the original hot node fail or relinquish control. This model is easy and is similar to many traditional models for shared storage on a SAN as well. This approach allows the use of regular (non-clustered) filesystems such as XFS or ZFS.</p>
			<p>The other approach is a live/live system where all nodes replicating the storage can read and write at any time. This requires the same clustered filesystem support that we would need with any shared block storage. Just like with a SAN being used at the same time by two nodes, the nodes in an RLS cluster will need to communicate by storing their activity data in a special area of the clustered file system. </p>
			<p>Replicated local storage can give us many of the benefits typically associated with a SAN or other external storage, namely the ability for multiple nodes to access data at once. While also having the benefits of locality including improved performance and resilience because there are fewer dependencies. Of course, the replication traffic has its own overhead, and this has to be considered. There are many ways that replication can be configured, some with very little overhead, some with a great deal.</p>
			<p>It is common to feel that replicated local storage is new or novel or in some way unusual. Nothing could be further from reality. In fact, what is rarely understood, is that for high reliability storage systems RLS is always used. Whether it is used locally (that is, directly attached to the compute systems) or if it is used remotely (meaning that the remote storage uses RLS to make itself more reliable), RLS is at the core of nearly any true high availability storage layer.</p>
			<p>RLS comes in multiple flavours, primarily Network RAID and RAIN. We could call it Network RAIN when used in this situation, but we do not. Unlike RAID, which is nearly always local only, RAIN is nearly always used in an RLS situation and so the network nature of it is nearly assumed, or at least the network option of it.</p>
			<p>RLS in so many forms on Linux that we cannot really talk about all of the options. We will have to focus on a few more common ones. RLS is an area where there are many open sources as well as commercial and proprietary solutions with a wide variety of performance, reliability, and features; and implemented in often very different ways. RLS can add <a id="_idIndexMarker269"/>rather a new level of complexity to any storage situation because you have to consider the local storage communication, the replication communication, any potential network communication between nodes and remote storage (that is local to another node), and how the algorithms and protocols interact with each other.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor086"/>DRBD</h2>
			<p>The first and <a id="_idIndexMarker270"/>simplest RLS <a id="_idIndexMarker271"/>technology on Linux is <strong class="bold">DRBD</strong> or the <strong class="bold">Distributed Replicated Storage System</strong>, which is a Network RAID layer baked right into the Linux kernel. Wikipedia states that DRBD is not <em class="italic">Network RAID</em>, but then describes it as exactly Network RAID. Whether it is or not might be little more than semantics, in practice it is indistinguishable from Network RAID in use, in practice, by description, and even under the hood. Like all RAID, DRBD consumes block devices and appears as a block device allowing it to be stacked anywhere in the middle of the storage stack just like regular RAID and LVMs.</p>
			<p>DRBD is built on a RAID 1 (mirrored RAID) mechanism and so allows for two to many nodes with each node getting a copy of the data. </p>
			<p>DRBD is very flexible and reliable, and because of its simplicity it is far easier for most system administrators to understand clearly as to how it works and fits into the storage infrastructure. But because DRBD is limited to full block device replication by way of mirroring, as<a id="_idIndexMarker272"/> is RAID 1, the ability to scale is quite limited. On its<a id="_idIndexMarker273"/> own, DRBD is very much focused on classic two node clusters or very niche use cases with a large number of compute nodes needing to share a small amount of identical data.</p>
			<h3>Making DRBD flexible</h3>
			<p>Because DRBD <a id="_idIndexMarker274"/>is just a software RAID <a id="_idIndexMarker275"/>tool, in effect, and because you have complete management of it, and because RAID acts as an LVM with total flexibility to sit anywhere in a stack, you can take DRBD and turn it into something far more scalable than it might first appear. But currently this process is all manual, although in theory you could script it or create tools to otherwise automate these kinds of procedures.</p>
			<p>One powerful technique that we can use is the concept of <em class="italic">staggering</em> our RAID 1 with extra logical block devices to mimic RAID 1E which operates essentially like RAID 1 but is scalable. This technique works by taking the physical storage space on an individual node and logically breaking it into two (or theoretically more) sections with an LVM technology. In a standard Network RAID setup, the entire space of the storage on node 1 is mirrored to the entire storage space on node 2. But now that we have split storage on each node, we mirror the first portion of node 1 to a portion of node 2; and node 2 does the same but with node 3; and node 3 does the same but with node 4; and this same process carries on indefinitely until whatever the terminal node number is does this with node 1 completing the <em class="italic">circle</em> and every machine has RAID 1 for its data, split between two other nodes as its mirrored pair. In this way, we can make a Network RAID 1 ring indefinitely large. </p>
			<p>This technique is powerful, to be sure. But it is extremely cumbersome to document and maintain. If you have a static cluster that never changes, it can work very well. But if you regularly grow or modify the cluster, it can be quite problematic.</p>
			<p>DRBD, and most Network RAID technologies, are typically blessed with good overall performance and, perhaps more importantly, rather predictable performance. DRBD, by its nature of presenting a final block device, is inherently local. In order to access DRBD resources remotely it would be necessary to use DRBD as a building block to a SAN device which would then be shared remotely. This is, of course, semantics only. DRBD is always local <a id="_idIndexMarker276"/>because to DRBD the SAN is the <a id="_idIndexMarker277"/>local compute node, the SAN interface is another layer higher up the proverbial stack and so while the SAN would be remote, DRBD would be local!</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor087"/>Gluster and CEPH</h2>
			<p>While two<a id="_idIndexMarker278"/> different <a id="_idIndexMarker279"/>technologies <a id="_idIndexMarker280"/>entirely, <strong class="bold">Gluster</strong> and <strong class="bold">CEPH</strong> are both free, open source, modern<a id="_idIndexMarker281"/> RAIN solutions designed for Linux that allow for high levels of reliability and high degrees of scalability. Both of these solutions at least offer the option of having storage be local to the compute node in question. Both are very complex solutions with many deployment options. We cannot simply assume that the use of either technology tells us that storage is going to be local or remote. Local is the more common application, by far, but both have options to directly build a remotely accessible and separate storage tier that is accessed over a network if designed to do so.</p>
			<p>These technologies are far too complex and full of options for us to dig into here. We will necessarily have to treat them at a very high level only, although this should be more than adequate for our needs.</p>
			<p>RAIN storage of this nature is the most common approach to handling large pools of servers (compute nodes) that will share a pool of storage. This technique gives the storage the opportunity to be local, to rebalance itself automatically in the event of a failure, but rarely will guarantee data locality. The storage across the group is a pool, only. So there can be an affinity for locality with data, but there is not the strict enforcement of locality as there is with DRBD. This gives more control to DRBD, but far more flexibility and better utilization with Gluster or CEPH.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor088"/>Proprietary and third-party open-source solutions</h2>
			<p>In addition to<a id="_idIndexMarker282"/> what comes baked in or potentially<a id="_idIndexMarker283"/> included with a Linux distribution are many third party components that you can install. Nearly all of these products will fall into the RAIN category and very in price, support, and capabilities. A few worth knowing the names of include <em class="italic">LizardFS</em>, <em class="italic">MooseFS</em>, and <em class="italic">Lustre</em>. </p>
			<p>It is impossible to cover the potential range of commercial products that may be or may become available. RAIN storage is an area of current development and still many vendors make products in this space but do not make them widely available. In some cases, you can find commercial RAID or RAIN systems that are only available when included with appliances of one type or another or when included in some other project. But all of these storage systems follow the same basic concepts and once you know what those are and how they can work, you can make good decisions about your storage systems even if you have not necessarily worked with a specific implementation previously.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor089"/>Virtualization abstraction of storage</h2>
			<p>It is all too easy <a id="_idIndexMarker284"/>to get lost when talking about storage and forget that most of the time, storage is not even something that we have to worry about as system administrators! At least not in the way that we have been approaching it.</p>
			<h3>The storage administrator</h3>
			<p>It is not uncommon<a id="_idIndexMarker285"/> in larger organizations to decide to separate storage and systems duties as there are so many complexities and nuances to storage that having a team that is dedicated to understanding them can make sense. If you have worked in a Fortune 500 environment you have probably witnessed this.</p>
			<p>Some of the biggest problems with this come from separating the people who deeply understand the workloads from some of the most important factors that determine the performance and reliability of those workloads. Separation often also requires that core architectural decisions be made politically rather than technically. If you use local storage, you cannot separate the storage and systems teams in any realistic way. Because of this, many organizations have used often terrible technical design decisions to create skill silos within their organizations without considering how this would impact workloads. The deployment of SAN technology is quite often done for this purpose. </p>
			<p>Irrespective of the efficacy of this approach, when in use this generally means that storage is taken out of the hands of systems administrators. This simplifies our role dramatically while simultaneously cutting us off at the knees when it comes to being able to provide ultimate value. We can request certain levels of performance or reliability and must trust that our needs will be met or that we, at the very least, will not be held accountable for their failures.</p>
			<p>Similarly, it is common to separate systems and platform teams. In this case we see the same effect. The platform team, which manages the hypervisor underneath the systems, will provide storage capacity to the systems team and systems must simple consume what is made available to them.</p>
			<p>In both of these <a id="_idIndexMarker286"/>cases storage is abstracted from the system and provided simple as a <em class="italic">blind block device(s)</em> to us on the systems team. When this happens, we still have to understand how underlying components might work, which questions to ask, and at the end of the day still have to manage file systems on top of the provided block devices. The block device interface remains the universal <em class="italic">hand off</em> interface from a storage or platform team to the systems team. </p>
			<p>An additional aside: the same thing will often happen to the platform team. They might have to take blind storage from a storage team, apply it to the hypervisor layer, then carve up that block device into smaller block devices to give to the systems team!</p>
			<p>In most cases today, our Linux systems will be virtualized in some manner. We have to understand storage all of the way down the stack because Linux itself may be the hypervisor (such as in the case of KVM) or be used to control the hypervisor (as is the case with Xen) or provide storage to a higher-level hypervisor (like VirtualBox) and in all these cases it is Linux managing every aspect of the storage experience potentially. Linux may also be being used to create a SAN device or storage layer in some other form. We have to understand storage inside and out, but the majority use cases will be that our Linux systems will be getting their storage from their hypervisor even if we are the managers of that hypervisor.</p>
			<p>While they can choose to behave in many different ways, most people set up hypervisors to act like an LVM layer for storage. They are a bit of a special case because they convert from block to filesystem, then back to block for the handoff to the virtual machine, but the concept remains the same. Some hypervisor setups will simply pass through a direct block connection to underlying storage whether a local disk, a SAN, or a logical volume from an LVM. These are all valid approaches and leave more options for the virtual machine to dictate how it will interact with the storage. But by and large having the block layer of storage terminate with the hypervisor, be turned into a filesystem, creating <em class="italic">block device containers</em> on top of the filesystem and allowing the virtual machines to consume those devices as regular block devices is what is expected from virtualization that many people actually refer to this artefact of virtualization approaches as being intrinsic to virtualization itself, which it is not. </p>
			<p>You can use this technique inside of a system as well. Examples of this include mounting file system file <a id="_idIndexMarker287"/>types<a id="_idIndexMarker288"/> like <em class="italic">qcow2</em>, <em class="italic">vhdx</em>, or <em class="italic">iso</em> files! Something that <a id="_idIndexMarker289"/>we do every day, but rarely think about or realize what we are actually doing.</p>
			<p>Obviously when getting our storage from the hypervisor, concerns about standard (non-replicated) local storage, replicated local storage, standard (non-replicated) remote storage, or replicated remote storage are all made at a different layer than the system, but the decisions are still made, and those decisions completely impact how our systems will ultimately run.</p>
			<p>We have learned <a id="_idIndexMarker290"/>about a lot of storage abstraction approaches and paradigms now with LVMs, RAID, and RAIN. Now we need to start to think about how we will put these technologies together to build our own storage solutions.</p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor090"/>Analyzing storage architectures and risk</h1>
			<p>Nothing <a id="_idIndexMarker291"/>creates <a id="_idIndexMarker292"/>more risk for our systems than our storage. That should go without saying, but it has to be said. Storage is where we, as system administrators, have our greatest opportunity to make a difference, and it is the place where we are mostly likely to fail and fail spectacularly.</p>
			<p>In order to address risks and opportunities in regard to storage, we must understand our entire storage stack and how every layer and component interact with each other. Storage can feel overwhelming, there are so many moving pieces and optional components.</p>
			<p>We can mitigate some of the overwhelming feelings by providing design patterns for success<a id="_idIndexMarker293"/> and<a id="_idIndexMarker294"/> understanding when different patterns should be considered.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor091"/>General storage architectures</h2>
			<p>There are two<a id="_idIndexMarker295"/> truly <a id="_idIndexMarker296"/>high-level <a id="_idIndexMarker297"/>axis in <strong class="bold">storage architecture</strong>: <em class="italic">local</em> versus <em class="italic">remote</em>, and <em class="italic">standard</em> versus <em class="italic">replicated</em>.</p>
			<p>Of course, the natural assumption for most people is to jump immediately to believing that replicated and remote are the obvious starting point. This is actually not true. This is probably the least sensible starting point for storage as it has the least likely to be useful combination of factors.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor092"/>Simple local storage: The brick</h2>
			<p>Believe it or not, the <a id="_idIndexMarker298"/>most commonly appropriate storage design for companies of all sizes is local, <strong class="bold">non-replicated storage</strong>! Bear in mind that <em class="italic">replicated</em> when we speak of storage architectures does <em class="italic">not</em> reference a lack of backups nor a local of local replication (such as RAID mirroring) but only refers to whether or not storage is replicated, in real or near-real time, to a second totally separate system.</p>
			<p>We will cover this again, and slightly differently, when we look at total system design rather than looking at storage in isolation. Like most things in life, keeping things simple generally makes the most sense. Replication sounds amazing, a must have feature, but replication costs money and often a lot of it and to do replication well often impacts performance, potentially dramatically.</p>
			<h3>Replicating disaster</h3>
			<p>A common <a id="_idIndexMarker299"/>mistake made in storage design is getting an emotional feeling that the more replication that we have, the more that we are shielded from disaster. To some degree this is true, replicated some files locally with RAID 1 does a lot to protect against an individual hard drive failing and remote replication can protect against an entire node failing, but neither does anything to protect against much more common problems like accidental file deletion, malicious file destruction, file corruption, or ransomware. </p>
			<p>If we do something simple, like delete a file that we shouldn't, then instantly our high-power replication mechanism will ensure that our deletion ripples through the entire system in a millisecond or two. Instead of protecting us, it might be the mechanism that replicates our mistake faster than we can react. Overbuilding replication mechanisms typically protects only against hardware failure and can quickly turn into a situation of diminishing returns. </p>
			<p>That first level of RAID might be highly valuable because hard drive failure remains a very real risk and even the tiniest drive hiccup can cause significant data loss. But replicating between nodes will only protect against entire system loss which is quite a bit less common. RAID protection is relatively cheap, often costing only a few hundred dollars to implement. Nodal replication, however, will require dramatically more hardware to <a id="_idIndexMarker300"/>achieve replication generally costing thousands or tens of thousands of dollars for a fraction of the protection that RAID is already providing.</p>
			<p>Mechanisms like RAID, especially RAID 1 (mirroring) are also extremely simple to implement and very straightforward. It is quite uncommon to encounter data loss caused by human error in mirrored RAID. The same cannot be said for replicated storage between nodes. There is far more to go wrong, and the chances of human error causes data loss is much higher. We do not simply mitigate risks by choosing to go this expensive route, we introduce other risks that we have to mitigate for as well.</p>
			<p>Many system administrators feel that they cannot use simple, local, non-replicated storage, and problems with company politics cannot be overlooked. If your company is going to <em class="italic">play politics</em> and blame you, as the system administrator, even when the mistake is not yours and your decision was the best one for the business, then you are forced to make dangerous decisions that are not in the interest of the profitability of the business. That is not something that a system administrator can control.</p>
			<p>As a system administrator, we can manage this political problem <em class="italic">in some cases</em> by presenting (and documenting well) risk and financial decisions to demonstrate why a decision that may ultimately have led to data loss to have still been the correct decision. No decision can every eliminate all risks, as IT professionals and especially as system administrators we are always making the decision as to how much risk to attempt to mitigate and at <a id="_idIndexMarker301"/>what financial cost should we do so.</p>
			<h3>Risk assessments</h3>
			<p>One of the <a id="_idIndexMarker302"/>hardest, yet most important, aspects of IT and especially systems administration is doing risk assessments to allow for proper planning. Risk is rarely taught either formally or organically. This is an area where nearly all businesses fail spectacularly and IT, where risk is absolutely key to everything that we do, is generally left with no training, no resources, and no support.</p>
			<p>Teaching risk is a career in and of itself, but a few techniques that we should be using all of the time can be covered here. At its core, risk is about assigning a cost that we can apply against projected profits. </p>
			<p>We have two key aspects of risk. One is the chance that something bad will happen, the second is the impact of that event. The first we can express as a matter of <em class="italic">happens X times per year</em> if you find that handy. The second can be expressed in monetary terms such as <em class="italic">it will cost $5,000</em>. If something will happen once a decade then you could say it is .1x per year. So, something that impacts us for five grands would have an annual cost of $500. This is ridiculously simplistic and not really how risk works. But it's an unbelievable useful tool in expressing risk decisions to management where they want millions of factors distilled to a single bottom line number.</p>
			<p>Now we have to take our numbers that show the cost of a risk mitigation strategy. For example, if we are going to implement a replication technology that costs $300/year in licensing and requires ten hours of system administration time per year at $120/hour as can project a cost of mitigation to be $1500/year. </p>
			<p>Next weeks need a mitigation effectiveness. Nothing is really one hundred percent. But a good replication strategy might be 95% or a typically one might be around 65% effective. With these numbers we can do some hard math.</p>
			<p>We know that we are at risk of losing roughly $500 per year. If we spend $1500 per year, we can 95% surely stop the $500 loss. $500 * .95 = $475. So, take $1500-$475=$1025 of loss per year caused by the risk mitigation strategy. These are numbers you can take to a CFO. Do this math, you should be able to show savings or protection, not a loss. If you show a loss then you really, really need to avoid that plan. It means that the risk protection mechanism is, for all intents and purposes, representative of a <em class="italic">disaster</em> simply by implementing it.</p>
			<p>Math, it might sound trite to say, but the average system administration and even the average CFO will often run from using basic math to show if an idea is good or bad and go purely on emotion. Using math will protect you. It means you can go to the CFO and CEO and stand your ground. You cannot argue with math. Show them the math, if they decide that the math is wrong, have them work the numbers. If they decide to ignore the math, well, you know what kind of organization you work for and you should really think long and hard about what kind of future there is at a company that thinks that profits are not their driving factor. And if something goes wrong in the future and you get blamed, you can pull out the math and ask, <em class="italic">if this was not the right decision, why did we not see it in the math when we made the decision?</em></p>
			<p>Nothing feels better that defending successfully a seemingly crazy decision that has been backed by math. Show that you are doing the best job possible. Do not just say it, do not make <a id="_idIndexMarker303"/>unsubstantiated claims. Use math and prove why you are making decisions. Elevate the state of decision making from guesswork to science.</p>
			<p>Not all workloads can simple be treated this simply. But vastly more than are normally assumed can. This should be your standard assumption unless you have solid math to show otherwise. Or you are dealing with a situation that goes beyond math, such as life support systems where uptime and reliability are more important than money can every describe. Otherwise, use math.</p>
			<p>This simplest of all storage approaches is easily thought of as <em class="italic">the brick</em>. It is simple, it is stable, it is reliable, it is easy to backup and restore, it is easy to understand and support. This solution is so effective today, especially with modern storage technologies and hardware, that<a id="_idIndexMarker304"/> I will state that it is appropriate for 85-90% of all production workloads, regardless of company size, and at least 99% of small business workloads.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor093"/>RLS: The ultra-high reliability solution</h2>
			<p>What workloads and <a id="_idIndexMarker305"/>situations do not make sense for the simple architecture above almost always fall into this category: <strong class="bold">replicated local storage</strong> (<strong class="bold">RLS</strong>). RLS allows for highly available storage with great performance at reasonable cost. Nothing will match the performance and cost of straight local storage that we mentioned first, but if you need higher availability and better reliability than that solution can provide, this is almost certainly the solution for you.</p>
			<p>RLS provides the highest level of reliability available because it has fewer moving parts. A remote high availability solution must necessarily deal with distance, cabling, and network protocols as additional components at a minimum, and typically with networking equipment (like switches) additionally, all above and beyond the risks of RLS. Remember that remote storage solutions wanting to accomplish true high availability are going to have to do so by using RLS locally in their own cluster, and then making that cluster of storage available remotely over a network so you have all of the complications and any potential problems with RLS, plus the overhead and risks of the remote access technology.</p>
			<p>If straight local storage with no remote replication takes ninety percent of all workloads, standard RLS much take ninety percent of what remains (the two should take about ninety nine<a id="_idIndexMarker306"/> percent together.) When doing proper planning, these two options are so simple, cost effective, and safe that they are just impossible to beat under normal circumstances. These are your break and butter options.</p>
			<h3>Alternative storage reliability</h3>
			<p>While RLS might <a id="_idIndexMarker307"/>feel like the absolute end all, be all of storage protection, it really is not. It is great when you have to rely on the storage layer to handle reliability. But in that regards, it is a kludge or a band aid, not the ultimate solution.</p>
			<p>In a perfect world we have storage mechanisms that are a layer higher than our actual storage layer with things like databases. A database management system is able to do a much better job at maintaining availability and coordinating data replication between nodes than a blind block replication mechanism can ever do. Putting replication where the application intelligence is just makes sense.</p>
			<p>Ideally, applications would use their databases as their layer for storage and state reliability and let intelligence systems that know how the data is used replicate what matters. Databases are one of the most ideal mechanisms for replication because they know the data that they have and are able to make good decisions about it.</p>
			<p>Because of this, many enterprise applications do not use any form of storage or even systems replication whatsoever. Using <em class="italic">unreliable</em> systems and highly reliable <em class="italic">applications</em> is a solid strategy and offers benefits that you can get in no other way. Because of this, we <a id="_idIndexMarker308"/>can sometimes ignore high availability needs at the raw storage layer and just focus on performance.</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor094"/>The lab environment: Remote shared standard storage</h2>
			<p>This architecture <a id="_idIndexMarker309"/>is very popular because it checks all of the boxes for salespeople: <em class="italic">expensive</em>, <em class="italic">risky</em>, and <em class="italic">confusing</em>. It is true, salespeople push this design more than any other because it generates so many ways to bill the customer for additional services while passing all responsibility on to the customer.</p>
			<p>All architectures have a legitimate place, more or less, in an ecosystem and this one is not an exception. But before we state a use case we should state where its benefits lie: non-replicated remote storage is <em class="italic">slower</em>, <em class="italic">riskier</em>, and more <em class="italic">expensive</em> (on the surface) than local storage mechanisms. Where do we find value for such a design?</p>
			<p>Primary the benefits are in lab, testing, and other environments where the value to data durability is negligible, but we can benefit from large environments where storage can be carved out judiciously to create maximum cost savings at a large scale.</p>
			<p>Nearly every large environment has a need for this kind of storage at some point in their collection of workloads. The key is to identify where this mix of needs is sensible and not to attempt to apply it where it is inappropriate. Because this kind of storage is cheap at large scale (but outrageous expensive at small scale) and because managers so often mistake remote shared storage for the <em class="italic">magic box that cannot fail</em> it is often used where it is least applicable. There is no magic, it is a poor technology for most production workloads and should be chosen only with extreme caution.</p>
			<p>The rule of thumb here is that you should only use this type of storage if you can decisively prove that it makes sense - that even standard reliability and performance are not valuable. If you have any doubts, or if the organization has any doubts, then choose a different storage architecture before making a mistake with this one means maximum risk. Performance is a <em class="italic">soft failure</em>, it is easy to correct after the fact and it normally has marginal impact if you get it wrong. Data loss is a <em class="italic">hard failure</em> where getting it wrong is not a graceful failure but a catastrophic failure and there is little ability to correct it later.</p>
			<p class="callout-heading">Fail gracefully</p>
			<p class="callout">We cannot always <a id="_idIndexMarker310"/>avoid failure. In fact, much of the time skirting failure is a critical part of our job. In order to make this work we have to understand how to fail well and a key component of that is the idea of <em class="italic">failing gracefully</em>. And in storage, this is an area where this concept is more pronounced than in other areas.</p>
			<p class="callout">The idea with failing gracefully is that if we fail, we want it to be in a small, incremental way rather than in a tragic, total disaster kind of way. Many storage decisions that we make are designed around this. We know that we might get things wrong. So, we want to make sure that we are as close to correct as possible, but also with taking into consideration <em class="italic">what if</em> we are wrong.</p>
			<p class="callout">In this way we tend heavily towards RAID 10 in RAID and towards local storage in architecture. We want solutions that, if we are wrong, result in us being too safe rather than losing the data because we thought that it would not matter.</p>
			<p>While it takes a lot more than just bad storage decisions to determine an entire architecture, remote non-replicated storage is the foundation point of the popular design used by vendors and <a id="_idIndexMarker311"/>resellers which they typically call a <strong class="bold">3-2-1 architecture</strong> and what IT <a id="_idIndexMarker312"/>practitioners called the <strong class="bold">Inverted Pyramid of Doom</strong>.</p>
			<p>This architecture is traditionally deployed broadly but is by no small margin the least likely architecture to ever me appropriate in a production environment. It is slow, it is complex, it is <a id="_idIndexMarker313"/>costly, and it carries the highest risks. It makes sense primarily in lab environments were recreating the data being stored is, at most, time consuming. This is an architecture truly designed around the needs of typically non-production workloads.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor095"/>The giant scale: Remote replicated storage</h2>
			<p>Our last main <a id="_idIndexMarker314"/>architecture to consider is the <em class="italic">biggest</em> of them all. Remote replicated storage. It might see like this would be the storage architecture that you would see in every enterprise, and while not exactly rare, it is seen far less commonly than you would guess.</p>
			<p>Remote replicated storage is the costliest to implement at small scale but can become quite affordable at large scale. It suffers from extra complexity over RLS (which means less reliable) and lower performance than either RLS or straight local storage and so only makes sense when cost savings is at a premium, but a certain degree of reliability is still warranted. A bit of a niche.</p>
			<p>Considering that the two local storage architectures were granted ninety nine percent of workload <a id="_idIndexMarker315"/>deployments (by me, of course) between them, this architecture gets most of the last percentage that is left, at least in production.</p>
			<h3>The safest system is only so safe</h3>
			<p>One of my more<a id="_idIndexMarker316"/> dramatic stories from my decades as a system administrator comes from a time that I was working at a university library as a consultant. I was brought in to work on some large-scale databases in this two-admin shop. The senior admin was out on vacation and only the junior admin was still around. The environment used a highly redundant and high availability SAN system on which all systems depended. There was an extreme amount of protection of this system from UPS to generators to hardware redundancy all at great expense.</p>
			<p>While I was there, the junior admin decided to do some maintenance on the SAN and, for whatever reason, accidentally clicked to delete all of the volumes on the SAN. This was, of course, an accident but a very careless one by someone who was assuming that every possible failure scenario was carefully guarded against. But all of the high availability, all of the redundancy, all of the special technology did nothing to address simple human error.</p>
			<p>With one click of her mouse, the entire library's computer systems were gone. The applications, the databases, the users, the logs. All of it. Everything was dependent on a single storage system and that system could be turned off or, in this case, deleted with essentially no effort by someone with access. All of the eggs were in a single backet that, while made very sturdily, had a big opening and could easily be turned upside down.</p>
			<p>To make matters far, far worse the junior system administrator was not emotionally prepared for the event and was so terrified of losing her job that she had to be hospitalized and all of the potential IT staff that might have been able to have stepped in to assist were instead engaged in getting her an ambulance. Through poor technological planning, and through poor human planning, an easily avoidable disaster that should have only turned into a minor recovery disaster turned into a huge outage. Luckily there were good backups, and the system was able to be restored relatively quickly. But it highlighted well just how much we often invest in protecting against mechanical failure and how little we address human frailty and, in reality, it is human error that is far more likely to be the cause of a disaster than machines failing.</p>
			<p><em class="italic">Storage architectures</em> and <em class="italic">risk</em> is the hardest part of storage design. Drilling down into filesystem details is generally fun and carries extremely little risk to us as system administrators. If we pick EXT4 when BtrFS would have been best, the penalty is probably nominal, so much so that we would never expect anyone to ever find out that we did not make the perfect decision. But choosing the wrong storage architecture could result in massive cost, large downtime, or big time data loss. </p>
			<p>We really have to take the time to understand the needs of our business and workloads. How much risk is okay, what performance do we need, how do we meet all needs at the optimum cost. If we do not know the answers then we need to find out. </p>
			<p>Best practice is, as<a id="_idIndexMarker317"/> always, to take the time to learn all business needs, learn all of the available factors and apply. But that is very hard to do in practice. Some rules of thumb to assist us will come in very handy.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor096"/>Storage best practices</h2>
			<p>Attempting to<a id="_idIndexMarker318"/> distill storage into <strong class="bold">best practices</strong> is rather hard. At the highest level, the fundamental rule to storage is that there are no shortcuts, you need to understand all aspects of the storage infrastructure, understand the workloads, and apply that combined knowledge allow with an understanding of institutional risk aversion to determine workload ideals.</p>
			<p>Going further, best practices include:</p>
			<ul>
				<li>RAID: If data is worth storing, it is worth having on RAID (or RAIN.) If you are questioning the value of having RAID (at a minimum) on your servers, then reconsider storing the data at all.</li>
				<li>RAID Physicality: Both hardware and software RAID implementations are equally viable. Determine what factors apply best to your needs.</li>
				<li>LVM: Like general virtualization which we will touch on in future chapters, storage virtualization is not about providing a concrete feature set that we need on day one. It is about providing mechanisms to protect against the unknown and to be flexible for whatever happens in the future. Unless you can present an incredible strong argument for what LVM is not needed, use it.</li>
				<li>Filesystem: Do not be caught up in hype or trends. Research the features that matter to you today and that are likely to protect you against the unknown in the future and use a filesystem that is reliable, robust, and tested where you can feel confident that your filesystem choice is not going to hamper you long term.</li>
				<li>Storage Architecture: Unless you can prove that you require or financially benefit significantly from remote storage, keep your storage access local. And unless you can demonstrate clear benefit from nodal replication, do not replicate between nodes. Simple trumps complex.</li>
			</ul>
			<p>As a system administrator you might deal with storage design decisions infrequently. But no matter how infrequent, storage design decisions have some of the most dramatic impacts on our <a id="_idIndexMarker319"/>long-term system compared to any other decisions that we make. Take your time to really determine what is needed for every workload.</p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor097"/>Storage example</h1>
			<p>We should step back <a id="_idIndexMarker320"/>and put together an example of how these pieces might fit together in a real life scenario. We cannot reasonably make examples for every common, let alone plausible, storage scenario but hopefully we can give a taste of what we are talking about in this chapter to make it all come together for you.</p>
			<p>To keep things reasonably simple, I am going to work as generically as possible with the absolutely most common setup found in small and medium businesses. Or at least what probably should be the most common setup for them.</p>
			<p>Smaller businesses generally benefit from keeping their designs quite simple. Lacking large, experienced staff and often at high risk from turnover, small businesses need systems that require less maintenance and those that can easily be maintained by consultants or staff that may not possess tribal knowledge of the environment.</p>
			<p>For these kinds of environments, and also for many larger ones, hardware RAID with hot and blind swappable drives are important. They allow hardware maintenance to be done by bench professionals without a necessity to engage systems administration tasks. This becomes extremely critical when dealing with colocation or other distant facilities. In these cases, someone from the IT department may have no way to be physically involved at all.</p>
			<p>So, we start with some general case assumptions. At the physical layer we have eight hard drives. These can be spinning drives, SSD, NVMe, any block device. It does not really matter. What matters is that we have multiple, but we want them to all act as a single unit.</p>
			<p>So, we add to this a hardware RAID controller. This RAID controller we use to attach all of the drives and set it to put them into an appropriate RAID level. This could be any number of options, but for this example we will say that they are in RAID 10.</p>
			<p>From the very beginning of our example, without having even installed Linux or anything of the sort, we have used the hardware of our system to implement our first two layers of storage! The physical devices, and the first abstraction layer.</p>
			<p>We will not show actually inserting the drives here, that is purely a manual process and chances are your server vendor has done this already for you, anyway. </p>
			<p>As for setting up the RAID itself, every controller and vendor is slightly different, but the basics are the same and the task is always extremely simple. That is much of the point of the RAID controller - to reduce the amount of knowledge and planning necessary around RAID operations to the bare minimum both up front and during operational phases. For our <a id="_idIndexMarker321"/>example here, to make things easier to demonstrate, we are going to assume that we are dealing with a single array that is <em class="italic">not</em> the array from which our operating system is running so that we can show some of the steps more easily from the command line. But this is just an example.</p>
			<p>Remember that hardware RAID is different for every vendor and potentially every product by a vendor. So you always need to be aware of exactly how your specific product works. </p>
			<p>Also, we should note that to the RAID controller, each drive attached to it is a block device. This is the unique case where the block device interface, pretending to be a physical hard drive, is actually and truly a hard drive! In every subsequent case we will be using software to implement the interface of a hard drive but the drive that we represent will be logical, not physical:</p>
			<p class="source-code">=&gt;ctrl slot=0 create type=ld drives=2I:1:5,2I:1:6,2I:1:7,2I:1:8 raid=1+0</p>
			<p>This is a real syntax for a real-world RAID controller. Typically, you will do this task graphically from a GUI. But sometimes you will want to use a command line utility. When possible, I work from the command line, it is more repeatable and far easier to document.</p>
			<p>Once we are past this phase of initial hardware configuration then we can proceed with the Linux specific portions of the example.</p>
			<p>Hardware RAID controllers typically create their own naming conventions in the /dev filesystem. In our example case, the controller uses the <strong class="source-inline">cciss</strong> syntax and created device <strong class="source-inline">c0d1</strong> under that system. All of these will vary depending on the control and configuration that you use.</p>
			<p>Next, we are going to create a logical volume layer on top of the RAID layer to allow us more flexibility in our storage system. To do so we must start by adding the newly created device to Linux' LVM system as <em class="italic">a physical device</em>. We do this with the <strong class="source-inline">pcvreate</strong> command and the path to our new device:</p>
			<p class="source-code"># pvcreate /dev/cciss/c0d1</p>
			<p>Very fast and<a id="_idIndexMarker322"/> easy. Now the LVM subsystem is aware of our new RAID array device. But of course, all that LVM knows and cares about is that it is a block device. That it is a RAID array specifically is actually not something that LVM can detect, nor does it matter. The point here is that it is abstracted and can be utilized the same no matter what it is. The speed, capacity, and safety characteristics are encapsulated in the RAID layer and now we can think of it purely as a hard drive as we move forward.</p>
			<p>Another interesting point here is that when using a hardware RAID controller this abstract and virtualized hard drive representation is only a logical hard drive, but as a block device it is actually physical! Mind blowing, I know. It's really hardware, it just is not a hardware hard drive. Ponder on that for a moment.</p>
			<p>How that our RAID array is under LVM's management we can add the drive to a volume group. In this example we will be adding it to a new volume group that we are creating just for the purposes of this example. We name this new group <strong class="source-inline">vg1</strong>. Here is an example command doing just this:</p>
			<p class="source-code"># vgcreate vg1 /dev/cciss/c0d1</p>
			<p>Okay, now we are getting somewhere. The capacity of the individual physical hard drives being combined by the RAID controller into a single logical drive under LVM control is now in a capacity pool or <em class="italic">volume group</em> where we can start to carve out actually useful subsets of that capacity to use on our server.</p>
			<p>With the volume group created, all that is left is to make the actual <em class="italic">logical volumes</em>. Remember that logical volumes have replaced <em class="italic">partitions</em> as the primary means of dividing a block device into consumable portions that we can use. For our example we are going to do the absolute simplest thing and tell the LVM system to make just one logical volume that is as large as possible; that is, using 100% of the available capacity of the volume group (which is currently at 0% utilization as this is the very first thing that we will have done with it.):</p>
			<p class="source-code"># lvcreate -l 100%FREE -n lv_data vg1</p>
			<p>This command tells LVM to create a new logical volume, using 100% of the free space that is available, in volume group <strong class="source-inline">vg1</strong>, and name the new logical volume <strong class="source-inline">lv_data</strong>. That is it. We now have a logical volume that we can use! It should be obvious that we could have made<a id="_idIndexMarker323"/> a smaller logical volume, say of 50% of the available space, and then made a second one, also of 100% of what was remaining after that to give us two equal sized logical volumes. </p>
			<p>Remember that an LVM system like the one found here in Linux, gives us flexibility that we would often lack if we were to apply a filesystem directly to the physical drives or even to the virtual drive presented by the RAID controller hardware. The LVM system lets us add more physical devices to the volume group, for example, which gives us more capacity for making logical volumes. It will let us resize the individual logical volumes, both growing or shrinking them. LVM will also allow us to snapshot a logical volume which is very useful for building a backup mechanism or preparing to do a risky system modification so that we can revert quickly. LVM does very important things.</p>
			<p>Now that <strong class="source-inline">lv_data</strong> has been created we will need, in most cases, to format it with a filesystem in order to make it truly useful. We will format with XFS. In the real world today, XFS would be the most likely to be recommended filesystem for general purpose needs:</p>
			<p class="source-code"># mkfs.xfs /dev/vg1/lv_data</p>
			<p>Very simple. In a few seconds we should have a fully formatted logical volume. In applying the filesystem format we stop the chain of block device interfaces and now present a filesystem interface which is the change that allows applications to use the storage in a standard way instead of using block devices as are used by storage system components.</p>
			<p>Of course, one last step is necessary, we have to mount the new filesystem to a folder to make it usable:</p>
			<p class="source-code"># mkdir /data </p>
			<p class="source-code"># mount /dev/vg1/lv_data /data</p>
			<p>That is it! We just implemented a multi-layer abstraction based storage system for one of the most common system scenarios. We have built an XFS file system on top of LVM logical volume management on top of a hardware RAID controller on top of multiple individual physical hard drives. </p>
			<p>Because each layer uses the block device interface, we could have mixed and matched so many more additional features. Like using two RAID controllers and merging their capacity with the volume group. Or making multiple volume groups. We could have made many <a id="_idIndexMarker324"/>logical volumes. We could have used software RAID (called MD RAID in Linux) to create RAID using the output of the two RAID controllers! The sky is really the limit, but practicality keeps us grounded.</p>
			<p>At this point if you <em class="italic">cd /data</em> you can use the new filesystem just as if it has always been there. That it is a new filesystem, that it is built on all these abstraction layers, that there are multiple physical devices making all of this magic happen is completely hidden from you at this point. It just works.</p>
			<p>Now in the past, if this was 2004, we would generally stop here and say that we have described what a real world server is likely going to look like if it is implemented well. But this is not 2004 by a long shot and we really need to talk more about how we are likely going to see our storage used in the most common scenarios. So today we need to think about how our virtualization layers are going to use this storage, because things get even more interesting here.</p>
			<p>We will assume that our /data filesystem that we just created will be used to store the drive images of a few virtual machines. Of course, these drive images are just individual files that we store in the filesystem. Very simple. No different than creating and storing a text file in /data (except VM drive images tend to be just a tad larger.)</p>
			<p>What is neat about a drive imagine (this could be a QCOW, VHD, ISO, or other) is that they sit on top of a filesystem but, when opened by a special driver that is able to read them, they present a block interface again! That is right. We have gone from block to block to block to block to filesystem to block again! In some unique cases we might not even use a hypervisor but might use this new block device file somewhere in our regular server. Windows does this commonly with VHD files as a way to pass data around in certain circumstances. MacOS does this as their standard means of creating installer packages. On Linux it is far less common but just as available.</p>
			<p>But assuming that we are doing something normal, we will assume that we are running a hypervisor, KVM almost certainly, and that the virtual machines that are going to run on KVM are going to use disk image files storage on our newly minted file system. In this case, much of what we have done here is likely to happen yet again inside of that virtual machine.</p>
			<p>Some portions would not be very sensible to recreate. The physical drives are already managed by a physical RAID controller. The speed, capacity, and reliability of the storage is already established by that system and does not need to be duplicated here. The standard <a id="_idIndexMarker325"/>approach is for a single drive image file to be presented to the operating system running in a virtual machine as a single block device. No different than how our operating system was presented with the block device from the hardware RAID controller.</p>
			<p>Now inside of the virtual machine we will often run through the same exercise. We add the presented block device as an LVM physical volume. Then we add that physical volume to a volume group. Then we carve out one or more logical volumes from that volume group. We then format that logical volume with our filesystem of choice and mount it. Of course, typically much of that is not done by hand as we have done here but rather than the installation process automating much of it.</p>
			<p>We can add more steps such as using MD RAID. Or we can use fewer, such as by skipping LVM entirely. We could do all of the same steps with just a single hard drive and no RAID controller. This would be far less powerful physically, but all of the examples would work the same at a technical level. We could use VLM on the physical machine but not in the virtual machines, or vice versa! The flexibility is there to do what is needed. It is all about understanding how block devices can be layered, how a filesystem goes on a block device and how block files can turn a filesystem back into block devices!</p>
			<p>Abstraction and encapsulation are amazingly powerful tools in our IT arsenal and rarely are they so tangible.</p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor098"/>Summary </h1>
			<p>If you have survived to the end of this chapter and are still hanging in with me, congrats, we made it! Storage is a big deal when it comes to systems administration and likely no other area that you manage will you be able to bring as much value to your organization. </p>
			<p>We have covered storage basics building on the concepts of block device interfaces, abstraction techniques, filesystems and their interfaces, and used these concepts to investigate multi-device redundancy and how it can be used to build complex and robust data storage, and how storage access across devices can be handled to meet any potential need. My goal here has been to give you the knowledge necessary to think carefully on your own about your storage needs for any given workload, and an understanding of availability technologies and how you can apply them to meet those goals most effectively.</p>
			<p>Never again should you see storage as a magic black box or a daunting task that you dread to tackle. Instead, you can see storage as an opportunity to shine and to demonstrate how proper system administration best practices can be applied to maximize whatever storage factors matter most for your workload without simply throwing money at the challenge or worse, simply ignoring it and hoping that you can find another job before things fall apart.</p>
			<p>In our next chapter, we are going to look at system architecture at an even higher level. Many of the most interesting concepts from this chapter will be recurring there. System architecture relies on storage architecture very heavily and many redundancy and system protection paradigms are shared. It can be quite excited to see how good storage design elements can lead to a truly high performance, highly available, and cost-effective final solution.</p>
		</div>
	</div></body></html>