<html><head></head><body>
		<div id="_idContainer061">
			<h1 id="_idParaDest-175"><em class="italic"><a id="_idTextAnchor184"/>Chapter 13</em>: Understanding cgroup Version 2</h1>
			<p>In this chapter, we'll look at <strong class="bold">cgroup</strong> <strong class="bold">Version 2</strong>. We'll see how it's different from <strong class="bold">cgroups</strong> <strong class="bold">Version 1</strong>, and how it improves upon Version 1. After that, we'll take a brief look at how to work with it. We'll wrap up by converting the <strong class="bold">AlmaLinux</strong> machine to use cgroup Version 2. Learning how to use cgroup Version 2 will be very helpful to developers of new software, as well as to <strong class="bold">Linux</strong> administrators who want to be prepared for the future.</p>
			<p>By the way, that's not a typo that you see in the chapter title. One of the Version 2 changes is in the official name of the technology. So, we have <em class="italic">cgroups</em> Version 1, and <em class="italic">cgroup</em> Version 2. Strange, but true. (I didn't explain this before, because I didn't want to create more confusion).</p>
			<p>Specific topics in this chapter include:</p>
			<ul>
				<li>Understanding the need for Version 2</li>
				<li>Understanding the improvements in Version 2</li>
				<li>Setting resource limits on rootless containers</li>
				<li>Understanding <strong class="bold">cpuset</strong></li>
				<li>Converting RHEL 8-type distros to cgroup version 2</li>
			</ul>
			<p>With the introduction out of the way, let's get started.</p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor185"/>Technical requirements</h1>
			<p>This time, we'll use a <strong class="bold">Fedora</strong> <strong class="bold">virtual machine</strong> that's set to use as many CPU cores and as much memory as you can spare. (I'll still have mine set to use four CPU cores and eight GB of memory.) So, download your favorite spin of Fedora, and create a virtual machine from it.</p>
			<p>For the <em class="italic">Understanding cpuset</em> section, it would be helpful to have a host computer with at least two physical CPUs. I realize that not many people will have access to a machine like that, and that's okay. I do have such a machine, so I can show you what you need to see.</p>
			<p>We'll also use the AlmaLinux machine for a couple of brief demos.</p>
			<p>All right, let's get with it.</p>
			<p>Check out the following link to see the Code in Action video: <a href="https://bit.ly/3xJNcDx">https://bit.ly/3xJNcDx</a></p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor186"/>Understanding the need for Version 2</h1>
			<p>As good as cgroups <a id="_idIndexMarker454"/>Version 1 is, it does have a few rather serious flaws. Let's take a quick look.</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor187"/>Version 1 complexity</h2>
			<p>To begin <a id="_idIndexMarker455"/>with, Version 1 has too many resource controllers and too many attributes per controller. Very few people use more than just the <em class="italic">Big Three</em> controllers that we covered in <a href="B17491_12_Final_NM_ePub.xhtml#_idTextAnchor164"><em class="italic">Chapter 12</em></a>, Controlling Resource Usage with cgroups Version 1. Some unnecessary controllers have been removed from Version 2.</p>
			<p>There's also too much complexity with the Version 1 hierarchy, which makes it a bit confusing to use and can hurt performance. To see what I mean, think back about what we saw in the Version 1 <strong class="source-inline">cgroup</strong> filesystem. You saw that each resource controller has its own subdirectory, as we see here:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/Figure_13.1_B17491.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.1<a id="_idTextAnchor188"/> â€“ The resource controllers for version 1 on Ubuntu</p>
			<p>In <a href="B17491_12_Final_NM_ePub.xhtml#_idTextAnchor164"><em class="italic">Chapter 12</em></a>, <em class="italic">Controlling Resource Usage with cgroups Version 1</em>, we also saw that when we set a <strong class="source-inline">CPUQuota</strong> for <a id="_idIndexMarker456"/>Vicky, it appeared in her <strong class="source-inline">user-1001.slice </strong> subdirectory that's under the <strong class="source-inline">cpu/user.slice </strong> subdirectory, like this:</p>
			<p class="source-code">vicky@ubuntu2004:/sys/fs/cgroup/cpu/user.slice/user-1001.slice$ cat cpu.cfs_quota_us </p>
			<p class="source-code">200000</p>
			<p class="source-code">vicky@ubuntu2004:/sys/fs/cgroup/cpu/user.slice/user-1001.slice$</p>
			<p>Then, when we set a <strong class="source-inline">MemoryMax</strong> restriction, it showed up under the <strong class="source-inline">memory</strong> subdirectory, like this:</p>
			<p class="source-code">vicky@ubuntu2004:/sys/fs/cgroup/memory/user.slice/user-1001.slice$ cat memory.max_usage_in_bytes </p>
			<p class="source-code">30994432</p>
			<p class="source-code">vicky@ubuntu2004:/sys/fs/cgroup/memory/user.slice/user-1001.slice$</p>
			<p>Okay, you'll never guess what happened when we set Vicky's <strong class="source-inline">BlockIOReadBandwidth</strong> parameter. That's right, it shows up under the <strong class="source-inline">blkio</strong> subdirectory, like this:</p>
			<p class="source-code">vicky@ubuntu2004:/sys/fs/cgroup/blkio/user.slice/user-1001.slice$ cat blkio.throttle.read_bps_device </p>
			<p class="source-code">8:0 1000000</p>
			<p class="source-code">vicky@ubuntu2004:/sys/fs/cgroup/blkio/user.slice/user-1001.slice$</p>
			<p>So you see, Vicky's settings are in three different places, which means that the operating system has to look in all three places to get them all.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">I accidentally used Vicky's login window instead of my own for these screenshots, but that's okay. It shows you that Vicky can see the settings in her own cgroup files. Of course, she<a id="_idIndexMarker457"/> can't change the settings, because she doesn't have the correct root privileges.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor189"/>Version 1 attribute filenames</h2>
			<p>Another<a id="_idIndexMarker458"/> problem with Version 1 is that there's no consistent naming convention for the attribute files of the different resource controllers. For example, setting <strong class="source-inline">MemoryMax</strong> on Version 1 places a value in the <strong class="source-inline">memory.max_usage_in_bytes</strong> file, as we see here for Vicky:</p>
			<p class="source-code">donnie@ubuntu2004:/sys/fs/cgroup/memory/user.slice/user-1001.slice$ cat memory.max_usage_in_bytes </p>
			<p class="source-code">30789632</p>
			<p class="source-code">donnie@ubuntu2004:/sys/fs/cgroup/memory/user.slice/user-1001.slice$</p>
			<p>However,Vicky's <strong class="source-inline">CPUQuota</strong> setting shows up in the <strong class="source-inline">cpu.cfs_quota_us</strong> file, as we see here:</p>
			<p class="source-code">donnie@ubuntu2004:/sys/fs/cgroup/cpu/user.slice/user-1001.slice$ cat cpu.cfs_quota_us </p>
			<p class="source-code">200000</p>
			<p class="source-code">donnie@ubuntu2004:/sys/fs/cgroup/cpu/user.slice/user-1001.slice$</p>
			<p>As we'll see in a few moments, naming conventions are a lot more consistent with Version 2.</p>
			<p>Okay, let's get to<a id="_idIndexMarker459"/> the real root of the problem by talking about <em class="italic">rootless containers</em>.</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor190"/>No support for rootless containers</h2>
			<p>As you <a id="_idIndexMarker460"/>saw in <a href="B17491_05_Final_NM_ePub.xhtml#_idTextAnchor063"><em class="italic">Chapter 5</em></a>, <em class="italic">Creating and Editing Services</em>, we can<a id="_idIndexMarker461"/> use <strong class="bold">Red Hat</strong>'s new <strong class="source-inline">podman</strong> to create and run <strong class="bold">Docker</strong> containers without either root privileges or membership in the docker group. However, with cgroups Version 1, it's not possible for a non-privileged user to set runtime resource limits when creating a container. For example, let's go to the AlmaLinux machine and create a new user account for my buddy, Pogo, by doing:</p>
			<p class="source-code">[donnie@localhost ~]$ sudo useradd pogo</p>
			<p class="source-code">[donnie@localhost ~]$ sudo passwd pogo</p>
			<p>Look at what happens to the poor guy when he tries to create a container with a 50% <strong class="source-inline">CPUQuota</strong>:</p>
			<p class="source-code">[pogo@localhost ~]$ podman run -it --cpu-period=100000 --cpu-quota=50000 ubuntu /bin/bash</p>
			<p class="source-code">Error: OCI runtime error: container_linux.go:367: starting container process caused: process_linux.go:495: container init caused: process_linux.go:458: setting cgroup config for procHooks process caused: cannot set cpu limit: container could not join or create cgroup</p>
			<p class="source-code">[pogo@localhost ~]$</p>
			<p>Alas, poor Pogo doesn't have root privileges. So, he can create and run <strong class="source-inline">podman</strong> containers, but he can't set any resource limits for them.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Actually, with cgroups Version 1, it <em class="italic">is</em> possible for a non-privileged user to set runtime resource limits on rootless <strong class="source-inline">podman</strong> containers. But, it requires that you delegate this ability to non-root users. With cgroups Version 1, that constitutes a security hazard because it could allow someone to create a container that could freeze your system. So, we're not going to do it (We'll talk more about delegation in just a bit).</p>
			<p>Now, let's contrast<a id="_idIndexMarker462"/> that with what we see on the Fedora machine, which is running a pure cgroup Version 2 environment.</p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor191"/>Understanding the improvements in cgroup Version 2</h1>
			<p>Version 2 is a bit<a id="_idIndexMarker463"/> more streamlined and simpler to understand. At the time of writing, Fedora, <strong class="bold">Arch</strong>, and <strong class="bold">Debian 11</strong> are the only three Linux distros of which I know that run cgroup Version 2 by default (that will likely change by the time you read this).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">It is possible to convert RHEL 8-type <a id="_idIndexMarker464"/>distros, such<a id="_idIndexMarker465"/> as <strong class="bold">Alma</strong> and <strong class="bold">Rocky</strong>, over to a pure Version 2 setup. Unfortunately, the RHEL-type distros use an older implementation of Version 2 that still doesn't have all of the resource controllers that we need enabled. So, to see everything that we need to see, we'll use Fedora.</p>
			<p>To begin, let's log in to the Fedora machine and create a user account for my buddy Pogo (Pogo is the awesome opossum who comes in through my cat door at night to chow down on the cat food â€“ Yes,seriously.) Then,Then, have Pogo log in from a remote terminal (Note that on Fedora, you might have to start and enable the <strong class="source-inline">sshd</strong> service first.) On your own local terminal, look at the cgroup filesystem, which looks like this:</p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/Figure_13.2_B17491.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.2 â€“ The cgroup filesystem on Fedora</p>
			<p>The attribute files <a id="_idIndexMarker466"/>that we see here are for the global settings, which we don't really care about for now. What I really want you to see is under the <strong class="source-inline">system.slice</strong> and <strong class="source-inline">user.slice</strong> subdirectories. Let's look at the <strong class="source-inline">user.slice</strong> subdirectory first.</p>
			<p>Under the <strong class="source-inline">user.slice</strong> subdirectory, you'll see lots of files for things that can be set at the user slice level. At the bottom, we see the subdirectories for both Pogo and me, as we see here:</p>
			<p class="source-code">[donnie@fedora user.slice]$ ls -l</p>
			<p class="source-code">total 0</p>
			<p class="source-code">-r--r--r--. 1 root root 0 Jul 30 16:55 cgroup.controllers</p>
			<p class="source-code">. . .</p>
			<p class="source-code">. . .</p>
			<p class="source-code">-rw-r--r--. 1 root root 0 Jul 30 16:55 pids.max</p>
			<p class="source-code">drwxr-xr-x. 5 root root 0 Jul 30 17:24 user-1000.slice</p>
			<p class="source-code">drwxr-xr-x. 4 root root 0 Jul 30 17:09 user-1001.slice</p>
			<p class="source-code">[donnie@fedora user.slice]$</p>
			<p>Each of<a id="_idIndexMarker467"/> these user slice subdirectories contains attribute files for all of the resource controllers, as we see here:</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/Figure_13.3_B17491.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.3 â€“ Resource controllers for user-1001.slice</p>
			<p>So now, all of the<a id="_idIndexMarker468"/> applicable settings for a particular user would be contained in the <strong class="source-inline">user slice</strong> directory for that user. The operating system now only has to look in one place to get all of the settings for a user. </p>
			<p>Next, have Pogo log in from a remote terminal. Then, in your own terminal window, set <strong class="source-inline">CPUQuota</strong> for Pogo. The good news is that the command to do that is exactly the same as it was in Version 1. If you don't remember, the command is:</p>
			<p class="source-code">[donnie@fedora ~]$ sudo systemctl set-property user-1001.slice CPUQuota=40%</p>
			<p class="source-code">[sudo] password for donnie: </p>
			<p class="source-code">[donnie@fedora ~]$</p>
			<p>Then, do a <strong class="source-inline">daemon-reload</strong>. Once that's done, look at the <strong class="source-inline">cpu.max</strong> file in Pogo's user slice directory, which should look like this:</p>
			<p class="source-code">[donnie@fedora ~]$ cd /sys/fs/cgroup/user.slice/user-1001.slice/</p>
			<p class="source-code">[donnie@fedora user-1001.slice]$ cat cpu.max</p>
			<p class="source-code">40000 100000</p>
			<p class="source-code">[donnie@fedora user-1001.slice]$</p>
			<p>The <strong class="source-inline">40000</strong> figure <a id="_idIndexMarker469"/>represents the 40% <strong class="source-inline">CPUShare</strong>, and <strong class="source-inline">100000</strong> represents the time interval over which <strong class="source-inline">CPUShare</strong> is measured. The default time setting, which you see here, is 100 milliseconds (you can change that time interval, but you'll likely never need to).</p>
			<p>You would also set Pogo's memory limit the same you did with Version 1, as we see here:</p>
			<p class="source-code">[donnie@fedora user-1001.slice]$ sudo systemctl set-property user-1001.slice MemoryMax=1G</p>
			<p class="source-code">[sudo] password for donnie: </p>
			<p class="source-code">[donnie@fedora user-1001.slice]$</p>
			<p>This time, the setting shows up in Pogo's <strong class="source-inline">memory.max</strong> file, as we see here:</p>
			<p class="source-code">[donnie@fedora user-1001.slice]$ cat memory.max</p>
			<p class="source-code">1073741824</p>
			<p class="source-code">[donnie@fedora user-1001.slice]$</p>
			<p>Now, understand that this <strong class="source-inline">MemoryMax</strong> setting is a hard limit. In other words, Pogo absolutely cannot use more memory than what <strong class="source-inline">MemoryMax</strong> allocates. If you look in the <strong class="source-inline">systemd.resource-control</strong> man page, you'll see other options that are available for Version 2 and that aren't available for Version 1. (Note that this man page always refers to cgroup Version 2 as the <em class="italic">unified control group hierarchy</em>.) One such parameter is <strong class="source-inline">MemoryHigh</strong>, which is more of a soft limit. <strong class="source-inline">MemoryHigh</strong> would allow Pogo to exceed his memory allocation if it's unavoidable, but his processes would be throttled until his memory usage goes back down to within his allocation. This makes it easier for a system to deal with temporary spikes in memory usage for any given process or user.</p>
			<p>Version 2 also has the <strong class="source-inline">MemoryLow</strong> and <strong class="source-inline">MemoryMin</strong> parameters, which cause a process to reclaim memory from unprotected processes if the amount of free memory for the protected process drops to the specified threshold. If you want to control swap memory usage, Version 2 lets you do that with the <strong class="source-inline">MemorySwapMax</strong> parameter.</p>
			<p>Setting limits on <strong class="source-inline">block I/O</strong> usage is a bit different, because the parameter names have changed. To<a id="_idIndexMarker470"/> limit Pogo's read bandwidth, we'll first use <strong class="source-inline">df</strong> to see what drive devices we have, as we see here:</p>
			<p class="source-code">[donnie@fedora ~]$ df -h | grep -v tmpfs</p>
			<p class="source-code">FilesystemÂ Â Â Â Â Â SizeÂ Â Used Avail Use% Mounted on</p>
			<p class="source-code">/dev/sda2Â Â Â Â Â Â Â Â 21GÂ Â 2.6GÂ Â Â 18GÂ Â 13% /</p>
			<p class="source-code">/dev/sda2Â Â Â Â Â Â Â Â 21GÂ Â 2.6GÂ Â Â 18GÂ Â 13% /home</p>
			<p class="source-code">/dev/sda1Â Â Â Â Â Â Â 976MÂ Â 256MÂ Â 654MÂ Â 29% /boot</p>
			<p class="source-code">[donnie@fedora ~]$</p>
			<p>The desktop versions of Fedora now use the <strong class="source-inline">btrfs</strong> filesystem by default, which is why we just see regular drive partitions instead of logical volumes. (There's no need to use logical volumes with <strong class="source-inline">btrfs</strong> because it has its own built-in drive pooling mechanism.) If you're using <strong class="bold">Fedora Server</strong> though, you'll still see <strong class="source-inline">ext4</strong> and logical volumes. Anyway, we see that the <strong class="source-inline">/home/</strong> directory is mounted on the <strong class="source-inline">/dev/sda</strong> drive, which of course is where Pogo's home directory is. (As we saw with version 1, you can set a rate limit on an entire drive, but not on a specific partition of that drive.)</p>
			<p>We'll now use the <strong class="source-inline">IOReadBandwidthMax</strong> parameter to limit the rate at which Pogo can transfer files, like this:</p>
			<p class="source-code">[donnie@fedora ~]$ sudo systemctl set-property user-1001.slice IOReadBandwidthMax="/dev/sda 1M"</p>
			<p class="source-code">[sudo] password for donnie: </p>
			<p class="source-code">[donnie@fedora ~]$</p>
			<p>Note that because there's a blank space in the <strong class="source-inline">/dev/sda 1M</strong> parameter, you have to surround it with a pair of double-quotes (<strong class="source-inline">""</strong>) when you set this from the command line.</p>
			<p>Next, look at the <strong class="source-inline">io.max</strong> file in Pogo's user slice directory, which should look like this:</p>
			<p class="source-code">[donnie@fedora user-1001.slice]$ cat io.max</p>
			<p class="source-code">8:0 rbps=1000000 wbps=max riops=max wiops=max</p>
			<p class="source-code">[donnie@fedora user-1001.slice]$</p>
			<p>Here, we see another benefit of using Version 2. Instead of having four separate attribute files for the four available parameter settings, as we had with Version 1, Version 2 places the <strong class="source-inline">IOReadBandwidthMax</strong>, <strong class="source-inline">IOWriteBandwidthMax</strong>, <strong class="source-inline">IOReadIOPSMax</strong>, and <strong class="source-inline">IOWriteIOPSMax</strong> settings all in one file.</p>
			<p>Also, note<a id="_idIndexMarker471"/> that the <strong class="source-inline">8:0</strong> we see at the beginning of the line in this <strong class="source-inline">io.max</strong> file represents the major and minor numbers of the entire <strong class="source-inline">sda</strong> drive, as we see here:</p>
			<p class="source-code">[donnie@fedora dev]$ pwd</p>
			<p class="source-code">/dev</p>
			<p class="source-code">[donnie@fedora dev]$ ls -l sd*</p>
			<p class="source-code">brw-rw----. 1 root disk 8, 0 Jul 31 14:30 sda</p>
			<p class="source-code">brw-rw----. 1 root disk 8, 1 Jul 31 14:30 sda1</p>
			<p class="source-code">brw-rw----. 1 root disk 8, 2 Jul 31 14:30 sda2</p>
			<p class="source-code">[donnie@fedora dev]$</p>
			<p>Okay, if you really want to, you can play around with <strong class="source-inline">stress-ng</strong> for Pogo as you did for Vicky in <a href="B17491_12_Final_NM_ePub.xhtml#_idTextAnchor164"><em class="italic">Chapter 12</em></a>, <em class="italic">Controlling Resource Usage with cgroups Version 1</em>, but I'm not going to repeat the directions for that here. </p>
			<p>The main thing to know about setting limits on services is that each system service has its own subdirectory under the <strong class="source-inline">/sys/fs/cgroup/system.slice/</strong> directory, as we see here:</p>
			<p class="source-code">[donnie@fedora system.slice]$ pwd</p>
			<p class="source-code">/sys/fs/cgroup/system.slice</p>
			<p class="source-code">[donnie@fedora system.slice]$ ls -l</p>
			<p class="source-code">total 0</p>
			<p class="source-code">drwxr-xr-x. 2 root root 0 Jul 31 14:30Â Â abrtd.service</p>
			<p class="source-code">drwxr-xr-x. 2 root root 0 Jul 31 14:30Â Â abrt-journal-core.service</p>
			<p class="source-code">drwxr-xr-x. 2 root root 0 Jul 31 14:30Â Â abrt-oops.service</p>
			<p class="source-code">drwxr-xr-x. 2 root root 0 Jul 31 14:30Â Â abrt-xorg.service</p>
			<p class="source-code">drwxr-xr-x. 2 root root 0 Jul 31 14:30Â Â alsa-state.service</p>
			<p class="source-code">drwxr-xr-x. 2 root root 0 Jul 31 14:31Â Â atd.service</p>
			<p class="source-code">drwxr-xr-x. 2 root root 0 Jul 31 14:30Â Â auditd.service</p>
			<p class="source-code">. . .</p>
			<p class="source-code">. . . </p>
			<p>Within each of <a id="_idIndexMarker472"/>these subdirectories, you'll see the same attribute files that you saw for Pogo. Also, the procedure for setting limits on services is the same as it was for Version 1, so I also won't repeat any of that.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Be aware that certain parameters that you may be used to using under cgroups Version 1 have been renamed for cgroup Version 2. Specifically, the <strong class="source-inline">CPUShares</strong>, <strong class="source-inline">StartupCPUShares</strong>, and <strong class="source-inline">MemoryLimit</strong> parameters in Version 1 have been replaced by <strong class="source-inline">CPUWeight</strong>, <strong class="source-inline">StartupCPUWeight</strong>, and <strong class="source-inline">MemoryMax</strong>, respectively. Also, all Version 1 parameter names that have the <strong class="source-inline">BlockIO</strong> prefix have been replaced with parameter names that have the <strong class="source-inline">IO</strong> prefix.</p>
			<p>All righty, now that we know about the cgroup Version 2 filesystem, let's see if we can let Pogo set<a id="_idIndexMarker473"/> some resource limits on a rootless container.</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor192"/>Setting resource limits on rootless containers</h1>
			<p>A few <a id="_idIndexMarker474"/>moments ago, I told<a id="_idIndexMarker475"/> you about the concept of <em class="italic">delegation</em>. Normally, you need root privileges in order to set any resource limits. However, you can delegate this chore to non-privileged users. The best news is that unlike delegation under cgroups Version 1, delegation under cgroup Version 2 is perfectly safe.</p>
			<p>To see the default setting, open the <strong class="source-inline">/lib/systemd/system/user@.service</strong> file, and look for the <strong class="source-inline">Delegate=</strong> line in the <strong class="source-inline">[Service]</strong> section. The applicable lines should look like this:</p>
			<p class="source-code">[Service]</p>
			<p class="source-code">. . .</p>
			<p class="source-code">. . .</p>
			<p class="source-code">Delegate=pids memory</p>
			<p class="source-code">. . .</p>
			<p class="source-code">. . .</p>
			<p>By default, Fedora only allows non-privileged users to set resource limits for memory and for the maximum number of running processes. We need to edit that to include the <strong class="source-inline">cpu</strong>, <strong class="source-inline">cpuset</strong>, and <strong class="source-inline">io</strong> resource controllers, like this:</p>
			<p class="source-code">[donnie@fedora ~]$ sudo systemctl edit --full user@.service</p>
			<p>Edit the <strong class="source-inline">Delegate=</strong> line so that it will look like this:</p>
			<p class="source-code">Delegate=pids memory io cpu cpuset</p>
			<p>Save the file and do a <strong class="source-inline">daemon-reload</strong>. Note that if any users are logged in, they might have to log out and log back in again for this to take effect.</p>
			<p>Keep Pogo's original login window open, and then open a second one for him. He'll create a container in one window, and look at the container information in the second window. Have Pogo create an <strong class="bold">Ubuntu</strong> container, like this:</p>
			<p class="source-code">[pogo@fedora ~]$ podman run -it --cpu-period=100000 --cpu-quota=50000 ubuntu /bin/bash</p>
			<p class="source-code">root@207a59e45e9b:/#</p>
			<p>Pogo is setting a <strong class="source-inline">CPUQuota</strong> of <strong class="source-inline">50%</strong> over a 100-millisecond time interval. In Pogo's other login window, have him view the information about his container. He'll first do a <strong class="source-inline">podman ps</strong>, like this:</p>
			<p class="source-code">[pogo@fedora ~]$ podman ps</p>
			<p class="source-code">CONTAINER IDÂ Â IMAGEÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â COMMANDÂ Â Â Â Â CREATEDÂ Â Â Â Â Â Â Â Â STATUSÂ Â Â Â Â Â Â Â Â Â Â Â Â PORTSÂ Â Â Â Â Â Â NAMES</p>
			<p class="source-code">207a59e45e9bÂ Â docker.io/library/ubuntu:latestÂ Â /bin/bashÂ Â Â 55 minutes agoÂ Â Up 55 minutes agoÂ Â Â Â Â Â Â Â Â Â Â Â Â Â funny_zhukovsky</p>
			<p class="source-code">[pogo@fedora ~]$</p>
			<p>Pogo didn't <a id="_idIndexMarker476"/>assign <a id="_idIndexMarker477"/>a name to this container, so <strong class="source-inline">podman</strong> randomly assigned the name <strong class="source-inline">funny_zhukovsky</strong>. (Remember that Pogo is a opossum, so don't be too hard on him for forgetting to assign a name.) Now, have Pogo inspect the inner workings of this container, using whatever container name that came up for you:</p>
			<p class="source-code">[pogo@fedora ~]$ podman inspect funny_zhukovsky</p>
			<p>There's a lot of output here, but you only need to look at two lines. Keep scrolling down, and you should find them. They should look like this:</p>
			<p class="source-code">"CpuPeriod": 100000,</p>
			<p class="source-code">"CpuQuota": 50000,</p>
			<p>So far, so good. But, here's where things get a bit tricky. It's just that the attribute file for this container is buried deep within the cgroup filesystem where it's hard to find. Fortunately, Pogo is a more clever opossum than I thought he was, so he found a way to cheat. He knew that the <strong class="source-inline">50000</strong> text string would only show up in one of the attribute files under his user slice directory, so he used <strong class="source-inline">grep</strong> to find it, like this:</p>
			<p class="source-code">[pogo@fedora ~]$ cd /sys/fs/cgroup/user.slice/user-1001.slice/</p>
			<p class="source-code">[pogo@fedora user-1001.slice]$ grep -r '50000' *</p>
			<p class="source-code">user@1001.service/user.slice/libpod-207a59e45e9b14c3397d9904b41ba601dc959d85962e6ede45a1b54463ae731b.scope/container/cpu.max:50000 100000</p>
			<p class="source-code">[pogo@fedora user-1001.slice]$</p>
			<p>At last, Pogo found the attribute file:</p>
			<p class="source-code">[pogo@fedora container]$ pwd</p>
			<p class="source-code">/sys/fs/cgroup/user.slice/user-1001.slice/user@1001.service/user.slice/libpod-207a59e45e9b14c3397d9904b41ba601dc959d85962e6ede45a1b54463ae731b.scope/container</p>
			<p class="source-code">[pogo@fedora container]$ cat cpu.max</p>
			<p class="source-code">50000 100000</p>
			<p class="source-code">[pogo@fedora container]$</p>
			<p>That wraps <a id="_idIndexMarker478"/>it <a id="_idIndexMarker479"/>up for rootless containers. So now, let's get set to talk about <strong class="source-inline">cpuset</strong>.</p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor193"/>Understanding cpuset</h1>
			<p>When you're<a id="_idIndexMarker480"/> dealing with a server that's running lots of containers and processes, it's sometimes beneficial to assign a container or a process to a certain CPU core or set of CPU cores. On a machine with more than one physical CPU, it might also be beneficial to assign a memory node, as well. To see what I'm talking about, install <strong class="source-inline">numactl</strong> on your Fedora machine, like this:</p>
			<p class="source-code">[donnie@fedora ~]$ sudo dnf install numactl</p>
			<p>Use the <strong class="source-inline">-H</strong> option to look at the hardware list, like this:</p>
			<p class="source-code">[donnie@fedora ~]$ numactl -H</p>
			<p class="source-code">available: 1 nodes (0)</p>
			<p class="source-code">node 0 cpus: 0 1 2 3</p>
			<p class="source-code">node 0 size: 7939 MB</p>
			<p class="source-code">node 0 free: 6613 MB</p>
			<p class="source-code">node distances:</p>
			<p class="source-code">nodeÂ Â Â 0 </p>
			<p class="source-code">Â Â 0:Â Â 10 </p>
			<p class="source-code">[donnie@fedora ~]$</p>
			<p>There's one <strong class="source-inline">NUMA</strong> node, which<a id="_idIndexMarker481"/> is <strong class="source-inline">node 0</strong>, and which is associated with four CPUs. Well, in reality, there's only <em class="italic">one CPU</em> that has <em class="italic">four CPU cores</em>. We also see the amount of memory that is assigned to this node.</p>
			<p>So, now you're saying, <em class="italic">But Donnie, what is this NUMA business, and why should I care?.</em> Okay, <strong class="bold">NUMA</strong> stands <a id="_idIndexMarker482"/>for <strong class="bold">non-uniform memory access</strong>. It has to do with how the operating system deals with memory on machines with more than one physical CPU. On systems with only a single CPU, such as your Fedora virtual machine, NUMA doesn't do anything for us, because there's only one memory node. On machines with more than one CPU, each CPU has its own associated memory node. For example, check out this photo of one of my junk motherboards:</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/Figure_13.4_B17491.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.4 â€“ A dual-CPU motherboard</p>
			<p>There are two<a id="_idIndexMarker483"/> CPU sockets, each with its own bank of memory sockets. Each bank of memory constitutes a NUMA node. Now, let's look at one of my running multi-CPU systems, which is an old <strong class="bold">Hewlett-Packard</strong> workstation that's running with two quad-core <strong class="bold">AMD</strong> <strong class="bold">Opterons</strong> and <strong class="bold">Fedora 34</strong>:</p>
			<p class="source-code">[donnie@fedora-teaching ~]$ numactl -H</p>
			<p class="source-code">available: 2 nodes (0-1)</p>
			<p class="source-code">node 0 cpus: 0 2 4 6</p>
			<p class="source-code">node 0 size: 7959 MB</p>
			<p class="source-code">node 0 free: 6982 MB</p>
			<p class="source-code">node 1 cpus: 1 3 5 7</p>
			<p class="source-code">node 1 size: 8053 MB</p>
			<p class="source-code">node 1 free: 7088 MB</p>
			<p class="source-code">node distances:</p>
			<p class="source-code">nodeÂ Â Â 0Â Â Â 1 </p>
			<p class="source-code">Â Â 0:Â Â 10Â Â 20 </p>
			<p class="source-code">Â Â 1:Â Â 20Â Â 10 </p>
			<p class="source-code">[donnie@fedora-teaching ~]$</p>
			<p>So, we see two NUMA nodes this time. The even-number CPU cores are assigned to <strong class="source-inline">node 0</strong>, and the odd-number CPU cores are assigned to <strong class="source-inline">node 1</strong>.</p>
			<p>By default, most processes run under a randomly chosen CPU core or set of CPU cores upon startup. Sometimes, the operating system might move a running process from one core or set of <a id="_idIndexMarker484"/>cores to another. On a normal workstation like I'm running here, that doesn't matter. But, it might matter on a server that's running lots of processes. You could possibly improve efficiency and performance by assigning certain processes to their own dedicated CPU cores and NUMA nodes. If you're dealing with cgroups Version 1, you'll need to jump through hoops and perform unnatural acts to make this work, because the Version 1 <strong class="source-inline">cpuset</strong> controller doesn't directly work with <strong class="source-inline">systemd</strong>. With cgroup Version 2, it's a breeze. It's just a matter of either using <strong class="source-inline">systemctl set-property</strong> to set the <strong class="source-inline">AllowedCPUs=</strong> and <strong class="source-inline">AllowedMemoryNodes=</strong> parameters, or setting them in the <strong class="source-inline">[Service]</strong> section of the service file. </p>
			<p>Now, even though you only have one CPU for your Fedora virtual machine, you can still try this to see what it looks like. First, install the <strong class="bold">Apache</strong> web server by doing:</p>
			<p class="source-code">[donnie@fedora ~]$ sudo dnf install httpd</p>
			<p class="source-code">[donnie@fedora ~]$ sudo systemctl enable --now httpd</p>
			<p>Next, assign the Apache service to CPU cores <strong class="source-inline">0</strong> and <strong class="source-inline">2</strong>, like this:</p>
			<p class="source-code">sudo systemctl set-property httpd.service AllowedCPUs="0 2"</p>
			<p class="callout-heading">Reminder</p>
			<p class="callout">As before, remember to surround any set of parameters that contains a blank space with a pair of double-quotes.</p>
			<p>Now, pretend that this virtual machine has more than one NUMA node, and assign the Apache service to NUMA node <strong class="source-inline">0</strong>, like this:</p>
			<p class="source-code">sudo systemctl set-property httpd.service AllowedMemoryNodes=0</p>
			<p>These two commands will affect the <strong class="source-inline">cpuset.cpus</strong> and <strong class="source-inline">cpuset.mems</strong> attribute files, as you see here:</p>
			<p class="source-code">[donnie@fedora httpd.service]$ pwd</p>
			<p class="source-code">/sys/fs/cgroup/system.slice/httpd.service</p>
			<p class="source-code">[donnie@fedora httpd.service]$ cat cpuset.cpus</p>
			<p class="source-code">0,2</p>
			<p class="source-code">[donnie@fedora httpd.service]$ cat cpuset.mems</p>
			<p class="source-code">0</p>
			<p class="source-code">[donnie@fedora httpd.service]$</p>
			<p>On my trusty<a id="_idIndexMarker485"/> dual-CPU Hewlett-Packard, I instead modified the <strong class="source-inline">httpd.service</strong> file to add these two parameters. The two new lines look like this:</p>
			<p class="source-code">. . .</p>
			<p class="source-code">. . .</p>
			<p class="source-code">[Service]</p>
			<p class="source-code">. . .</p>
			<p class="source-code">. . .</p>
			<p class="source-code">AllowedCPUs=0 2</p>
			<p class="source-code">AllowedMemoryNodes=0</p>
			<p class="source-code">. . .</p>
			<p class="source-code">. . .</p>
			<p>So, in both examples, I'm allowing Apache to use CPU cores 0 and 2, which are both associated with NUMA <strong class="source-inline">node 0</strong>.</p>
			<p class="callout-heading">Pro Tip</p>
			<p class="callout">You can separate the core numbers in your list with either a comma or a blank space, or use a dash (-) to list a range of CPU cores. Also, note that you do not surround the 0 2  in double-quotes when you add this AllowedCPUs= parameter to the unit file.)</p>
			<p>After a <strong class="source-inline">daemon-reload</strong> and restart of the Apache service, we should see the appropriate attribute files show up in the <strong class="source-inline">/sys/fs/cgroup/system.slice/httpd.service/</strong> directory. Again, here's what the <strong class="source-inline">cpuset.cpus</strong> file looks like:</p>
			<p class="source-code">[donnie@fedora-teaching httpd.service]$ cat cpuset.cpus</p>
			<p class="source-code">0,2</p>
			<p class="source-code">[donnie@fedora-teaching httpd.service]$</p>
			<p>Cool. Apache is running on CPU cores <strong class="source-inline">0</strong> and <strong class="source-inline">2</strong>, just like we want. Now, let's look in the <strong class="source-inline">cpuset.mems</strong> file:</p>
			<p class="source-code">[donnie@fedora-teaching httpd.service]$ cat cpuset.mems</p>
			<p class="source-code">0</p>
			<p class="source-code">[donnie@fedora-teaching httpd.service]$</p>
			<p>Again, it's just what<a id="_idIndexMarker486"/> we want to see. Apache can now only use NUMA <strong class="source-inline">node 0</strong>. So, thanks to cgroup Version 2, we have achieved coolness with the bare minimum of effort.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">NUMA doesn't mean that a process that's running on one CPU can't access memory that's in the NUMA node for another CPU. By default, any process can access all system memory on all NUMA nodes. You would use the <strong class="source-inline">AllowedMemoryNodes</strong> parameter to change that.</p>
			<p>So, now you're wondering, "<em class="italic">Can I use cgroup Version 2 on my RHEL 8-type machine?</em>". Well, let's take a look.</p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor194"/>Converting RHEL 8-type distros to cgroup version 2</h1>
			<p>It's an easy matter to convert a Red Hat Enterprise Linux 8-type distro to cgroup Version 2. Step 1 is to edit the <strong class="source-inline">/etc/default/grub</strong> file on your AlmaLinux machine. Find the line that starts with <strong class="source-inline">GRUB_CMDLINE_LINUX=</strong>. At the end of that line, add <strong class="source-inline">systemd.unified_cgroup_hierarchy=1</strong>. The whole line should now look like this:</p>
			<p class="source-code">GRUB_CMDLINE_LINUX="crashkernel=auto resume=/dev/mapper/vl-swap rd.lvm.lv=vl/root rd.lvm.lv=vl/swap rhgb quiet systemd.unified_cgroup_hierarchy=1"</p>
			<p>Next, rebuild the <strong class="source-inline">GRUB</strong> configuration, like this:</p>
			<p class="source-code">[donnie@localhost ~]$ sudo grub2-mkconfig -o /boot/grub2/grub.cfg</p>
			<p>Reboot the machine, and then look in the <strong class="source-inline">/sys/fs/cgroup/</strong> directory. You should now see the same filesystem that you see on the Fedora machine. However, don't be too disappointed if you can't get all of the previous labs to work. It's just that the RHEL 8-type distros all use an older version of the Linux kernel, which doesn't yet have all of the cgroup resource controllers enabled. Will they ever be enabled in the RHEL 8 distros? Well, maybe. Red Hat's policy is to stick with one certain kernel version for the whole ten-year lifespan of each major release of RHEL. So, all of the RHEL 8-type distros will be stuck on the old kernel version <em class="italic">4.18</em> until they reach end-of-life in 2029. Sometimes, Red Hat will backport features from newer kernels into their older RHEL kernel, but there's no guarantee that they'll do this with any newer cgroup Version 2 code. At any rate, once you've seen what you need to see on your AlmaLinux machine, feel free to delete the edit that you made to the <strong class="source-inline">grub</strong> file, and rebuild the GRUB configuration. This will convert the machine back to using cgroups Version 1.</p>
			<p>So, have you<a id="_idIndexMarker487"/> seen enough about<a id="_idIndexMarker488"/> cgroups? Hopefully not, because it's something that's worthwhile learning in depth. But, there's lots more that we need to cover, so let's go do that.</p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor195"/>Summary</h1>
			<p>In this chapter, we've learned a lot about cgroup Version 2. We started with a discussion about the deficiencies in cgroups Version 1, and how cgroup Version 2 is better. Then, we looked at how to allow non-privileged users to set resource limits on their containers, and how to use the <strong class="source-inline">cpuset</strong> resource controller. Finally, we took a brief look at how to convert a RHEL 8-type machine to use cgroup Version 2.</p>
			<p>Once again, I'm reading your mind, and you're wondering why cgroup Version 2 hasn't yet been universally adopted if it's so good. Well, it's just that certain critical programs and services, especially containerization services, are still hardcoded to use Version 1. Fortunately, the situation is improving, and it's a safe bet that Version 2 will become the standard within our lifetimes.</p>
			<p>All right, this concludes Part 2 of this tome. Let's start Part 3 with a discussion of <strong class="source-inline">journald</strong>. I'll see you there.</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor196"/>Questions</h1>
			<ol>
				<li value="1">Which of the following statements is true?<p>A. You can safely use <strong class="source-inline">podman</strong> under cgroups Version 1 to set resource limits on rootless containers.</p><p>B. You can safely use <strong class="source-inline">podman</strong> under cgroup Version 2 to set resource limits on rootless containers.</p><p>C. You can't set resource limits on <strong class="source-inline">podman</strong> containers.</p><p>D. No special privileges are required to set resource limits on rootless <strong class="source-inline">podman</strong> containers.</p></li>
				<li>What is the difference between <strong class="source-inline">MemoryMax</strong> and <strong class="source-inline">MemoryHigh</strong>?<p>A. <strong class="source-inline">MemoryMax</strong> is a hard limit, and <strong class="source-inline">MemoryHigh</strong> is a soft limit.</p><p>B. <strong class="source-inline">MemoryHigh</strong> is a hard limit, and <strong class="source-inline">MemoryMax</strong> is a soft limit.</p><p>C. They both do the same thing.</p><p>D. Neither one does anything.</p></li>
				<li>Which of the following statements is true about delegation?<p>A. It's perfectly safe for both cgroups Version 1 and cgroup Version 2.</p><p>B. It's only safe for cgroups Version 1.</p><p>C. It's never safe to use delegation.</p><p>D. It's only safe for cgroup Version 2.</p></li>
				<li>What is the first step for converting a RHEL 8-type system to cgroup Version 2?<p>A. Edit the <strong class="source-inline">/etc/grub.cfg</strong> file.</p><p>B. Edit the <strong class="source-inline">/etc/default/grub</strong> file.</p><p>C. Edit the <strong class="source-inline">/boot/grub2/grub.cfg</strong> file.</p><p>D. Edit the <strong class="source-inline">/boot/grub</strong> file.</p></li>
			</ol>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor197"/>Answers</h1>
			<ol>
				<li value="1">B</li>
				<li>A</li>
				<li>D</li>
				<li>B</li>
			</ol>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor198"/>Further reading</h1>
			<p>A Red Hat blog post about cgroup Version 2:</p>
			<p><a href="https://www.redhat.com/en/blog/world-domination-cgroups-rhel-8-welcome-cgroups-v2">https://www.redhat.com/en/blog/world-domination-cgroups-rhel-8-welcome-cgroups-v2</a></p>
			<p>An <strong class="bold">Oracle</strong> blog post about why Version 2 is better than Version 1:</p>
			<p><a href="https://blogs.oracle.com/linux/post/cgroup-v2-checkpoint">https://blogs.oracle.com/linux/post/cgroup-v2-checkpoint</a></p>
			<p>Comparing Version 1 and Version 2:</p>
			<p><a href="https://chrisdown.name/talks/cgroupv2/cgroupv2-fosdem.pdf">https://chrisdown.name/talks/cgroupv2/cgroupv2-fosdem.pdf</a></p>
			<p>Using cgroup Version 2 for rootless Docker containers:</p>
			<p><a href="https://rootlesscontaine.rs/getting-started/common/cgroup2/">https://rootlesscontaine.rs/getting-started/common/cgroup2/</a></p>
			<p>The current adoption status of cgroup v2 in containers:</p>
			<p><a href="https://medium.com/nttlabs/cgroup-v2-596d035be4d7">https://medium.com/nttlabs/cgroup-v2-596d035be4d7</a></p>
		</div>
	</body></html>