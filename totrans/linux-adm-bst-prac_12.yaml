- en: '*Chapter 8*: Improving Administration Maturation with Automation through Scripting
    and DevOps'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*：通过脚本编写和DevOps提升自动化以改进管理成熟度'
- en: I think that it is safe to say that for most of us in system administration
    that scripting and automation and where we naturally gravitate towards for thinking
    of what creates the best opportunities for overall system improvement. This might
    be treated, and automation is very important, without question, but it is not
    the end all of system administration either. It is safe to say that the more that
    we learn to script and automate, the more that we have free time to focus our
    energies on tasks that only humans can do while also developing a deeper appreciation
    for what developers do which is always be helpful for those of us in IT.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，对于我们大多数系统管理员来说，脚本编写和自动化是我们自然倾向于思考如何改善整体系统的最佳机会。这可能会被视为一种重要的工作方式，自动化毫无疑问非常重要，但它也并不是系统管理的终极目标。可以说，随着我们不断学习脚本编写和自动化，我们将拥有更多的自由时间，集中精力去处理只有人类才能完成的任务，同时也能更深入地理解开发人员的工作，这对于我们IT领域的从业者总是非常有帮助的。
- en: System automation is an area where it becomes much easier to obtain bragging
    rights as to what our daily task list looks like. When sitting around having beers
    at the proverbial system administrators cocktail lounge, we get little satisfaction
    over telling our compatriots how we wrote some really clean and easy to read documentation.
    But when we explain how we wrote a long, complicated script that takes hours of
    our weekly workload and turns it into a task that is run magically by the computer's
    scheduler we get kudos, attention, streamers, a ticker tape parade, fellow administrators
    buying us rounds of our favorite beverage and, if we are truly lucky, a piñata.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 系统自动化是一个领域，在这里我们更容易获得炫耀的资本，展示我们每天的任务清单。当我们坐在比喻的系统管理员鸡尾酒酒吧里，向同伴们讲述我们如何编写了一些非常清晰且易于阅读的文档时，我们并不会感到满足。但当我们解释如何编写一个复杂的脚本，将我们每周工作量中需要几小时的任务，转化为计算机调度器自动运行的任务时，我们会获得称赞、关注、彩带、报纸横幅游行、同事们为我们买来一杯杯最爱的饮品，如果非常幸运的话，还有一个打击乐器。
- en: Automation is typically the area of administration that most of us find to be
    both the most exciting, and the scariest. There are more building blocks, more
    concepts to understand, than in other areas of system administration. For the
    most part, system administration is a lot like taking a history class where yes,
    there are real benefits to knowing more pieces of history so that you have a larger
    context when learning something new, but generally you can learn about any specific
    event without a large understanding of all of the events related to it and that
    led up to it and still come away having learned something valuable and essentially
    understanding it. You will not be lost when learning Roman history just because
    you did not grok Greek history first. But scripting and automation is a lot more
    like math class where if you fail to learn addition, then learning how to find
    the square root is going to be completely impossible. Scripting is a skill that
    builds on top of itself and to be really useful you will want to learn a bit of
    it.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化通常是我们大多数人在系统管理中既感到兴奋又感到害怕的领域。与系统管理的其他领域相比，自动化涉及更多的构建模块和需要理解的概念。大部分情况下，系统管理很像上历史课，确实，了解更多历史片段有助于你在学习新内容时拥有更大的背景，但通常你可以单独学习任何特定事件，而不需要全面了解与之相关的所有事件和导致这些事件发生的背景，依然能够从中学到有价值的东西，并且本质上理解它。当你学习罗马历史时，即便你没有先学希腊历史，也不会感到迷失。然而，脚本编写和自动化更像数学课，如果你没有学会加法，那么学会如何求平方根将是完全不可能的。脚本编写是一项累积的技能，要真正发挥作用，你需要学一些基础内容。
- en: We are going to start by looking at unscripted command line administration in
    comparison to working with a graphical user interface and use that as a foundation
    to move into automation itself.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过对比未编写脚本的命令行管理与使用图形用户界面（GUI）来作为基础，进一步学习如何进入自动化领域。
- en: 'In this chapter we are going to learn about the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: 'The GUI and the CLI: Administration best practices'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图形用户界面与命令行界面：管理最佳实践
- en: Automation maturity
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化成熟度
- en: '**Infrastructure as Code** (**IaC**)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施即代码**（**IaC**）'
- en: '**Documentation First Administration** (**DFA**)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档优先管理**（**DFA**）'
- en: Modern tools of automation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代自动化工具
- en: 'The GUI and the CLI: Administration best practices'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图形用户界面与命令行界面：管理最佳实践
- en: If you are coming to Linux from the Windows world, you may be excused from realizing
    that nearly everything should be done from the command line, not from the graphical
    user interface. But really, even on Windows, Microsoft has been very clear, for
    a very long time, that the desktop experience is really for end users and not
    for system administrators and that they recommend either using PowerShell as the
    administration tool of choice when working on a local system directly or any number
    of remove management tools that connect via an API. Microsoft pushes quite hard
    to encourage those installing their systems for the past few generations to install
    their operating systems and hypervisors without graphical user interfaces at all.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是从Windows环境转向Linux，你可能没有意识到几乎所有操作都应该通过命令行完成，而不是图形用户界面。但实际上，即便在Windows上，微软早就明确表示，桌面体验主要是为最终用户设计的，而不是为系统管理员设计的，并且他们推荐在直接操作本地系统时，使用PowerShell作为首选管理工具，或者使用通过API连接的远程管理工具。微软强烈建议过去几代安装其操作系统的人，在安装操作系统和虚拟机管理程序时完全不使用图形用户界面。
- en: Graphical User Interfaces, or GUIs as we will call them now to keep things short,
    present a lot of problems for system administrators.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图形用户界面，或者现在我们简称为GUI，给系统管理员带来了许多问题。
- en: The first issue with GUIs is bloat. During the installation of an enterprise
    operating system, when a GUI is available it is often more than half of all of
    the code that will be deployed in a system. Every additional line of code means
    more data that we have to store, the more we store the more we have to back up;
    more code to worry about having bugs or flaws or intentional backdoors; more code
    to patch and maintain and so on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: GUI的第一个问题是膨胀。在安装企业操作系统时，如果有GUI可用，它往往占据了系统中一半以上的代码。每增加一行代码，就意味着我们需要存储更多的数据，存储的数据越多，我们就需要备份更多；更多的代码意味着更多的bug、缺陷或故意的后门要担心；更多的代码需要打补丁和维护，等等。
- en: Next is performance. GUIs require much more compute power and memory consumption
    while running than do non-GUI systems. It is not uncommon for a GUI to require
    2GB or more of additional system memory above and beyond what is needed for the
    system workloads. This might sound trivial, but especially if we are dealing with
    many systems consolidated onto a single piece of hardware it can add up very quickly.
    If we have twenty server virtual machines running on a single physical server
    it might not be uncommon in the Linux world for the average workload to only be
    between two and four gigabytes of memory. Adding two more gigabytes to each system
    would mean not only a nearly fifty percent increase, but forty gigabytes across
    the machines.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是性能。GUI（图形用户界面）在运行时需要的计算能力和内存消耗要比非GUI系统大得多。GUI通常需要额外2GB或更多的系统内存，这比系统工作负载所需的内存还要多。这听起来可能微不足道，但特别是在多个系统合并到一台硬件上时，这个问题可能会迅速加重。如果我们有二十台虚拟服务器运行在同一物理服务器上，那么在Linux环境下，平均工作负载可能仅为2到4GB的内存。每个系统再增加2GB内存，不仅意味着接近50%的增长，而且在所有机器上将增加40GB的内存。
- en: Consolidation and the age of squeezing systems
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统整合与压缩系统的时代
- en: In the 1990s and 2000s, before the prevalence of virtualization, we were in
    an era where servers were gaining performance rapidly, but each individual system
    would only run a single operating system no matter how small the workloads on
    that system were. As systems became more powerful, much faster than software used
    more resources, there was a strong trend towards allowing system bloat because,
    at least when it came to hardware, it did not matter very much.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在1990年代和2000年代，虚拟化普及之前，我们处于一个服务器性能快速增长的时代，但每台系统只能运行一个操作系统，无论该系统的工作负载有多小。随着系统性能的增强，远远超过了软件使用资源的速度，系统膨胀的趋势越来越强烈，因为至少在硬件方面，膨胀并不那么重要。
- en: CPU and memory resources would normally come in discrete chunks and to have
    enough we would normally have to overbuy. It was rare to run a system close to
    its limits because it was so difficult to expand systems in those days. So a system
    would typically have a lot of spare resources by design to allow for a large margin
    of error and, of course, growth. Because of these factors, running a GUI on a
    server was more or less trivial.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CPU和内存资源通常是以离散块的形式提供的，要拥有足够的资源，我们通常需要购买更多。过去，系统很少会接近其极限运行，因为那个时候扩展系统非常困难。因此，系统通常设计得有很多备用资源，以便留有大幅度的误差空间，当然也为将来的增长留有余地。由于这些原因，在服务器上运行GUI几乎是微不足道的。
- en: Many factors have changed since those days. We could probably write an entire
    book just discussing why the industry temporarily moved from command line after
    decades of using nothing else and for a small blip from the mid-1990s to the early
    2000s had GUIs seemingly taking over as the dominant approach in server management
    only to go right back to the command line by around 2005\. Ignoring social trends
    driving changes we are concerned here with capacity concerns.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自那时以来，许多因素发生了变化。我们或许可以写一本书，专门讨论为何在使用命令行数十年后，行业短暂地从命令行转向图形用户界面（GUI），尤其是从1990年代中期到2000年代初，GUI似乎一度成为服务器管理的主流方式，直到2005年左右又重新回到命令行。忽略社会趋势推动的变化，我们在这里关注的是容量问题。
- en: Once virtualization became mainstream, and even more so as cloud computing began
    to become a major trend, the availability of spare resources for operating systems
    ceased to be a common thing, almost overnight. This might seem counterintuitive,
    given that virtualization inherently gives us more flexibility and power. But
    it also gives us the ability to scale down very effectively, and this is something
    that we did not have before. With virtualization we are rarely in a position of
    having dramatically excess system resources, especially predictably excess resources,
    and so there is a huge advantage to keeping individual virtual machines as lean
    as possible, and that means not running an enormous and largely useless GUI process.
    Very small businesses that still cannot combine their workloads to reach the effective
    lower bounds of a single server remain an exception to this rule and still have
    plenty of overhead to implement GUIs unless they would be good candidates for
    cloud computing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦虚拟化成为主流，尤其是随着云计算开始成为一个重大趋势，操作系统的空闲资源几乎在一夜之间变得不再常见。这看起来似乎与直觉相反，因为虚拟化本身为我们提供了更多的灵活性和计算能力。但它也赋予了我们非常有效地缩减规模的能力，这是我们之前所没有的。通过虚拟化，我们很少会处于拥有大量过剩系统资源的情况，特别是那些可以预测的过剩资源，因此保持每个虚拟机尽可能精简就显得尤为重要，这意味着不运行一个庞大且大多无用的GUI进程。仍然无法将工作负载合并到单一服务器的有效下限的小型企业是这一规则的例外，它们仍然有足够的开销来实现GUI，除非它们是云计算的良好候选者。
- en: In a traditional business, where there are multiple servers, a major advantage
    of virtualization is consolidation and avoiding the installation of GUIs may mean
    that fifty or sixty workloads can be installed on a single physical server instead
    of thirty or forty on the exact same hardware. This equates to the need to buy
    fewer servers and that means cost savings not just from lowering purchase costs,
    but also lowering power consumption, cooling costs, datacenter real estate costs,
    software licenses, and even IT staff.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的企业环境中，多个服务器是常见的，虚拟化的一个主要优势是整合，避免安装GUI意味着可以在一台物理服务器上安装50或60个工作负载，而不是在相同硬件上安装30或40个。这相当于减少了购买服务器的需求，从而节省了成本，不仅降低了采购成本，还减少了电力消耗、制冷费用、数据中心房地产费用、软件许可费用，甚至IT人员成本。
- en: If we look at an example on public cloud computing, we can see the advantages
    of not having a GUI even more easily. Small workloads, which could include email
    servers, web servers, proxies, telephone PBXs, and on and on might only cost between
    five and ten dollars per month to run on their own. Adding a GUI will easily cause
    the cost of a cloud hosted virtual machine to double from five to ten or ten to
    twenty and so forth as the GUI creates a need for more CPU power, more storage,
    and most importantly, much more memory. It does not much effort to see how moving
    from ten dollars per month for a workload to twenty dollars will add up exceptionally
    quickly. As most cloud-based workloads are quite small adding a GUI to each one
    could have staggering capacity consequences as much as doubling the compute cost
    of a company's infrastructure!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以公共云计算为例，我们可以更容易地看到没有GUI的优势。小型工作负载，比如邮件服务器、Web服务器、代理服务器、电话交换机等，每月运行成本可能仅为5到10美元。如果加上GUI，云托管虚拟机的成本可能轻松翻倍，从5美元涨到10美元，或从10美元涨到20美元等等，因为GUI需要更多的CPU、存储，最重要的是更多的内存。很容易看出，从每月10美元到20美元的工作负载成本上涨会迅速积累。由于大多数基于云的工作负载都非常小，给每个工作负载添加GUI可能会对容量产生巨大的影响，甚至可能使公司的基础设施计算成本翻倍！
- en: The lack of appropriateness for a GUI is so dramatic that many vendors have
    traditionally not even provided a mechanism for attaching to a GUI in the cloud
    space. Amazon famously did not make GUIs possible on their standard cloud instances
    effectively forcing organizations to learn command line and even more advanced
    techniques involving management without a login. But nearly all cloud users opt
    for remote logins via a technology such as SSH. The cloud did more than anything
    else to demonstrate the risks and costs of the GUI.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: GUI 不适用的程度如此之大，以至于许多供应商传统上甚至没有为云空间提供连接 GUI 的机制。亚马逊以其不允许在标准云实例上使用 GUI 而闻名，这实际上迫使组织学习命令行，甚至是更先进的管理技术，而不需要登录。但几乎所有云用户都会选择通过如
    SSH 之类的技术进行远程登录。云计算比任何其他方式都更能证明 GUI 的风险和成本。
- en: Prior to ubiquitous virtualization and cloud computing system administrators,
    especially those in the Windows world, would argue that GUIs just did not add
    that much overhead and that if they did anything to make someone's job easier
    that they were worth it. That myth has been exposed and no one can honestly make
    this claim today. GUIs are clearly nonsensical in any broad sense.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在虚拟化和云计算普及之前，系统管理员，尤其是那些在 Windows 环境中的管理员，会争辩说 GUI 根本不会增加太多开销，如果它们能让某人的工作变得更轻松，那它们就是值得的。这个神话已经被揭穿，今天没有人能诚实地再做这种声明。GUI
    在任何广泛意义上都是不合逻辑的。
- en: Often the biggest selling point for command line management at a managerial
    level is about security. A GUI presents a much larger attack surface for a malicious
    actor to use to attempt to breach a system. All of that extra code alone makes
    things much easier for a would-be attacker. And, of course, the functionality
    of a GUI has to make for very enticing attack surfaces just by the nature of needing
    to have more means of being accessible. More lines of code, more access methods,
    more management paths, lower performance and more all come together for overall
    increased security risk. Taken all together it may not be incredibly major, but
    the increase in risk is real and is measurable or, at least, estimable.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于管理层来说，命令行管理的最大卖点通常是关于安全性的。GUI 为恶意攻击者提供了更大的攻击面，增加了突破系统的可能性。仅仅是多出的那些代码，就让潜在攻击者的工作变得更加容易。而且，当然，GUI
    的功能本身就需要更多的访问手段，这自然形成了非常诱人的攻击面。更多的代码行、更多的访问方式、更多的管理路径、更低的性能等，都共同增加了整体的安全风险。综合来看，虽然这可能不是非常严重，但风险的增加是真实的，也是可以衡量的，或者至少可以估算的。
- en: The final major point as to why command line management has become the *de facto*
    standard is efficiency. Yes, the very reason that so many point to as to why they
    chose to keep a GUI. The reality is that system administration is not a casual
    task, nor one where you can effectively just poke around and guess about what
    settings should be wear. To do the job well, or even safely, you must have a pretty
    solid understanding of a large number of items from operating specifics to general
    computing and networking understandings.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 命令行管理成为*事实上的*标准的最终主要原因是效率。是的，正是许多人选择保持图形用户界面（GUI）的理由。现实是，系统管理不是一项轻松的任务，也不是你可以轻松地四处摸索并猜测应该使用什么设置的工作。要做好这项工作，甚至做到安全，你必须对大量的项目有相当扎实的理解，从操作细节到一般的计算和网络知识。
- en: The GUI in management was traditionally promoted as a tool for those that were
    not used to an environment to be able to be effective quickly with less training
    and knowledge. A great concept if you are talking about a janitor. In system administration
    the last thing that we want is someone without deep knowledge and experience being
    able to act like they know what they are doing. This is dangerous on many levels.
    GUIs, sadly, actually make it much harder for many organizations to evaluate which
    candidates are even minimally qualified for a technical position.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 管理中的 GUI 传统上被宣传为一种工具，旨在帮助那些不习惯环境的人能够更快地高效工作，减少培训和知识要求。如果你谈论的是清洁工，这个概念是很不错的。在系统管理中，我们最不希望看到的就是没有深厚知识和经验的人能够表现得像是他们知道自己在做什么。这在多个层面上都是危险的。遗憾的是，GUI
    实际上让许多组织更难评估哪些候选人至少符合技术职位的基本资格。
- en: Not only does a GUI pose a risk that someone without proper knowledge will start
    poking around, but for someone who knows what they are attempting to do the command
    line is vastly faster. It is faster for performing simple tasks, for performing
    most complex tasks, and it is far easier to script or automate. Command line management
    flows so easily directly into scripting that people often fail to be able to tell
    the two apart. If you ever truly compare tasks, it is not unheard of for command
    line work to take less than ten percent the amount of time that it takes to do
    the same task with a GUI!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图形用户界面（GUI）不仅存在着未经授权的人员可能会随意操作的风险，对于那些知道自己在做什么的人来说，命令行的速度要快得多。它在执行简单任务、执行大多数复杂任务时更快，而且更容易编写脚本或自动化。命令行管理与脚本编写如此契合，以至于人们常常分不清两者的区别。如果你真正对比任务，你会发现命令行工作完成相同任务所需的时间通常不到使用GUI的百分之十！
- en: Command line is not just more efficient for a system, it also makes multi-system
    management much easier as commands can be duplicated across systems in ways that
    GUI actions cannot be. Command line management can also easily be recorded, catalogued,
    searched, and so forth. It is plausible to do the same with a GUI but it requires
    long video recording which results in large amounts of storage needs and no simple
    way to parse or turn into documentation, and so on.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 命令行不仅对系统更高效，它还使多系统管理变得更加容易，因为命令可以在不同的系统间复制，这些是GUI操作所无法做到的。命令行管理还可以轻松录制、归档、搜索等等。虽然在GUI中做同样的事也是可能的，但需要长时间的视频录制，这会导致大量存储需求，而且没有简单的方法来解析或转化为文档等。
- en: In the more modern era, we have also begun to face the problem of needing to
    perform most or all system administration remotely. This inadvertently played
    right into the command line's hand. The amount of data that needs to be sent,
    and the sensitivity to network lag for the command line are far smaller than for
    a GUI. A remote GUI session to a server generally uses a noticeable amount of
    network traffic. Remote GUI sessions are video streams, generally in pretty high
    resolution. In some cases, even a single user can cause network issues, especially
    if the server exists in a location with bad Internet access. The standard method
    for remote command line management is SSH.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代时代，我们开始面临需要远程执行大部分或所有系统管理任务的问题。这无意中恰好迎合了命令行的优势。命令行需要传输的数据量和对网络延迟的敏感度远小于GUI。远程GUI会话通常会消耗显著的网络流量。远程GUI会话是视频流，通常具有相当高的分辨率。在某些情况下，即使是一个用户也能引发网络问题，尤其是当服务器位于网络连接较差的地区时。远程命令行管理的标准方法是SSH。
- en: SSH remote sessions will work just fine even over an archaic dial up Internet
    connection. And even the slowest modern Internet service is enough to handle scores,
    if not hundreds, of simultaneous SSH users. This is something that you generally
    cannot do with remote GUI sessions. Command line is nearly as effective over a
    tiny Internet connection from the other side of the globe while GUI remote management
    suffers noticeably from any network blips, limitations, or distance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 即使通过古老的拨号上网连接，SSH远程会话也能正常工作。即使是最慢的现代互联网服务，也足以处理数十甚至数百个同时在线的SSH用户。这是一般远程GUI会话做不到的。命令行在全球另一端通过微弱的网络连接几乎同样有效，而GUI远程管理则容易受到网络波动、限制或距离的明显影响。
- en: Command line is here to stay, but it is important to really understand why.
    It can be easy to forget that it is far more than just one or two small factors.
    There are really good reasons why you should be using command line whenever possible.
    Moving back and forth is not conducive to learning to be more efficient in either
    approach, as well. From a personal level it is expected that you would want to
    avoid the use of GUIs as much as possible so that you can focus on learning command
    line skills. Using the command line consistently is needed to really become efficient.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 命令行将持续存在，但理解其存在的真正原因非常重要。我们容易忽视它远不止依赖于一两个小因素。实际上，有很多充分的理由说明为什么在可能的情况下你应该使用命令行。来回切换并不利于提高这两种方法的效率。从个人角度来看，理应尽可能避免使用GUI，以便专注于学习命令行技能。持续使用命令行是提高效率的必要条件。
- en: Now we must acknowledge that there are a number of different command line options
    for Linux. We can use `BASH`, `Fish`, `zsh`, `tcsh`, PowerShell and more. Linux
    is, as we know, all about options and flexibility. This is a situation where less
    is probably more. Some of these shells are very nice and useful but, we must remember,
    that we are system administrators, and we need to make sure that we are totally
    familiar with the tools that we are likely to have access to in an emergency.
    Moving between shells is not particularly hard, especially in the Linux use case,
    but we should still be wary of spending time learning the nice keyboard shortcuts
    and auto-completion and other perks of a shell-like Fish or `zsh` because we may
    not be able to use those skills in the next job and that always has to be a consideration.
    And, in the case of an emergency if you were to get called to work on a system
    that you have not had a chance to set up previously you may be stuck with no option
    except for BASH. For me, this means that BASH is the only tool that I want to
    be learning.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须承认，Linux有许多不同的命令行选项。我们可以使用`BASH`、`Fish`、`zsh`、`tcsh`、PowerShell等。正如我们所知道的，Linux是关于选项和灵活性的。这是一个“少即是多”的情况。有些Shell非常好用且有用，但我们必须记住，我们是系统管理员，我们需要确保对在紧急情况下可能接触到的工具非常熟悉。在Shell之间切换并不特别难，特别是在Linux环境下，但我们仍然应该警惕花时间学习像`Fish`或`zsh`这样的Shell中的便捷键、自动补全和其他特性，因为我们可能在下一个工作中无法使用这些技能，这始终是一个需要考虑的因素。而且，如果在紧急情况下你被叫去处理一个你之前没有机会设置的系统，你可能唯一的选择就是BASH。对我来说，这意味着BASH是我唯一想学习的工具。
- en: And there you have it. All of the logic and reasoning so that you can go back
    to management and explain why you need to be working from the command line, why
    you need staff that works from the command line, and why your systems should rarely
    get installed with any GUI environment at all. In our next section we are going
    to talk about maturity levels in automation for systems.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。所有的逻辑和推理都在这里，这样你就可以回到管理层，解释为什么你需要从命令行工作，为什么你需要那些能从命令行工作的员工，为什么你的系统应该很少安装任何GUI环境。在下一节中，我们将讨论系统自动化的成熟度等级。
- en: Automation maturity
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化成熟度
- en: While there is no formal system for measuring automation maturity levels, there
    are some basic concepts of automation maturity that we can discuss. The idea here
    is that organizations sit, more or less, along a continuum from having no automation
    to being fully automated with most organizations sitting somewhere in the middle,
    but more likely to be towards no automation than towards being fully automated.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有正式的系统来衡量自动化成熟度，但我们可以讨论一些自动化成熟度的基本概念。这里的观点是，组织在自动化程度上大致处于一个连续体中，从没有自动化到完全自动化，大多数组织处于中间位置，但更可能趋向于没有自动化，而不是完全自动化。
- en: Not every organization needs to be, or even should be, completely automated.
    But in general, more automation is better when the cost to implement the automation
    is low enough to do so. Automation is not free, and it is quite possible to find
    organizations investing more in automating a process than it would cost to perform
    the duty manually over the lifespan of a system. We do not want to automate blindly
    only for the sake of automating.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不是每个组织都需要，甚至应该，完全实现自动化。但一般来说，当自动化实施成本足够低时，更多的自动化是更好的。自动化并不是免费的，实际上，组织可能会发现，投入自动化一个流程的成本要高于手动执行这个任务的成本，尤其是在系统的生命周期内。我们不希望仅仅为了自动化而盲目自动化。
- en: Typically, however, what we find is organizations skipping automation in nearly
    all cases and uses manual labour with all of its costs and risks instead. There
    is a natural tendency towards this because in the moment, any task will be easier
    if done manually. If we do not look ahead and invest, we would simply never automate,
    and this is often how companies view IT needs. If a task takes one hour to do
    manually and three hours to automate, that's the time of three tasks and hard
    to justify ignoring the fact that the same task will happen monthly and in four
    months would not only have been less effort to have automated, but the automation
    would make the task more reliable and consistent.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常情况下，我们发现组织几乎在所有情况下都会跳过自动化，而选择使用人工劳动，承受所有成本和风险。这是一种自然倾向，因为在当下，任何任务如果手动做都会更简单。如果我们不着眼未来并进行投资，我们根本就不会去自动化，而这也是许多公司看待IT需求的方式。如果一个任务手动完成需要一小时，自动化需要三小时，那就相当于三个任务的时间，忽略这个事实是很难证明合理的，因为同样的任务每个月都会发生，四个月后，自动化不仅会减少工作量，而且自动化还能让任务更可靠和一致。
- en: Nearly any organization will benefit from automating more than they do today.
    There is no need to look at automation and feel that it is an all or nothing proposition.
    Automate what you can, skip what you cannot do. Be practical. Hit the low hanging
    fruit first. The more than you automate the more time you have to automate other
    things in the future. You will improve your automation skills as you practice,
    as well, making each new automation something easier than the last. Automation
    is a great example of that kind of thing that is really hard to do the first time
    but gets progressively easier and easier until it is just the obvious way to approach
    things and becomes second nature.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎任何组织都将从比今天更多的自动化中受益。没有必要把自动化看作是非做不可的全有或全无的选择。可以做的就自动化，做不了的就跳过。要务实。首先做最容易实现的部分。你自动化的越多，未来你就能有更多的时间去自动化其他事情。随着实践的积累，你的自动化技能也会提升，使得每一次新的自动化都比上一次更容易。自动化正是一个典型的例子，第一次做起来确实很难，但随着时间的推移，它会变得越来越容易，直到它成为处理事情的显而易见的方法，并且变成第二天性。
- en: Automation maturity is not exactly a direct continuum with each step *more mature*
    than the last. For example, if we look at scheduling tasks and scripting tasks,
    each of these can be done independent of the other. Both are useful on their own.
    We can script complex operations and run them manually. Or we can schedule simple,
    discrete commands to trigger things without human intervention. We can then put
    the two together to automatically kick off complex scripts that do many things
    at once. Which one do we consider first and which second is just arbitrary.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化的成熟度并不完全是一个直接的连续体，每一步都比上一步*更加成熟*。例如，如果我们看看调度任务和脚本任务，这两者可以独立于对方执行。每个任务单独完成都有其用处。我们可以编写复杂的操作脚本并手动运行它们，或者可以调度简单的独立命令来触发事情的发生，无需人工干预。然后，我们可以将两者结合起来，自动启动执行多项任务的复杂脚本。我们首先考虑哪个，第二考虑哪个其实是随意的。
- en: Local and remote automation
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地与远程自动化
- en: In case it is not overly obvious, we have the choice of implementing automation
    either locally with tasks scheduled or triggered to run on the server in question,
    or we have the ability to push our automation from an outside source which can
    give us a sort of centralization of automation. There is a sort of hybrid approach
    where a local scheduler or agent reaches out to a central server to request automation
    information which is technically still local, just with a centralized store to
    mimic control.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这不够明显的话，我们有两个选择：要么在本地实现自动化，通过调度任务或触发任务在目标服务器上运行，要么我们可以从外部来源推动自动化，从而实现某种形式的自动化集中化。还有一种混合方法，本地调度器或代理向中央服务器请求自动化信息，虽然从技术上讲仍然是本地的，只不过是有一个集中存储来模拟控制。
- en: Often overlooked is the advantage of local automation being able to run even
    if remote systems become unavailable even to the point of the local system completely
    losing networking. My favorite task to keep local regardless of other automation
    decisions is system reboots. While it would be convenient to centralize reboots
    and I have seen organizations opt to do so I very much appreciate having a local,
    forced reboot that happens at least weekly and sometimes even daily. This gives
    me great peace of mind that even if something completely crashes on a server if
    it is still functional in any way that eventually a reboot process is going to
    make an attempt at restarting the machine and hopefully bringing it back online.
    A very niche need and one that may never be important to you, but I have witnessed
    systems become inaccessible for remote management while still providing their
    workloads and an automated, locally scheduled reboot brought them back online
    and making them accessible again.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 常常被忽视的是本地自动化的优势，即即使远程系统不可用，甚至本地系统完全失去网络连接，本地任务仍然能够运行。我最喜欢保留为本地自动化的任务之一就是系统重启。虽然将重启集中管理会更方便，且我见过一些组织选择这么做，但我非常欣赏有一个本地的、强制性的重启任务，它每周至少会执行一次，有时甚至是每天。这让我非常安心，即使服务器出现完全崩溃的情况，只要它以任何形式仍能运行，最终重启过程都会尝试重新启动机器，并希望能够让它重新上线。这是一个非常小众的需求，可能对你来说永远不重要，但我见过有些系统即使远程管理无法访问，仍能继续提供工作负载，而一个自动化、本地调度的重启任务将它们重新带回线上，并使其再次可访问。
- en: An increasingly popular happy medium approach is to have a central control repository
    that contains all of the desired automation which is then pulled by an agent on
    the end points being automated. This repository contains both the automation itself,
    such as scripts, as well as the scheduling information or triggers. Then the information
    is actualized by a local script that is able to keep functioning independently
    even if the remote repository fails or becomes unavailable. In this way you only
    really risk losing access to update the list of scheduled tasks to make changes
    to them or to the schedule. As long as you do not need to send out new updates
    to the automation you do not have to worry about your repository being offline.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一种越来越流行的中庸之道是拥有一个中央控制库，包含所有所需的自动化内容，然后由终端的代理拉取进行自动化。这些库既包含自动化本身，如脚本，也包含调度信息或触发器。然后，信息通过本地脚本得以实现，即使远程库出现故障或不可用，本地脚本仍能独立运行。通过这种方式，你真正面临的风险只是无法更新调度任务列表、对它们或调度进行更改。只要不需要发布新的自动化更新，就不必担心库会离线。
- en: Command line
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命令行
- en: 'Not so much a legitimate maturity level but more of a basic starting point,
    so we could think of this as a level zero, is moving to the command line and the
    use of a proper shell environment for interactive (that is: non-automated.) As
    we just discussed, being on the command line and learning command line syntax
    and tools is the fundamental building block on which all subsequent automation
    is going to be based. Understanding how to do tasks at the command line, how to
    manipulate text files, how to filter logs, and other common command line tasks
    will build quickly into obvious automation capabilities.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅仅是一个合法的成熟度级别，更像是一个基本的起点，因此我们可以把它看作是零级，指的是进入命令行并使用一个适当的 Shell 环境进行交互式操作（即：非自动化）。正如我们刚刚讨论的，处于命令行并学习命令行语法和工具是所有后续自动化构建的基础。理解如何在命令行上执行任务、如何操作文本文件、如何过滤日志以及其他常见的命令行任务，会迅速发展成显而易见的自动化能力。
- en: Scheduled tasks
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定时任务
- en: The best and easiest place to begin with automation is the scheduling of tasks.
    This may sound like the simplest and most obvious step, but surprisingly many
    organizations never even get this far. Linux has long been a stronghold of reliable,
    easy to manage local scheduling with `cron` having been built into not only Linux,
    but essentially all UNIX systems for almost half a century as it was released
    in 1975\. Cron is fast and efficient, ubiquitous and well known. Any experienced
    UNIX admin should be able to at least schedule a basic task when needed. Cron
    even handles tasks happening at boot time.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 开始自动化的最佳且最简单的地方就是任务调度。这听起来可能是最简单、最显而易见的一步，但令人惊讶的是，许多组织甚至没有做到这一点。Linux 长期以来一直是可靠且易于管理的本地调度的堡垒，`cron`不仅内建于
    Linux 中，几乎所有的 UNIX 系统都可以使用它，已经有近半个世纪历史（自 1975 年发布以来）。Cron 快速高效、无处不在且广为人知。任何有经验的
    UNIX 管理员都应该能够在需要时至少调度一个基本任务。Cron 甚至能处理启动时的任务。
- en: Simple tasks of all sorts can be run through `cron`. Common tasks used in most
    environments could include system updates, data collection, reboots, file cleanups,
    system replications, and backups. You can schedule anything, of course, but these
    are some good ideas for first time automaters looking for obviously recurring
    system needs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Another common area for simple scheduled tasks are code updates via a repository
    like when we pull new code via `git`. Tasks such as code updates and subsequent
    database migrations can all be easily scheduled.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Scripting
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we say automation everyone always immediately thinks about scripting. At
    the end of the day, nearly everything in automation is scripting either directly,
    or under the hood somewhere. Scripting delivers the power when we want to move
    beyond the simplest of tasks or just calling scripts that someone else has made.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: We cannot possibly teach scripting itself here, that is an entire topic in and
    of itself. Scripting is the closest that IT comes to crossing paths with the software
    development world. Where does combining IT command line tasks together turn into
    programming? Technically it is all programming, but an incredibly simplistic form
    of programming focused purely on system management tasks.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Typically, on Linux we write scripts in the BASH shell. BASH is a very simple
    language designed to be primarily used interactively as a live shell, BASH is
    how we assume all command line interactions with Linux will be performed. BASH
    is relatively powerful and capable and nearly any script can be written in it.
    At least when starting out, most Linux admins will turn to the BASH shell that
    they are already using in their command line environment and add scripting elements
    organically to move from a single command, a few basic commands strung together,
    and into full scripting just a little at a time.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Any shell, such as `tcsh`, `ksh`, `fish`, and `zsh`, will allow you to script
    and in many cases with more power and flexibility than you can with BASH. Traditional
    shells, like `tcsh`, `ksh`, and BASH, can be very limiting and cumbersome to attempt
    to use for advanced scripting. Apple for its macOS UNIX operating system has recently
    moved to `zsh` to modernize it compared to other UNIX systems. Typically, a Linux
    system is not going to have a more modern, advanced shell installed by default,
    even though they are easily available on essentially any Linux based operating
    system.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: You may work in an environment where an alternative shell is consistently provided
    or offered, or you may have the option of adding it yourself. If so, and especially
    if you will be doing cross platform scripting with macOS you might consider using
    `zsh` instead of BASH, or if you are doing a lot of Windows scripting its native
    shell PowerShell is also available on Linux.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: PowerShell on Linux
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the weirdest things that you may ever encounter is the idea of running
    Microsoft's PowerShell on Linux. Many people are confused and believe that it
    does not even work. PowerShell does actually run just fine on Linux. The problem
    with PowerShell on Linux is that PowerShell users on Windows actually spend essentially
    no time at all learning PowerShell and nearly all of their time learning a range
    of CommandLets or small programs that can be called by PowerShell and easily combined
    with other little programs to give power to the system.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: On Linux, of course, the same thing happens. If you are scripting on Linux you
    will surely be using tools like sed, awk, cut, head, tail, grep and so forth.
    These tools are a lot like CommandLets, but are actually just every day system
    executables. If you were to port `BASH` or `zsh` over to Windows you would find
    that the tools that you are accustomed to using on Linux were still not available.
    That is because they are tiny programs that you are calling from BASH, not part
    of BASH itself. BASH is just the programming language.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The reverse is also true. If you run PowerShell on Linux you still use sed,
    awk, cut, grep, head, tail and on and on. It is the language that has changed,
    not the operating system's suite of tooling and components.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: So, while there can be value in learning one scripting language and attempting
    to use it over and over again between different operating systems, there is not
    the value that one might assume. You will likely spend far more time tripping
    over integration quirks, misunderstandings, and poor documentation than you could
    ever recuperate from language learning efficiency. Books, online guides, example
    code and so on will never work for you if you try to use PowerShell on Linux.
    There will assume that you are trying to do Windows tasks with access to Windows
    tools, always. PowerShell is, at its core, designed to be a truly modern shell
    that uses operating system objects to do its heavy lifting. BASH instead is focused
    on text processing and manipulation as Linux is traditionally built on text files
    and needs a scripting engine that will easily accommodate that.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Using something so foreign as PowerShell on Linux is a great tool for exposing
    where different components that we often see simply as *the command line* or *the
    shell* for what they are. If we use `zsh` on Linux, nearly everything that BASH
    has built in is replicated in `zsh`, and they both conventionally use the same
    operating system tools. PowerShell has few, if any, replicated built in commands
    and no shared conventions making it painfully obvious what was coming from the
    shell and what is part of the operating system outside of the shell.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: In general, however, it is most advised to do all scripting in languages that
    are well supported in your environment. Just like we have said in other areas
    of system administration, it is important to use tools that are ubiquitous, well
    understood, and appropriate for the environment. For most that means BASH exclusively.
    BASH is the only scripting environment that is going to be absolutely available
    on every Linux system that you ever encounter. Other shells or scripting languages
    might be common, but none other are so universal.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: When BASH proves to be too limiting for more advanced scripting it is uncommon
    to turn to another shell, such as `zsh`, as other shells remain very uncommon
    and generally lack in the extensive power that you are likely looking for once
    you are abandoning BASH. Traditionally it has been non-shell scripting languages
    that are used as BASH alternatives for advanced scripting such as **Python**,
    **Perl**, **Tcl**, **PHP**, and **Ruby**. Ruby has never gained much favor. PHP,
    while very common for certain tasks is pretty rare as a general system automation
    language. Perl and Tcl have fallen out of favor dramatically, but at one time
    Perl was the clear leader in system automation languages. That leaves Python as
    a very clear front runner for advanced scripting needs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Python has many advantages overall. It is decently fast. It is available on
    nearly any platform or operating system (including all Linux, alternative UNIX,
    macOS, and Windows.) It is quite easy to learn (often used as a first language
    for new programmers to learn.) It is very often already installed because a great
    many applications and tools on Linux depend on Python so you will regularly find
    it already installed even when it is not intentionally installed as a standard.
    Because it is used so commonly for these tasks, it has become increasingly well
    suited to the role as documentation in how to use Python in this way has grown
    and other tools written around it have sprung up.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'At this time, nearly all system scripting for Linux is done in BASH when possible
    and in Python when more power or flexible is really needed. All other languages
    are really niche use cases. This means that BASH and Python also have additional
    reasons that we should be strongly considering them when choosing languages for
    our own scripting: standardization.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: System automation is different than general programming. With broader programming
    developers spend years learning multiple languages, language families, constructs,
    and spend all of their time in their programming environments. Moving between
    languages, learning a new one, adapting to language changes and so on are all
    part of the daily life of a developer and the overhead to move between languages
    is very low. For a system administrator this is a bit different. In theory we
    spend very little time learning programming and rarely are exposed to any real
    variety of languages. So for administrators, having just one or two languages
    that you use is important for being able to find resources, examples, peer review,
    and others to provide support for our automation in the future. These are certainly
    not the only acceptable languages, but they do hold rather sizable advantages
    over most other options.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Of course, scripting is a very general topic and scripts can run from being
    just a few lines of simple commands run sequentially to giant programs full of
    complex code. Growing your skill in scripting is a topic all to itself and well
    worth investing significant time into. Good scripts will generally include their
    own logging mechanisms, error detection, functions, reusable components, and more.
    You can essentially invest indefinitely in greater and greater programming skills
    to apply to system automation scripts.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Developer tooling for script writing
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether you are just writing a very simple script or working on a masterpiece
    of automation to be passed down from generation to generation of system administrators
    in your organization, it may be worth taking an additional step to learn something
    about tooling used in software development to potentially aid in the script writing
    process.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: On the simplest side are tools like integrated development environments or IDEs
    that can make writing code faster and easier and help you to avoid errors. Developers
    nearly always use these tools, but system administrators will often overlook them
    as they feel that they write scripts so little of the time that learning another
    tool may not be worth it. And perhaps it is not, but the more tooling you learn
    the more likely you are to use it and to write more scripts. A good IDE can be
    free and quite easy to use, so is a good starting point as you can integrate one
    into your process without really spending any money and only a few minutes to
    download and install one.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: The other truly enormous toolset that developers almost universally use and
    system administrators rarely do are code repositories and version control tools
    like Git and Mercurial. With tools like these, and a central hosting of your code
    which is often associated with these tools, we can really leap forward in our
    script writing and management. These tools are also really useful for the management
    of other forms of textual data in our environments. Linux especially uses text-based
    configuration files which can be treated just like scripts and kept under version
    control and stored in version control systems. An excellent use of cross-domain
    skill sharing.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Version control is certainly the most must have technique from the software
    development world for use in our own scripting. Version control allows us to track
    our changes over time, to test code and have a simple ability to roll back, it
    allows for integrating multiple team members into the management of the same scripts,
    it tracks changes by user, empowers code review and auditing, simplifies data
    protection and deployment and so much more. If you use only one major development
    technique, this is the one to use. At first it will feel cumbersome, but quickly
    it will become second nature and make so many things that you do so much easier.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: The development world has many other tools that we might potentially use like
    continuous integration, automated deployments, and code testing that all might
    provide to be useful depending on the scripting that we do, but nearly all of
    those are niche and completely optional even in a very heavily automated environment.
    Learning about these tools can expose you to options that may or may not make
    sense for your workflow, and will also give you great insight into how your potential
    development teams may be working.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Look to software engineering as a source of ideas about how to better approach
    your own script writing, but do not feel that you need to or even should adopt
    every tool and technique. Scripting for automation and product development do
    have overlap, but are ultimately different activities.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: There is no secret to scripting other than just doing it. There are many good
    books available and resources online. Start with the simplest possible projects,
    look for opportunities to do scripting where you might have done work manually
    before. System tasks such as deployment or system setup checklists can be a great
    place to start. Or scripts to deploy a set of standard tools. Or perhaps a script
    to collect a specific set of data from many machines. I often find myself scripting
    data processing tasks. Once you start looking you will likely find many places
    where some new scripting skills can be put to work.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: One of the best places to start using unscheduled scripting is for basic build
    and installation tasks. Using scripts to perform initial system setup and configuration
    including installing packages, adding users, downloading files, setting up monitoring,
    and so on. These tasks generally offer large benefits at relatively little effort
    and can serve as being essentially a form of documentation listing any packages
    and configuration changes needed for a system. Documentation done in this way
    is highly definitive because it truly documents the actual process used rather
    than one that is intended or expected.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Documentation first
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In software engineering circles there is a concept of writing tests to verify
    code. While not perfect, running tests makes it far less likely for software to
    have bugs because there are tests that look for expected behavior and ensure that
    it is happening. We can still have bugs, this is anything but a guarantee, but
    it is a great step. After decades of writing tests for code, the idea that it
    would be feasible to write tests before writing code was floated and in research
    it is sometimes found that in doing so not only are bugs reduced but code writing
    efficiency can actually improve simply because test writing encourages thinking
    about problem solving in good ways. Test-first coding was considered a breakthrough
    in approaching software development.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: This concept can carry over to the system administration world, in a manner
    of speaking, with the use of what I call documentation-first engineering. In this
    concept we start by writing documentation and then using that documentation to
    build the system. If it is not documented, we do not build it. Like test-driven
    coding, this approach forces us to think about how we want a system to work ahead
    of time which gives us another opportunity to make sure that what we are doing
    is well planned and sensible. And it allows us a chance to verify that our documentation
    is complete and sensible. When we write documentation after the fact it is far
    easier to make documentation that cannot really be followed for completing a task.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, such as those with low automation levels, this could mean simply
    documenting what we can in a wiki or word processor document and working from
    that as we deploy a system. If we have higher levels of automation then we may
    actually write code as documentation that builds our systems for us. Since the
    code itself functions as the documentation it is not just documentation-first,
    but the documentation actually does the work which absolutely guarantees that
    the documentation is complete and correct!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: It is an intrinsic nature of automation to encourage better documentation and
    to move from documenting after the fact to before the fact and on to using the
    documentation itself as the build mechanism. This also means that we can potentially
    see double gains in efficiency as version control and backups and other mechanisms
    that we want to use for both documentation and scripting can be automatically
    applied to both.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Using advanced tools for our scripting may also be considered a higher step
    of automation maturity in a way.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Scripting combined with task scheduling
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The hopefully obvious next step is to take task scheduling and our new found
    scripting knowledge and combine the two for even more power. Making complex tasks
    and making them able to run automatically without any human intervention.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Common tasks to automate in this manner will often include software updates.
    Having a script that looks for the latest updates, downloads them, prepares the
    environment, and deploys them all automatically on a schedule is very handy. Nearly
    any complex set of tasks that should be performed together can be scheduled in
    this way whether it is every minute or just on the third Tuesday of the month.
    Scripts are also very good for dealing with conditional situations where actions
    should only be performed under certain conditions such as only if storage is beyond
    a certain level or if certain people are logged in.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Almost a special case, and therefore well worth mentioning I believe, is using
    scheduled scripts to manage backups or replication.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: State management
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most amazing changes that we have experienced in system automation
    is the introduction of state machines and state management for systems. State
    can be a difficult concept to explain as this falls far outside of the normal
    thought processes in IT and system administration. State is often seen as the
    future of systems, however.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'In traditional systems administration and engineering we talk and think about
    tasks: how do we make a system get from point A to point B. In state theory, we
    do not talk about the *how* of managing systems. Instead, we focus only on the
    intended results or *resultant state.*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '*To think of it in another way: we start focusing on ends, instead of focusing
    on means. We move from process oriented to being goal oriented.*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: This approach forces us to really change nearly everything that we think about
    and know about systems. It is a game changer in every sense and frees us, as humans,
    to focus far better on what we are good at while allowing the computer to do far
    better what it is good at.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: All of this magic is done by what is called a *state machine.* In the context
    of system administration, a state machine is an agent or portion of code that
    is given a document or series of documents that dictate the desired state of the
    system. State can refer to nearly anything about a system such as what packages
    are installed, what the current patch level is, the contents of configuration
    files, the ports open in the firewall, and the list of which services should be
    running.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: A state machine will take this documentation as to the intended state of the
    machine and guarantee (or at least attempt) to ensure that the system is in the
    state desired. If a package is missing, it will install it. If a service is not
    running, it will start it. If a configuration file is incorrect, it will correct
    it. The opposite is also true, if a program appears that is not supposed to be
    installed it will be removed. If a service starts that is not supposed to run,
    it will be shut down.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The state machine typically runs every so many minutes or seconds and scans
    its state file and determines how the system should be, then scans the system
    to verify that everything it knows about how it is, and how it should be, match.
    It then takes whatever corrective action is required. Of course, under the hood,
    this is all done by complex scripts and system tools, that when assembled provide
    the power to enforce state. The degree to which corrective action can be taken
    by the state machine is determined by the power of the scripts that it has access
    to use. It is not unlimited, but generally on Linux a state machine will have
    enough power to do whatever is effectively needed in a real world, non-attack
    scenario and will remain highly effective at slowly or thwarting many less intensive
    attacks as well.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: In this way, in theory, a state machine keeps a system in a nearly constant
    state that we desire. With state machines we spend our time writing the documentation
    of how we want a system to be and we let the state machine itself worry about
    how to make the system behave in the desired way. This includes the initial setup
    of a machine to take a basic, vanilla operating system install and turn it into
    a functional component of a specific workload. State machines work at the hypervisor
    and cloud levels as well, allowing us to maintain a standard conceptual approach
    not just inside of an individual system, but at the platform level providing the
    systems in the first place.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: The end of the login
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The nature of state management is to encourage, if not enforce, the end of the
    concept of logging into a server for administration purposes altogether. Before
    we think of eliminating logins, however, state management systems serve to improve
    traditional remote management through state.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally the biggest risks in remote administration are the needs to open
    ports and to have those ports open flexibly to allow for management from whatever
    location is necessary at the time. The idea of opening very few ports and locking
    them to a single IP address sounds like great security in theory, but is all but
    useless in real world practice. Having the flexibility for an administrator to
    log in quickly, from wherever they are at the time of an emergency, either requires
    too much exposure or far too many steps to limit access.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Enter state management. With state management a system can be instructed via
    the state definition file stored centrally in a repository to enable the SSH service,
    open a random port that gets used for that SSH service, and to lock it to the
    current IP address of an administrator or group of administrators. And the version
    control system will easily track that the change was requested, when it was requested,
    and by whom. In theory there could trivially be an approval step included as well
    as part of the mechanism. Once recorded the system would authorize access for
    the specified administrator(s). And once they were done, or on a set schedule,
    the state management system will revert the changes, after it has all been documented,
    and completely close off all avenues of access. The potential for enhanced security
    is incredible.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: But that is only an interim step. With full state management we should, in theory,
    never need to log into a system at all. We should be able to do perform any management
    steps necessary via the state system itself or, even more appropriately, those
    steps should be being performed automatically by the state engine to ensure that
    proper state is maintained.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: To fully enable a login-less mechanism of this nature we have to combine something
    like state management with concepts that we talked about in the last chapter such
    as having remote log collection and alerting so that even for tasks like capacity
    planning that there is no need to be physically logged into an individual system.
    To traditional system administrators this often sounds like blasphemy and all
    but impossible, but this is how many companies operate today and it is entirely
    possible with the right work being done up front.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: For systems running on physical hardware inside of the office this might sound
    like overkill, and perhaps it is. For systems running on cloud servers this is
    highly practical in many cases. Human intervention should not be needed for a
    properly tested, documented, and running system. Manual management is difficult
    to document, very difficult to repeat, and highly error prone. Of course, human
    intervention can always be saved as a last resort, but the ability to remove it
    completely and depend on redeployment as a last resort is very obtainable today.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Automation maturity models give us a sort of roadmap of how to get from where
    we are to where we hope that we could be. Certainly not every organization has
    to get to the point where state management is handling all of their needs. Not
    every environment even needs to be scripted! Most organizations will continue
    to benefit no matter how far our maturity level is taken.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: The final level of the maturity model we are saving for its own section. Taking
    what we have learned and applying the techniques we arrive at...
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as code
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Taking concepts that we have discussed here and looking at them another way,
    we discover the concept of *infrastructure as code.* Meaning that we can write
    code or configuration files that represent the entirety of our infrastructure.
    This is powerful and liberating.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to confuse infrastructure as code concepts with state machine concepts
    because they will, in many cases, overlap quite extensively. There are critical
    differences, however.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as code can go hand in hand with state machines, but state machines
    do not allow for imperative system definitions. Infrastructure as code can be
    used to define state, also known as a declarative approach to infrastructure as
    code, or an imperative approach by which operations are defined rather than final
    state, making it feel much more like traditional systems administration where
    we focus on the means rather than the ends or the *how* rather than the *goal*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Platforms and systems
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Infrastructure refers to both the systems that we run, that is the operating
    system containers, as well as the platforms, that is hypervisors and physical
    machines, on which they run. For most of what we are looking at in this section
    and certainly concepts like infrastructure as code, the applicability is equal
    to both aspects of infrastructure.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: The tools and techniques that we apply at the system level will work with our
    platform level and vice versa. That means that we do not just get to rely on these
    awesome tools and techniques for configuring our operating systems and applications,
    but they can be used to actually deploy and build the virtual machine containers
    (both full virtualization and containerization) in both cloud and traditional
    non-cloud environments.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Applying the same or similar tooling across these two domains means greater
    overall power through conceptual integration and a more complete picture of our
    infrastructure as a whole. A key benefit in much of modern computing is moving
    from purely seeing operating systems as the building blocks for workloads to also
    seeing the hypervisor as playing a direct role in workloads as well. Instead of
    the hypervisor simply providing a space for an operating system to contain a workload,
    the hypervisor or hypervisor cluster can serve as a platform that is workload
    aware and act as a tool in dividing up resources to provide as a workload component
    level.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: For example, the hypervisor or even hypervisor cluster level management will
    be aware that it is providing workload containers for application servers, proxies,
    processing nodes, storage, backups, databases, and so forth. Because the platform
    level is workload aware, it can then make intelligent provisioning decisions as
    to not only what kinds of resources will be needed, but also onto which nodes
    a workload should be deployed. If we have a three node hypervisor cluster and
    we provision three application virtual machines our provisioning should know to
    spread these out with only one virtual machine per node to increase redundancy;
    and it should know to do the same with the accompanying database that feeds those
    application servers. But it should also know to keep one application server on
    the same node as one database and to configure those to talk to the local database
    instance rather than a randomly non-local one. Bringing workload level awareness
    all the way down from the application, through the operating system, and down
    to the platform means better performance, with less effort, and more data protection.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Typically, as we move into infrastructure as code, we naturally begin to merge
    systems and platform administration groups because it just makes sense to see
    these as two parts of a larger, holistic infrastructure vision.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Imperative infrastructure as code design gives us many of the upfront benefits
    that we also saw with state machines, but require less work to set up initially
    and much more work to maintain over time. Because of this, imperative systems
    were considered normal when infrastructure as code was first introduced but as
    the market matured and declarative (stateful) tool sets and pre-built imperative
    structures were made available, the shift to declarative infrastructure as code
    was inevitable.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, of course, all systems like this are going to be imperative.
    Any declarative system ultimately uses pre-defined imperative steps to arrive
    at the desired state. So in order to have a declarative system for us to use,
    either we or someone else has to wrote imperative scripts and tools that get us
    from many different starting points and result in the same ending point. It takes
    a lot of time and testing to build these components even once a base state engine
    has been designed.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: These scripts have to exist for every task. If you want to define that a file
    must exist then we need tools under the hood that check for the existence of the
    file, logic to determine what to do if the file does not exist, how to find it,
    how to copy it, where to put it, what to do if a file copy fails, and so forth.
    This is probably the simplest use case and you can easily imagine how much more
    complicated it is to deal with any other task. Even a simplistic declarative system
    is going to be made from a myriad of imperative scripts that the administrator
    may never know about. Or it could be built by the administrator for very specific
    needs unique to the systems in question. The concept is flexible, but complex.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: In theory we can have declarative state management without going as far as to
    have infrastructure as code. As odd as it may sound, it is actually somewhat common
    for those starting out with state systems to do so almost manually, attempting
    to issue stateful commands in a nearly imperative way to inform the system of
    desired state without documenting all aspects in code before doing so.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: It is also common for the tools for these techniques to be deployed, and even
    used, but in a very incomplete way. This can be due to frustration, a lack of
    planning, internal corporate politics, you name it. Because working completely
    in this mode is so intensive and requires great planning up front it can be very
    difficult to obtain adequate time and buy-in from the powers that be to allow
    for complete implementations. Because of this we often see these tools being used
    to roll out basic, well-known functions using pre-built third party scripts, but
    complex configurations often unique to an organization still being done in a traditional
    way. The rush to get systems deployed will often drive this behavior.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to what is really the key best practice around infrastructure
    as code. What matters most is getting broad organizational buy-in and investing
    in proper documentation and code completeness prior to workloads being put into
    production.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Like any code that we would talk about in software engineering circles, our
    infrastructure as code requires testing before being used. Testing system administration
    code is often far easier than other types of applications, however. Creating our
    code for a new workload then attempting to deploy that workload from scratch is
    quite straightforward and can be attempting over and over again until results
    are perfect. Then system modifications, breaks, and other potential scenarios
    can be tested to ensure that the system will respond well under potential real-world
    problems.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: One of the great benefits of this type of testing is that the function of our
    code is to create infrastructure out of, roughly, nothing. So all we have to do
    is start with a blank slate and, if things are working correctly, our infrastructure
    will create itself and we can test from there.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: It is true that even if we cannot go as far as we would like by creating an
    entire infrastructure that is self-creating, healing, configuring, and destroying
    we can at least use these tools and techniques to go part of the way. Starting
    with simply building the infrastructure from scratch and not maintaining it or
    decommissioning it is an excellent step.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as code as disaster recovery
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have touched on this concept elsewhere and will really dig into it in the
    next chapter on Backups and Disaster Recovery, but it is so important that we
    have to discuss it whenever we talk about a constituent component of disaster
    recovery in modern systems. As we have talked about infrastructure as code we
    keep talking about the automated creation of systems where none existed previously.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: This is exactly what is needed in a disaster recovery scenario. Our systems
    are gone and we need to bring them back. Having our systems, their standard files,
    their configurations, and more all stored as code in a place where it can easily
    be recovered or even better, not lost at all during a normal disaster, means that
    we are ready to build a new system, anywhere that we want, at the drop of a proverbial
    hat.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Because these types of build systems spend most of their time building workloads
    for testing and production deployment, they are easily justified for being fast,
    efficient, easy to use, well understood, and heavily tested. A system built in
    this way will be built identically during testing, the same in production, and
    the same during an emergency disaster recovery. We do all the hard work to make
    the system fast and repeatable up front so when something terrible happens we
    do not have to deviate from the established process.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Traditional system restores after a disaster require building systems through
    a process that is unique and nothing like the process through which the systems
    were built initially. This is highly error prone for so many reasons. It is a
    process that rarely gets good testing or documentation. It is typically done without
    proper planning and under a high degree of stress. This is the worst possible
    time to be experiencing these conditions. There is so much to go wrong at a time
    when there is the highest pressure to get everything right.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding those problems by creating a system that builds itself the first time,
    the second time, every time, almost instantly and completely identically is truly
    a big deal. If anything justifies the benefits of automation and infrastructure
    as code, it is this. Having the confidence that your systems are going to come
    back fast and correctly is something that most companies do not have today. The
    fear that backups are not going to work, that the knowledge of how to get a system
    up and running correctly configured for a workload is missing, that licenses or
    system details are not documented, or that necessary packages are not readily
    available is dramatic. Instead of overlooking those problems and hoping for the
    best we have a modern approach that makes these issues simply go away.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Getting our organization to the level of truly implementing infrastructure as
    code is a tremendous step, but this is the future. Companies that do this have
    faster builds, faster recoveries from disaster, better security, are more agile,
    scale faster, and all of that means have a better chance of making more money.
    At the end of the day, our only job as system administrators is to do our job
    in such a way that we can increase the bottom line of the organization through
    our efforts, no matter how indirect and impossible to measure that they may be.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what the techniques are it is time to talk about the actual,
    already existing, ready to be tested tools that make things like state machines
    and infrastructure as code possible.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Modern tools of automation
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of this power comes primarily from modern tools that have been being introduced
    into the realm of system administration over the last fifteen to twenty years.
    The Linux world has been very fortunate to have been at the forefront of this
    movement since the very beginning. This comes naturally both because the Linux
    community tends to be one that thrives on and focuses on innovation, but also
    because the intrinsic nature of a system built around command line interfaces
    and simple text files for configuration and software repositories all make for
    vastly simpler automation. The design of Linux may not have been intentional to
    encourage automation, but nearly every major aspect of both the system implementation
    and the behavior of the ecosystem have led to it having the most ideal combination
    of factors to nearly always make it the leader in new automation tools and strategies.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Configuration management systems
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: New tools are always arising and techniques do vary over time so making a definitive
    list here is not possible, but there are some important tools that have managed
    to make a name for themselves to a point that they are worth mentioning as starting
    points for an investigation into the tooling that will likely make sense for your
    environment. The biggest tools for this type of automation, for infrastructure
    as code, at the time of this writing include **Chef**, **Puppet**, **SaltStack**,
    **CFEngine**, **Ansible**, and **Terraform**. The oldest of these, **CFEngine**,
    is so old that it was first introduced in its earliest form less than two years
    after the first Linux kernel was introduced!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: All of these tools, often referred to as configuration management systems, share
    the common approach of allowing you to write code and configuration files to define
    your infrastructure environment and they manage the automation of the infrastructure
    based on that code. Most of them offer multiple functional behaviors such as the
    option to function either imperatively or declaratively. Most also offer the option
    to either pull configuration from the systems being managed or to push configuration
    from a central management location. So you get the range of functional options
    plus a range of product options.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: The biggest differences between these products really come down to the style
    of coding that is used to document your infrastructure and the cost, support,
    or licensing of the products. Most of the products in this space either started
    as or moved to being open source. Open source makes a lot of sense as the space
    has matured quickly and management tooling somewhat naturally tends towards open
    source as it is mostly made by end users or those that came from open source communities
    and is used only in technical circles. Closed source products naturally lend themselves
    towards more customer-visible products where managers, rather than engineers,
    are choosing them. A key strength to most infrastructure as code tools is that
    they are free, open source, and often included in the Linux distribution so engineering
    and administration teams can generally choose to test and deploy them even into
    production without management needing to approve or even be aware that they are
    doing so. Because of the licensing and use cases it is little different than needing
    to deploy OpenSSH or any other standard component used in day-to-day system administration.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Of course the easiest approach here is to simply read about a few tools, download
    and install a few, and see what you like. Working with more than one is not a
    bad thing and most of the concepts will carry from one to another, even if the
    style of scripting and documentation vary. Many of these tools are cross platform
    and while they are generally designed with Linux as the primary platform both
    for deployment and to be managed, it is not uncommon for other platforms, especially
    BSD but also Windows, macOS, and others, may be able to be managed as well. Consider
    the possibility of choosing a platform that will be expandable to meet all of
    your organization's needs well into the future. If you are a heterogeneous shop
    you may want to invest your technical know-how into a platform that could be managing
    everything across the domains rather than only managing Linux, even if Linux is
    where you start.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Desktops are servers
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thinking of desktops as a form of server can be a bit confusing, but in a way,
    they are. They are simply one to one end user GUI servers that are generally deployed
    to desktops or homes rather than in the datacenter. This is a trivial bit of semantics
    until we start trying to understand how desktops may or may not fit into our greater
    support strategies.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: But when we consider that desktops are truly just a special class of servers
    it quickly becomes apparent that it makes sense to potentially group them in for
    common administration as well. Of course if we are using a Linux distribution
    for a desktop this is much more obvious than if we are using Windows, for example,
    but the truth remains the same. If we are going to potentially support Linux and
    Windows servers both using the same tooling, there is really no barrier to doing
    the same for desktops regardless of the operating system(s) deployed there.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditionally, but for no reason of which I am aware, servers and desktops
    have been managed using very different toolsets. I can hypothesize many reasons
    why this might be. Typically, two separate teams provide this management and each
    chooses its own tooling independently. Servers grew up in one world while desktop
    support grew up in another. Vendors want to sell more tools and catering to hubris
    always makes sales easy. And the most likely factor: servers are typically administered
    from the command line and most desktop support teams expect to work purely from
    a GUI.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: When we step back and look at desktops (and other end user devices) as if they
    are servers it is easy to see that the same tools to manage, document, and monitor
    servers work equally well with desktops. There is no real reason to treat them
    differently. Of course desktop support teams will always require the ability to
    remotely see, and probably interact with, the end user's graphical desktop to
    be able to assist the end user's issues directly, but that is a different need
    entirely from administration duties.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Desktops can be, or even should be, managed using the same advanced tools and
    techniques like infrastructure as code and state machines as we would with any
    other server. That is correct, when we understand that they are actually a server,
    then rules that apply to all servers also apply to desktops. Good semantics make
    everything easier to understand.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: In fact, there is even a good argument that end user devices are among the most
    valuable to apply these techniques to because they are the most likely to need
    to be rebuilt, modified, share profiles, undergo changes, be compromised, or need
    to be managed while not in communication with central services. This last point
    is especially valuable because state machines that keep functioning with their
    current state will continue to provide security and self-healing characteristics
    for machines that are off of the network and can enforce policies independently.
    This is possible to do in more traditional ways but is harder and less likely
    to be done.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Because of the nature of how infrastructure as code systems tend to be deployed
    it is far more likely, as well, for these systems to keep functioning when a desktop
    or laptop is off of the corporate LAN compared to traditional management tooling
    that is generally built entirely around the LAN concept. Because end user devices
    traditionally have a high frequency of either moving on and off of or simply never
    existing on the LAN having tools that are not LAN-centric is typically much more
    important in the end user space than in the datacenter space already.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: In some ways, mobile device management, or MDM as it is generally known, is
    an attempt to make tools that work more likely infrastructure as code and state
    machines, but that are presented more like traditional tools and sold through
    more traditional channels with a focus solely on end user management. These tools
    have been successful, I feel, because they are copying much of the technology
    and common approaches from this space and once we work with these tools we typically
    find that mobile device management tools do not make sense unless we are lacking
    these capabilities in our organizations.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Many of the benefits of any new system, of course, come from convention rather
    than strict definition. One of the largest conventions in the infrastructure as
    code space is the move from LAN-centric to network agnostic system management
    deployments. It is common, and nearly expected, that the tooling for infrastructure
    as code systems will be hosted in some public form whether on cloud, VPS, or colocation,
    and kept external to any LAN(s) that may exist for your organization.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Hosting management infrastructure outside of the LAN means that any LAN-centric
    ties that we might otherwise make accidentally or casually are no longer possible
    unless we deploy a LAN extension technology like a VPN. This convention naturally
    moves us away from deploying technology that only works inside of a LAN or that
    uses the LAN boundaries as security features. Eliminating the LAN boundaries frees
    us to manage multiple sites, mobile users, even multiple organizations transparently
    from a single platform, as long as those systems use the Internet. Traditional
    systems break under so many circumstances, while also having common security vulnerabilities
    both because of the tendency to break easily or have gaps in utilization, and
    because LAN-centric thinking is not good security practice.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Version control systems
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The other category of tools that we should really address within the topic of
    automation is code repositories and version control systems. These are technically
    two separate things, but almost always go hand in hand.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: At their core, version control systems simply keep track of the changes made
    to documents that we have so that we can track essential data such as who made
    a change, when it was made, and what the document looked like before, at the time
    of, and at a later date than the change. This alone is quite powerful, but most
    any system that does this today also serves to distribute the code and the version
    control so that it can be used by multiple people, in multiple places. And in
    being able to do that, can also be used to populate a central repository which
    can be treated as a master location for storage so that no single person's endpoint
    needs to be considered vital for the purposes of data protection. That central
    location becomes a location for backups and recovery, as well!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Version control systems influence many modern document systems and we covered
    both in our earlier chapter on documentation, but in this context we could look
    at real world products such as Google Docs, Microsoft Office Online, and Zoho
    Docs, all of which present traditional document file types or interfaces, but
    all provide version control of those documents. These are very clunky to use for
    the purpose of coding and code management, but all will serve in a pinch if you
    are looking to just get started quickly using what you already have deployed.
    These systems are essentially copying the mechanisms of traditional code version
    control systems and applying them to spreadsheets and word processing.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Since these office document types are so well known, it is almost easier to
    picture these as the standard (they are not) and thinking of code version control
    systems as being a code-specific modification of those document tools (they are
    not, they came first.) These systems generally (and by generally, I mean every
    situation that I am aware of) work with standard text files and so can be used
    with any text editing tools whether you work with something basic like *vi* or
    *nano* directly on the Linux command line or you work with robust tools like *Atom*
    or *MS Visual Studio Code* that provide fully graphical coding environments with
    deep editing awareness and features, you can use version control systems. Some
    advanced environments will actually have version control integrated directly into
    the applications so that you can automate the entire process from a single place
    and make it look and feel much closer to the office style tools!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'In a practical sense, at this time, two protocols for version control have
    risen so far to the top that it almost feels like there are only two choices really
    left on the market: *git* and *mercurial.* In reality, there are many, but only
    these two require mentioning. Feel free to research other tools and protocols,
    but make sure that these two are included in any research short list that you
    might have. Both are free and work similarly and allow for all of the features
    that you often expect today including central repositories, copies on end user
    machines, automated deployments, version meta data, and so on.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the protocols that are used, much of the power stemming from version
    control systems today come from the online repository services that power them.
    Of these there are more and you can run your own in house as well. The two key
    players are Microsoft's GitHub and the open source GitLab. Both are hosted services
    with extensive free offerings, and GitLab also offers their software for free
    that you can host yourself if this is a business or technical requirement for
    your environment. These two services, and others like them, provide central git
    and Mercurial repository locations, a centralized location for backups, a simple
    web GUI for code manipulation and management, and a raft of processes, tools,
    and services around code automation. Much of which is likely overkill or useless
    in a system administration environment, but much of it does have a potential use.
    You can certainly get the benefits that you need without these types of services,
    but it is far harder to do so and nearly all successful environments have been
    relying on them for years. They are almost always free for the needs of system
    administration, do not avoid them. As hosted services, their use is yet another
    means of *breaking free* from LAN thinking, as well.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: There is not much of best practices or even rules of thumb to discuss when talking
    about tools. Testing multiple tools, keeping up with the market as to what is
    available and what is nearly developed, evaluating the tools that make sense for
    your organization, and learning your chosen tools inside and out are all standard
    good approaches.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Rules of Thumb:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Management tools should almost always be open source. This is an area where
    security is of the absolute utmost importance and where licensing limitations
    create security risks on their own. So open-source matters more here than in most
    areas.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy management tools in a network-agnostic way. This means deploying them
    on the Internet in a place that is accessible to reasonably any machine located
    anywhere. Avoid any semblance of requiring or relying upon traditional LAN networking
    for connectivity or security unless absolutely necessary.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Best Practice**: Keep code of all types under version control and in a repository.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Now we have covered the techniques and talked briefly about a handful of real
    world tools that you can use to initiate your investigations into tools that you
    want to try to deploy and learn for your own environment.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we have looked at why automation is important. We investigated
    how we should approach automation and where to look to get started. We discussed
    maturity modeling. We delved into the rather complex topics of state machines
    and infrastructure as code. And finally we tackled actual tools that you can download
    and learn today to take your coding to a totally different level, entirely.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'In our next chapter we are going to be going into one of the absolutely most
    important, and most commonly avoided, topics in system administration: backups
    and disaster recovery. Do not try to skip over this coming chapter, if there is
    one thing that we need to get right in administration, it is our ability to avoid
    or recovery from a disaster.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
