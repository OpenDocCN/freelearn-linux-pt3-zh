# 第四章：设计系统部署架构

我们如何部署系统决定了这些系统今后将表现如何以及它们的弹性将持续多久。了解设计组件和原则对我们理解如何设计将承载我们工作负载的平台至关重要。请记住，归根结底，只有位于堆栈顶端的应用程序才真正重要 - 在应用程序之下的一切，无论是操作系统、虚拟化程序、存储、硬件还是其他工具，都只是用来实现最终应用级工作负载所需功能的工具。很容易感觉到这些其他组件的重要性，但实际上它们并不重要。换句话说，重要的是结果，而不是达到结果的途径。

在本章中，我们将首先查看系统的构建模块（除了我们在上一章中广泛讨论的存储外），然后将所有这些组件作为一个整体来查看它们，以了解它们如何形成我们应用工作负载的强大载体。接下来，我们将进行需求分析。最后，我们将继续将这些组件组装成满足这些需求的架构设计。

通过本章的学习，您应该对此感到自信，尽管 Linux 可能只是我们应用程序堆栈中的一个片段，但您已准备好正确设计整个堆栈，以满足工作负载目标。尽管从技术上讲，这种设计大部分并不严格属于系统管理（或工程），但它通常由系统管理员来处理，因为只有极少数组织拥有来自其他部门高技能和全面知识的员工。系统团队处于所有组件的交汇点，并在两个方向上（向上到应用程序和向下到虚拟化程序、硬件和存储）具有最大的单一角色可见性。系统团队被赋予更大的设计任务是很自然的。

在本章中，我们将学习以下内容：

+   虚拟化

+   容器化

+   云和**虚拟专用服务器**（**VPS**）

+   自有、托管和混合托管

+   系统设计架构

+   风险评估和可用性需求

+   可用性策略

# 虚拟化

二十年前，如果你问一个普通的系统管理员什么是虚拟化，他们可能会一头雾水。自从 IBM 在 1965 年首次在其大型计算机系统中引入虚拟化技术以来，我们就已经在 IT 领域中拥有了这些技术，但对于普通公司来说，这些技术相对较少且难以获得，直到像*VMware*和*Xen*这样的供应商在千禧年之交将它们带入了主流市场。企业领域在 20 世纪 90 年代已经使用了许多这些技术，但对它们的了解并没有广泛传播开来。

时代已经变了。自 2005 年以来，虚拟化已经广泛普及并被广泛理解，每个平台和所有价格点都有选择，没有人因为技术或财务上的限制而需要避免实施这项技术。在其核心，虚拟化是创建一个*软件中的计算机*（在实际硬件的顶部）并呈现标准虚拟硬件集的抽象层。执行虚拟化的软件称为**hypervisor**

在上一章中，我们反复谈到接口以及某些东西如何消耗或呈现自己作为磁盘驱动器或文件系统，例如。hypervisor 是一种呈现*计算机接口*的软件，这意味着它不仅仅呈现硬盘驱动器，而是像整个计算机一样运行。如果您从未使用过或思考过虚拟化，这可能看起来非常复杂和令人困惑，但实际上，这是一个通常使计算变得更简单和更可靠的抽象。就像抽象存储的技术（如**逻辑卷管理器**和**RAID**系统）一样，在其成熟和理解之后，计算机级虚拟化也被证明是非常有价值的。

在本章中，我们将讨论两种类型的 hypervisor，它们分别被称为 Type 1 和 Type 2 hypervisor。所有的 hypervisor 都呈现同样的东西：一个*计算机*。但 Type 1 和 Type 2 hypervisor 之间的区别在于它们消耗的内容。

## Type 1 hypervisor

有时称为*裸金属*hypervisor，Type 1 hypervisors 旨在直接在系统硬件上运行，但当然也可以在任何表示自己为能够运行的硬件（如另一个 hypervisor！）上运行。因此，Type 1 Hypervisor 不是应用程序，并且不运行在操作系统之上，因此只需要担心与将安装的物理设备的硬件兼容性。

一般来说，Type 1 hypervisor 被认为是真正适合生产的唯一类型，因为它们直接安装而无需任何不必要的软件层，因此可以更快、更小和更可靠。

Type 1 hypervisor 最初更难工程化，因此最早期的 hypervisor 通常是其他可以将工作传递给操作系统的类型。但事实上，是 Type 1 hypervisor 的引入和足够多的不同产品供应商使市场成熟，鼓励了在 2000 年代极端转向虚拟化。

## Type 2 hypervisor

与裸金属 hypervisor 不同，Type 2 hypervisor 是一个安装在操作系统上的应用程序。这意味着 hypervisor 必须等待操作系统提供资源，与其他应用程序竞争资源，并要求操作系统本身稳定，除了 hypervisor 稳定外，还需要使工作负载在其上运行。

当虚拟化技术相对较新时，尤其是在微型计算机领域，由于成本更低且更易于制造，并且几乎不需要硬件支持来完成其工作，因此类型 2 虚拟化监视程序更为常见。类型 2 虚拟化监视程序让裸金属操作系统来处理供给驱动程序和硬件检测、任务调度等繁重工作。所以在 2000 年代的大部分时间里，我们看到类型 2 虚拟化监视程序在推动虚拟化技术采用中扮演了主要角色。它们易于部署，非常容易理解，因为它们只是一种在操作系统之上部署的应用程序，任何人都可以在现有的台式机甚至笔记本电脑上安装一个来尝试虚拟化技术。

到了 2000 年代末，技术发生了相当大的变化，软件得到了先进和成熟的发展，几乎所有计算机都获得了某种程度的虚拟化硬件支持，允许虚拟化监视程序使用更少的代码同时获得更好的性能。类型 1 虚拟化监视程序迅速扩展，到了 2010 年之前，在生产环境中使用类型 2 虚拟化监视程序的想法几乎是不可想象的。类型 1 虚拟化监视程序提供了一个统一的标准操作系统安装目标，将大部分繁重工作从操作系统转移到监视程序，被普遍认为是更佳选择。因为监视程序控制裸金属，能够适当调度系统资源，并从系统中获得最大性能。监视程序预计只占操作系统大小的一小部分。这意味着它几乎只是虚拟化操作系统和物理系统之间的一个垫片（一层极少代码的基本功能层，对于运行在其上的操作系统基本不可见）。这最小化了膨胀和功能，而操作系统通常需要庞大、复杂和功能丰富以在大多数情况下完成其工作。

类型 2 虚拟化监视程序已被证明在实验环境中非常有用，尤其是在需要从个人计算环境（例如台式机或笔记本电脑）进行测试或学习的情况下，或者对于需要完全禁用或甚至在不再需要时可能移除监视程序的特殊临时工作负载也很有用。但对于生产服务器环境来说，只有类型 1 虚拟化监视程序才真正合适。

虚拟化技术通常与两项最佳实践相关联。

+   除非有要求使您无法这样做，否则应虚拟化每个系统。在实际操作中，您几乎永远不会真正看到这条规则的有效例外。

+   服务器始终使用类型 1（裸金属）虚拟化监视程序。

## 虚拟化监视程序的类型令人困惑。

在现实世界中，要检测什么是 Type 1 虚拟化管理程序可能相当困难。按定义，虚拟化管理程序实际上没有任何自己的最终用户界面。这使得我们必须解释它，但并不是我们真正看到的东西。即使是真正的操作系统也很难指出并说“看，它在这里”，因为我们真正看到和触摸的是作为应用程序在操作系统顶部运行的壳或桌面环境，而不是操作系统本身。对于虚拟化管理程序来说，我们看到的任何界面，无论是什么样的，都必须由运行在操作系统上的东西来呈现，而不是直接在虚拟化管理程序上运行的东西。

当然，虚拟化管理程序需要某种接口来与我们互动。它们处理这个接口的方式差异很大，即使是同类型的虚拟化管理程序，构建方式也不尽相同。在幕后，它们总是在裸金属上运行，但它们可以使用几种不同的架构来处理它们所需的所有功能。每种不同的架构都有不同的机会，展示给最终用户的方式也不同 - 这意味着一个坐到系统前的最终用户可能会体验到完全不同的界面，假装是它们或者可能并不是它们。

在早期的 Type 1 虚拟化管理程序中，运行一个虚拟机（虚拟化管理程序顶部运行的虚拟计算机的名称）通常会赋予它控制虚拟化管理程序的权限。这使得虚拟化管理程序可以尽可能精简，并且可以使用现有工具完成像展示用户交互 shell 这样的大任务，而无需重新发明轮子。采用这种方法意味着早期的虚拟化管理程序工程工作最小化，使得虚拟化本身成为关键关注点。

随着时间的推移，开始出现了替代方法。在虚拟化管理程序顶部运行完整操作系统以充当最终用户接口似乎是资源的浪费。后来的虚拟化管理程序采用创造性方法来解决这个问题，使得虚拟化管理程序本身更重，但总体系统的重量减少。

虚拟化往往令人困惑，供应商很少有理由想要揭示其系统的内部工作原理。因此，误用术语或建议虚拟化管理程序的工作方式与实际不同，无论出于营销原因还是为了试图简化对不太了解的客户的系统。今天有许多虚拟化管理程序，未来可能会有更多。然而，在生产企业环境中，我们预计会定期看到四种，我们将简要分析每种工作方式。没有一种方法是最佳的，这些只是解决同一个问题的不同方式。

## VMware ESXi

今天的虚拟化市场领导者。VMware 是最古老的虚拟化产品之一，随着时间的推移，其设计也有所变化。最初，VMware 遵循经典设计，采用了极简的超级监控程序和一个*隐藏*的虚拟机，其上运行了一个简化版的 Red Hat Enterprise Linux，为用户提供了与平台交互的外壳界面。

如今，VMware ESXi 将一个小型 shell 构建到超级监控程序本身中，仅提供足够的潜在用户交互能力来处理最简单的任务，如检测正在使用的 IP 地址或设置密码。其他所有功能通过从外部工具调用的 API 处理，允许最重要的用户界面部分完全保留在物理客户工作站上，而不是在超级监控程序上。

## Microsoft Hyper-V

尽管晚于企业级 Type 1 虚拟化的竞争，微软选择了经典方法，始终运行一个虚拟机，其中包含一个简化版的 Windows，提供了安装 Hyper-V 后用户所见的图形用户界面。这个第一个虚拟机是默认自动安装的，不需要付费许可，不包含 Windows 品牌，相反它被命名为*物理*机器，这使得它看起来好像根本没有虚拟机，而是在 Windows 安装之上运行的 Type 2 超级监控程序，但实际并非如此。这只是基于复杂的命名和不必要的混乱标准安装过程而看似如此。毋庸置疑，Hyper-V 是一种真正的 Type 1 虚拟化监控程序，以最经典的方式运行。

## Xen

与 VMware 同样来自早期时代的 Xen 也采用了经典方法，但与 VMware 不同的是，多年来一直保持这种方式。然而，与 Hyper-V 不同的是，Xen 的安装过程更加手动化，用于提供用户交互的第一个虚拟机完全不被隐藏，事实上是完全暴露的。这意味着在超级监控程序安装过程中，一个虚拟机会被自动创建（因此它总是第一个），并且该虚拟机被特别授予直接操作控制台的特殊权限。因此，一旦启动，您看到的是虚拟机本身的控制台，因为超级监控程序除此之外没有其他控制台。

*你*甚至可以选择在管理虚拟机中使用不同的操作系统！然而，在实践中，Xen 始终与 Linux 作为其控制环境一同使用。其他操作系统大多是理论上的。这种暴露使得像 Hyper-V 这样的隐藏经典系统变得更加令人困惑，因为在 Xen 中，一切是如此显而易见。

由于 Xen 与 Linux 紧密结合，对于 Linux 系统管理员来说，了解 Xen、通过 Linux 管理 Xen 和 Xen 架构至少有些知识可能很有价值。这种紧密结合并不意味着在使用 Xen 时系统和平台团队会相互交织，但它增加了这种可能性。

## KVM

最后，我们来谈谈**基于内核的虚拟机**（**KVM**）。KVM 之所以特别，有几个原因。首先，它独特地将虚拟化监视程序（hypervisor）合并到操作系统本身中。其次，它与 Linux 一起工作。与其他虚拟化监视程序不同，其他程序中平台管理者负责管理虚拟化监视程序，系统管理员负责管理操作系统层面。而在这里，这两种角色必须合而为一，因为这两个组件已经合并为一个。在使用 KVM 时，无法分开操作系统和虚拟化监视程序。KVM 将虚拟化监视程序直接嵌入 Linux 内核中。它只是内核的一部分，始终存在。

这种方法具有明显的好处。它简化了整个系统，并提供了几乎每种不同方法的优点，只有相对较少的注意事项。当然，也存在注意事项，例如虚拟化监视程序安装可能导致膨胀，增加潜在攻击面的真实风险。KVM 最大的好处可能是利用 Linux 系统管理员和现有知识的生态系统，因此在管理运行在裸机上的系统的某些更复杂的方面，如文件系统和其他存储架构决策、驱动程序支持和硬件故障排除中，与 Linux 共享，从而获得了一个庞大的基础平台和支持网络。

由于 KVM 的易用性和广为人知的许可（由于其包含在 Linux 中），以及广泛的组件可用性，当供应商希望创建自己的虚拟化监视程序平台时，KVM 已成为远远最受欢迎的选择。许多云端大型供应商、虚拟化监视程序管理或超融合空间已利用 KVM 作为其系统的基础，并在其上进行了定制化。

## 虚拟化只是为了整合吗？

大多数人问你为什么*费心*虚拟化，答案总是相同的：*因为它允许你在单个物理设备上运行多个不同的工作负载。* 毫无疑问，当适用时，整合是巨大的好处，但将其陈述为唯一的甚至关键的好处意味着我们忽略了大局。

虚拟化为了节省成本通过合并的持久神话似乎没有人能够驱散。虚拟化的性质对于普通人，甚至是普通的 IT 专业人士来说都过于复杂，它真正提供的东西仍然被广泛误解。虚拟化的真正价值在于它创建的抽象层，这提供了对未知的避险方式 - 一种使系统部署更灵活、更可靠，同时总体付出更少努力的方法。虚拟化为未来可能发生的未知事件提供了更多选项，这是你无法预计的。

对于虚拟化的一个核心挑战是它提供了太多不一定相互关联的好处。大多数人希望得到一个简单、稳定的答案，不想理解虚拟化的工作原理以及为什么添加额外代码层实际上会使系统更简单、更可靠。这些都有点过于复杂。事实上，虚拟化有很多好处，每一个都通常足以证明始终使用它，并且基本没有任何限制。虚拟化几乎没有成本，基本没有性能开销，不会增加管理复杂性（确实会在某些领域增加，但在其他领域减少，从而总体减少）。

不可避免地，人们会问是否存在虚拟化不适合的特殊工作负载。当然存在特殊情况。但地球上每个 IT 店铺的下一个反应是宣称他们独特且唯一，在他们的服务器需求中虚拟化不合理 - 然后他们可靠地声明一种标准虚拟化的股票和绝对理想工作负载，适用于几乎每个人，并且远离特殊情况或免除最佳实践的例外。相信我，你不是这条规则的例外。每次都虚拟化每个工作负载。没有例外。

大多数人提到他们避免虚拟化的例子通常是虚拟化处理不当的例子。其他非虚拟化相关的错误，如选择不良供应商、错误大小的服务器或选择大幅超支的存储层而需要精简的情况，可能会发生，无论有无虚拟化。无论是系统管理员、平台团队还是硬件采购者，他们仍然必须像没有虚拟化时一样做同样高质量的工作。虚拟化并非万能药，但不能成为消除我们工作需要做好的银弹的借口。这只是错误的逻辑。

现在我们应该对虚拟化有一个清晰的理解，而不仅仅是对其作用的浅显了解，并且知道为什么我们要将其用于所有生产工作负载。无论是在家庭实验室工作，还是在大型生产环境中运行，虚拟化应该很快成为第二天性。让虚拟化成为一个不言而喻的结论，只需要担心一些细节问题，比如存储配置、虚拟机监控程序选择或管理工具的使用。接下来，我们将看看虚拟化的替代方案及其近亲——容器化。

# 容器化

有些人认为容器是一种虚拟化形式，有时称为 C 类虚拟化或操作系统级虚拟化。近年来，容器已成为一种独立的存在，特定的容器使用场景成为了热门话题，以至于容器这一概念本身几乎被遗忘。然而，容器代表了一种极为有用的（或作为传统虚拟化的替代）形式。

基于容器的虚拟化与传统虚拟化不同，在传统虚拟化中，每个系统硬件的各个方面都由虚拟机监控程序以软件的形式进行复制，并且每个实例或虚拟机（在谈论容器时通常称为虚拟环境（VE））在其上运行时，都是唯一存在的。虚拟机之间没有任何共享的内容，按定义，任何支持虚拟化硬件的操作系统都可以在其上运行，就像它运行在裸机上一样。

基于容器的虚拟化完全不使用虚拟机监控程序，而是由一款软件构建，这款软件在操作系统中进行强烈的资源隔离，允许安装并运行各个虚拟机，就好像它们是完全独立的实例一样，但在后台，所有作为容器运行的虚拟机共享主机的内核实例。由于这个原因，安装任何任意操作系统的能力受到限制，因为只能安装能够共享同一内核的操作系统到单个平台上。

由于没有虚拟机监控程序，且所有系统共享单一内核，大多数容器系统的开销几乎为零，这使其非常适合许多高要求的任务。容器在 Linux 环境中尤其受欢迎，今天有很多容器选项。容器系统几乎完全没有系统开销，曾经是这种方法的一个关键特性，但随着系统从资源紧张到在许多情况下具有丰富的计算能力，挤压硬件每一滴性能的价值开始减弱，相比之下，完整虚拟化所带来的更大灵活性和隔离性变得更加重要。因此，曾被认为是最伟大的虚拟化选项往往被忽视，甚至被遗忘！

容器不需要任何特殊的硬件支持，完全通过软件实现，使得它们能够在更广泛的平台上运行（例如，很容易在老旧的 32 位 Intel 硬件或 Raspberry Pi 上实现），且成本更低。这使得它们在硬件加速广泛可用之前的时代变得非常重要。

在我们关心的 Linux 世界中，多个不同的基于 Linux 的操作系统可以在同一个容器主机上运行，因为只需要共享内核。因此，在单个容器主机上运行 Ubuntu、Debian、Red Hat Enterprise Linux、SUSE Tumbleweed 和 Alpine Linux 等虚拟机毫无问题。

Linux 拥有（也可能是个负担）几乎每个技术领域都具有大量的选项，容器化也不例外。Linux 上存在多个开源和商业容器产品，但今天无可争议的冠军是 **LinuX Containers**（**LXC**）。LXC 的独特之处在于它完全集成在 Linux 内核中，因此使用它其实只是简单地启用，不需要额外的软件或内核修改。如果你打算在 Linux 上实现真正的容器，可能性很大就是 LXC。几乎所有基于 Linux 的操作系统都完全支持 LXC。

### 容器简史

完全虚拟化由 IBM 在 1960 年代引入，但由于其复杂性，直到 1990 年代末期，借助高端硬件支持和大量特定的软件，才使其得以普及并进入主流服务器市场。

容器首次出现在 1979 年的 System 7 UNIX 中，使用的是名为 *chroot jails* 的机制，按今天的标准看起来相当简陋，但在功能上非常接近现代容器。在 UNIX 世界中，容器（无论是哪种类型）几乎一直都有存在。1999 年，真正的现代容器可以说从 FreeBSD 的 Jails 开始引入，随后其他 UNIX 平台如 Solaris 的 Zones 和 Linux 的 OpenVZ，最终 LXC 开始出现。到了 2000 年代中期，容器已经无处不在，并且在完全虚拟化技术真正起步之前就非常流行。

容器在 2013 年经历了一次复兴，伴随着 Docker 的引入。尽管 Docker 并不完全是一个容器，但“容器”这个术语如今更多地与 Docker 联系在一起，而非与真正的容器。早在 Docker 之前，容器从未被认为是特别吸引人，相反，它们更多是基本的进程隔离工具，承担着为操作系统执行初步安全工作的职责，唯一的例外可能是 Solaris Zones，在 ZFS 发布时曾一度受到大力推广。

如今，因 Docker 的流行及其与容器的紧密关联，大多数人（甚至包括系统管理员！）在提到容器时，会想到 Docker，而不是那些已经存在几十年的真正容器。

我们不能谈论容器而不提到 Docker。今天，Docker 是与容器最相关的名称，且这个称号当之无愧。Docker 最初作为一组构建在 LXC 之上的扩展而出现，为应用程序提供极端的进程隔离以及*打包*的库环境。尽管 Docker 使用容器，最初是 LXC，现则是其自有的容器库，但 Docker 本身是一个应用隔离环境，提供的服务范围比 LXC 更为有限。使用 LXC，你可以部署一个操作系统（不包括内核），并且几乎将其视作传统的完全虚拟化。而 Docker 则是部署应用或服务，而非操作系统。两者的范围不同，因此 Docker 更适用于*第五章*，*补丁管理策略*。由于 Docker 是应用层容器化，它通常会运行在虚拟机上，无论是完全虚拟化还是容器化，以提供其底层操作系统组件。

一般来说，容器代表了一个受信任、成熟且超高性能的虚拟化选项（或替代方案）。尽管在能力上更为有限（例如，一个 Linux 容器主机只能运行 Linux VE，而 FreeBSD VE 是不可能的），但它们更容易维护、更快速、通常也更稳定（因为出错的可能性更小）。容器可以更快地创建、启用或关闭、修补，资源使用上更灵活（它们不需要完全虚拟化所需的严格 CPU 和内存分配），需要的技能较少，运行时开销更小。容器的唯一显著限制是无法混合操作系统工作负载，甚至内核版本。如果你所做的任何事情需要特定的内核版本（且该版本在整个平台中并不统一），或者需要自定义编译内核、GUI、ISO 或类似的完整安装，那么容器显然无法满足你的需求。但如果你正在处理一个纯 Linux 环境，所有工作负载都是 Linux 且能够共享内核，这种情况并不罕见，那么容器将是理想选择。

至少目前为止，容器无法利用操作系统的图形界面，因此它们被限制用于纯文本界面（即 TTY）的服务器工作。因此，容器不是图形终端服务器或 **虚拟桌面实例** (**VDI**) 部署的可选方案。这类工作负载仍然需要完整虚拟化，直到有人为此开发出解决方法。但由于这通常不是一个高度渴望支持的工作负载，因此很少有人会在这个已经容易解决的问题上投入大量精力，只为了能够说他们用容器解决了它。

通过使用容器，你可以让操作系统本身充当虚拟机管理程序，但它仍然是操作系统，并且可以使用所有常规的 Linux 工具和技术进行管理，因为系统仍然是 Linux。你无需学习或维护单独的虚拟机管理程序。这样，利用现有的 Linux 技能和工具的能力非常强大。

必须注意的是，由于 KVM 和容器都以标准的裸机 Linux 安装为基础，并且都直接集成在原生的 Linux 内核中，因此在同一主机上同时运行完整虚拟化和容器化并不罕见，且容器中运行原生的 Linux 工作负载是完全可行的。这确保了较低的开销和更大的灵活性，并且非 Linux（主要是 Windows）或自定义内核的 Linux 系统可以在 KVM 上作为完整虚拟机运行。如果你觉得混合使用更适合你的需求，就没有必要仅选择其中一种方法。容器技术如今已经变得极为流行且重要，但传统意义上对容器的使用已经大大减少。这主要是由于对术语和技术的误解，导致即使在非常合适的情况下，容器也常常被忽视。特别是作为 Linux 系统管理员，考虑在你的环境中使用容器而非传统虚拟化，可能会非常有益。将容器作为你工具箱中的另一个工具，可以让你变得更加灵活和高效。

接下来，我们将结合所学的虚拟化和容器知识，并加上管理方面的内容，来学习云计算。

# 云与 VPS

任何关于虚拟化的讨论，今天都不可避免地会引导我们进入云计算。云计算已经成为那个热衷了十年的流行词，人人都想要，大多数人都在使用，但却没有人知道它是什么，意味着什么，或者为什么要使用它。没有什么技术比云计算更加被误解，同时又被广泛谈论。因此，我们有很多内容需要讨论，其中大部分是为了澄清误解和滥用术语。

### 云的奇异困惑

这是一种罕见的组合，既是高度技术化的，又与正常的商业对话不相关，同时几乎在所有层级上都被讨论得像是一个随意的高层非技术性商业决策。考虑到只有极少数 IT 专业人士对云有任何深入的理解，而且更少人能清晰地理解何时选择它，普通的非技术性中层经理人会把这个术语当作讨论邮票价格一样轻松地使用，真是让人难以理解。那些人到底以为自己在讨论什么？没人真正知道。我是说真的，真心的。

问一群经常谈论云的人，把他们分开，避免彼此互相影响。现在，让他们描述云对他们来说意味着什么。你大多数时候会听到一些胡言乱语，显然如此。但当你深入探讨时，你会得到各种各样、彼此完全不同的描述和答案，而听别人讨论云的人通常会说，他们相信那些人说的都是在说同一件事，通常那个*一个东西*是他们都完全没有意思的，更别提是其中的一个人了。实际上，云对几乎每个人来说意味着不同、随机且毫无意义的东西，没有任何规律可言。

如果云这个术语有一个音乐对应物，那就是阿拉尼斯·莫里塞特（Alanis Morissette）的《Ironic》，这首歌的唯一讽刺之处就是标题。云这个术语也是如此，人人都在用，没人知道它是什么意思。就像“讽刺”一样。

多年来，云通常被用来表示*托管*。简单地用另一个没有任何理由的新词替代一个已建立的、广为人知的行业术语。当然，云根本不意味着那样。这种可怕的误解导致了*没有云，只有别人的计算机*的迷因。当然，即使稍微了解一下云的人，听到有人说出这种话时，也会感到羞愧，因为他们完全误解了云的含义。

今天，你更有可能听到“云”用来表示*由 HTML 构建*，我不是在开玩笑。有时它意味着*平台独立*，其他时候它表示*订阅定价*。你可以列出所有可能的含义，而“云”已经被用来指代其中任何一个。你唯一可以确定的是，没人，永远不会，真正意味着云。它可能意味着几乎任何其他东西，但从来不意味着这个术语实际所指的内容。

围绕云定义的广泛狂热唯一的好处是，没有哪个替代定义足够普遍，能够崛起并超越真正的云。然而，问题已经严重到一个程度，你作为 IT 专业人士，除非和一个真正博学且值得信赖的技术同事交流，否则无法和任何人使用*云*这个术语，因为你知道他们真的知道它是什么意思。

真正令人惊讶的是，*云*的使用已经取代了像*协同*这样的术语，成为商业中默认的笑话——也就是说，只有那些真正迷失的人才会使用这个术语。*云*复杂且被完全误解，已经成为如此常识，以至于你无法用它来传达一个想法。它已经成为一种语言中的*标志*，用来指代那些仅仅是在说管理术语，却完全不明白自己在说什么的人，而他们没有意识到其他人正默默地嘲笑他们的无能，但你几乎能听到它不断地被重复！无论每个人都知道他们在误用它，*云*依然具有一种上瘾的特性，并持续不断地被使用。

从这番看似抱怨的话（确实是抱怨）中，你应该记住的最重要的一点是，只有最顶尖的专业人士才能理解“云”这个术语，而只有受过良好教育的 IT 专业人士才能理解云计算。避免使用这个术语，因为无论你认为自己能像别人一样错误地使用它，你都不能。没有任何方式能正确使用云这个词，因为每个人都认为自己知道它是什么意思，尽管他们都认为它是某种独特的东西。

当你必须出于某种原因提到云计算时，使用更完整的术语，比如*云计算*或*云架构*，以澄清你实际上是指云，而不是随便抛出一个词让听者自行解读。

从很多方面来看，描述云计算几乎比描述它是什么更容易，尤其是因为每个人都认为它是某种东西。云计算与托管无关，与网页无关，甚至与互联网无关（有时互联网被称为“云”，而不是“一种云”）。我们不能在这里详细解释云计算的每一个可能方面，也没有必要，因为大多数云计算与系统无关，因此也与 Linux 无关。但我们应该在一定程度上讨论它对 Linux 系统管理员的意义，何时适用等问题。

首先，我们必须从**NIST**（美国**国家标准与技术研究院**）的云计算抽象定义开始，这一定义基于亚马逊最初的定义。始终牢记，云计算是一个真实的、严格的技术术语，由亚马逊为真实世界的架构所创造，因此它有一个严格、不可替代的定义，任何误用、误解或其他人试图篡改它的使用都不会改变它所指代的极其具体的事物。那些不了解云计算的人常常争辩说它是一个可以按自己理解的方式使用的宽泛术语，但事实并非如此。它不是一个偶然进入词汇表的随机英语单词，它在首次使用前就在行业内被精确地定义过。

NIST 的定义如下：*云计算是一种模型，用于启用无处不在、便捷的、按需的网络访问，以共享的可配置计算资源池（例如，网络、服务器、存储、应用程序和服务）为基础，这些资源可以在最小的管理工作或服务提供者交互下迅速配置和释放。这个云模型由五个基本特征、三种服务模型和四种部署模型组成。*

对我们作为系统管理员来说，定义中最重要的部分是涉及*资源池*和*快速配置与释放*的部分。因此，我们可以快速创建和销毁的共享资源（意味着服务器、CPU、RAM、存储、网络）。虽然云计算的定义远不止这些，但这些是最基本的。如果你立刻想到“*这听起来像是虚拟化已经做的事情*”，你是对的，确实有很大的重叠，虚拟化是云计算的关键构建模块（包括完全虚拟化和/或容器）。如果你想到“*等一下，资源池我可以快速构建和销毁—这些特征对我或我以前工作过的任何环境来说都不太有用*”，你也是对的。云计算并不适用于传统工作负载或环境，它是围绕着特定用途的应用架构设计的，这种架构只有少数企业能够在任何规模上加以利用。

使得*云*的使用更加混淆的是，许多供应商（并且在逻辑上是合理的）自己将云作为其产品的一个组成部分。因此，当你问你的供应商某个产品是否基于云时，他们可能是在回答是否将云作为产品本身提供给你，或者他们可能是在回答是否在工具的构建过程中某个地方使用了云。这两种情况都适用于大多数人提问的方式，而且由于没人真正知道云是什么，没人确切知道你到底在问什么或想要了解什么。让我举一个牵强但合理的例子。

如果我购买一个遗留应用程序，比如一个使用 MS SQL Server 和旧版 Delphi（Objective Pascal）前端的客户端-服务器应用程序，然后使用一个真正的云产品创建虚拟机并部署服务器端组件，这样我们就可以真正说我们是在云上构建了解决方案。然而，最终的产品从任何角度看都不是云计算。仅仅因为架构的某一部分是在云上构建的，并不意味着最终产品就是云。云只是堆栈中的一层。

对我们这些系统管理员来说，我们关心的是一种叫做**基础设施即服务**（**IaaS**）的云类型。这是一个高级的说法，实际上指的是基于云的虚拟机。其他类型的云，如**平台即服务**（**PaaS**）和**软件即服务**（**SaaS**），在云计算领域非常重要，但只有当系统管理员不在时才存在。如果我们是 PaaS 或 SaaS 的系统管理员，那么，对我们而言，云就是工作负载，而不是云。如果我们不是 PaaS 或 SaaS 系统的系统管理员，那么作为系统管理员，我们不需要讨论这些系统，因为它们与我们的角色无关。

从我们系统的角度来看，云几乎就像是一个先进的、灵活的虚拟化层。当然，像虚拟化一样，我们可能会被要求实施云平台。这是完全不同的一回事，值得写一本书（甚至两本）。但从实际角度来看，系统管理员可能会从一个虚拟机监控器、容器引擎或通过云接口编排的任何一个平台中获取资源。对我们来说，这一切都是一样的——它们都是我们部署操作系统的机制。因此，从纯粹的系统管理员角度来看，可以把云计算看作与任何其他虚拟化技术没有区别，因为它们本质上是一样的。唯一的区别是它是如何管理的，以及如何交由我们处理。

从实际操作来看，我们可能需要深度参与有关是否使用云计算与其他虚拟化获取路径的决策。像任何商业决策一样，这归根结底是评估在一个价格点下所提供的性能和特性，并将其与同一价格点下其他选项的性能和特性进行比较。就是这么简单。但是，鉴于我们之前提到的所有原因，在云计算方面，我们往往需要与大量的误解和对“魔法”的信仰作斗争。因此，我们需要稍微谈一谈云计算，尽管我们本不应该这么做，因为这些误解如此普遍且根深蒂固。

首先，有一种观念认为云计算是便宜的。虽然在特殊情况下，云计算可能节省大量资金，但这很少发生。云计算通常是一种极其昂贵的产品，之所以被选择，是因为它提供了很大的灵活性，从而可以减少整体购买量以满足相同的需求。云计算的提供成本非常高，因此供应商被迫收取比其他架构更多的费用以向客户提供（请记住，*供应商*可能是你们内部的云部门，云计算并不意味着外部供应商）。

### 水平可扩展的弹性工作负载

尝试描述云计算所应对的工作负载对于那些不熟悉某些类型应用架构的人来说可能是一项挑战。我们需要稍作停顿，深入探讨一些应用概念，才能真正理解我们作为系统团队是如何相关的，并看到为什么不同的方案在这个层次上对我们的平台决策起着如此重要的作用。

在传统的应用设计中，期望是将整个应用程序仅运行在一个或几个操作系统实例上。通常，一个实例会作为数据库服务器，另一个作为应用服务器。可能会有更多的角色，且你可能会有冗余的数据库服务器或类似的配置，但本质上，可用的实例数量是相当有限且静态的。一旦部署，实例的数量就不会再变化。在许多情况下，整个应用程序会存在于一个单一的操作系统实例中。

扩展传统应用程序主要侧重于提高单个操作实例的能力。这可能通过更快的 CPU、更多少量的 CPU、更多的 CPU 核心、更大的内存、更大的存储或更快的存储来实现。或者，正如我们通常所说的，如果你需要服务器做更多事情，你就需要一台更强大的服务器。这种性能提升方式能够产生很大的效果，因为高端服务器非常强大，且很少有公司需要运行超出单个大型服务器性能能力的工作负载。这种扩展方式被称为垂直扩展，意味着我们在*框架内*提升单线程或单个服务器的性能。这种扩展方式无疑是最容易实现的，并且适用于任何类型的应用程序，无论其设计如何（这就是你提升视频游戏性能或任何桌面工作负载的方式）。

对大多数人来说，专为垂直扩展设计的工作负载是他们唯一了解的工作负载类型。当然，对于在桌面上工作的终端用户而言，一切都是垂直的。即使是系统管理员，也几乎完全需要管理仅为这种扩展方式构建的应用程序。几乎所有内部部署的应用程序都假设这就是你扩展的方式，直到最近，许多开发人员才较为熟悉其他的替代方案，而且仍然有很多（可能大多数）人并不熟悉，尽管那些熟悉的人在媒体上更为突出。

另一种方法是设计能够通过添加额外、独立的操作实例来扩展的应用程序。例如，运行多个数据库服务器实例（可能在一个集群中）不仅仅是为了提高弹性，同时也为了提高性能。运行多个应用程序服务器，可以通过简单地添加更多的操作系统实例来运行应用程序，同时保持每个单独实例的规模较小。在传统的应用程序架构中，我们可能需要一台拥有四个高性能 CPU 和 1TB 内存的应用服务器来处理我们的应用负载。一个水平可扩展的应用程序可能会使用 16 台较小的服务器，每台服务器有 64GB 内存和较小的 CPU 来处理相同的负载。传统上，我们会说我们的系统是*向上扩展*，但通过添加更多实例，我们说它们是*向外扩展*。当然，你总是可以同时进行*向上和向外扩展*，这意味着增加每个单独实例的资源，同时也增加实例的数量。

正如你可以想象的那样，我们在现实世界中作为终端用户或系统管理员使用的少数应用程序设计上并没有考虑或能够有效利用水平扩展（*scale out*）平台，即使能做到也是有限的。这要求应用程序架构师、分析师和开发人员从一开始就为这种部署方式进行规划。而再多的规划或期望也不能使每个工作负载都能够以这种方式扩展。

一些应用程序，如常见的基于 Web 的业务流程、大多数网站、电子邮件系统等，非常适合这种设计，你可以轻松地找到或制作这些类型的应用程序，以利用这些资源。其他应用程序，如财务处理或库存控制系统，可能会因为设计上的局限性而面临困难，要能够实现这种扩展可能需要更多的工作，甚至有可能根本无法做到。

仅仅因为开发团队设计了一个工作负载，使其具备水平扩展性，并不意味着该工作负载本身就能实现这一点。这可以通过一个简单的例子来说明。你创建了一个帮助人们选择健康早餐的网站，你将其市场定位于美国。从东部时间早上 6 点到下午 2 点（即加州人吃完早餐时），网站非常繁忙，但在这些时间之外，网站运行非常缓慢。另一方面，另一个网站帮助人们选择任何一餐的食物，并且面向全球市场。这个第二个网站的访问量虽然没有第一个网站那么大，但全天保持着相对一致的流量。第一个网站可以利用扩展性，而第二个网站无法做到这一点，因为其所需资源几乎没有变化。

水平可扩展工作负载的关键优势在于它们可以迅速增长。添加一个额外的操作系统实例（或者额外的一百个！）是简单且不具破坏性的。相比之下，向现有服务器添加更多的 CPU 或 RAM 则既困难又缓慢。水平扩展的下一步是让它变得具有弹性。为了具备弹性，系统不仅要能快速扩展，还要能够在容量发生变化时迅速收缩：即在不需要时关闭并销毁不必要的操作系统实例。这正是云计算的独特优势，为弹性、水平可扩展的工作负载提供按需容量，使你可以在需要时使用资源，不需要时停止使用。

垂直扩展资源提供的成本远低于水平扩展资源。你可以通过尝试组装几台规格大致相同的计算机来验证这一点。除非在极少数情况下，构建一台大型服务器的成本是几台小型服务器的一小部分，这在实际经济学中是如此。单一系统只需要一个操作系统和应用实例，而多个系统则需要每个系统都加载相同操作系统和应用程序到内存中，浪费了许多资源。此外，管理一个*更快*的系统比管理多个较慢的系统要少得多。管理人力也是如此。管理一个高效、快速的员工比管理多个较慢的员工要少得多，后者需要协调完成同样的工作。

因此，要使水平扩展系统成为一个合理的选择，它必须能够利用扩展和收缩两种方式，拥有能够有效利用这一点的工作负载使用场景，并且这一点要足够大，以克服传统设计的低成本、低开销和高度简化。除非你的工作负载满足所有这些要求，否则云计算根本不应成为你的考虑对象。它根本不适用。尽管媒体和趋势驱动的 IT 专业人士常常会误导人们认为云计算无所不在，但实际上，只有少数工作负载能够有效地利用云计算，而且只有少数企业拥有这些稀有的工作负载。

当然，我们谈论的是云计算的 IaaS 方面。在其他云计算领域，只有应用部分暴露给业务，通常是基于云的。但这与我们在这种场景中讨论的内容基本无关，决策过程也完全不同。

第二，存在一种观点认为云计算是可靠的。云计算的定义或设计中完全没有任何东西暗示它具有可靠性。事实上，这与所有标准的云计算思维完全相悖。云计算，由于它是专门为扩展设计而构建的，因此假设任何冗余或可靠性都必须内建于应用程序本身，因为扩展要求——你必须将这一点包含在应用程序内部，以便扩展能够正常工作。因此，在系统或平台层级包含冗余是没有意义且适得其反的。对于云计算的基本理解，应该使我们感到惊讶，如果有人期望在这一层级上提供超出最低要求的冗余。在现实世界中，云计算资源通常比传统的服务器资源更加脆弱，正是因为这个原因。云计算假设可靠性不是非常重要，或者它在技术栈的其他地方得到提供。云计算只是最终系统的一个构建块，绝不是一个完整的解决方案。当然，理论上，可能会出现一个高可用性的云计算服务商，但他们的成本和性能限制将使其很难在几乎完全由价格驱动的市场中竞争。

第三，存在一种观点认为云计算具有广泛的适用性，认为每个公司都应该使用它，并且它正在取代所有其他架构。实际上，这完全不正确。云计算从现在开始已经存在超过十五年（在写作时），并且在那个周期的初期它就迅速渗透了市场。今天，云计算已经成熟并广为人知。那些将要迁移到云端（或为之设计）的大多数公司和工作负载已经完成了迁移，新的工作负载也在以大致稳定的速度在云端创建。云计算的行业饱和率或多或少已经实现。随着旧的工作负载退休或在其他压力下屈服，一些新的工作负载将会迁移到云端。有些工作负载会回流，因为一些过于热衷的云计算爱好者和受流行词驱动的管理者会从云端迁移回来，吸取没有理解或规划的教训。将标准工作负载迁移到云计算而不进行重新设计通常会带来高昂的成本和风险。但总体来说，云计算已经稳定在一个已知的饱和点，计算世界将保持现状，直到另一个激动人心的范式转变发生。基本上，我们今天在高级定制内部软件和大规模多客户软件中看到的东西非常适合云计算范式，而传统的单客户工作负载仍然在传统范式下最为有利。这一切都正如当初云计算首次宣布时所预测的那样。

使用云计算并不需要任何特定的技能或培训，尽管许多行业人士为了销售认证和培训课程，可能希望我们相信这一点。事实上，了解云计算的真正含义通常足以让你有效地使用它。也就是说，个别云供应商平台（例如**Amazon**的**AWS**或**Microsoft**的**Azure**）如此庞大且复杂，以至于获得供应商认证和培训在了解如何与他们的产品接口交互时确实具有实际价值。但需要明确的是，培训的价值在于学习如何与特定供应商合作，而不是学习云计算本身。

这并不改变这样一个事实：大多数寻求从云计算中获得重大价值的组织，很可能需要通过深度的供应商集成来实现，而这几乎肯定需要对特定供应商的产品知识进行投资。

云计算是一组令人惊叹的技术，服务于一个极其重要的目的。当你的工作负载适合云计算时，其他任何东西都无法与之媲美。无论你是建立自己的私有云还是使用公共共享云，无论你是将云托管在内部还是让托管公司处理数据中心组件，云计算可能是某些工作负载的正确技术。通过你在这里的理解，你应该能够评估自己的需求，判断云计算是否可能发挥任何合理作用，并能够查看实际供应商和成本，进行与其他选项的比较，并做出基于数学的合理评估。

现在我们了解了云计算，我们可以回顾一下虚拟专用服务器的旧概念，并探讨它们为何与今天的云计算紧密相关，但实际上并不相同。

## 虚拟专用服务器（VPS）

与 IaaS 云计算类似，甚至在事物的维恩图中几乎重叠的现代概念是虚拟专用服务器（VPS）。VPS 实际上早于云计算，源于更简单的虚拟化（或容器化）技术，允许供应商（当然也可以是内部部门）从更大的共享环境中为客户划分单独的虚拟机。客户无需提供完整的服务器，而只需购买供应商服务器的一个小片段或几个片段来满足他们的需求。

正如我提到的，这听起来与我们刚才描述的 IaaS 云非常相似，实际上它确实是如此。实际上，许多人使用 IaaS 云时，实际上是在不自知的情况下使用其 VPS 方面。VPS 背后的理念是允许公司以通常所需的物理服务器规模的很小一部分来购买服务器级别的资源。如果你回想一下我们讨论虚拟化时提到的，如何通过使用虚拟机监控程序（hypervisor），我们可能将一台物理服务器转换成一百台虚拟机，每台虚拟机运行自己独立的操作系统，那么我们可以将这些资源卖给一百个不同的客户，每个客户都能在自己的安全空间内运行一台小型服务器。这使得小公司或有小需求的公司能够以任何现实的预算购买企业级数据中心和服务器硬件的容量与价格。

在继续之前，我们需要快速进行一次 VPS 与 IaaS 云的对比分析，以了解为什么 VPS 与云计算如此常被混淆，并且它们为何经常相互竞争：

+   **首先，目标**：IaaS 云的目标是通过自动化提供按需快速创建和销毁资源的能力——主要供最大型的组织或工作负载使用。VPS 背后的目标是将传统服务器资源划分，以便小型组织和/或工作负载能够负担得起。一个追求的是最大的规模和最复杂的系统，另一个追求的是最小的规模和最少的复杂性。一个预计应用程序和基础设施团队双方都需要进行定制化工程工作，而另一个则期望传统应用程序，并且任何团队无需特殊知识或额外配合。

+   **其次，接口**：在云计算中，期望和目标是系统能够自我配置（并且自我销毁）。云并不是为了让人类手动交互来请求资源，也不是为了配置它们，也不是为了决定何时需要更多（或更少）的资源，也不是为了在完成后销毁它们。因此，云的重点是提供 API，使软件能够处理资源配置。VPS 的设计目标是像任何正常的虚拟机一样工作，由人工启动构建，安装操作系统，可能进行操作系统配置，并关闭虚拟机，最后在不再需要时删除它。云产品通常不提供与虚拟化硬件的任何直接交互，例如访问控制台，因此需要控制台级别交互（如图形用户界面）的一些系统是无法实现的。要成为 VPS，必须提供控制台和图形用户界面访问，才能以标准方式完全模拟硬件设备。如果你能使用普通服务器，那么你就能使用 VPS。

+   **第三，资源配置**：云服务假设需要快速配置。当然，“快速”是一个相对的概念。但在云生态系统中，系统必须能够在几分钟甚至几秒钟内从首次请求到完全功能的状态，这几乎是理所当然的。在 VPS 的世界里，虽然我们总是希望一切能尽快准备好，但在开始手动安装操作系统之前，等待几分钟的时间是很常见的，甚至可能需要几十分钟。我们假设云实例是通过软件创建的，而 VPS 实例则是由人工手动创建的。

+   **第四，计费**：由于云计算的价值假设来源于其能够快速创建和销毁的特性，从而保持成本的可控性，因此计费必须是细化的才能实现这一点。为此，计费通常按分钟或可能是小时的增量来处理，或者以其他极短的单位如处理器周期来计算。VPS 有时也会按这些短的时间增量收费，但也可以轻松使用较长的时间间隔，如按天或按月收费，因为它不是一个快速创建和销毁的服务。（我们可以说，云服务倾向于无状态，而 VPS 则倾向于有状态。）

使 VPS 和 IaaS 云服务更难区分的一个原因是，今天几乎所有的 VPS 提供商都使用 IaaS 云服务作为其 VPS 配置的底层机制，而大多数 IaaS 云服务提供商也选择额外提供 VPS。这种情况注定会发生，原因有两个。首先，VPS 提供商使用云服务，因为这是一种非常合理的方式来构建 VPS（如果你考虑一下 VPS*供应商*的需求，它们听起来与云服务的目标非常相似），并且由于 VPS 是建立在云计算之上的，你可以将它作为云的一部分来宣传，并成为一种*简化的云资源接口*。对于云服务提供商来说，提供 VPS 也是有道理的，因为大多数寻找云服务的客户根本不知道它是什么，只是出于政治而非商业或技术原因使用它，因此提供一个简单的服务让他们可以从你这里购买并使用你的资源（因为云服务非常复杂难懂）能够帮助你捕获大多数的收入。

像 Amazon 这样的供应商曾经没有提供 VPS 服务，如果你并非真正需要云服务，使用他们的资源是非常困难的。为了解决这个问题，Amazon 将 LightSail 作为 VPS 产品加入到他们的云产品之上。

其他云服务提供商，如*Digital Ocean*、*Linode*和*Vultr*，将 VPS 作为其主要产品，并几乎完全专注于它，同时将其云界面静静地放在一旁，让真正需要云服务的客户能够找到它，而那些原本打算使用 VPS 的人则能立即获得所需的服务。

VPS 是现实世界中企业运行工作负载的最受欢迎且最有效的方式之一，尤其是对小型公司而言，但任何规模的公司都可以利用它们。云计算是有效的，但主要适用于特殊情况的工作负载。大多数宣称正在利用云计算的公司实际上正在使用 VPS，并且甚至没有意识到他们完全忽视了云计算。

值得注意的是，当我们谈论云采用率时，我们面临一个根本性的问题：没有人知道云计算是什么，包括那些认为自己当前在使用或没有在使用云计算的人！像亚马逊这样的供应商可以告诉你他们有多少客户，但他们不能告诉你这些客户是在将他们的产品作为云使用，还是以其他方式使用云计算。在关于云采用的调查中，几乎没有合理的机会，能够确保被询问的人、问卷调查的人以及阅读采纳率报告的人，三者对云计算的理解足够深入，能够有意义地回答或提问，实际上，通常他们都根本不知道被问到的问题是什么。所以，关于云采用率的任何信息都接近完全没有意义。没有任何真实的机制可以让任何个人或组织真正了解云计算生态系统的真实面貌。如果一群松鼠对一堆仓鼠进行天体物理学调查，然后把结果交给一群超级活跃的小狗来解读，你会得到同样有意义的数据。人们喜欢报告和数据，很少关心调查本身是否有任何实际意义。

在这一点上，你可能会对 IT 和业务领域中对云计算的理解状况感到既不知所措又沮丧，但作为 Linux 系统管理员的你，现在应该准备好去解释它，评估它，了解它所构建的基础，并知道何时以及如何选择将其用于自己的工作负载，何时则应考虑选择 VPS。

# 本地部署、托管和混合托管

既然我们已经讨论了这么多构成我们平台的底层组件，用以部署操作系统的基础，接下来我们终于可以讨论这些系统应该存在在哪里！

至少，这些是我们讨论的最简单的主题。物理位置容易解释，尽管许多企业在实践中对此感到困惑。从概念上讲，我们通常只考虑两种工作负载位置，即本地部署和非本地部署。尽管如此，这可能有点复杂，因为公司拥有多个地点，所以对某一个地点来说是非本地的，在另一个地点可能就被视为本地的。然而，我们通常认为本地部署是指公司所有的站点，而非本地部署则指由第三方运营的任何站点。因此，我们通常将非本地物理性称为托管，因为物理系统是由我们委托托管的。然而，也有一些原因，这种说法可能会误导人。

大多数人认为，当系统托管在本地时，这也意味着它将由内部团队操作。这通常是对的，但由第三方团队管理本地系统并不鲜见，尤其是在高性能或高安全性环境中。例如，如果你需要亚马逊特定范围的云计算产品，但又无法允许任何外部托管，你可以让亚马逊在你的本地运行 AWS 云实例。这远非低成本或简单，至少需要一个小型的自包含数据中心及其所有相关人员！

托管在实践中变得更加复杂，但核心问题始终如一：划分点。当我们决定将系统托管到外部时，问题迅速变成了定义哪些部分由托管服务提供商提供，哪些由我们自己提供。

在最极端（也是不切实际）的情况下，你可以租一座房子、办公室或储藏室，并根据需要提供自己的机架、服务器、互联网、冷却、供电等，但如果我们这样做，可以说我们实际上已经将这个地方变成了我们的本地场所。说得好。

传统上，人们认为几乎所有工作负载都应该在本地运行。这是因为一个非常简单的原因：早期的商业网络没有互联网连接，因此将其托管到其他地方几乎是不可能的，或者至少不实际。再加上在互联网初期，广域网连接速度慢且不稳定，使得远程服务器几乎无法使用。而且，软件是围绕局域网网络特性构建的，与今天不同的是，今天任何优质的企业软件都假定它需要在长距离连接上（最有可能是互联网）有效地运行。

由于这些旧的假设，服务器需要位于员工工作的办公室这一“部落知识”被代代相传，很多人未曾对此进行评估。随着 2000 年代初期的到来，这一信息从普遍正确转变为几乎不再正确。

今天，大多数工作负载通过互联网有效运行，因此可以几乎位于任何地方。使用某种形式的外部托管或不基于任何特定公司位置的集中式托管如今已成为常态，而不是例外。

在所有本地和外部托管的评估中，我们有一些普遍的因素：谁会访问数据，以及从哪里访问，延迟和带宽如何影响应用性能，哪些访问数据的人具有优先权，以及在不同位置的性能问题成本如何。

没有固定的规则，我们只是需要尽可能仔细地考虑尽可能多的因素。场内和场外解决方案仅仅是位置，应当视作如此。使用企业级场外数据中心可能非常重要，特别是如果我们在场内没有真正的服务器机房。而且灾难恢复在场外位置可能会更好。但如果服务器离得很远，用户体验是否足够好呢？这些问题都是情境性的问题，需要从当前的业务基础设施状况以及未来短期的角度来回答。

## 合租机房

当一个站点提供了房地产、冷却、电力、互联网、机架、网络等服务，而我们提供自己的物理服务器时，这被称为合租机房。合租机房是我们作为系统管理员，在保留使用任何对我们的组织和工作负载有意义的硬件灵活性的同时，获得企业级数据中心服务的最流行和有效的方式之一。合租机房对从非常小的企业到绝对巨大的企业都有效。没有任何公司会超越合租机房可能带来的价值，政府也一样。这是一种没有*上限*的策略。

合租机房是将 IT 设备移出场地的最有用和有效的形式之一，因为它允许 IT 部门保留对硬件采购和配置的完全控制，不仅仅是对系统，还有网络和设备的控制。只有支持技术硬件的非 IT 职能是由合租机房提供的。这使得合租机房能够专注于强大的设施管理技能，而 IT 部门则保留所有 IT 职能和灵活性：基本上，你可以通过第三方远程完成原本希望自己内部团队做的事情，假设你有足够的业务量来这么做。预计合租机房提供商也会有*远程操作人员*，在必要时帮助完成一些基础工作任务，比如更换或验证电缆、重启设备等。

灵活性是合租机房的关键。不论是因为你希望定制自己的硬件、维护遗留系统、使用特殊硬件（例如 IBM Power 硬件），还是希望将现有的整个环境从场内迁移到合租机房，合租机房都能提供这种自由。大多数合租机房设施也允许不同规模的部署，从为你托管一台 1U 服务器，到为你提供部分机架（通常有十分之一、四分之一、半个、全机架等规格），甚至提供能够容纳多个机架的笼子，甚至出租整个数据中心楼层！

合作托管面临的最大挑战是没有有效的方法去实现极小规模，因为你可以合理托管的最小规模是单个服务器。如果你的需求小于这个规模，那么合作托管通常难以为你提供具有成本效益的解决方案。但不要仅仅因为认为它昂贵就拒绝合作托管。我经常为那些认为合作托管太昂贵的公司做这些计算，结果发现合作托管的成本不到他们其他提案成本的一半，同时还能提供更多的增长灵活性，而无需额外支出。大多数人认为服务器比实际更贵，合作托管的费用比实际更高。合作托管的费用常常不恰当地与传统系统挂钩，好像只有二十年的旧设备才能进入数据中心，而人们往往设想的是几十年前的成本模型。二十年前，服务器比现在贵得多，使用寿命也明显更短，数据中心空间的成本也比今天高。就像 IT 中的一切一样，成本随着时间的推移已经下降，对于任何规模的工作负载，合作托管通常比大多数替代方案便宜得多。

合作托管只是将系统托管到远程的其中一种方法。其他方法如公共云、托管云和基于云的 VPS 系统是标准的替代方案。

关于本地性的最大挑战，实际上都是与理解当前市场上不同方法的定价相关，并能够评估在本地或远程托管设备的好处和注意事项，以及这些设备应该是专用的还是共享的。现在你应该准备好做出评估，并根据你的部署情况做出适当的选择。接下来，我们将深入探讨平台级系统设计架构这一更复杂的话题。

# 系统设计架构

系统管理中最具挑战性的一部分就是处理系统架构这个广泛的概念。在某些情况下，我们的预算非常低，或者我们的需求非常简单，以至于我们根本不需要考虑除了最基本的选项之外的任何事情。但对于许多系统来说，我们有更广泛的需求，且需要考虑的因素非常多，使得系统架构在许多方面都可能变得具有挑战性。

我们现在理解了平台概念、本地性以及通常与提供一个可以安装操作系统的环境相关联的服务范围。接下来，我们需要开始描述如何将这些概念结合到现实世界中，形成可用的设计。大多数系统设计其实就是常识和实用性。记住，任何事情都不应像魔法或黑匣子一样神秘，如果我们从供应商那里获得服务，他们使用的技术和选项范围与我们是一样的。

我们将在下一部分讨论风险和可用性，但在此之前，我们应该提到这一点，以便更清晰地说明为什么系统设计依赖这些数据：我们为整体系统增加的任何冗余（无论是为了性能还是风险降低）都可以在不同层次上进行。主要是在系统层（我们当前讨论的层面），或者在应用层（我们无法控制的层面）。因此，即使在需求最苛刻的高可用性工作负载情况下，我们也可能不需要一个强大的底层系统设计，在思考设计选项时必须考虑这一点。

我将分析常见的设计方法，以便我们理解它们如何最好地应用于不同的场景。这些是包括存储和计算的物理系统架构。假设在每种情况下都使用某种抽象技术，即虚拟化和/或容器化，因此在每个案例中不会逐一提及。

## 独立服务器，也叫“雪花服务器”

这个设计可以说是最基础不过的了。简单的服务器，作为所有其他设计的基准。自包含的“一体化”服务器，将存储和计算都集成在一个机箱内。没有外部依赖，没有集群，也没有冗余（单一机箱外部的冗余）。当然，我们假设遵循了标准硬件实践，比如最基本的 RAID 和双热插拔电源供应等。

如今，许多 IT 人员会对尝试使用独立服务器表示反对，但他们不应该这么做。经典的单一服务器是一种强大、有效的设计，适合大多数工作负载。这应该是*首选*设计，默认的起点，除非你有特殊需求做其他事情。

由于其简单性，单服务器设计具有最好的成本比，较其他因素相比，它们比看起来更强大，且性能优秀。许多人认为服务器是一种脆弱的设备，而多年前它们确实是这样；但这种印象源自 1980 年代和 1990 年代初的服务器。到了 1990 年代末，服务器技术逐渐成熟且可靠，今天在良好维护的服务器上，故障率极低。认为单一服务器风险很高的观点已经过时了，但像我们行业中的许多事情一样，过时的观念常常残留下来，并在导师与学员之间传递，而没有重新评估这些因素是否仍然成立（而且在许多情况下甚至没有最初的评估，根本不知道这些观点是否曾经成立）。

与任何其他方法相比，单个服务器受益于拥有更少的组件和更低的复杂性，并且由于部件失败较少且配置错误较少，使其更加容易实现真正的可靠性：这就是为什么我们有时将其称为*砖块*方法。砖块简单而有效，虽然它们可能失败，但很少失败。情感上常常将复杂性与健壮性联系在一起，但实际上简单性更加可取。复杂性本身就是其敌人，不必要的复杂系统会带来不必要的风险（和成本）。

尽管由于许多原因难以衡量，我们通常假设一个良好维护和支持的独立服务器可以提供接近五个九的平均可用性（即每年约一小时的停机时间）。在任何业务中，很少有工作负载不能很好地应对这种停机时间。独立服务器面临的困难是这只是一个平均值（当然），我们将有孤立系统经历更高的停机时间，而其他系统则完全没有停机时间。

简单性还带来了性能。通过路径中减少组件，单个服务器具有出色的性能。试图获得超过单个服务器所能实现的总体性能是困难的。单个服务器为我们提供了几乎任何方法中最低的延迟和几乎最佳的吞吐量。

当涉及到单个服务器系统时，使用数学和逻辑来解释它是否合理是很重要的。许多人在系统架构方面依赖情感，这是不应该发生的。我们对系统设计的关注点是性能和可用性，这些纯粹是数学组成部分。情感在这里没有任何作用，事实上，它们是我们的敌人（就像它们是任何业务流程的敌人一样）。

单个服务器的扩展能力远远超出大多数人的想象。我经常听到的论点是，他们无法考虑单个服务器，因为他们的需求*如此巨大*，但随后部署的系统仅仅是标准规模的一小部分，更不用说单个服务器可以达到的最大规模了。请记住，与横向扩展相比，纵向扩展效果显著，并且通常也具有成本效益。最大的单个服务器系统可以支持数百个最强大的 CPU 和许多 TB（甚至更多）的 RAM。挑战不在于找到一个足够大的单个服务器来完成任务，而是找到任何能够有效利用如此强大能力的工作负载的任务位置！

独立服务器的一个重要优势是，每台物理设备可以根据其工作负载的需求进行扩展和定制设计。因此，不同的服务器可以使用不同的 CPU 到 RAM 到存储比例，不同的服务器可以使用不同的 CPU 代数或架构，一个系统可能使用大容量硬盘，而另一个则使用小巧但速度极快的固态存储。调整非常简单。

## 简单并不一定意味着简单

拥有独立的服务器并不意味着我们放弃所有我们可能认为需要从更复杂设计中得到的选择和灵活性。这只是意味着我们必须以不同的方式考虑它们。关于独立服务器的许多关切很可能源于一个虚拟化技术尚未普及，网络相对较慢的时代。今天我们拥有虚拟化、快速存储和快速网络，这些可以在一定程度上改变游戏规则。

我们将服务器称为独立的，是指它们的架构，一切都是自包含在一块硬件中的。这并不意味着我们没有（或不能有）多个服务器。相反，像世界五百强公司经常会拥有数千台独立的服务器。使它们独立的是它们彼此之间没有依赖。其中一台的完全失效（或被盗）不会对另一台产生负面影响。

一个小型组织可能会选择依赖单个独立服务器来支持他们的整个业务，并完全依赖备份和恢复到替换硬件的能力，如果灾难发生。这是一种完全有效且相当常见的方法。

如果您的组织规模较大，或者工作负载需要更快的可用性保护，那么运行多个标准服务器是标准做法。这会在物理硬件设备之间分担负载，并且由于虚拟化，提供了通过快速在其他硬件上重建丢失工作负载来缓解硬件故障的自然方式。如果部署密度过高，备用硬件也是一种选择。借助现代存储、网络、系统管理和备份技术，恢复许多工作负载可能只需几分钟，即使完全硬件故障也通常只会带来微不足道的系统影响。事实上，将备份存储在其他独立节点上可以实现丢失系统的几乎即时恢复，同时保持强大的解耦。

独立的服务器也并不意味着没有统一的管理形式。像 ProxMox 或 VMware vSphere 这样的工具允许管理的集中化，同时保持系统硬件的独立性。现代工具使得管理庞大的独立服务器群变得非常简单。

独立服务器的几乎每个方面都可以通过增加更多内容和使其更复杂来改进，它始终引领的两件事是成本和简单性。在商业上，没有其他方法能够可靠地保持我们的成本或我们的简单性如此之低，这通常是最重要的因素。

## 许多对多服务器和存储

随着公司成长，可能会有机会整合架构中的不同方面以节省成本。分离网络和存储是最常见的做法。创建一个计算节点层和一个存储节点层可以带来很大的灵活性。其主要好处是能够轻松移动资源，并更好地利用系统。

例如，一个组织可能需要十五个物理计算节点（传统服务器），但只需要六个存储节点（SAN 或 NAS）来支持它们。每个独立系统可以轻松定制扩展，并且不需要与池中的其他系统匹配。通过这种方式，这种方法与独立服务器方法并没有太大区别。

应当注意的是，存储层相较于计算层，在做这件事时存在更大的风险，原因有两个。首先，存储是有状态的，而计算是无状态的，这意味着我们不仅需要保护系统的可用性（正常运行时间），还需要保护数据的存储，因此存在数据丢失的风险——这里有更多的东西需要保护。其次，存储比计算更复杂，两个层级上的硬件和软件相同意味着存储层更容易因复杂性而发生故障。这些风险在独立服务器中也存在，但当它们被组合成一个单一机箱时，可能更难理解风险发生的位置，即使我们知道整体的风险结果是什么。

在最简单的实现方式中，我们会有一个计算服务器节点和一个存储节点（通常是 SAN 阵列），并通过一根直连电缆（如以太网、eSATA、FC 等）直接连接它们。这实际上是一个假设场景，因为它显然是膨胀且不合逻辑的，缺乏规模化，但我们可以从这个例子中学习，看看如何将单一独立服务器设计，在没有任何规模效益的情况下，简单地将机箱数量翻倍，从而管理并增加系统设计的物理复杂性和逻辑复杂性。

通常，这种设计的最大优势是能够大规模整合存储，将尽可能多的存储推入单个节点，同时拥有许多小型到中型（一个或两个 CPU）的服务器，允许工作负载在这些服务器之间流动，以便最佳地平衡工作负载。这种方法具有灵活性，通常具有成本效益，并且使大规模扩展变得相对简单。

超越独立服务器的设计意味着我们开始引入需要讨论的依赖关系。至少，当我们转向一个多节点系统时，就会有节点间连接的复杂性（这些连接可能只是简单的电缆，也可能更复杂，比如通过某种交换网络），以及配置节点之间的通信所带来的复杂性，还有额外组件故障的风险。

这种设计实际上并没有解决风险问题，反而比标准的独立服务器更具风险。这就是为什么将独立服务器作为基准，并从这个基准点出发发现风险变化是很重要的。在一个*网络*系统设计中，没有冗余，因此每个工作负载完全依赖于它所关联的计算节点和存储节点。这个风险可能在计算节点之间是均匀的，或者每个工作负载所在的节点可能有独特的存储配置，从而导致同一服务器上不同工作负载的风险差异很大。在这里，风险变得更加复杂，因为我们必须处理计算节点、存储节点、两者之间的连接以及配置的累计风险！每个单独的部分都非常难以单独衡量——将它们放在一起时，我们大多数情况下只能从相对的角度来看，并理解它比独立服务器更具风险。

## 将世界视为工作负载

系统架构是*按工作负载*来设计的，没有必要让组织中的所有工作负载，甚至单一计算节点上运行的所有工作负载，都共享相同的架构。混合搭配完全可行，而且在某些情况下是常见的。每个工作负载应根据其自身需求进行评估，然后评估整体架构。

经常被忽视的是，可以为不太重要的工作负载使用复杂且不太可靠（但在大规模情况下可能更便宜）的网络设计选项，同时在相同的计算节点上为更关键的工作负载提供极其快速和/或可靠的本地存储。在需要存储合并的一个大环境中，混合搭配可以是一种强有力的策略，这样做不会危及少数高度关键的服务。

同理，每个工作负载都可以有自己的备份、复制、故障转移和其他灾难应对的风险缓解策略。共享一个计算节点通常不会对工作负载间如何处理可靠性和可用性提出过多要求。当然，通常所有工作负载都会以相同的方式处理，要么是出于标准化和简化的愿望，也常常是因为误解了每个工作负载可用的定制化范围。人们常常认为选择一个系统设计是一个非此即彼的决定，但事实并非如此。

网络系统设计的主要挑战在于，任何获得的效率提升都必须抵消由需要更多节点（分离计算和存储意味着同样的任务需要额外的硬件机箱和操作系统）所带来的额外成本，并且在任何规模下，都会需要更多的网络设备来处理互联连接。网络设备可以是一个简单的以太网交换机，也可以是复杂的光纤通道或 Infiniband 交换机集群。交换机不仅代表了额外的购买成本，还增加了硬件故障的风险，并且在较小程度上也增加了配置的复杂性。通常会购买冗余交换机来降低硬件风险，但这会增加成本和配置复杂度。即便是在极其庞大的环境中，这也意味着额外的成本和风险，这些往往是难以克服的。

### 末日倒金字塔：集群化计算与高风险存储，亦称 3-2-1

遗憾的是，倒金字塔（又称 3-2-1 或 IPOD）传统上在 2000 年代和 2010 年代的大多数时间里，一直是中小企业中最常部署的架构，同时也是普通工作负载需求的最糟糕设计决策的典型例子。它也是供应商和转售商最大化利润的设计，因此这是每个人都希望你购买的方案。

IPOD 设计与上述的网络系统设计的区别在于，计算层是集群化的，以实现高可用性，而存储层则不是。如我们在上次的设计中讨论的那样，存储既需要更好地保护，又更容易发生故障。通常，网络层（即提供计算与存储之间连接的层）也会进行集群化以实现高可用性。每一层的节点数量决定了所使用的命名约定：3-2-1 指的是设计中有三个（或更多）计算节点，连接到两个冗余交换机，所有这些都依赖于一个单一的存储设备，通常是一个 SAN。

从架构图来看，IPOD 是一个金字塔，宽的部分在上面，一切都平衡在顶点上。因此有了“倒金字塔”这一术语，这种设计的目的就是尽可能地增加成本和风险，因此才有了“末日”的绰号。

### 自上而下的冗余

为什么像 IPOD 这样明显不切实际的设计在传统上如此流行？答案需要我们理解几个因素。首先，冗余、风险和系统设计是大多数企业，甚至企业内部的大多数 IT 部门，缺乏培训且普遍没有意识的领域，因此容易成为供应商进行操控的目标。

真正的技巧来自于两点：语言学和自上而下视角带来的简化。语言学的技巧发生在*冗余*这个词的含义上，因为这个词并不像大多数人理解的那样，其系统*确实有冗余*，但几乎是无意义的。所以，当客户说“*我需要冗余*”时，他们实际上是指“*我需要高可用性*”，而这使得供应商可以声称有冗余，忽略实际需求。语义在所有商业领域都非常重要，IT 领域尤其如此。

系统的自上而下的视角来源于我们如何看待架构。作为 IT 专业人员，我们知道我们应该*从侧面*看待我们的架构，也就是说，层层分明地看待其可靠性，知道计算建立在网络之上，网络建立在存储之上。但如果供应商想要引导客户相信系统有强大的冗余性，他们会展示一个*自上而下*的视角，只显示有冗余的计算层。其他层次则过于复杂，常常被各方忽视，认为它们是*黑箱，做着魔法般的工作*。忽略困难的部分，仅专注于冗余性不太重要的简单部分，这使得误导客户变得非常容易。

当然，如果我们真的停下来思考一下，真正重要的是整个系统的总体可靠性。被任何单一层次的事情分散注意力只会让我们误入歧途。我们需要了解所有的层次，以及它们如何相互作用，才能确定整体的可靠性。但人们有强烈的情感驱动，常常认为某一层是极其可靠的（比如这里的计算层通常是），然后觉得整个系统因此也必须是极其可靠的。但事实恰恰相反。系统的整体可靠性主要由最脆弱的层决定，而不是最可靠的层。如你之前所记，系统的风险是累积的。你将所有的风险合并，因为每一层都百分之百依赖于其他所有层，一旦任何一层失败，整个系统都会失败。你可以通过一个思维实验轻松证明这一点……如果某一层的失败概率是 100%，而所有其他层的失败概率是 0%，那么系统依然会 100%失败。那些几乎可靠的层对抵消不可靠层的作用几乎为零。

冗余本身是一个危险的词。在一般的英语用法中，冗余意味着你拥有某个事物的多个副本，而实际上只需要更少的副本。这可能意味着当某个副本失败时，另一个副本可以作为替代或备份，但这并不是必然的，且这个词常常被用来表达其他意思。例如，在 RAID 中，RAID 0 有多个磁盘（冗余），但冗余越多，风险反而越高，而不是越低。RAID 1 则恰恰相反。在那里，冗余是它的对立面，甚至在同一个上下文中也是如此。这真正体现了 IT 中语义学的重要性（实际上在商业中，甚至是生活中也是如此）。人们常常把冗余作为可靠性的替代词，但两者的含义是截然不同的。使用你真正想表达的术语，你将获得更准确的信息。

IPOD 设计中最大的问题是其实用性。如果我们从可靠性的角度来看，它比网络系统设计更安全，因为至少有一些层包含了高可用性措施，即使并非所有层都有。但这确实是正确的，但也容易让人产生误导。网络系统设计的目的是通过牺牲高风险来节省成本，相对于简单的独立服务器设计，使用“比某个根本不打算设计为安全的系统更安全”这一说法，并不完全错，但在这种语境下讲述时，常常会激发一种情感反应——让人觉得 IPOD 更安全，但这并不等同于*更安全*。如果我们将 IPOD 的可靠性与独立服务器进行比较，它的安全性会显得非常差。请记住，我们一开始就指出，低成本、简单的独立服务器是我们比较的基准。IPOD 的问题在于，它的风险极高，接近网络系统设计的风险，而它的成本又远高于网络系统设计，通常也比独立服务器设计要高，而且给 IT 团队带来了更多的复杂性和工作量。正是这种高风险和高成本的持续结合，使得 IPOD 成为一个有问题的设计，并且通常被认为是现实世界中最糟糕的设计。

在生产环境以外，IPOD 通常非常适合大规模实验室环境，其中容量最为重要，而可靠性则不那么关键。通过灵活扩展计算能力，使用一个统一的、低成本、可靠性较低的存储层，可以使大规模实验室变得更加实惠。

## 分层高可用性

从我们已经看到的内容派生出的逻辑系统设计，是将网络系统设计的各个独立层以及“毁灭金字塔”计算层中的高可用性集群应用到所有层，从而得到高可用性的存储层、高可用性的网络层和高可用性的计算层。通过这种方式，我们可以拥有大规模的计算、存储和网络，而不会对任何单独的层次产生高度关注。

每一层仍然依赖于其他每一层，因此这三层高度可用的层仍然必须被评估，并且每一层的风险需要加在一起。所以，尽管我们几乎可以肯定地让任何单独的一层比单一的独立服务器更可靠，或者甚至远远更可靠，但当我们将每一层的风险累积起来，再加上将这些层结合在一起所带来的额外复杂性风险时，它可能不再比独立服务器更可靠。

## 可靠性是相对的

在讨论可靠性和这些不同的架构时，我们必须记住要进行同类比较，而不是苹果和橙子的比较。当我们说一个单独的服务器具有一定的可靠性，而使用标准高可用性技术集群的服务器具有相对较高的可靠性时，我们假设这些服务器在个体可靠性上大致相同。在大多数情况下，这种假设是正确的。无论是计算节点、网络硬件，还是存储节点，在大致相同的价格范围内，我们获得的硬件和软件质量相似，并且故障特性也大致相同。因此，这些质量*相同的*不同设备的可靠性大致相同，网络硬件是最可靠的（最简单的），而存储节点则是最不可靠的（因为它们最复杂）。

然而，我们可以大幅度地操控这一点。一台五千美元的服务器通常会比一台五十万美元的服务器不那么可靠（和性能差）。然而，每一台都是独立的单体服务器。所以，显然，我们必须同时考虑架构可靠性（我们所设计的系统的可靠性）和个体组件的可靠性。

这里常见的一个问题是，*你得到的就是你付出的* 这一原则完全不适用，你很容易找到一些极为昂贵的单机系统，不论是计算节点还是存储节点，它们的可用性都不高，甚至可能比普通设备还不可靠！由于可靠性很难衡量，甚至更难证明，供应商几乎没有动力告诉我们真相。供应商有很强的动力告诉我们任何能够让我们花更多钱的事情，无论是让我们觉得传统服务器比实际更脆弱，还是对那些本质上是由稻草（以及猪）构建的设备作出狂野的高可用性声明。

所以我们必须小心，考虑所有的因素。我们必须理解，保护单一机箱（垂直可靠性）与保护多个机箱（水平可靠性）是不同的。单一机箱的可靠性对于某些组件（如冗余电源、高质量组件和镜像 RAID 存储）通常非常强大，但对于其他组件（如 CPU、内存、主板）则往往比较复杂和问题多发。而单一机箱系统虽然操作更简单，但无法像多个机箱那样处理一些关键问题，比如物理损坏（水、火、叉车等）。

我们还必须敏锐地意识到，市场营销人员和销售人员常常利用对可靠性的混淆作为销售策略，并推销像*双控制器*系统这样的概念，声称它几乎不可能出现故障，但却没有科学或数学依据来支持这一说法。双控制器系统实际上只是将系统水平扩展到单一机箱内部，具有前者的所有复杂性，并且缺乏后者的物理保护。任何基于误导性信息进行销售的产品，都更有可能做得很差，因为这意味着供应商不太可能对质量设计承担责任。

尤其是在 2010 年代初期，服务器供应商常常推出那些标榜为高可用或*无法失败*的产品，这些产品甚至连传统服务器的基础可靠性都无法接近。由于客户无法自行验证这一点，他们通常只能相信供应商的说法，如果企业因此亏损，指责和推诿是常见的后果。

这种方法必然是我们能够合理组建的最昂贵的设计，因为我们需要在每一层都拥有多个设备，并且还需要技术来创建每一层的集群。这最适合非常大的系统，其中每一层的规模足够大，可以在各个层面实现规模效益。

值得注意的是，几乎所有基于云的系统都运行在这种架构上，这是由于它们的巨大规模。当然，并非所有的云都使用这种架构，因为云可以使用任何架构，但这无疑是大型公共云实施中最常用的架构，并且通常也会出现在中等规模的私有云实施中。然而，许多云确实在独立服务器上运行，即使在巨大的规模下也是如此。

## 超融合

我们将要看的是最后一种架构类型——超融合架构，它将复杂设计的趋势推向了一个极简的方向。超融合作为一种架构并不新颖，但在过去几十年里几乎被完全忽视，直到 2010 年代中期才经历了某种复兴，现在，它与独立服务器一起，成为了系统架构设计的基石。

超融合（Hyperconvergence），也叫 HC 或 HCI，将其他更复杂架构中的计算和存储节点重新组合成单一服务器（或者你也可以把它理解为将独立的服务器通过工程冗余添加高可用性，而不增加不必要的复杂性）。超融合为我们提供了两全其美的解决方案，既有独立服务器的简洁性，又有类似分层高可用性的高可用性选项。

超融合既简单又高效，这让它变得很难解释。关键策略是利用独立方法现有的性能和成本节约，同时尽可能少地做事，仍能增加高可用性集群。通过将多个独立节点进行集群（那它们还算独立吗？）我们使所有组件都具备高可用性，同时减少了所需的总组件数。

当正确实施时，数据也可以保证保留在计算节点本地，即使存储在节点间复制以实现存储高可用性，这不仅意味着我们能够像独立服务器一样获得存储的高性能，还意味着我们可以避免跨节点的依赖，让任何节点即使在所有其他节点和/或连接它们的网络发生故障时，依然能够继续工作！这意味着，与所有其他系统设计不同，我们仅在独立服务器设计的基础上增加了更多的弹性！这非常重要。所有其他设计都必须投入大量精力来克服它们自己引入的脆弱性，且有可能未能充分克服这些问题，导致其风险可能比我们什么都不做时还要高。

因此，超融合作为独立设计的逻辑延伸，可能是任何大规模系统设计中最适用的方案。虽然超融合常被认为仅限于小型系统，但它可以扩展到集群技术的极限——这一点适用于所有设计选项。所以所有标准设计的规模大致相同，而这个规模通常远大于实际应用中单一系统的需求。

高可用性与基础集群

在本节中，我们假设集群（无论是计算、存储还是网络）是为了使系统在至少这一层面上具有高可用性而进行的。然而，高可用性集群并不是唯一的集群形式。在所有这些设计中，包括独立设计，我们都可以将通用集群作为管理层来管理多个系统。这可能会让人困惑，因为“集群”这个术语可以有多种含义。

## 系统设计架构中的最佳实践

系统设计的最佳实践是尽可能将架构保持简单，以满足需求，但又不能更简单。记住，简单是一种优势，而非限制。复杂性应尽量避免，因为复杂性会带来成本和风险。

在任何评估中，从砖块开始。就一个，单独的独立服务器。简单而有效。现在评估一下，这是否满足你的需求？如何通过多花钱更好地满足你的需求？

如果需要高可用性，请评估超融合架构。从架构角度来说，没人能比它更可靠了。

如果你有特殊情况需要在大规模下进行成本节约，其他设计可能会适用。但请记住，无论情感上感觉多么可靠，或销售人员如何推销该解决方案，超融合架构在设计上从根本上是无法被超越的可靠性。确保任何不是这两种起点之一的设计，都要在全面了解所有风险和成本的基础上使用。

我们覆盖了大量的材料，也颠覆了很多传统思维。令人遗憾的是，*传统思维*在这里等同于*盲目忽视需求和风险逻辑*，但事实就是如此。这是一个非常困难的话题，因为它对大多数人来说既陌生，又是一个非常专业的技能。而且在许多情况下，你会遇到很多来自他人的反对，他们难以评估或沟通风险，也无法将风险信息转化为可执行的商业决策。

此时你已经拥有了物理设计系统所需的工具和知识。这是一个庞大的话题，可能值得时不时地重新审视。这是非常基础的知识，为我们提供了在“象征性堆栈”上构建可靠系统的起点。现在，我们知道如何根据不同的需求设计不同的系统架构，接下来我们将探讨风险本身，学会询问什么样的风险缓解措施最适合我们。

## 风险评估和可用性需求

在我们设计系统架构的核心任务中，就是将业务需求（关于性能和可用性）与我们对风险的理解结合，并且像所有商业（因此也是 IT）中的事情一样，评估成本。在上一部分中，我们已经讨论了很多关于风险的内容。我们必须这样做——风险和性能几乎定义了我们在设计阶段的所有内容（除了严格的能力和功能）。

如果我们询问企业关于风险的看法，我们几乎总是会收到两种标准回答之一：*我们不愿意花费任何成本来缓解风险*，或者*我们不能承受停机，必须百分之百的时间保持在线*。这两种回答显然是疯狂的，任何商业人士或 IT 专业人士都不应该说出这样的答案，然而，这几乎是你得到的唯一答案，完全没有任何指导意义。它们代表着管理层完全*忽视* IT，将所有决策风险都留给 IT，而管理层却没有提供任何指导。

我们有一些基本的指导原则，几乎总是可以应用的。在范围的*低*端，经验法则是，如果数据足够有价值，值得一开始就存储，那么至少它是值得备份的。这是数据和可用性保护中最简单的方面，如果你的公司认为他们存储的数据甚至不值得备份，你应该问问自己，你为什么在这里工作。确实有一些极特殊的情况，存储的数据确实是暂时的，不需要备份，但这种情况是如此独特和罕见，以至于可以安全地忽略不计。

在范围的另一端，没有任何一个系统，无论在哪里，都重要到不值得任何形式的停机。首先，完全避免停机是不可能的。没有人能够在任何资源的情况下做到这一点。我们可以使一个系统变得极其可靠，并能够从几乎无尽的潜在场景中轻松恢复，但无论是政府、军队、秘密集团、外星物种、投资银行，还是其他任何机构，都不可能满足小型企业经常要求的那些没有任何讨论的要求。为了使系统可靠所能投入的理论最大值，就是相关公司的全部价值，即使最大的公司将每一分钱都投入到提高可靠性而不做任何其他投资，风险仍然存在，无论多么微小。

风险与收益递减

尝试投资于风险缓解技术是一件非常困难的事情，因为随着系统变得越来越可靠，将“针”向更高可靠性移动的成本变得越来越昂贵。例如，获取一台构建良好的独立服务器，可能为我们提供高达五个 9 的可用性，而无需任何特殊的*高可用性*特性。

我们可能发现我们需要更高的可用性，也许是六个或七个 9。为了获得这种数量级的可靠性提升，几乎肯定需要至少双倍于独立服务器的硬件投资。这可能完全符合我们的需求，但每单位工作负载的成本大幅上升。

如果我们想要再进一步，提升一个数量级，价格又会再一次跳升。我们投入更多的钱，得到的保护却越来越少。

因此，由于我们的企业很少愿意或能够清楚地定义我们的风险规避水平，通常需要由 IT 部门，具体来说是由系统管理团队，代表管理层执行这一极其重要的任务。这通常需要一些数学计算、大量与公司各个部门的访谈、一些常识，当然，还有一些猜测。与风险打交道时，需要保持一种逻辑性的视角，尽管情绪化的反应是非常诱人的，这也是商业中常犯的错误。企业主或经理往往会情绪化地反应，要么认为用于保护的资金没有直接产生收入，因此不值得投入；要么认为他们的业务不值得保护，从而过度支出，试图通过展示公司具有价值来给人留下深刻印象，因为停机对公司来说将是极其可怕的事情。

当然，良好的管理总是会在风险评估任务中发挥重要作用。这不应当由 IT 部门承担。虽然 IT 部门具有深刻的洞察力，并且在任何风险讨论中是一个宝贵的贡献者，但真正拥有完整风险图景的，是核心管理、运营和财务部门，他们才是制定公司基础设施风险策略所必需的部门。

在评估工作负载时，我们必须尝试评估停机会给我们的企业带来什么真正的成本。这在几乎任何情况下都不是一件简单的事情，但这是我们需要理解的内容，以便有任何合理的方式来讨论风险。大多数企业希望将停机成本简化为最简单的术语。因此，每分钟或每小时的美元损失通常是讨论停机时间的方式。例如，失去公司的主要业务应用程序将使企业每小时损失一千美元。

尽管简单，几乎没有现实中的工作负载会在每小时内均匀地损失收入。在现实中，通常会看到一条复杂的曲线。例如，在最初的几分钟，甚至可能是几个小时，我们可能几乎看不到任何收入损失。但随后，我们通常会看到一个峰值，因为停机时间足够长，首先会被客户察觉，接着引起客户的关注。缺乏信心和缺乏操作的峰值往往会相对迅速地达到顶峰。接着，长期的收入损失通常会在几天或几周后开始显现，因为客户流失。但这个曲线对每个企业来说都是不同的。当然，绘制出所有停机场景的完整曲线图是困难的，可能也不实际，但企业应该能够预测沿时间线的关键转折点，这些点代表了影响行为的重大变化。

我们常常容易将故障看作是全有或全无的情况。基本上，忽视工作负载，将整个公司视为完全瘫痪，就像僵尸末日发生了，所有员工都被感染了一样。能够如此影响企业的工作负载是非常罕见的。例如，如果公司失去了电子邮件工作负载，可能会带来影响，但由于电子邮件不是实时的，可能需要几个小时甚至几天才会实际造成收入损失（但如果电子邮件用于实时竞标，失去几分钟可能会产生非常大的影响——这完全取决于具体情况）。但假设电子邮件无法在几个小时或几天内恢复，正常的企业将立即通过其他渠道开始缓解电子邮件工作负载的损失。也许员工们会面对面交谈，或者使用公司即时通讯工具。也许销售团队开始通过电话联系客户，而不是发送电子邮件。在失去工作负载的情况下找到解决办法，往往比人们想象的要容易和有效，直到通过分诊过程来查看实际的恢复可能是什么样的。

## 工作负载相互作用

另一个我们需要理解的内容是工作负载之间是如何相互作用的。作为系统管理员，我们可能对技术依赖关系有深入的了解，比如一个关键的 ERP 或 CRM 系统依赖于另一个应用程序（如电子邮件或财务系统）才能正常运行，如果其中一个系统出现故障，另一个也会受到影响。这是工作负载依赖关系的一个重要方面，但只需提到它，大家通常就能理解。更难理解的是系统的人类工作流程相互依赖性。

有些工作负载可能是完全独立的。有些可能会显著依赖于其他工作负载。其他的工作负载可能会重叠并提供非正式的风险缓解。

我们先来看第三种情况。如今许多组织可能拥有传统电话、电子邮件、几种类型的视频会议解决方案，以及一些即时通讯工具。即使在一个小型组织中，也很容易因为许多工具捆绑了大量功能而随意地拥有多个重叠的解决方案。在这种情况下，失去电子邮件可能在很长一段时间内影响不大，因为内部通讯可能会转移到即时通讯工具上，客户通讯可能会通过电话或视频会议进行。大多数组织都有能力通过使用其他可用工具来应对某个系统的故障。

但反过来也是成立的。两个技术上没有关联的系统，比如 CRM 和电子邮件，可能无法直接连接，但人类的工作流程可能需要同时使用它们，而失去其中任何一个可能在功能上等同于失去两者。因此，我们必须考虑所有的使用案例以及所有的应对措施。

这种相互作用的知识将帮助我们决定如何合理地部署某些工作负载。例如，如果电子邮件和即时消息在危机期间会相互重叠，那么尽量解耦它们可能更为合适，以便在其中一个的硬件或软件发生故障时，不会波及到另一个系统。

如果我们有一些系统，例如电子邮件和客户关系管理（CRM）系统，两个系统互为依赖，那么将这两种工作负载组合在一起共享失败域可能是完全合理的。举个例子，如果我们有两台独立的服务器，一台运行 CRM，另一台运行电子邮件，那么每台服务器都会有自己的故障风险，并且几乎可以肯定，这些故障不会在重叠的时间发生。每个工作负载都有相同的预期年停机时间 X。两个工作负载的总停机时间是 2X。这个数学很简单。将这两个工作负载合并到同一台服务器上，因为每个工作负载仍然保持相等的年停机风险，即 X，那么合并后的总停机时间仍然是 2X。但第一个例子的有效停机时间是 2X（或者 1.99999X，因为停机重叠的几率非常小），而第二个例子的有效停机时间仅为 1X。我们是怎么做到的？不是通过减少单一风险，而是通过降低有效风险——即影响到业务最终结果的风险。在背后，我们确实降低了风险，因为一台服务器的停机风险只有两台相等服务器的一半，原因就是故障的设备数减少了一半。

即使是公司完全停运，也不一定意味着完全的损失。让员工回家享受一个突如其来的假期，可能会降低保险费用并提升士气。给员工一两天的时间待在家里，可能会重新激发他们的活力，他们可能会在系统恢复后高兴地回到工作岗位，更高效地工作，或者为弥补失去的业务投入一些额外时间。我们在评估失败工作负载所带来的损失时，必须考虑缓解策略。一些企业可能只是效率下降，而其他企业则可能会失去客户。

当然，我们也必须考虑相反的可能性。如果你是一家严重依赖客户对高可用性认知的企业，即使是一个小小的停机也可能产生广泛的影响呢？也许你的整个业务每小时仅生成一千美元的收入，但即便是两个小时的停机（在这种情况下，我们假设停机最多只会损失两千美元）也可能导致客户信心的丧失，从而引发客户流失，造成几十万甚至几百万美元的损失！

所有这些损失都只是估算。即使停机事件真的发生了，也没有可靠的方式来知道如果没有发生停机，收入会是多少。所以如果我们在事件发生后无法确定这个数字，那么显然，在事件只可能发生、尚未发生之前，我们也无法确定这一数字。结论是……估算风险是非常困难的。

在一个大型组织中，可以考虑在周末与来自不同部门的员工进行 *假设* 游戏。模拟一些 *X 或 Y 失败了* 的场景，并尝试在小范围内进行几乎真实世界的模拟来减轻问题。你能用某个工具继续工作吗？哪些部门会变得功能失效，哪些仍能正常运行，你的客户如何看待这种情况？这种类型的 *游戏* 最适合由强大的规划者与感知者（例如：急救专家）结合进行。规划者思考风险策略并编写程序，而感知者则不做计划，而是在现场进行战术工作，找出如何利用现有工具继续工作。

## 定义高可用性

我在 IT 领域最喜欢的一句名言来自约翰·尼科尔森，他说过 *高可用性不是你能购买的东西，而是你需要做的事情*。我们常常会把高可用性看作是 IT 中的一个内在需求，认为它复杂到无法理解，因此会被那些将未经验证的 *高可用性* 标签贴在产品上的厂商所欺骗，甚至仅仅把它作为产品名称的一部分，假装购买某个产品就能实现高可用性，然而从逻辑上讲，这几乎是不可能的。比如，想象一下购买一架高可用性的飞机。虽然你可以使一架飞机比另一架飞机更可靠，但你整体的可靠性和安全性主要来自飞行员，而非飞机本身。在 IT 领域也是如此。即便是做工最好的产品，如果操作不当，其效果也微乎其微。一个没有备份的百万美元集群，可能比一个备份完善的台式机还不安全！

所以，首先我们需要建立一个衡量的基准。在上一节中，我们提到独立服务器基础设施作为我们的基准。这一基准必须代表我们所说的 *标准可用性*。我们现在有两种方法来衡量这种可用性。一种是通过给出一个 *九* 数字，来以绝对的方式表示可用性，通过行业证据，维护良好、做工精良的独立服务器可以达到五个九的可用性，这大致意味着每年有六分钟或更少的计划外停机时间（计划内的维护停机时间可能是潜在问题，但并不包括在这样的可靠性数字中）。

现在请记住，当我们谈论服务器或系统设计架构时，我们并不包括最终的工作负载，只是指提供底层系统的平台，平台上将安装虚拟机监控程序。所以，基本上是指硬件可用性。在其上运行的任何软件可能有自己的可靠性问题，平台的稳定性无法解决来自最终工作负载中糟糕代码带来的不稳定问题。

另外一种，通常也更有用的方式来看待系统架构的可靠性，不是以无法衡量的绝对标准，而是通过相对方式将不同设计进行比较。没有人真正知道系统的可靠性是什么。服务器供应商并没有在对我们保密，他们只是不了解。每一个小小的系统配置差异都会产生非常不同的可靠性数据，就像我们对飞机的说法一样，操作系统的用户对可靠性产生了最大的影响。一个拥有完美数据中心、持续现场支持、能及时响应警报并且有备件随时待命的公司，可能会从同一台困在没有空调、充满灰尘并且通常被忽视的机房里的服务器中挤出完全不同的可靠性数据。影响因素实在是太多了。即使我们能够 somehow 考虑所有潜在的变化，为了获得关于如此复杂且故障率如此低的系统的有意义统计数据，我们也需要运营数千台甚至数万台服务器超过十年，才能收集到有用的数据，而这些数据过十年后都会过时。所以，实际上，这些都无法被衡量。

所以，我们最重要的工具不是以*九个*为单位谈论，这是一个伟大的营销工具，也是一些深陷于复杂流程（如六西格玛）的管理者喜欢重复的内容，但在这个语境下并没有实际意义。相反，我们应该着眼于系统与基准的偏差数量级。比基准可用性高得多的系统可以被归类为*高可用性*，而比基准可用性低得多的系统可以被归类为*低可用性*，任何与基准大致相同的系统则保持*标准可用性*。超出这些一般术语之后，讨论几乎变得不可能。

类似超融合的系统设计通常被归类为*高可用性*，因为它是最可靠的设计方法。而 IPOD 通常被归类为*低可用性*，因为它更接近于最不可靠的设计方法——网络系统设计。分层集群通常被认为是高可用性，但不*如超融合*那样高。当然，在这种情况下，我们仅仅考虑系统设计的可用性，而忽略单个组件。如果我们在 IPOD 的每一层都使用极高可用性的单个组件，理论上可以将其恢复到标准可用性，但可能需要付出极大的成本。

从相对角度考虑可靠性要比从绝对角度考虑更有价值。单纯通过常识和风险的定位、风险缓解、风险积累等设计要素，几乎可以轻松看出一个独立服务器、一个 IPOD 以及超融合系统在可用性上的明显*高*、*中*、*低*差异。这不需要特别的培训或数学能力，就能明了每个系统之间的区别以及如何提升组件的整体质量会移动绝对可靠性数值，而相对可靠性却不变。最终，我们所能知道的就是这些。

通过了解停机对我们业务的财务影响，即使只是一个非常粗略的估算，我们在决定如何投资风险缓解时就有了参考依据。我们永远不应在风险缓解上投入超过计算风险所显示的潜在损失。这听起来很明显，但在许多公司中这是一个常见的评估难点。例如，如果一次可能的停机会导致我们损失一千美元，而有效地保护这一停机的费用是两千美元，我们就绝不应考虑花钱来缓解这个风险。

我们应该把风险缓解看作是一种停机本身，从数学角度来看，这是为了让计算变得更容易理解。通过良好的风险缓解，我们现在会承受最低的财务损失（例如花费一千美元）来防止一次可能导致十万美元损失的大规模停机。前期成本是确定的，而未来风险仅仅是一个可能性。因此，任何风险缓解的投入都应该远低于其所要保护的潜在损害。

我用来形容为了缓解风险而支付的费用超出了潜在损害的类比已经用了多年：*这就像今天为了避免明年可能会得头痛而朝自己脸上开枪*。

在比较标准可用性系统和高可用性系统时，我们可能只是在讨论每年平均几分钟的停机时间差异。因此，高可用性必须迅速证明其成本和复杂性的合理性。对于一个巨大的公共网站，如果仅仅一两分钟的不可用就可能导致数百万的购买损失，或者更糟糕的是，损害客户信任，那么即使节省的时间看起来微不足道，仍然可以轻松证明在高可用性系统上的大量支出是合理的。但对于一个服务员工的内部系统而言，客户信任并非关键因素，且停机不会导致用户转向竞争对手（例如：电子邮件系统、财务系统、CRM 系统等），即便是每天失去几分钟，甚至一年，也可能对财务没有实际影响，重金投入来保护这些系统反而是浪费。

那么，我们如何将这一切应用到最佳实践中呢？困难的答案是，风险评估和随之而来的系统设计是非常艰难的工作。确定风险是一个漫长的过程，涉及大量的数学、逻辑，并在某种程度上需要猜测。这要求我们深入了解我们的业务和技术栈。它要求我们在各个层面和各部门与业务进行互动，整合通常是孤立的信息。它迫使我们将其他风险评估与逻辑和预期的情感反应进行对比。

经验法则告诉我们，我们部署的大多数系统应该是独立服务器，几乎所有剩下的系统应该是超融合的。这两种标准模式代表了现实世界中正确设计的近乎全部内容。所有其他设计实际上都被归类为极为小众的用例，其中 IPOD 是最典型的*反模式*，除了极端特殊情况外，绝对不要去做。

本章我们覆盖了很多内容。但现在我们已经知道如何做出风险判断，如何基于这种评估设计架构。我们理解了如何以及为何使用不同种类的虚拟化，为什么我们总是进行虚拟化。而且我们知道如何评估云和局部性在部署中的使用。现在，将这一切结合起来！我们在决定每个工作负载的部署时都会使用这些工具！有这么多选择，但这正是让我们的职业充满挑战和成就感（也是让我们值回薪水的原因）。

# 总结

总结来说，系统架构是复杂的，要求我们真正深入了解业务需求、运营方式，与组织中的关键角色沟通并获取意见，并广泛地考虑技术构建块，构建能够以最低成本提供所需性能和可靠性的解决方案。

我们研究了虚拟机和容器的基本组件，现在应该能够为它们的使用辩护，并在它们之间做出正确选择，还能使用传统容器而不被较新的应用容器搞混。而且我们学到了局部性。你应该能够穿越复杂的语言迷宫，理解经理们讨论服务器资源的部署和所有权时的难点，分析成本和风险，并为组织找到合适的选择。共址、云、传统虚拟化、本地部署，所有这些选择你都能理解。

最后，我们来谈谈系统设计和架构。将我们系统的物理和逻辑组件整合，构建一个完整的功能平台，能够提升我们的工作负载，而不是削弱它们。这是一个很长的章节，涉及了许多非常少被单独教授的话题，更不用说将它们结合起来了。这些都是非常难的话题，可能在继续之前值得再次深入探讨这些内容。

对于许多从事系统管理的我们来说，可能几乎永远不会使用本章节中的内容。而对于其他人来说，这可能是几乎每天都需要的技能。这些话题通常能让你通过展示能够将看似平凡的技术细节与系统设计决策结合，解决关键的组织需求，彻底提升你的职业生涯。在本书的所有话题中，这个话题可能是最能让你脱颖而出，跨越组织边界的一个。

我希望通过这里提供的信息，您能够过滤掉销售和市场营销的虚假信息，运用严密的逻辑和推理，并在永恒的概念上进行构建。花时间真正理解故障域、附加风险、虚假冗余等，将使你在信息技术旅程的几乎每个方面变得更加出色，无论你的目标是纯粹的技术性，还是梦想着坐上董事会的椅子。

在我们的下一个章节中，我们将回到看似更为平凡的话题——系统补丁管理，并从高层次的系统策略转向实际操作中的安全性和稳定性战争。
