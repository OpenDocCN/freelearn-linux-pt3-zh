- en: '*Chapter 5*: Patch Management Strategies'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 5 章*：补丁管理策略'
- en: In day-to-day operations of your Linux systems probably the most common task
    for an average system administrator is going to be patch management (aka patching.)
    Unlike the Windows and macOS worlds, it is standard for the system administrator
    to handle a broad variety of operating system and application patching tasks covering
    both primary and third-party ecosystems. It is also standard for there to be built
    in, and sometimes third party, application management ecosystems to assist with
    this potentially daunting task.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Linux 系统的日常操作中，平均系统管理员最常见的任务大概是补丁管理（即补丁更新）。与 Windows 和 macOS 世界不同，系统管理员通常负责处理各种操作系统和应用程序的补丁更新任务，涵盖了主系统和第三方生态系统。同时，系统中通常内置或通过第三方提供应用程序管理工具，以协助完成这项可能令人生畏的工作。
- en: Patching, and of course system updates, are large parts of what we do and while
    it may feel mundane it is very important that it be something that we get right.
    And production Linux systems today have become much more complex and diverse than
    they were just ten years ago. And of course, patching has become more important
    than ever, something that we expect to only see increase over time as well.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 补丁管理，当然还有系统更新，是我们工作中的重要部分，虽然它可能感觉平凡，但确保它做得正确非常重要。而今天的生产 Linux 系统比十年前更加复杂和多样化。补丁管理也变得比以往任何时候都更为重要，我们预计这一趋势随着时间的推移只会越来越明显。
- en: We will start by understanding how patches and updates are provided and what
    we mean by different installation methodologies.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先了解补丁和更新是如何提供的，以及我们所说的不同安装方法的含义。
- en: 'In this chapter we are going to learn about the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: Binary, Source, and Script Software Deployments
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二进制、源代码和脚本软件部署
- en: Patching Theory and Strategies
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 补丁理论与策略
- en: Compilation for the Administrator
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理员的编译工作
- en: Linux Installation and Redeployment Strategies
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux 安装与重新部署策略
- en: Rebooting Servers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重启服务器
- en: Binary, source, and script software deployments
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二进制、源代码和脚本软件部署
- en: Software can come in all shapes and sizes. So, software deployments are not
    a one size fits all affair. The standard means of deploying software are directly
    as a binary package, through source code that needs to be compiled into a binary
    package, or as a script. We will dig into each of these as it is necessary to
    understand what each is and when they might be appropriate.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 软件有各种形态和大小。因此，软件部署并不是一刀切的事。软件的标准部署方式有三种：直接作为二进制包、通过需要编译成二进制包的源代码，或者作为脚本。我们将深入探讨这些方式，因为理解每种方式的含义及其适用场景非常重要。
- en: Compiled and interpreted software
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编译型和解释型软件
- en: 'Many system administrators have never worked as developers and often are not
    aware of how software exists. There are two fundamental types of programming languages:
    compiled and interpreted.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 许多系统管理员从未作为开发人员工作过，通常也不了解软件的存在形式。编程语言主要有两种基本类型：编译型和解释型。
- en: Compiled languages are written in one language (source code) and run through
    a compiler to produce binary executable code that can run directly on an operating
    system. This can be an oversimplification, but we are not developers and need
    only be concerned with the original code being compiled into a binary format.
    For Linux, this format is called **ELF**, which stands for **Executable and Linkable
    Format**. Compiled binaries run on the operating system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 编译语言是用一种语言（源代码）编写的，通过编译器生成二进制可执行代码，这些代码可以直接在操作系统上运行。这可能是一个过于简化的说法，但我们不是开发人员，仅需要关注原始代码被编译成二进制格式即可。对于
    Linux，这种格式被称为**ELF**，即**可执行与可链接格式**。编译后的二进制文件在操作系统上运行。
- en: Interpreted languages are different. Instead of being compiled into a binary,
    they remain as written, and are processed in real time by a program called an
    interpreter which itself is a binary executable and which treats the code as an
    input file to process. So interpreted software requires that an appropriate interpreter
    for the language in which it is written is installed on the operating system in
    order for the software to work. For example, if you have a Python program that
    you want to be able to run, the system on which you want to run it will need to
    have a Python interpreter installed to process that file.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 解释型语言有所不同。它们不像编译成二进制那样，而是保持原样，并由一个叫做解释器的程序实时处理，解释器本身是一个二进制可执行文件，它将代码作为输入文件来处理。因此，解释型软件要求操作系统上安装适当的解释器，以便该软件能正常工作。例如，如果你有一个Python程序，并希望在某个系统上运行，那么你需要确保该系统上安装了Python解释器来处理这个文件。
- en: Both approaches to software are completely normal and valid. As system administrators,
    we will work with both all of the time. Modern computers (and interpreters) are
    so fast that performance is of little concern between the two types and other
    concerns (mostly of the developers) are generally more important in deciding how
    a given piece of software will be written.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种软件方法完全正常且有效。作为系统管理员，我们将始终与这两种方法打交道。现代计算机（和解释器）速度非常快，因此两者之间的性能差异几乎没有什么关切，其他因素（主要是开发人员的考虑）通常在决定软件如何编写时更为重要。
- en: Software is not quite this simplistic, there are bizarre concepts like code
    that looks like it is interpreted but is actually compiled at the last moment
    and run like a binary. Some languages like those based on .NET and Java are compiled,
    but not to a binary, and so are essentially an amalgam taking some benefits of
    both approaches.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 软件并不完全如此简单，存在一些奇怪的概念，比如看似解释型的代码，实际上却在最后一刻被编译并像二进制文件一样运行。一些像基于.NET和Java的语言，虽然是编译的，但并不是编译成二进制文件，因此本质上是结合了两种方法的一种混合体，兼具两者的优点。
- en: However, by and large we think of all software as either binary executable (runs
    directly on the operating system without assistance) or interpreted (requires
    a programming language environment or *platform* on which to run, on top of the
    operating system.) For the purposes of understanding code deployment, languages
    like .NET and Java, as well as **JIT** (**just in time**) compiled ones like Perl
    are lumped with interpreted languages due to behavior.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常我们将所有软件视为二进制可执行文件（直接在操作系统上运行，无需帮助）或解释型软件（需要操作系统上的编程语言环境或*平台*来运行）。为了理解代码部署的目的，像.NET和Java这样的语言，以及**JIT**（**即时编译**）的语言，如Perl，由于其行为，通常被归类为解释型语言。
- en: Common languages that are generally precompiled include C, C++, Rust, and Go.
    Common languages that are interpreted, or act as though they are, include Python,
    PHP, Perl, and Ruby. To make matters more confusing, any interpreted language
    can be compiled. A standardly compiled language could even be interpreted! It
    is less *what a language does* as much as *what is it doing in this specific situation*?
    Essentially, any given language *can* be compiled or interpreted depending on
    how we treat it in practice. However, in the real world, no one is interpreting
    C++, and no one is compiling Python. But if you wanted to, it is possible.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的预编译语言包括C、C++、Rust和Go。常见的解释型语言，或者看起来像是解释型语言的，包括Python、PHP、Perl和Ruby。更让人困惑的是，任何解释型语言都可以被编译。标准编译语言甚至可以被解释！更重要的不是*语言本身做了什么*，而是*在特定情况下它是怎么做的*？本质上，任何语言*都可以*根据我们在实践中的处理方式来选择编译或解释。然而，在现实世界中，没有人会解释C++，也没有人会编译Python。但如果你愿意，还是有可能的。
- en: As system administrators, we really have no say in how software is built, we
    simply have to deploy software as it is given to us. What we have to understand
    most is how much control and influence we may have on the total platform. If we
    run a binary application, it is possible that our only real options are around
    the version of the kernel that we run. But if we are installing a PHP script,
    we may have to decide how to install PHP as well, what version to run, which PHP
    provider, and so forth. It can become rather complex in some situations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作为系统管理员，我们实际上对软件的构建没有发言权，我们只需要按给定的方式部署软件。我们最需要理解的是，我们可能对整个平台的控制和影响力有多少。如果我们运行一个二进制应用程序，那么我们的实际选项可能仅限于我们所运行的内核版本。但如果我们要安装一个PHP脚本，我们可能还需要决定如何安装PHP，选择哪个版本，选择哪个PHP提供者，等等。在某些情况下，这可能变得相当复杂。
- en: The majority of software that we will deploy is going to be binary in nearly
    all business scenarios. Often we might not even know (or care) about specific
    software as so much of the process will typically be handled for us. It is quite
    common to have to install software blindly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在几乎所有商业场景中，我们部署的大多数软件将是二进制格式。通常，我们甚至可能不知道（或不关心）特定的软件，因为整个过程通常会由其他人处理。盲目安装软件的情况非常常见。
- en: It is increasingly common for system administrators to install software that
    is built of scripts which are readily readable code. These files are simply processed
    by an interpreter. So, the software never exists on disk in binary form. Since
    modern computer systems are so powerful, the seemingly problematic lack of efficiency
    in this process is often no problem at all. Many popular software packages are
    now written and delivered in this way, so most system administrators will commonly
    work with script installations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 系统管理员安装由脚本构成的软件的情况越来越常见，这些脚本是可以直接读取的代码。这些文件只是由解释器处理。因此，软件从未以二进制形式存在于磁盘上。由于现代计算机系统如此强大，尽管这一过程在效率上似乎存在问题，但通常并不成问题。现在，许多流行的软件包都是以这种方式编写和交付的，因此大多数系统管理员通常会与脚本安装打交道。
- en: In many cases, scripts are installed using the same automation methods as binary
    software packages making the entire process often transparent. As a system administrator,
    you might not always even know what kind of package you are deploying unless you
    dig into its underlying components. This becomes especially true of non-critical
    packages, and packages deployed using a dependency resolving system that handles
    any platform inclusion (for example, PHP or Python) for you, or if those dependencies
    were already installed for other components ahead of time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，脚本的安装与二进制软件包使用相同的自动化方法，这使得整个过程通常对管理员透明。作为系统管理员，你可能并不总是知道自己正在部署的是什么类型的软件包，除非你深入其底层组件。这种情况在非关键软件包中尤为明显，尤其是使用依赖解决系统部署的软件包（例如
    PHP 或 Python），它们为你处理了平台的集成，或者这些依赖项已经为其他组件提前安装好了。
- en: Today we expect that the installation of scripts will be a common task that
    may not represent the majority of all packages that are deployed to a system but
    that can easily form the majority of primary workload code on a system. By that
    I mean that the operating system, supporting utilities, and large system libraries
    will often be binary packages. But the final workload, for which we are running
    the system in the first place, will have a very good chance of being a script
    rather than a compiled binary.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们预期脚本的安装将成为一项常见的任务，这可能不会代表部署到系统上的大多数软件包，但它很容易构成系统上主要的工作负载代码。我的意思是，操作系统、支持工具和大型系统库通常是二进制软件包。但最终的工作负载，系统之所以存在，往往有很大可能是脚本，而非编译后的二进制文件。
- en: And, finally, the last type is the source code that cannot be run as is and
    must first be compiled into binary packages before being run. We are going to
    cover this topic, in depth, in just two sections, so I am only going to touch
    on it briefly here. You could argue that this approach is still a binary installation
    because the resulting deployed package is binary and that is totally correct.
    However, the workflow that must be followed to get and deploy that binary is quite
    different and makes this a valid tertiary deployment option. Some systems implement
    compilation steps automatically, and so it is plausible to have a deployment package
    that is compiling and installing a binary, and the system administrator is not
    even aware that it is happening.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最后一种类型是不能直接运行的源代码，必须先编译成二进制软件包才能运行。我们将在接下来的两个部分中深入讨论这个主题，因此这里只会简单提及。你可以争辩说，这种方法仍然是二进制安装，因为最终部署的软件包是二进制的，这完全正确。然而，为了获取和部署这个二进制文件，必须遵循的工作流程截然不同，因此这成为了一种有效的三级部署选项。一些系统会自动执行编译步骤，因此有可能出现一个部署包，它在编译并安装二进制文件的同时，系统管理员甚至都没有察觉。
- en: Misleading use of source installation
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 误导性的源代码安装使用
- en: For reasons we will dig into a little later in this chapter, installing from
    source code generally developed a bad reputation. In some ways this was deserved,
    and in some ways it was not.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们稍后将在本章深入讨论的原因，从源代码安装软件通常会有不好的声誉。在某些方面，这是应得的，而在某些方面，又并非如此。
- en: Because source based installation is, for all intents and purposes, unique to
    the free and open source software world it was heavily targeted by vendors and
    IT practitioners in the 1990s and 2000s in an attempt to discredit it because
    it was cutting heavily into more traditional closed source products (and the jobs
    of people who only supported that software.) This was, of course, completely fabricated
    but as source licensing is complicated to understand it is easy to instill fear
    and doubt into those that fail to grasp the nuances of the topic.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因为源代码安装在实际上是自由和开源软件世界的独特存在，它在 1990 年代和 2000 年代时遭到厂商和 IT 从业人员的猛烈攻击，目的是为了抹黑它，因为它严重影响了传统的闭源产品（以及那些只支持闭源软件的人的工作）。这当然是完全虚构的，但由于源代码授权复杂难懂，很容易让那些没有掌握该话题细微差别的人产生恐惧和怀疑。
- en: More importantly, however, source installation got a bad reputation because
    it was seen as being a generally unprofessional and unnecessary practice being
    promoted by system administrators who acted more like hobbyists and installed
    in this manner without real consideration for business needs. It was fun or looked
    impressive or their friends did it, so they did it, too. This, I am afraid to
    say, was broadly accurate. There was an era when lots of software was installed
    in unnecessarily complex and convoluted ways without regard for the business efficacy
    of the process. Not to say that source code compilation never has a place, it
    most certainly does. But even twenty years ago or more that place was in a niche,
    not the majority of deployment situations. So, in many ways, the bad reputation
    was earned honestly, but not completely.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，更重要的是，源代码安装被贴上了不专业且不必要的标签，原因是它被认为是由一些更像爱好者的系统管理员推动的，他们在没有真正考虑业务需求的情况下以这种方式安装。它有趣，或者看起来很有印象，或者是他们的朋友这么做，所以他们也这么做。我很遗憾地说，这种说法在很大程度上是准确的。曾经有一段时期，许多软件以不必要的复杂和繁琐方式安装，而不考虑这一过程对业务的有效性。并不是说源代码编译没有其存在的价值，它确实有。但即使在二十年前或更久，这种方式也只是局限在一个小众的场景中，而不是大多数的部署情况。因此，从很多方面来看，这个坏名声是实至名归的，但并非完全如此。
- en: Today, however, it is not a big deal as source code compilation is nearly forgotten
    and almost no one today knows the standard processes with which to do it and the
    installation of the necessary tools is often banned or at least the tools are
    not readily available making compilation quite difficult, if even possible. Only
    software that has a strong need to be compiled is distributed in this fashion.
    So, the market has all but eliminated this in practice.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，今天，源代码编译几乎被遗忘，几乎没有人知道如何使用标准流程来进行编译，并且相关工具的安装通常被禁止，或者这些工具并不容易获得，使得编译变得非常困难，甚至不可能。只有那些需要编译的强烈需求的软件才会以这种方式分发。因此，市场实际上几乎消除了这种做法。
- en: But the fear and shaming of those that used to do compilation often still exists.
    Saying that someone is performing a source code installation remains a derogatory
    statement. Sadly, in an attempt to discredit even more software today, it has
    become common to use the term not to reference software that has to be compiled
    from source into binary, but to refer to script-based software which does not
    have a compilation step in this way. The term source code implies that the code
    has to be turned from source into binary. Scripts are not considered source code,
    at least not in this context. Technically, however, they are the original code,
    the source, but the implied bad step does not exist. But few people would follow
    up and can be easily misled by this little linguist trick. So, it is an easy way
    to take a manager who is just looking for an excuse to make an emotional decision,
    rather than more difficult rational, one, and mislead them. It sounds reasonable,
    and few will bother to actually think it through.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，那些曾经进行编译的人仍然面临着恐惧和羞辱。说某人正在进行源代码安装，依然是一种贬低的说法。遗憾的是，为了进一步抹黑现今的软件，现在已经普遍将这个术语用于指代那些不需要通过源代码编译成二进制的脚本类软件。术语“源代码”暗示代码必须从源代码转化为二进制，而脚本在这个上下文中并不被认为是源代码。然而，从技术上来说，它们确实是原始代码、源代码，但不再有那个令人不悦的步骤。可是很少有人会进一步探讨这个问题，而这一小小的语言技巧很容易误导他人。所以，这成为了一种简单的手段，能够让一个仅仅寻求借口来做情绪决策的经理，被误导。这听起来很合理，而很少有人会真的去深思。
- en: The trick really comes from semantic shorthand, something that is always dangerous,
    especially in IT. The concern with self-compiling software has nothing to do with
    the availability of the source code, but from the need to compile it before using
    it. If that step did not exist, the existence of the source code is purely a positive
    for us. People then incorrectly refer to the compilation as a source code installation.
    This semantic mistake opens the doors for someone to take something that technically
    truly is a source code install, without compilation, and because the term has
    been used incorrectly for so long it becomes a negative connotation applied to
    the wrong thing, and no one understands why any of it is wrong or backwards.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个窍门实际上来自于语义简写，这是一个总是危险的东西，尤其是在IT领域。自编译软件的问题与源代码的可用性无关，而是与在使用之前需要编译它有关。如果没有这个步骤，源代码的存在对我们来说完全是积极的。人们随后错误地将编译过程称为源代码安装。这种语义错误为某些人打开了大门，他们将某个技术上确实是源代码安装的软件（没有编译）误解为安装源代码，而由于这个术语长时间被错误使用，它变成了一种负面的含义，应用到错误的事物上，最终没有人理解为什么这些东西是错的或反向的。
- en: Linux offers us many standard enterprise methods for software deployments. The
    plethora of options, while powerful, is making it far more difficult to standardize
    and plan for long term support.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 为我们提供了许多标准的企业级软件部署方法。尽管这些选项功能强大，但也使得标准化和长期支持的规划变得更加困难。
- en: One thing that is ubiquitous in all production Linux systems, regardless of
    the vendor, is a package management system that exists by default. More than anything
    else over the years, these package management systems have come to define one
    Linux based operating system from another. Several software packaging formats
    exist, but two, that is, DEB and RPM, have become dominant with all others remaining
    very niche.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有生产环境中的Linux系统中，无论供应商如何，默认都会存在一个软件包管理系统。多年来，这些软件包管理系统已成为区分不同Linux操作系统的关键因素。存在多种软件打包格式，但其中两种，即DEB和RPM，已经成为主流，其他格式则保持在非常小众的地位。
- en: It is increasingly common for Linux distributions to either have multiple software
    packaging systems or to use multiple formats under a single software package management
    system. This variety is good as it gives us more options for how we may want to
    maintain specific packages on our systems, but it also means more complexity as
    well.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在越来越多的Linux发行版要么有多个软件打包系统，要么在单一的软件包管理系统下使用多种格式。这种多样性是好的，因为它给我们提供了更多的选择来维护系统中的特定包，但也意味着更多的复杂性。
- en: As with all software deployments, we have a few standard concerns that are universal
    to all operating systems. First is whether software is self-contained or requires
    access to other packages or libraries. Traditionally, most software, especially
    on Linux and other UNIX-like operating systems, has been designed to reduce their
    size both to install and for the operating system itself, by utilizing extended
    system and third-party libraries (collectively called dependencies.) This means
    that we can have software that is as tiny as possible and other software that
    utilizes the same resources can share them on disk minimizing, sometimes significantly,
    how much we need to store and maintain. Updating a package or library for one
    piece of software will update it for all. The alternative is to have each individual
    software package come packaged with all of its own dependencies included with
    the package and available only within the singular package. This makes for much
    larger software installations and the potential for the same data to exist on
    the system multiple times. Possibly a great many times. This leads to bloat, but
    also makes individual software packages far easier to maintain as there is reduced
    interaction between different software components.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有软件部署一样，我们有一些对所有操作系统普遍适用的标准关注点。首先是软件是否自包含，或者是否需要访问其他包或库。传统上，大多数软件，尤其是在Linux和其他类UNIX操作系统上，都是通过利用扩展的系统和第三方库（统称为依赖关系）来减少其安装大小以及操作系统本身的大小。这意味着我们可以拥有尽可能小的软件，并且使用相同资源的其他软件可以共享它们在磁盘上的存储，从而最小化（有时显著地）我们需要存储和维护的内容。更新一个软件包或库时，所有使用该包的程序都会同步更新。另一种方式是让每个独立的软件包都携带自己的所有依赖项，并且这些依赖项仅在该软件包内可用。这会导致软件安装变得更大，并且可能会在系统中存在多次相同的数据。可能是很多次。这导致了臃肿，但也使得单个软件包更容易维护，因为不同软件组件之间的互动较少。
- en: For example, dozens or scores of software packages will potentially want to
    use the OpenSSL libraries. If each of twenty packages include OpenSSL, we have
    the same code stored on disk twenty times. Moreover, if an update for OpenSSL
    is released or, more importantly, if a bug is discovered and we need to patch
    OpenSSL we will need to patch it twenty times, not just once. Whereas if we used
    a single, shared OpenSSL library then whether we have one application that relies
    on it or one hundred, we would only need to patch the library to make sure that
    any bug or update had been addressed.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，数十个或更多的软件包可能都需要使用OpenSSL库。如果每二十个包中都有OpenSSL，我们将把相同的代码存储在磁盘上二十次。此外，如果OpenSSL发布了更新，或者更重要的是，如果发现了漏洞需要修补，我们就必须修补二十次，而不是一次。相比之下，如果我们使用一个共享的OpenSSL库，无论我们有一个应用依赖它还是一百个应用，我们只需修补这个库，确保所有的漏洞或更新都已解决。
- en: Both approaches are completely valid. It used to be that shared libraries were
    a necessary evil because system storage was small and shared resources allowed
    for not just reduced disk usage, but better memory optimization because potentially
    a shared library might be able to be loaded into memory and shared by multiple
    pieces of software there as well. Today, we generally have more storage and memory
    than we can practically use, and this small efficiency is no longer necessary,
    even if potentially nice. Non-shared approaches trade this efficiency for the
    stability and flexibility of each package having their own dependencies included
    with them so that conflicting needs or an unavailability of shared resources does
    not pose a problem.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都是完全有效的。过去，由于系统存储空间较小，共享库是一种“必要的恶魔”，因为共享资源不仅能减少磁盘使用，还能更好地优化内存，因为共享库有可能被加载到内存中，并且能被多个软件共享。如今，我们通常拥有更多的存储和内存，远超实际需求，这种小规模的效率优化已经不再必要，尽管它可能仍然有些许好处。非共享方法则将这种效率转化为每个软件包拥有自己的依赖项，从而避免了依赖冲突或共享资源不可用时带来的问题。
- en: The biggest advantage to shared resource approaches is probably that patching
    a known vulnerability can be far simpler. As an example, if we assume that OpenSSL
    (a broadly shared library) discovers a critical vulnerability and releases an
    update. If our systems have shared resources, we need only to find systems with
    OpenSSL installed and update that one package. All systems that depend on that
    package are automatically patched together. If OpenSSL were instead to be individually
    packaged with dozens of separate applications that all depend on it individually,
    we would need a way to identify that all of those packages use OpenSSL *and* patch
    all of them individually. A potentially daunting task. We rely on the package
    maintainers of every piece of software to do their due diligence, patch their
    dependencies quickly, and provide updated packages to us right away. Not something
    that happens too often.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 共享资源方法的最大优势可能在于，修补已知漏洞可能会更简单。举个例子，如果我们假设OpenSSL（一个广泛共享的库）发现了一个关键漏洞并发布了更新。如果我们的系统使用共享资源，我们只需要找到安装了OpenSSL的系统并更新该包。所有依赖该包的系统都会自动一起修补。如果OpenSSL被单独打包并与数十个不同的应用程序一起使用，我们就需要找到这些应用程序中的每一个，确认它们都在使用OpenSSL，并且要单独修补它们。这将是一个可能令人生畏的任务。我们依赖每个软件的包维护者进行适当的审查，快速修补依赖项，并立即提供更新的包给我们，但这并不是常有的事。
- en: 'Often systems with multiple packaging approaches will use one type of package
    management and software repository system, such as DEB, when there are shared
    system components and many dependencies to handle. They will use another package
    management system, such as SNAP, when they are going to keep all dependencies
    included in the final package. But it is far more complex than that makes it sound,
    for example, make a DEB package that include all dependencies or one that expects
    them to be provided externally and shared. It is only a convention that DEB tends
    to be shared libraries for software packages. In Linux we also have a completely
    different set of concerns that you would be used to if coming from a Windows or
    macOS background: a software ecosystem tied to the operating system itself. In
    Linux, we expect our operating system (for example, RHEL, Ubuntu, Fedora, SUSE
    Tumbleweed, and others.) to not only include the basic operating system functionality
    and a few extremely basic utilities, but also a plethora of software of nearly
    every possible description including core libraries, programming languages, databases,
    web servers, end user applications, and on and on. In many cases, you might never
    use any software that did not come packaged with your Linux distribution of choice,
    and when you do add in third party software it is often a key application that
    represents a strategic line of business application or, somewhat obviously, is
    bespoke internally developed software.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 系统通常会使用一种包管理和软件仓库系统，如 DEB，当存在共享系统组件和需要处理许多依赖项时。它们会在将所有依赖项包含在最终包中时使用另一种包管理系统，如
    SNAP。但这远比听起来复杂，例如，制作一个包含所有依赖项的 DEB 包，或者一个期望依赖项由外部提供并共享的包。DEB 通常作为软件包的共享库只是一种约定。在
    Linux 中，我们还面临着一组完全不同的关注点，如果你来自 Windows 或 macOS 背景，可能并不习惯：一个与操作系统本身紧密相连的软件生态系统。在
    Linux 中，我们期望操作系统（例如 RHEL、Ubuntu、Fedora、SUSE Tumbleweed 等）不仅包含基本的操作系统功能和一些极为基础的实用工具，还包括几乎所有可能描述的软件，包括核心库、编程语言、数据库、Web
    服务器、终端用户应用程序等等。在很多情况下，你可能永远不会使用任何没有与你的 Linux 发行版一起打包的软件，当你添加第三方软件时，它通常是一个关键应用程序，代表着战略性业务应用，或者显而易见，是内部定制开发的软件。
- en: Because of this, when working with software on Linux we have to consider if
    we are going to use software that is built into the operating system, software
    that we acquire and install independently (this would include bespoke software),
    or a mix of the two where many components of the software come from the operating
    system, but some are provided otherwise.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 Linux 上处理软件时，我们必须考虑是使用内置操作系统的软件，独立获取并安装的软件（这包括定制软件），还是两者的混合，其中软件的许多组件来自操作系统，但一些组件由其他方式提供。
- en: Digging into specifics of different packaging systems would not make sense,
    especially as they tend to overlap heavily in their general usage but be very
    unique as to real world usage. Now we know the options that exist for them. Software
    package management systems are more important in Linux than on other operating
    systems, such as Windows or macOS, because there is typically much more complexity
    in the big server systems that Linux tends to run, and the software being installed
    typically uses much broader sets of dependencies pulling components or support
    libraries from often many different projects. Linux packaging systems that maintain
    online repositories of the software, libraries, components, and so forth make
    this all reasonably possible.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 深入了解不同打包系统的具体细节没有太大意义，特别是因为它们在一般使用上往往有很大的重叠，但在实际使用中又非常独特。现在我们已经知道了它们存在的选择。软件包管理系统在
    Linux 中比在其他操作系统中更为重要，如 Windows 或 macOS，因为在 Linux 运行的大型服务器系统中，通常存在更多的复杂性，而安装的软件通常使用更广泛的依赖集，从许多不同项目中拉取组件或支持库。Linux
    打包系统维护在线软件、库、组件等的仓库，使这一切成为可能。
- en: Probably the most important aspect of the large Linux software package management
    systems and their associated software repositories is that they allow the distribution
    vendors to assemble and test vast amounts and combinations of software against
    their exact kernel and component selection and configuration providing a large,
    reliable platform on which to deploy solutions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 大型 Linux 软件包管理系统及其相关软件仓库最重要的方面可能是它们允许分发商在其精确的内核和组件选择与配置下，组装并测试大量的组合软件，从而提供一个庞大、可靠的平台来部署解决方案。
- en: Best practice here is difficult. Really, we are left only with rules of thumb,
    but very strong ones. The rule of thumb is to use the vendor repos whenever possible
    for as much software deployment as you can. This might seem simple and obvious,
    but surprisingly there are a great many people who will still go and acquire software
    through a manual means and install it without the benefit of vendor testing and
    dependency management.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，最佳实践是困难的。事实上，我们只能依赖经验法则，但这些法则非常强大。经验法则是尽可能使用供应商的仓库进行软件部署。这个方法看似简单和显而易见，但出乎意料的是，仍然有很多人会通过手动方式获取软件并安装，而没有供应商的测试和依赖管理的支持。
- en: The real best practice is, as you might expect, to get to know the package management
    ecosystem of your distribution(s) of choice so that you are well prepared to leverage
    their features. Features tend to include logging, version control, roll back,
    both patching and system update automation, check summing, automatic configuration,
    and more.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的最佳实践，正如你可能预期的那样，是深入了解你所选发行版的包管理生态系统，以便你能够充分利用其特性。这些特性通常包括日志记录、版本控制、回滚、补丁和系统更新自动化、校验和、自动配置等。
- en: The more common and foundational a software component is, the more likely you
    should have it supplied by the vendor as part of your distribution. The more niche
    and close to the line of business, the more likely that it will be acceptable
    to install it manually or through a non-standard process as end user products
    are far less likely to be included in a vendor software repository and are much
    more likely to have a need to carefully manage versions and update schedules rather
    than primarily caring about stability and testing with respect to the rest of
    the system.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 软件组件越常见和基础，越有可能由供应商作为分发的一部分提供。越是小众且接近业务核心，越有可能通过手动安装或非标准过程来安装它，因为最终用户产品不太可能被包含在供应商的软件仓库中，更有可能需要仔细管理版本和更新计划，而不仅仅是关注与系统其他部分的稳定性和测试。
- en: It is not uncommon for software vendors making products that are not included
    in distribution repositories to make and maintain their own repositories allowing
    you to configure their repository and still manage all installation and patching
    tasks via the standard tools.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 软件供应商通常会为那些不包含在分发仓库中的产品创建和维护自己的仓库，允许你配置其仓库，仍然通过标准工具管理所有安装和补丁任务。
- en: Software deployments are made up of so many special cases. It is tempting to
    want to delivery standard, clear *always do this* style of guidance, but software
    just does not work that way. Learn the tools of your system, use them when you
    can, be prepared to do or learn something unique for every workload that you have
    to deploy. Some, like WordPress, may turn out to be so standard that you never
    need to do anything but use the distributions own packages. Others may be so unique
    that you simple deploy a basic operating install, download the vendor's installer
    and it installs every needed piece of software, and potentially even compiles
    it! It just all depends, and more than anything else it will depend on how the
    software vendor chooses to build and package the software and if your distribution
    decides to include the package in the operating system or will require it separately.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 软件部署包含了许多特殊情况。人们很容易想要提供标准、明确的*总是这样做*的指导，但软件的工作方式并非如此。学习你系统的工具，能用时使用它们，为每个需要部署的工作负载准备做一些独特的工作或学习某些东西。有些，比如WordPress，可能会变得非常标准，以至于你只需要使用发行版自带的软件包即可。其他软件可能会非常独特，你可能只需要部署一个基本的操作系统，下载供应商的安装程序，它会安装所需的所有软件，甚至可能会编译它！这一切都取决于情况，最重要的是，它取决于软件供应商如何构建和打包软件，以及你的发行版是否决定将该软件包包含在操作系统中，或是否需要单独安装。
- en: Now that we have a scope of the software installation processes and considerations,
    we can dive into the real heart of our concerns with patching and updates.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了软件安装过程和相关注意事项，我们可以深入探讨关于补丁和更新的核心问题。
- en: Patching theory and strategies
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 补丁理论和策略
- en: One might think that patching is pretty straightforward, and that there would
    be little to discuss. This is not the case. In fact, if you talk to several system
    administrators you are bound to get some pretty widely varying opinions. Some
    people patch daily, some weekly, some wait as long as they can, some do so only
    haphazardly, and some believe that you should never patch at all (hey, it if isn't
    broke, don't fix it!)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能认为打补丁是非常直接的事情，不会有太多可讨论的内容。但实际上并非如此。如果你与几个系统管理员交谈，你肯定会得到一些相当不同的看法。有些人每天打补丁，有些人每周打补丁，有些人会尽量等到最后才打补丁，还有些人则是随机进行补丁操作，甚至有些人认为根本不应该打补丁（嘿，如果它没坏，为什么要修呢！）
- en: We should first establish why we patch our software. Patching, as opposed to
    updating or upgrading, implies that we are applying minor fixes to software to
    fix a known problem or bug but not to implement new features or functionality.
    Adding new features is generally considered to be an update.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该首先明确为什么要给软件打补丁。打补丁不同于更新或升级，它意味着我们对软件进行小范围的修复，以解决已知问题或漏洞，而不是实施新功能或特性。添加新功能通常被视为一次更新。
- en: Most software vendors and operating system vendors honor this system and maintain
    patching systems that only address security or stability issues in their software
    between releases. In the Linux ecosystem this is primarily tied to an operating
    system release. So, for example, if you use Ubuntu 22.04 and you use its own patching
    mechanisms to patch the software that comes with the distribution, then you will
    safely get nothing but security and stability fixes for the existing software
    versions and not new versions, features, or functionality. The logic here is that
    upgrading to a new version may break the software, change usability, or cause
    other products that depend on that software to fail.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数软件供应商和操作系统供应商遵循这一系统，并维持仅在版本之间解决安全性或稳定性问题的补丁系统。在 Linux 生态系统中，这主要与操作系统的发布版本相关。例如，如果你使用的是
    Ubuntu 22.04，并且通过其自身的补丁机制来修补随发行版一起提供的软件，那么你将仅获得现有软件版本的安全性和稳定性修复，而不会得到新版本、特性或功能。其逻辑在于，升级到新版本可能会导致软件出现问题、改变可用性，或使依赖该软件的其他产品失败。
- en: Upgrading to new versions are assumed to only happen when a new version of the
    operating system itself comes out and then the operating system and all the packages
    included in it can be upgraded together at the same time. This allows the operating
    system vendor to, theoretically, test the software together as a singular package
    to give the customer (you) confidence that all of your software components will
    work together even after you move everything to new versions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 升级到新版本通常只在操作系统本身发布新版本时发生，这时操作系统和所有其中的包可以同时升级。这样，操作系统供应商理论上可以将软件作为一个整体进行测试，以便让客户（你）确信，即使在将所有软件升级到新版本之后，所有的软件组件仍然能够协同工作。
- en: So, we assume that if a patch has been made available to us, then this indicates
    that a vendor, quite likely in conjunction with our distribution vendor, has identified
    a problem that needs to be fixed, a fix has been created, it has been tested,
    and it is now being distributed. However, even with testing, many eyes watching
    for errors, and the intent to do nothing but fix known bugs, things can still
    go wrong. Both the patch itself can be bad and the process of patching can run
    into unexpected problems. This means that we have to remain cautious about patching
    no matter how good the intentions are of those providing the patches.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们假设如果补丁已经提供给我们，这意味着供应商（很可能与我们的分发供应商合作）已经发现了需要修复的问题，已经创建了修复方案，进行了测试，现在正在分发补丁。然而，即使经过测试，众多目光监督错误，并且目标只是修复已知漏洞，事情仍然可能出错。补丁本身可能有问题，打补丁的过程也可能遇到意外的麻烦。这意味着，无论提供补丁的人有多么良好的意图，我们都必须对打补丁保持谨慎。
- en: When patching, we are left with two opposing concerns. One is that if the system
    is currently working for us, why introduce the risk (and effort) of the patching
    process when we do not have to. On the other hand, why keep running a system where
    known bugs are left exposed once a patch has been made available to us? We have
    to look at the concerns and pick a reasonable course of action.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在打补丁时，我们面临着两个相互对立的担忧。一方面，如果系统当前对我们来说是正常工作的，为什么在没有必要的情况下还要引入补丁过程的风险（和工作量）呢？另一方面，既然补丁已经提供给我们，为什么还要继续运行一个存在已知漏洞的系统？我们必须权衡这些问题，选择一个合理的行动方案。
- en: Risk aversion is really not a key concern here, we are not looking at expense
    versus risk but rather two nearly equal courses of action (from an effort and
    cost perspective) with two very different outcomes. We need to pick the approach
    that reduces risk the most for our business and that is all. It is not how risk
    averse we are but what our risk profile is like that matters most. If our business
    is heavily susceptible to small downtime events, then patching may be deprioritized.
    If our company has highly sensitive data that is a likely target or we are very
    sensitive to public relations blunders in the event of a breach, then we might
    patch very aggressively. To make a sensible determination we must understand how
    each approach creates and mitigates different risks and how those risks affect
    our specific organization.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 风险规避并不是这里的关键问题，我们并不是在权衡成本与风险，而是在面对两种几乎相等的行动方案（从努力和成本的角度来看），但它们会带来截然不同的结果。我们需要选择一个能够最大限度降低业务风险的方案，仅此而已。最重要的不是我们有多么规避风险，而是我们的风险特征是什么。如果我们的业务非常容易受到小规模停机事件的影响，那么可能会优先考虑推迟修补。如果我们公司拥有高度敏感的数据，这些数据可能成为攻击目标，或者我们非常关注在发生数据泄露时的公关危机，那么我们可能会采取非常积极的修补策略。为了做出明智的决定，我们必须了解每种方案是如何产生和缓解不同风险的，以及这些风险如何影响我们特定的组织。
- en: The risk of delayed patching
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推迟修补的风险
- en: Simply pushing off patching does not eliminate certain types of risks. It may
    prove to have benefits but may also introduce even more risks depending on the
    situation. Under normal circumstances, new patches are made available with great
    frequency, potentially as often as multiple times per day, but at least multiple
    times per month.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 单纯推迟修补并不能消除某些类型的风险。虽然这可能带来一定的好处，但也可能根据情况引入更多的风险。通常情况下，新的修补程序会频繁发布，可能一天发布多次，至少也会每月发布几次。
- en: If we patch often, such as once a week, then theoretically we will normally
    have to deal with extremely few patches at any given time and any break or incompatibility
    will be relatively easy to identify and to roll back as there are so few patches
    to work with.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们经常修补，比如每周一次，那么理论上我们通常只需处理极少的修补程序，在任何给定时刻，任何故障或不兼容问题都相对容易识别和回滚，因为需要处理的修补程序很少。
- en: If we save up patches over a period of time and only patch, for example, once
    a year then we have a few problems. First, the patching process may take quite
    some time as many patches may be needed. Second, if there is a break it may be
    very difficult to identify the offending patch as it could be lost in a sea of
    patches that all have to be deployed. And third, the greater volume of changes
    made at once, as well as the increased *drift* from any tested scenario (few,
    if any, vendors will test a system that is specifically as out of date as yours
    against a large volume of sudden changes) means that the chances of there being
    a break caused by the patching process is greater.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在一段时间内积累修补程序，并且只在比如说一年一次时修补，那么我们会面临一些问题。首先，修补过程可能需要相当长的时间，因为可能需要许多修补程序。其次，如果出现故障，可能很难找出导致问题的修补程序，因为它可能会被一大堆需要部署的修补程序所掩盖。第三，一次性进行的大量更改以及与任何经过测试的场景的*偏离*（几乎没有供应商会针对像你这样具体过时的系统进行测试，并与大量突如其来的更改进行对比）意味着修补过程引发故障的几率更大。
- en: Delaying patches, therefore, becomes a self-fulfilling prophecy in many cases
    where neigh sayers who avoid patching because *patches break things* will often
    see this come to pass because they create a situation where it is more likely
    to occur.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，推迟修补在许多情况下会成为一种自我实现的预言，那些因为*修补会破坏东西*而避免修补的人，往往会看到这一点发生，因为他们创造了一个更容易发生的局面。
- en: There is no one size fits all approach. Every organization has to tailor the
    patching process to meet their needs. Some organizations will avoid patching altogether
    by making stateless systems and simply deploying new systems that were built already
    patched and destroying older, unpatched instances neatly sidestepping the problem
    altogether. But not every workload can be handled that way and not every organization
    is able to make that kind of infrastructure leap to enable that process.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一刀切的方案。每个组织都必须根据自身需求定制修补流程。有些组织会通过构建无状态系统并简单地部署已经修补过的新系统，销毁旧的未修补实例，从而完全避免修补问题。但并不是每个工作负载都可以采用这种方式，也不是每个组织都有能力进行这种基础设施的跃迁，以支持这种流程。
- en: Some organizations run continuous patch testing processes to see how patches
    will be expected to behave in their environment. Some just avoid patching completely
    and hope for the best. Some patch blindly. We will discuss all of these options.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一些组织会进行持续的补丁测试，查看补丁在其环境中的表现。一些组织完全避免打补丁，抱着侥幸心理，期望最好。一些则盲目地打补丁。我们将讨论这些不同的选择。
- en: Avoiding patches because of Windows
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 因为Windows而避免打补丁
- en: A culture of *patching avoidance* has sprung up in recent years within the ranks
    of system administrators. Given how critical patching is in general and how central
    it is to our careers this seems counter-intuitive. No one could be as strong of
    a cheerleader for rapid, regular patching as the system administrators.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，系统管理员中出现了一种*避免打补丁*的文化。考虑到打补丁在一般情况下的重要性，以及它对我们职业生涯的核心作用，这似乎有些反直觉。没有人比系统管理员更支持快速、定期打补丁。
- en: In the Windows world, patching is very unlike what it is like in the Linux,
    or any other world. It is often delayed, secretive, slow, unreliable, and worst
    of all buggy and error prone. Patching in Windows was always problematic but during
    the 2010s became so bad that it is no longer deterministic, can take much longer
    than simply deploying new systems, and fails with great regularity.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows世界中，打补丁的过程与Linux或其他操作系统截然不同。它通常是延迟的、保密的、缓慢的、不可靠的，最糟糕的是，常常存在漏洞和错误。Windows的打补丁问题一直存在，但在2010年代变得如此严重，以至于它不再是确定性的，远远比直接部署新系统要花更长时间，而且经常失败。
- en: And failures with Windows patching can mean almost anything from the patch simply
    failing to install and needing to devote resources to getting it to work, to causing
    software to fail and no longer function. Some patches can take many hours to run
    only to fail and then take hours to roll back!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Windows打补丁失败可能意味着几乎任何情况，从补丁安装失败，需要投入资源使其正常运行，到导致软件故障，无法再继续使用。一些补丁可能需要耗费数小时来运行，但最终失败，然后又需要花费数小时回滚！
- en: Because of this it has become common and almost expected that system administrators
    in the Windows realm will range from gun-shy about patches, to practicing total
    avoidance. This has sprawled to not just include patches but full system version
    updates as well. So now finding Windows systems that are years or even a decade
    out of date is becoming commonplace creating more and more security vulnerabilities
    throughout the ecosystem.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正因为如此，Windows领域的系统管理员通常对补丁保持谨慎态度，甚至完全避免打补丁，几乎成为了一种普遍现象。这种现象不仅限于补丁，还扩展到系统版本更新。因此，现在发现许多Windows系统已经多年甚至十年未更新，已成为常态，导致整个生态系统中越来越多的安全漏洞。
- en: Microsoft then has responded, not by fixing the patching problems, but by attempting
    to force patching without permission, and obscuring the patching process creating
    even more reliability problems and in many cases breaking patch management systems
    so dramatically that even administrators who desire to stay fully updated are
    often unsure how to do so or are simply unable to do so.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 微软的回应不是修复打补丁的问题，而是试图强行进行未经授权的打补丁，并且通过模糊补丁过程，制造了更多的可靠性问题，甚至在许多情况下，打补丁管理系统发生了严重故障，以至于即使是那些希望保持完全更新的管理员，常常也不确定如何操作，或者根本无法做到这一点。
- en: These problems are unique to Microsoft today and are mostly unique to Microsoft
    in the modern era. We must not allow an emotional reaction to a uniquely bad situation
    influence our practices in the Linux or other realms that are not impacted nor
    influenced by Microsoft. Outside of Microsoft's isolated piece of the industry
    no other ecosystem has experienced these types of issues. Not in Ubuntu, Red Hat,
    Fedora, SUSE, IBM AIX, Oracle Solaris, FreeBSD, NetBSD, DragonFly BSD, macOS,
    Apple iOS, Android, Chromebooks, and on and on. Patching always carries risk and
    we should be aware of this, but Microsoft's problems are unique and have nothing
    to do with our practices in the rest of the industry.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题今天是微软特有的，并且大多数是现代时代微软所独有的。我们不能让对这种独特糟糕情况的情绪反应影响我们在Linux或其他领域的实践，这些领域并未受到微软的影响。在微软孤立的行业领域之外，没有其他生态系统经历过类似的问题。无论是在Ubuntu、Red
    Hat、Fedora、SUSE、IBM AIX、Oracle Solaris、FreeBSD、NetBSD、DragonFly BSD、macOS、Apple
    iOS、Android、Chromebook等，都没有出现过这种情况。打补丁总是存在风险，我们应该意识到这一点，但微软的问题是独一无二的，与我们在其他行业的实践无关。
- en: Patching can be automated or manual. Both approaches are perfectly fine. Automation
    requires less effort and can protect against patching being forgotten or deprioritized.
    In a large organization formal patching procedures that require manual intervention
    may be simple to ensure consistency, but in small organizations with just a few
    servers it can often be easy to overlook patching for months. When looking at
    manual versus automated patching just consider the potential reliability of the
    process and the cost (generally in time) of the human labor involved.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 补丁可以是自动化的，也可以是手动的。两种方法都完全可以接受。自动化需要更少的努力，并且可以防止忘记补丁或降低其优先级。在大组织中，要求手动干预的正式补丁程序可能很简单，以确保一致性，但在只有几个服务器的小型组织中，补丁可能很容易被忽视，甚至几个月都没有进行。当考虑手动与自动化补丁时，只需要考虑流程的潜在可靠性和涉及的人力成本（通常是时间）。
- en: The benefit to manual patching is that you have an opportunity for a human to
    *inspect* each package as it is patched and to test the system in real time as
    it occurs. If something was to go wrong, every detail of the patching process
    is fresh in their memory as they just performed it and they know exactly what
    to test and what to roll back or address if something fails.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 手动补丁的好处在于，你可以让人类在每个补丁安装时*检查*每个包，并在补丁进行时实时测试系统。如果出现问题，补丁过程中的每个细节都会历历在目，因为他们刚刚执行过，而且他们知道在出现问题时应该测试什么，回滚或处理什么。
- en: Automation benefits from happening automatically, potentially at very predictable
    times, and being able to happen even if no humans are present to do the work.
    Scheduling automated patching for evenings, overnight, weekends, or holidays can
    minimize impact to humans while speeding the patching process. Automated patching
    is unlikely to be missed and it is easy to send alerts when patching happens or
    when there are problems caused by patching.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化的好处在于它可以自动发生，可能在非常可预测的时间内，并且即使没有人类在场工作，它也能发生。将自动化补丁安排在晚上、过夜、周末或节假日进行，可以最大限度地减少对人类的影响，同时加速补丁过程。自动化补丁不太可能被遗漏，而且在补丁发生时或出现问题时，容易发送警报。
- en: In most cases automation is going to be preferred to manual patching simply
    because it is less costly and nearly all manual benefits can be automated in some
    form as well, such as by having a human on standby to receive alerts in case of
    problems.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，自动化补丁比手动补丁更受欢迎，因为它成本更低，几乎所有手动的好处都可以以某种形式自动化，比如通过让人类待命，以便在出现问题时接收警报。
- en: Testing patches is rarely feasible
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试补丁很少是可行的
- en: Everyone talks about how important it is to test patches. System administrators
    often demand it and attempt to refuse to apply patches until testing can be done
    (a convenient way to avoid having to patch systems at all.) And if patches ever
    go wrong, management will almost always demand to know why patch testing was not
    done beforehand.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人都在谈论测试补丁的重要性。系统管理员常常要求这样做，并且尝试拒绝应用补丁，直到能够完成测试（这是避免完全补丁系统的一个方便方式）。如果补丁出现问题，管理层几乎总是要求了解为什么之前没有进行补丁测试。
- en: 'Here is the harsh reality: there is no practical or realistic means of testing
    patches on any scale. There, I said it. Say it out loud, go tell your bosses.
    The cost, both in time and money, to test patches is much larger than anyone believes.
    In order to thoroughly test patches, we need a replicated environment that gives
    us a mirror of our production environment so that we can test the patches that
    we want to deploy against software, hardware, and configurations. Attempting to
    shortcut this process does not work as it is the interplay of all these parts
    that make testing important. If you change anything, you may totally nullify the
    benefits (or worse, create a false sense of security) to a very expensive process.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个残酷的现实：没有任何实际可行的手段可以在任何规模上测试补丁。说到这，我说出来了。大声说出来，去告诉你的老板们。测试补丁的成本，无论是时间还是金钱，都远远大于任何人所认为的。为了彻底测试补丁，我们需要一个复制的环境，这样才能给我们提供一个与生产环境相同的镜像，以便我们能够对要部署的补丁进行测试，涉及的软件、硬件和配置。如果试图简化这一过程，是行不通的，因为正是所有这些部分的相互作用使得测试变得重要。如果你改变了任何东西，可能会完全抵消这一非常昂贵过程的好处（或者更糟糕的是，产生一种虚假的安全感）。
- en: In a real-world environment, every system is effectively unique. There are exceptions,
    but generally, this is true. There are so many variables that are possible, including
    hardware manufacturing dates, varying parts, firmware versions, and on up the
    infrastructure stack (that is, code and components that sit closer to the final
    application at the top of the stack). Computer systems are so complex today that
    there are millions of variables that could result in a combination that causes
    a bug in a patch to be triggered.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实的环境中，每个系统基本上都是独一无二的。虽然有例外，但通常情况下，这是真的。有许多可能的变量，包括硬件制造日期、不同的零部件、固件版本，以及更高层次的基础设施堆栈（即，接近最终应用的代码和组件）。今天的计算机系统如此复杂，可能有数百万种变量，导致补丁中的某个
    bug 被触发。
- en: This is one of the reasons why virtualization is so important, it creates a
    middle layer of standardization that allows some of the more complex parts of
    the system to be standardized. So at least one portion, a very complex portion
    involving many drivers, can be reduced in complexity.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是虚拟化如此重要的原因之一，它创造了一个标准化的中间层，使得系统中一些更复杂的部分能够标准化。这样，至少有一部分，涉及许多驱动程序的复杂部分，可以简化复杂性。
- en: In very rare organizations real patch testing is done. Doing so is costly. Generally,
    this involves carefully replicating the entire production environment right down
    to matching hardware versions, firmware revisions, patch history, and so forth.
    Every possible aspect should be identical. Patches must then be run through a
    battery of tests quite quickly in order to test any given patch in the company's
    range of scenarios.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有组织会进行真正的补丁测试。这样做的成本很高。通常，这涉及到仔细复制整个生产环境，包括匹配硬件版本、固件修订、补丁历史等。每个可能的方面都应该是相同的。补丁必须通过一系列快速的测试，以测试公司范围内的各种场景中的给定补丁。
- en: In practical terms this means duplicating all hardware and software and having
    a team dedicated to patch testing. Very few companies can afford that and even
    fewer still could justify the small amount of potential protection that it might
    provide.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，这意味着要复制所有硬件和软件，并且需要有一个专门的团队进行补丁测试。很少有公司能承担这样的费用，能证明这种做法带来的一点点潜在保护的公司就更少了。
- en: Timeliness of patching
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 补丁的及时性
- en: Patching is an activity that generally has to happen extremely quickly, and
    as the industry matures the importance of rapid patching continues to increase.
    Those of us trained in the 1990s and earlier will tend to have memories of a time
    when patching a system had almost no time sensitivity because most computers were
    offline, and patches were almost exclusively for stability issues that if you
    had not experienced already that you were unlikely to experience. So, waiting
    months or years to patch a system, if you ever did, tended to not be a very big
    deal.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 补丁工作是一项通常需要非常迅速完成的活动，随着行业的成熟，快速补丁的重要性持续增加。我们这些在1990年代及之前接受过训练的人，通常记得那个时候补丁几乎没有时间敏感性，因为大多数计算机都是离线的，补丁几乎完全是为了稳定性问题，如果你没有遇到过问题，那就不太可能遇到。所以，等待几个月或几年再给系统打补丁（如果你做了的话），通常并不是一件大事。
- en: Oh how times have changed. Software is so much bigger and more dynamic today,
    there are so many more interconnected layers, and all but the rarest of computer
    systems are now on the Internet and potentially exposed to active security threats
    and a dynamically changing environment all of the time. Everything, as pertains
    to patching, has been flipped on its ear over the past two decades, although most
    of the significant changes had happened by around 2001.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，时代变化多么迅速。今天，软件变得更加庞大和动态，层级之间的相互关联也更多，而且除了最罕见的计算机系统，几乎所有系统都连接到互联网，并且始终可能面临活跃的安全威胁和动态变化的环境。关于补丁的所有事宜，在过去二十年中发生了翻天覆地的变化，尽管大部分重大变化在2001年左右就已经发生了。
- en: Today patching tends to focus heavily on shoring up security gaps that have
    been discovered recently and every step of the process is one of rushing the patch
    to market before the bad guys are able to discover the vulnerability and exploit
    it. Bad actors know that patches will come for any vulnerability that they find
    quite quickly and so exploitation is all about speed. In some cases, it is the
    release of a patch itself that alerts the greater community to the existence of
    a vulnerability and so the action of releasing a patch triggers a sudden need
    for everyone to apply said patch in a way that was not necessary just hours before.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，修补工作往往主要集中在弥补最近发现的安全漏洞，每一步都在急忙将补丁推向市场，以防坏人发现漏洞并加以利用。恶意行为者知道，对于任何漏洞，补丁都会很快发布，因此漏洞利用完全是关于速度。在某些情况下，正是补丁的发布本身提醒了更广泛的社区漏洞的存在，因此发布补丁的行为触发了每个人都必须应用该补丁的紧迫需求，而这在几个小时前是完全不必要的。
- en: Because of this, patching quickly is incredibly important. Attacks based on
    a patch are most likely to either already be occurring and will increase in a
    last-ditch effort to leverage a soon-to-be-dwindling vulnerability or will soon
    start as a previously unknown vulnerability becomes public knowledge. For many,
    this means that we want to consider patching in terms of hours rather than days
    or longer. We still have to consider potential stability risks or impacts that
    might occur during production hours, so patching immediately is rarely an option,
    but it is certainly possible when it makes sense.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，快速修补漏洞极为重要。基于漏洞修补的攻击很可能已经在发生，并且会在即将消失的漏洞最后一刻加剧，或者会在一个之前未知的漏洞成为公开知识后迅速开始。对许多人来说，这意味着我们希望将修补工作以小时而非天数来考虑。我们仍然需要考虑在生产时间内可能发生的潜在稳定性风险或影响，因此立即修补通常不是一种选择，但在合理的情况下，当然是可能的。
- en: In Linux, because patching is generally quick and easy and almost always reliable,
    it is reasonable to consider patching throughout the production day in some cases,
    and daily patching in most other cases. Potentially smart approaches might include
    using a built-in randomizer to patch, somewhat randomly (for reasons of system
    load reduction) every four to eight hours, or having a scheduled patch time every
    day at ten in the evening or other appropriate time.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux中，由于修补一般既快速又简便，且几乎总是可靠的，因此在某些情况下，可以考虑在生产日内进行修补，在大多数其他情况下进行每日修补。可能的智能方法包括使用内置的随机生成器，每四到八小时随机修补（为了减轻系统负载），或者每天晚上十点或其他合适的时间进行计划性修补。
- en: In extreme environments, patching on a weekly schedule is possible and this
    was popular in the enterprise just one to two decades ago. Today, waiting up to
    six days to patch a known vulnerability borders on the reckless and should be
    done only with caution. Six days is a very long time in the world of exposed system
    vulnerabilities.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在极端环境下，按周进行修补是可行的，这在一到二十年前的企业中很流行。如今，等待长达六天来修补已知漏洞几乎是鲁莽的，应该小心进行。六天在暴露系统漏洞的世界里是非常长的一段时间。
- en: The choice of time frames is generally based on workload patterns. Some workloads,
    like email or an inventory control system, might have little susceptibility to
    momentary disruption and can be rolled back or failed over quickly. So, patching
    these in the middle of the day might make sense. Most companies have their workloads
    have a cyclical use pattern throughout the day and can predict that an application
    becomes very lightly used from one to two in the morning, or perhaps that by seven
    in the evening every single user has signed out and even a ten-hour outage would
    be noticed by no one.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 时间框架的选择通常基于工作负载模式。一些工作负载，如电子邮件或库存控制系统，可能对短暂的中断不太敏感，可以快速回滚或切换。因此，在一天中午间进行修补可能是合理的。大多数公司都有一天内周期性的工作负载模式，可以预测某个应用程序在凌晨一到两点使用非常少，或者也许到晚上七点每个用户都已注销，甚至十小时的停机也不会被人察觉。
- en: Whether or not intra-day patterns exist or not we almost always see inter-day
    patterns on a weekly basis. A workload might be light on the weekends or heavy
    during the week or vice versa. Once in a while, especially with financial applications,
    the pattern is more monthly based with the beginning of the month probably seeing
    a heavy load and the middle of the month being light.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是否存在日内模式，我们几乎总是会看到按周计算的日间模式。某些工作负载可能在周末较轻，而在工作日较重，反之亦然。有时，尤其是在金融应用程序中，模式更加偏向月度，通常月初的负载较重，而月中较轻。
- en: Any given workload will typically need to be assessed to understand when patching
    is reasonable and practical. Sometimes we have to be creative with our timing
    to be able to get patching in when the workloads allow. If workloads cannot be
    interrupted for patching, then they cannot experience other, less planned, downtime
    events either and we should have a continuity plan that allows us to patch, repair,
    or failover in case anything happens. In most cases we can, when necessary, do
    zero impact patching through these methods.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 每个工作负载通常需要评估，以了解何时进行补丁是合理且可行的。有时我们必须在时间安排上富有创意，以便在工作负载允许时进行补丁。如果工作负载无法中断进行补丁，那么它们也不能经历其他更不计划的停机事件，我们应该有一个连续性计划，允许我们在发生任何问题时进行补丁、修复或故障切换。在大多数情况下，我们可以通过这些方法进行零影响的补丁操作。
- en: The old idea that patching can be saved as a special monthly activity or done
    only when a specific need is identified for the organization is no longer realistic.
    Waiting that long leaves systems dangerously exposed and any workload that claims
    to have only one time a month when it can be patched should be questioned as to
    how any workload can be both important, and unable to be patched. Conceptually
    the two things cannot go together. The more important a workload is, the more
    important that it be patched in a timely fashion.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 过去那种认为补丁可以作为一个特殊的每月活动来保存，或者只有在识别到组织的特定需求时才进行补丁的想法，已经不再现实。等待这么长时间会使系统处于危险暴露之中，任何声称每月只有一次补丁机会的工作负载都应该质疑，怎么会有既重要又无法进行补丁的工作负载。从概念上讲，这两者是无法并存的。工作负载越重要，及时进行补丁就越重要。
- en: A common excuse for slow patching processes is that testing is required before
    rolling out a patch. On the surface this makes sense and sounds reasonable. You
    could probably sell this idea to non-technical management. There is nothing wrong
    with wanting to test a patch. But we have to consider that we are already, presumably,
    paying (or getting for free) a distribution vendor to test patches before we receive
    them. Those enterprise operating system vendors (Canonical, IBM, SUSE, and others.)
    have far more skills, experience, and resources to test patches than any but the
    largest organizations. If our testing does not add something significant to the
    extensive testing that we are already relying on them for, then our own internal
    testing process should be avoided, in order to avoid wasting resources and putting
    the organization at unnecessary risk by not getting potentially critical patches
    deployed promptly.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的缓慢补丁流程的借口是，补丁发布前需要进行测试。表面上看，这有道理，听起来也合情合理。你可能能将这个想法卖给非技术管理层。想要测试补丁本身没有问题。但我们必须考虑到，我们已经在支付（或免费获得）分发供应商来测试补丁，等我们收到补丁时。那些企业级操作系统供应商（如Canonical、IBM、SUSE等）拥有比任何但最大规模的组织更多的技能、经验和资源来测试补丁。如果我们的测试没有为我们已经依赖的广泛测试增加什么重要的内容，那么我们自己的内部测试过程应该避免，以免浪费资源并让组织面临不必要的风险，延误可能至关重要的补丁部署。
- en: Rapid, light testing can be reasonable if it is kept light enough and never
    used as an excuse to avoid timely patching. A common approach to useful patch
    testing is to have just a few systems that represent typical workload environments
    in your organization that run demonstration workloads where you can test each
    patch before it is rolled out to allow you to observe a successful install and
    test the patches at the highest level. This could be as little as a single virtual
    machine in a smaller organization. Consider if any testing at all is valuable,
    and if it is, keep the testing as light as you can to ensure that production patching
    happens as quickly as it can within the confines of the needs of your workloads.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果保持足够轻量并且绝不作为推迟及时补丁更新的借口，快速而轻量的测试是合理的。一种常见的有用补丁测试方法是，选取几个代表你组织中典型工作负载环境的系统，运行演示工作负载，在补丁发布之前先对每个补丁进行测试，以便观察安装成功并在最高级别进行测试。这可以在小型组织中仅仅是一台虚拟机。考虑是否有任何测试是有价值的，如果有的话，尽量保持测试轻量化，以确保生产环境中的补丁更新尽可能迅速地完成，前提是满足工作负载的需求。
- en: Standard patching strategies will also generally suggest that you start either
    with highly vulnerable workloads or with low priority workloads to focus on shoring
    up exposures or using low priority workloads as tests of Guinea pigs for more
    critical workloads. If your environment has one hundred virtual machines to patch,
    you can probably arrange a schedule that allows you to patch systems that are
    not critical first and slowly build up confidence in the patching process as you
    approach the patching of more critical or fragile workloads.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的补丁策略通常会建议你先从高风险的工作负载开始，或者从低优先级的工作负载开始，专注于修复漏洞，或者使用低优先级工作负载作为测试对象，为更关键的工作负载做准备。如果你的环境中有一百台虚拟机需要补丁，你大概可以安排一个时间表，首先补丁那些不那么关键的系统，随着对补丁流程信心的逐步建立，慢慢接触更多关键或脆弱的工作负载。
- en: Consider patching to be unquestionably one of the most important, and truly
    simple, tasks that you will be doing as a system administrator. Do not allow emotions
    or irrational advice from businesses or system administrators that do not understand
    what patching is (or influence from Windows admins) to lead you astray. Patching
    is hyper-critical and any organizational management that is not supportive of
    patching processes does not understand the risk and reward valuation of the process
    and we need to be prepared to explain it to them.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 把补丁管理看作是系统管理员最重要且简单的任务之一。不要让情绪或那些不了解补丁管理（或受Windows管理员影响）的企业或系统管理员的非理性建议将你引偏。补丁管理是至关重要的，任何不支持补丁管理流程的组织管理层都不了解该过程的风险与回报，我们需要准备好向他们解释这一点。
- en: Find a patching process that meets the needs of your organization. You do not
    need to patch on a rigid schedule, you do not need to patch the way that other
    organizations do. Find the testing that is adequate for you and the manual or
    automated patching process that keeps your systems updated without overly impacting
    the organization.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 找到一个满足你所在组织需求的补丁管理流程。你不需要按照固定的时间表进行补丁更新，也不需要按照其他组织的方式进行补丁管理。找到适合你的测试方法以及手动或自动补丁流程，以确保你的系统在不对组织造成过度影响的情况下保持更新。
- en: We should have a pretty solid handle on patching concepts at this point and
    you probably even feel a certain sense of urgency to go examine your current servers
    to see how recently they have been patched. This is understandable and if you
    want to look at them now before continuing on, I am happy to wait. Better safe
    than sorry. When you return, we will talk about software compilation and relate
    that process to patching.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，我们应该对补丁的概念有一个相当扎实的理解，你可能甚至已经感到一种紧迫感，想去检查一下你当前的服务器，看看它们最近是否有更新。这是可以理解的，如果你现在想查看一下它们，在继续之前，我愿意等你。安全总比后悔好。等你回来后，我们将讨论软件编译，并将该过程与补丁管理联系起来。
- en: Compilations for the administrator
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理员的编译工作
- en: It was not all that long ago when major system administrator guidelines included
    a requirement that any mid-level or senior administrator had to be well acquainted
    with the details of standard software compilation processes. Of course, all knowledge
    is good, and we would never say that it should not be learned at all. However,
    even at the time, this seemed like an odd amount of *under-the-hood* development
    knowledge and knowledge about the packaging of individual software solutions expected
    to be known by a person in a non-development role.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: It would not be unlike if when ordering a new car from your favorite car company
    that it was expected to be delivered as parts and that every potential new car
    owner would be expected to assemble the car before driving it. It is important
    to note that this process was only ever possible in the open-source world and
    that the majority of software outside of the Linux space and even a significant
    portion within it cannot be compiled by the system administrator at all. So, the
    entire concept of this requirement was for an almost niche scenario and was not
    broadly applicable to system administration in the general sense, which alone
    should have been a serious red flag to organizations promoting this as an education
    and certification standard.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: As a system administrator, the idea that we would need to take source code from
    developers and custom compile it, using a compiler that we provide, in our own
    environment feels almost absurd. In the Windows world this would be all but impossible,
    who would have access to an appropriate compiler or any of the necessary tools?
    In Linux and BSD systems it is often plausible because a compiler or multiple
    compilers may be included with the base operating system.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: There was a time when compilation on target systems had some value. Often, generically
    compiled software was inefficient by necessity in order to support nearly any
    potential hardware, and with on-target compiling, we could leverage every last
    bit of performance from our very specific hardware combination. However, this
    was also in an era of long compile times due to limited CPU power and would result
    in systems taking potentially days to install rather than minutes. Software deployment
    was a lengthy, complex, and error-prone task. In the modern world, it would be
    almost unthinkable to waste the time and resources necessary to compile most software.
    When we can deploy entire fleets of servers in just a few minutes while using
    extremely little system resources, tying up many CPU cycles and spending hours
    to do the same task does not make sense from a resource perspective alone.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: However, compilation carries far more risks than just wasting time and system
    resources. It also means that we, potentially, get slightly different resulting
    software on different systems that we run, between times that we deploy, from
    systems that we have tested, or, more importantly, from systems that the vendor
    has tested. As discussed earlier, with the challenges that we face with testing
    patching scenarios, compilation makes testing vastly harder. Likewise, patching
    becomes much harder.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Some software requires compilation today for deployment, generally software
    that leverages specific hardware drivers. This kind of software tends to be desktop
    software and not something that we would see on a server. That does not mean that
    we will never encounter it, but it should be quite uncommon. It is not technically
    wrong to self-compile software, but you should only be doing so if it is absolutely
    required and serves a very necessary purpose. It should not be done casually and
    there is no reason for a system administrator today to have any knowledge of the
    compilation process.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Several additional new factors have arisen that make compilation as a standard
    process less possible. Twenty years ago, we had one major compiler in the Linux
    ecosystem, and it was assumed that all software being deployed as code would be
    written in C and targeted (and tested) by the development team against that one
    compiler. There was a very standard toolchain that convention said would be what
    was used, and it was almost always correct. Today, not only are there multiple
    standard compilers now being used with different projects using different ones
    for testing, but also several common compiled languages that are now in common
    use with their own compilers and tool chains.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Compilation was never a single, standard process as was implied. However, the
    convention meant that it was very nearly so. Today. it is a scattered process
    for all of the reasons mentioned earlier. In addition to this, many projects that
    do require compilations by end users now build in a compilation process so that
    it acts much like a standalone deployer rather than requiring the system administrator
    to have any knowledge of the compilation or even be aware that the software being
    deployed is compiling! The world has changed in nearly every way.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: In the real world I have worked on tens of thousands of servers across thousands
    of clients and have not seen the assumed *standard* compilation process used in
    any organization for more than seventeen years and in the years before that it
    was still so rare as to be able to be generally ignored and always used under
    questionable circumstances when a system administrator was acting as if the business
    was a personal hobby rather than a serious business.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: The compilation era
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the early days of Linux, being able to compile your own software commonly
    was a major feature compared to closed source systems, such as Windows and Netware,
    where compilers were not free, and the operating system code was not made available.
    It meant being able to move between different architectures without too much effort
    and at the time system resources were at a premium so the small performance advantage
    possible from unique compilation sometimes meant a real difference in software
    performance. System administrators used to be passionate about compiler flags
    and versions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: This trend was so dramatic that even entire operating systems were released
    around the concept. Most notably was Gentoo Linux where the entire operating system
    was custom compiled every time that it was deployed. This often led to people
    discussing how many days it would take to install a full operating system. The
    investment in initial installation was significant.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: In the 1990s, it was not uncommon for operating system installations to take
    many days. We rarely virtualized and so installations were often from physical
    media onto unique hardware which was time consuming even when things went well
    and there were often installation hiccups that would cause you to have to attempt
    an install multiple times. In that environment, also taking the time to compile
    software, or even the entire operating system, was not as crazy as it is today.
    But rest assured, it was not completely sane, either.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: The same era that saw compilation make sense because of resource constraints
    coincided with the pre-internet office world and the nascent Internet world where
    environmental threats looking to exploit unpatched computers were rare. Of course,
    computer viruses existed and were well known, but the ability to avoid them by
    not sharing physical media between systems allowed for high confidence in avoiding
    infection when well managed. So, the difficulties of patching custom compiled
    software did not present a problem most of the time. This was the era of *set
    and forget it* software. Compiled software was more likely to be forgotten than
    to be maintained. The effort of installation made the potential effort of patching
    monumental and very risky. When you custom compile code yourself there is a lot
    that can go wrong that could result in you being left without a working system.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: CPU and RAM resources used to be so tight on the majority of systems that, in
    most cases, having to wait days longer to be able to put a system into production
    was considered worth it, even if only to gain one percent in additional performance.
    What was often ignored was the fact that if it took all of that time and all of
    those resources to compile the initial installation, that those resources or more
    might be needed over and over again in the future to compile future patches or
    updates. This carried a truly enormous risk and would commonly result in a complete
    lack of patching as, once deployed, there was little means to bring the system
    down for hours or days to attempt a compilation step in the hopes of being able
    to update the system.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，CPU和RAM资源在大多数系统上都非常紧张，通常情况下，能够多等几天才能将系统投入生产是值得的，即使只是为了获得额外的百分之一的性能提升。往往被忽视的是，若编译初始安装需要消耗那么多时间和资源，那么未来编译补丁或更新时，可能需要相同甚至更多的资源。这带来了巨大的风险，通常会导致完全没有补丁更新，因为一旦系统部署完成，几乎没有办法停机几个小时或几天进行编译步骤，希望能更新系统。
- en: Like so much in IT, there is a strong desire to build a house of cards and hope
    that, when it falls down, it is long enough in the future to qualify as someone
    else's problem. And sadly, that became a viable strategy as companies rarely associate
    failures with those that caused them and often blame, at random, whoever is at
    hand. Because of this, creating risky situations is often beneficial because any
    benefits will be attributed to the person who implemented it, and any disasters
    it causes will be attributed randomly at a later date.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息技术领域，常常有一种强烈的愿望，想要建立一座纸牌屋，并希望当它倒塌时，足够远的时间可以让它变成别人的问题。遗憾的是，这种做法逐渐成为一种可行的策略，因为公司很少将失败与造成失败的人联系起来，而是常常随机地责怪身边的任何人。正因为如此，创造风险情况通常是有利的，因为任何收益都会归功于实施者，而任何灾难则会在稍后随机归咎于某人。
- en: A system administrator today would still do well to learn traditional software
    compilation, if only to understand historical perspective and be prepared for
    any unusual situation that might arise. But today (and for many years previously)
    compiling our own software as a standard process should be avoided as a rule of
    thumb, it is simply not a good use of resources and introduces too much risk for
    no real benefit.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现代的系统管理员仍然应该学习传统的软件编译方法，哪怕只是为了理解历史背景，并为可能出现的任何异常情况做好准备。但今天（以及多年来），将编译自己的软件作为标准流程应当避免，作为经验法则，这根本不是一种资源的好利用方式，而且会引入太多风险而没有实际的好处。
- en: When compilation is done today you cannot expect to be able to follow a generic
    process blindly as was often assumed (incorrectly) in the past. That convention,
    that protected so many administrators who just got lucky, is not a convention
    any longer. To compile software today we need instructions and guidance from the
    developers to have any hope of realistically knowing what tools, configurations,
    libraries, languages, and external packages may be required and how to make compilation
    work. There are so many potential moving parts and all of the knowledge here is
    developer knowledge, not IT knowledge at all, let alone system admin knowledge.
    System administrators have so much information that they need to know and deeply
    understand, it is one of the largest and most challenging technical arenas. The
    idea that system administrators should additionally learn deeply the knowledge
    and skills of a totally different field never made any sense. Why would developers
    even exist if all system administrators could perform these jobs in addition to
    their own duties? Software engineering is a huge field that requires immense knowledge
    to do well; it is both utterly absurd and insulting to developers to imply that
    a different discipline can perform their role casually without needing the same
    years of training and full-time dedication.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Compilation by engineering department
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a middle ground that we should mention that can make sense. That is
    having an engineering group that takes software in code form from developers (internal
    bespoke teams or open-source projects generally) and performs internal compilation
    either as an additional security step or for an extreme level of performance tuning
    or simply because the software in question requires special compilation such as
    when tied to kernel versions or drivers.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: By having a central group that is doing internal compilation and packaging source
    code into resultant binaries for the administration team they allow system administration
    processes to remain focused on standard, repeatable, and fast to deploy binary
    mechanisms while the organization gets the advantages of custom compilation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: This approach is typically reserved for only the largest of organizations capable
    of maintaining rapid software packaging workflows so that patching, testing, and
    customization happen almost as fast as it would, should it be done by the software
    vendors. At large scale it can be beneficial.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: This process works because it does not put software compilation and packaging
    into the hands of administrators where it is awkward and highly problematic.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Best practice is to compile software only when it is a requirement and not to
    do so otherwise, unless you have a department that can handle packaging compiled
    software rapidly and reliably fast enough to account for proper patch management
    needs and the overhead of doing so is justified by a savings at scale.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Compilation is useful knowledge, but just because you *can* compile software
    does not mean that you *should*. Keep this skill in your back pocket for when
    you really need it or are directed to do so by your software vendor. Next, we
    will talk about how to deploy our Linux distribution itself and, even more importantly,
    how to *redeploy* a distribution after a disaster has struck.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Linux deployment and redeployment
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today we have many potential methods for deploying, and just as importantly,
    redeploying, our servers (or our workstations, for that matter.) In my opinion,
    this topic was relatively unimportant in the past because most companies depended
    on *slow deployment* methods and their disaster recovery models depended on restoring,
    rather than redeploying, their systems. But there are so many more modern disaster
    recovery methods today that depend on the ability to rapidly deploy servers that
    we have to look at this topic with a new eye.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: With modern deployment technologies and techniques, it is not uncommon to be
    able to deploy a new base operating system in a matter of seconds when in the
    past, even heavily automated systems would often take many minutes if not hours
    (not even considering the possibilities that would come with custom compiled systems!).
    Of course, computers are just faster today, and this plays a role in speeding
    deployments. Vendors have improved installation procedures as well. This is not
    unique to Linux, but nearly any operating system.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Even doing the most traditional or *ISO-based install* where we take installation
    media in the form of a DVD image and install from USB media or virtual media,
    we can generally do a full operating system install from scratch, manually, in
    a matter of minutes. Perhaps ten to fifteen minutes on normal hardware. This is
    pretty fast compared to how installs were done nearly twenty years ago.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: It was traditional in larger environments to use something akin to a response
    file to make these installation processes faster and more repeatable. This process
    would generally mean storing your installation ISO files somewhere on your network
    and storing a set of installation instruction files somewhere and defining systems
    in a list somewhere else often listing MAC addresses to assign appropriate configuration
    files to use. Essentially just automating the human responses used when performing
    a traditional install. Effective, but clunky.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Today any form of ISO, or similar media-based installation is typically reserved
    for truly manual installations that are generally done by very small companies
    (that only ever build a few servers, and each is likely completely unique anyway)
    or special situations. There is nothing wrong with the older response automation
    methodology, but so many newer options exist that it has simply fallen by the
    wayside and continues to lose traction as a popular installation method. It was
    truly most effective in the pre-virtualization days when few installation automation
    options existed, and installations were necessarily one operating system per physical
    device making MAC address-based management highly effective. Today this would
    work effectively for platform (hypervisor) installation, but not so much for operating
    system installation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: The advent of virtualization meant two big things changed. First, hypervisors
    installed to the bare metal of a physical server are rarely customized outside
    of the most basic options making the need for automation less (and the entire
    installation process is much smaller). And second, that operating system installation
    is now done in a non-physical space opening the field to more esoteric installation
    techniques than were previously available.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: In the virtual space, we can continue to install systems manually, and many
    people do. We can continue to use response files, and while I know of no one that
    continues to do this, I am confident that it is still widely practiced, especially
    as so many large-scale semi-automated deployment systems for this were in place
    already. However, now, we have readily available options to use pre-built system
    images that can be called from a library to install even faster. With a method
    such as this, an already built system is simply copied or cloned to make a new
    system. The initial image can be preconfigured with the latest patches and custom
    packages and often installed in under a minute; sometimes, in just a few seconds.
    If it is using certain kinds of storage, it can perform so quickly as to appear
    instantaneous. With containers, we can, sometimes, observe new systems initialized
    so quickly that we can barely detect that there is a build process at all (because,
    for all intents and purposes, there is not).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: What method you choose to use for deploying your servers is not the most important
    factor. All these deployment methods, and more, have a place. What we do want
    to consider is how quickly and reliably new systems can be built using the processes
    that you choose. When a new workload is needed, it is good to know that we can
    build a new server in a certain amount of time and feel confident in the final
    configuration of it. If a well-documented manual process achieves an acceptable
    result, then that is fine.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: For many businesses, the ability to deploy rapidly is not very important. What
    becomes extremely important is the ability to redeploy. Of course, for certain
    types of applications, like those well suited to cloud computing, rapid initial
    deployments are absolutely critical, but this remains a niche and not the norm
    and will for the foreseeable future. But redeploying implies typically that some
    level of disaster has struck and that a system has to be returned to functionality
    and in that case, it is exceedingly rare that we are not under pressure to put
    the system back into production as quickly as possible.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: So, it tends to be that redeployment speed, rather than deployment speed, is
    what matters more in our environments. However, because disaster recovery is rarely
    thought about in this way, the importance of this is often ignored during the
    only time that it can be realistically affected – meaning, we can only really
    address this during our initial design and implementation phases but typically
    ignore it until it is too late to effectively change. Additionally, redeployments
    need to be done with confidence so that what we have built quickly is built as
    we expect and behaves as we expect. Under these rushed and pressured conditions,
    it is easy to miss steps, ignore processes, take shortcuts, and make mistakes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: The faster and more automated our systems are, the better chance we have to
    being able to turn out the same identical system time after time even under highly
    pressured circumstances. We should be planning for this situation when we make
    our initial deployment plans. Being able to recreate systems, without needing
    to resort to backup and restore mechanisms, can be a game changer for many companies
    who often feel forced to rely on a single approach to bring systems back online,
    when in many cases, superior alternatives may exist. We will dig much deeper into
    this when we talk about backups in a future chapter.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Processes that allow us to recovery quickly also give us greater flexibility
    throughout our professional workflows. The ability to test patches, deployments,
    configurations, build temporary systems, and so forth. Flexibility is all about
    protecting against the unknown.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Best practices in deployment processes are all about evaluating the time to
    engineer reliability deployment methods that work best for your environment and
    to streamline this as is sensible to allow for being able to restore a baseline
    operating system in a time period that makes the most sense for your environment.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Some environments are able to build new servers, and configure them for their
    necessary workloads, so quickly that they actually choose to do this over performing
    activities such as patching, updates, or potentially even reboots! Instead, they
    will rapidly build and deploy a completely new virtual machine or container and
    destroy the old one. A really effective practice if you can make the process work
    for you.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: In our next section, we will discuss the importance of rebooting and testing
    your environment under regular, frequent, and planned conditions.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Rebooting servers
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ask your average system administrator, or even a non-technical but interested
    third party, and they will tell you the importance of long uptimes on servers
    and how they want to see those ultra-high *time since reboots* on them. It feels
    natural, and nearly everyone brags about it. *My servers have not needed a reboot
    in three years!*
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: There are two key problems with this, however.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The first problem is that *time since reboot* carries no business value, and
    business value determines IT value. So why should we care, let alone brag, about
    something that has no value? It might be interesting to know how long a system
    has managed to stay online, but an investor is not going to reap a reward from
    the fact that a computer system has gone an extended period of time without a
    reboot. We work for the good of the business, if we start to care about something
    other than resultant business value, we have lost our way. This happens when we
    focus on means instead of ends, server uptime easily carries an emotional value
    that *feels like it* might lead to good things and so we, as humans, often like
    to put proxies in place in our minds to simplify evaluating results and uptime
    is easily seen as a proxy for stability which is seen as a proxy for business
    value. All of this is false, none of those proxies are correct and, even worse,
    might be inverted.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The second problem is the big one - high uptime itself represents a risk. The
    risk of the unknown. Our system changes over time from wear and tear on the hardware
    to patches, updates, and general changes to software. There can be data corruption
    or unrecoverable read errors. There might be configuration changes that do not
    work as expected. There are so many things that can happen regardless of if anyone
    has made intentional system changes or not. And the longer that we go from our
    last reboot, the less confidence that we can have that we know how the system
    will react.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: We always have to be confident, within reason, that a reboot will work. We cannot
    always control when a reboot will happen. We can attempt to have redundant power
    and redundant components, but every system has a chance of restarting and when
    they do, we want to have a high degree of confidence that it will reboot smoothly.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: The process of rebooting triggers many risks for a server. It reloads a lot
    of data from disk or other storage locations that likely has not been read in
    its entirety since the last reboot so the potential for corruption or other storage
    problems is heightened. It puts stress on the physical system, especially if a
    physical restart is coupled with the reboot. This is the time that it will be
    discovered that a file is missing, a file has corrupted, or a memory stick has
    finally started to fail.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: At first glance we might think that intentionally rebooting a system seems crazy,
    why would we want to encourage a failure to happen? But this is exactly what we
    want. We intentionally induce the potential for failure in order to hopefully
    avoid it at other times.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: The logic here is that we reboot at a time that is convenient or safe, *a green
    zone*. If at the time that we reboot we have a hardware failure or discover an
    unknown software problem, it is at a time where we know what triggered the problem
    (planned reboot), how long it has been since the last reboot (hopefully not very
    long), what changed in between (in the case of a software or configuration problem),
    or that a reboot was the exposure event for hardware failure. This should happen
    at a time that we have designated for fixing a potential problem.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Finding your green zone
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your maintenance window or green zone is a designated time at which a workload
    is accepted to be unavailable in order to allow for regular administration tasks.
    Typical tasks might include a reboot, software installation, patching, database
    re-indexing, disk defragmentation, or other intensive tasks. Every industry, company,
    and even individual workload will be expected to have different time(s) when it
    is appropriate to assign a green zone. Do not expect that the correct green zone
    from one company will apply to a different company or even from one business unit
    to another within a single company.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: A common green zone is weekends. For many companies some, if not all, of their
    workloads can safely be unavailable from Friday evening until Monday morning without
    any business impact. Often, no one outside of IT would even be aware. A good strategy
    if this is the case is to perform any patching or similar tasks almost immediately
    upon the commencement of the green zone on Friday evening and then follow that
    work immediately with a reboot. If the reboot causes anything to fail, you have
    more than two and a half days to get it back up and running before anyone comes
    in and complains that systems are down. Two days of impact-free time allows for
    far better repairs than attempting to get a system back up and running when pressure
    is high, money is being lost, and the business is pushing for answers from the
    same team they want focused on fixing the problem.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: In my own experience, I once managed an application in which we measured database
    records that were known to have a consistent period of zero use for at least one
    minute every week, across all customers of the system. This gave us an effective
    zone of just sixty seconds, but if we planned carefully, we were able to do system
    reboots, software updates, patching, and more in that window. It was hardly convenient,
    but it was very cost effective compared to asking customers to give us a universal
    maintenance window or to run extra systems to cover for that one minute.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Green zones can be creative and they might not be when you expect. They might
    be easy like long weekends, or maybe it happens during a recurring Tuesday lunch
    meeting. Work with your workload customers to learn when a workload is unused
    or not at risk.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: This is really all about planning. Only trigger problems when we are actually
    available to recover the system. This is so that we are far less likely to encounter
    have the same thing happen at a time when perhaps we are not available, or the
    workload is heavily in use. Never let a business say that a workload is too important
    to have planned downtime, that is oxymoronic. We have planned downtime specifically
    because workloads are critical. If a workload does not matter, then saving maintenance
    until things fail is not a problem. The more critical a workload is, the more
    planned downtime is crucial. In fact, any workload given no downtime (at least
    at a system level) could be designated as non-critical or non-production level
    simply from being given no planned maintenance.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Compared to a car, the more important a vehicle is to you the more likely that
    you will take it out of service to have regular maintenance like oil changes and
    brake checks. You do this because planned maintenance is trivial, and an unplanned
    seized engine is not. We know naturally that it is better to maintain our cars
    than to let them fail. It is no different with our servers.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding planned downtime is planning for unplanned downtime
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If a resultant workload has no reasonable allowance for downtime, then additional
    strategies are necessary. If you are Amazon running a global online store, for
    example, even a minute of downtime might cost you a great many sales. If you are
    an investment bank, a minute of downtime could mean that orders are not completed
    properly, and millions of dollars could be lost. If you are a hospital, a minute
    might mean critical life support fails and deaths occur. And if you are a military,
    a minute could cost you a war, so that there are types of workload outages that
    we truly want to avoid is not in dispute. Clearly there are times when we need
    to go to extreme measures to make sure that downtime does not happen.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, we need high availability at a level that allows us to take
    any arbitrary component of the infrastructure offline. This could be storage,
    networking, or any number of platform and compute resources. That means some amount
    of high availability at the application level so that we are able to properly
    patch everything from hard drive firmware to application libraries at the highest
    level and everything in between.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Critical workloads need smooth running well secured infrastructure to keep them
    running. A good maintenance plan is at the absolute heart of making workloads
    reliable.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: There is no hard and fast rule about the frequency of reboots. But a good starting
    point is weekly and gauge from there what is appropriate for your environment.
    There is a tendency to opt for weekly or monthly because we often know that reboots
    are necessary, but we still think that they should be avoided. But, with rare
    exception, this is not true. We truly want to reboot as often as it is deemed
    safe and prudent to do so.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Rebooting monthly, under current patching regimes, is about the longest that
    you would want to consider waiting for a standard schedule. Remember that any
    schedule needs to have some accommodations for a system being missed and having
    to wait for an additional cycle. So, if you plan for monthly, you need to be accepting
    of some systems going two months, from time to time, without maintenance due to
    technical or logistical problems.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Weekly tends to be the most practical of schedules. Most workloads have a weekly
    usage pattern that makes it easy, or at least plausible, to allot a maintenance
    window. Weekly schedules are also good for users as they are easy to remember.
    For example, if a system reboots every Saturday morning at nine, users will get
    used to that and not try to use the system then even if they felt like working.
    It just becomes a habit. Weekly is frequent enough that the increased risk of
    the reboot process is very likely to evoke a pending hardware or software failure
    that would have otherwise occurred in the subsequent six days. This tends to be
    the best balance between convenience and reliability.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: We should always evaluate the opportunity to reboot more often. If our workload
    schedules allow for it, a daily reboot can be perfect, for example. This is how
    we generally treat end user workloads, encouraging systems to restart at the end
    of the day so that they are fresh and ready for the next day when staff arrive
    to work (whether virtual or physical does not matter.) Doing exactly the same
    with servers might make sense.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Your reboot schedule should take into account your application update schedule.
    If the software that you run updates only rarely then a monthly reboot might make
    more sense. If you have workloads receiving nearly daily updates, then combining
    system reboots with application updates might make sense.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: System reboots are especially important after software updates and primary workloads
    (generally assumed not to be managed by the operating system vendor) as there
    are so many possibilities for services to not start up as expected, to need additional
    configuration, or simply run into bugs when rebooting. If you do not reboot in
    conjunction with software updates you lack the full confidence of knowing that
    when it was installed that a reboot worked successfully. If an issue arises later,
    knowing that reboots were working when the last system changes were made can go
    a long way to hastening the recovery process.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: If forgetting to reboot systems regularly is a common problem, then forgetting
    to have reboot monitoring is nearly ubiquitous. Even those IT departments that
    take reboot schedules seriously often never think to add *uptime monitoring* to
    their list of sensors to monitor in their environment. Reboot monitoring is generally
    pretty simple and can be generally done quite loosely. For example, in many of
    my environments where we desire the servers to reboot every week, we add a sensor
    for *uptime exceeding nine days*. If our monitoring system determines that a server
    has been up longer than nine days, it will email an alert. Missing one reboot
    event is not a major problem, and this gives plenty of time to avoid false positives
    and plenty of time to plan for manual intervention to find what caused the planned
    reboot to fail and to get it fixed before the next planned reboot should happen.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Best practice here is to seek to reboot as often as is practical and to not
    avoid reboots for anything but a solid business need which cannot include the
    false need of *the system cannot go down*. Shoot for weekly or even daily and
    accept monthly if it is the best that can be mustered and be sure to add monitoring
    as a system not being rebooted is difficult to catch casually.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: System patching, updates, and reboots may feel very pedantic. And in some ways,
    I suppose that they are. But sometimes really important things can also be kind
    of boring. And really, patching and basic system maintenance should be boring.
    It should be predictable, reliable, and scheduled. And if at all possible, it
    should be automated.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Patching should not become a challenge or a scary proposition. With proper planning,
    backups, testing and so forth, it is generally easy to have a reliable patching
    and even update processes that very rarely experience major issues of any kind.
    If we fail to make our patching and updates regular and reliable, we will begin
    to fear the process which will almost certainly lead us to avoid it more which
    will just exacerbate the problem.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: In the modern world of computing, there is always someone looking to exploit
    our systems and while nothing can protect against every possible attack, we can
    heavily mitigate our exposure through rapid, regular, and reliable patching.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: You should now be confident to evaluate your workloads, get each up to date,
    and begin implementing a formal patching, update, and reboot schedule across your
    fleet.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter we are going to look at databases and how they should be
    managed from the perspective of system administration.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
