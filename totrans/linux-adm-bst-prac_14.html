<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer021">
			<h1 id="_idParaDest-225"><em class="italic"><a id="_idTextAnchor232"/>Chapter 11</em>: Troubleshooting</h1>
			<p>Few things are as challenging in systems administration as <strong class="bold">troubleshooting</strong> when problems have risen. Troubleshooting is hard at the best of times, but as system administrators our job is almost always to troubleshoot a system that is either currently running in production and has to remain functional while we attempt to fix some aspect of it or is currently down and we have to get it back up and running in production as quickly as possible. The ability to work at a reasonable pace without the business losing money actively as we do so typically does not exist for us or when it does, is the exception rather than the rule. Troubleshooting is hard, critical, and stressful.</p>
			<p>Troubleshooting involves more than just fixing an obvious technical problem, applying business logic is critical as well. We have to understand our troubleshooting in the greater context of the workload and the business and apply more than simple technical know-how. There is fixing a problem, and there is fixing the workload, and there is evaluating the needs of the workflow, and in the end there is maintaining the viability of the business. </p>
			<p>In this chapter we are going to cover the following topics:</p>
			<ul>
				<li>The high cost of disaster avoidance</li>
				<li>Triage skills and staff</li>
				<li>Logical approaches to troubleshooting</li>
				<li>Investigating versus fixing</li>
			</ul>
			<h1 id="_idParaDest-226"><a id="_idTextAnchor233"/>The high cost of disaster avoidance</h1>
			<p>In this chapter<a id="_idIndexMarker945"/> we are going to talk extensively about what to do after there has been a disaster. Throughout this book we consider ways to avoid disaster. Something that is easy to overlook is that there is a cost to protecting our workloads against failures and that we have to weigh that against the cost of the failure itself combined with the likeliness that that disaster will even happen.</p>
			<p>Too often we are told, or it is implied that disasters are to be avoided at all costs. This is crazy and should never be the case. Disaster avoidance has a cost, and that cost can be quite high. The disaster itself will have a cost and while that cost might be quite high, it is not always.</p>
			<p>The risk that we take is that the cost of avoiding a disaster is sometimes greater than the cost of the disaster itself. There was a time when it was common for companies to spend tens of thousands of dollars on fault tolerant solutions to protect workloads whose common failure scenarios would only cause a fraction of that cost in losses. The disaster was literally less of a disaster than the disaster avoidance was! And the disaster avoidance is a certain cost, a disaster is only a potential cost. If we equate both to costs we could simplify the evaluation to a simple question such as this: <em class="italic">Is it better to lose $50,000 today, or to maybe lose $10,000 tomorrow?</em> That makes it far easier than it would otherwise be and removes most emotional response.</p>
			<p>The phrase I like to use about overspending on disaster avoidance is that it is like shooting yourself in the face today, to avoid maybe getting a headache tomorrow.</p>
			<p>Never treat the disaster in a disaster prevention plan as a certainty, it is not. There is only a possibility that it will happen. Evaluate and use math. </p>
			<p>In this chapter we are going to look more deeply into how support should work in a business, improving support posture for your IT organization, learn about triage needs, discuss finding <a id="_idIndexMarker946"/>the right people for your troubleshooting team, and then delve into when to investigate, and when to fix our issues.</p>
			<p>This is some of the hardest, most ambiguous, and ultimately most important material for us to cover.</p>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor234"/>Sources of solutions</h1>
			<p>Where do we<a id="_idIndexMarker947"/> get the solutions to problems that arise when we are system administrators? I want to start this conversation with my own career anecdote, because I think that everyone gets very different perspectives on how IT support works in the broadest of senses and understanding different perspectives is important before we start to define what good looks like.</p>
			<p>When I first started working in IT, and for the first nearly two decades, it was an assumption that any and all issues would be resolved by the IT department. Of course, situations existed where applying patches, updated, or fixes from a vendor would be part of the process, but acquiring those patches, testing them, applying them, and so forth were always completely handled by the IT staff. Even the idea that you could ask a vendor to assist, guide, or advise was foreign let alone attempting to actually do so. Reaching out to a vendor for support was assumed to be an absolutely last resort situation and reserved only for those moments when it was believed that there was an unknown and as of yet unaddressed flaw in the hardware or software that had been identified and that it would be passed over to the vendor to fix that before handing it back to IT to apply said updates or fixes, if they ever became available.</p>
			<p>During this era IT, and especially system administrators, were expected to know systems inside and out, be able to address reasonably any issue that could arise, and quite frankly figure out what needed to be done. No ifs, no ands, and certainly no buts. If you did not know how a system worked or what might be wrong you were expected to use knowledge and logic and get to the bottom of it. A thorough understanding of how systems worked, even if specific details were sometimes lacking, and good logic essentially always would allow you to resolve an issue.</p>
			<p>It was not until the very end of the 2000s that I first encountered IT shops that would rely on vendors and resellers for some aspect of their support. The idea that systems were being run that the company and its IT organization (internal or external, does not matter) could not<a id="_idIndexMarker948"/> install, configure, and support was totally foreign to me and to most people I had been in the industry with for years. If you required support from the vendor sometimes, how did you not need it all of the time? What purpose was the IT team fulfilling if they were not the ones who possessed the requisite knowledge to implement and operate the systems that were under their purview? You need far more knowledge to plan and consider all solution options than it takes to fix the singular one that you finally deploy. If you need your vendor for day-to-day tasks, then you obviously lack the necessary skill sets and experience for the more critical high level decision making and that is something that a vendor cannot help with. Sadly, many organizations end up simply lacking all capability around solution planning and this explains why so many horrendous solutions that should obviously have been known to not meet the business needs get deployed.</p>
			<p>Today is seems that the world is focused on support coming from sales organizations and vendors, rather than from IT, but this creates two critical problems. First, what is IT even there to do if they are not in the critical support path. Are they even needed? And second, how is a vendor supposed to have the range of skills necessary to do internal IT needs when all they do is support products that the make? The disconnect here is significant.</p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor235"/>There is no magic support</h2>
			<p>There tends to be this sort of unspoken belief, often amongst managers but sometimes even from IT people, that there is a group of magic companies out there, in the technological arena that can provide more or less unlimited support in a way that internal IT cannot. It is imagined, I suppose, that these companies are not comprised of human beings or perhaps it is thought that the vendors of servers, storage, and operating systems have secret manuals full of information not released to the public that include secret codes that tell misbehaving systems to start working again.</p>
			<p>In the real world, hardware and software vendors know essentially nothing that their customers do not know, at least not when it comes to the operations of their devices. It is no different than if you called up Ford or Toyota and asked them how to drive a race car, instead of asking race car drivers. Sure, the car companies will have some people on staff with a good idea of how a car will be driven under performance conditions, but none of them can possess <a id="_idIndexMarker949"/>more knowledge than actual race car drivers and they certainly will not have as much expertise on the ground as the driver currently going around the track. </p>
			<p>Hardware and software vendors are just groups of people, made up from and hired from the same pool of IT talent that any other firm has access to, in fact it is likely if you have worked in the industry for any length of time that someone you know will go from working in the field to working for a vendor, or vice versa, and there is a decent chance that this will happen to you yourself. I myself have been an engineer at at least half a dozen of the big ten vendors in the industry and at none of them was there any special sauce knowledge doled out in secret. All of our customer support or even internal support knowledge was acquired the same way that it was by customers. Sometimes customers actually had more access to our own documentation than we did!</p>
			<p>If vendors knew how to make their products work so much better than they were working in the field, they would do anything that they could to get that information out there. Almost all major vendors have extensive documentation, training, certification, and other programs in the hopes that IT workers in the field will be able to do work on their own without anything going wrong at all and being able to fix it themselves whenever possible. It is not in their interest to have people say that the products do not work properly and that the vendor has to step in to fix things. The vendors desperately want IT to be able to properly deploy, configure, and maintain in the field without vendor involvement. That looks good for marketing and it generates the maximum profits. It also makes for the best relationship with IT which, by and large, is the biggest promoter and the biggest gatekeeper to hardware and software being purchased.</p>
			<p>Perhaps there is a mistaken belief that the best of the best IT staff will automatically be gobbled up by vendors leaving the rest of the field with only those that could not cut it. That might sound plausible, but it is anything but reality. First, few vendors have the deep pockets that people imagine and there are often customers far more willing and able to pay and attract the top tier talent. Second, vendor support work is often very different from normal IT work and few people drawn into IT in general enjoy it as much simply because the technical and business aspects are different, while highly related the two jobs differ in significant ways so there is no automatic flow back and forth. Third, extremely little support from any vendor is as technical as an IT role would be. Vendor roles tend to involve sales and account management, following detailed rules and scripts, and implementing things in a pre-defined way designed not to accommodate IT processes, but sales ones. Bottom line, the best IT people rarely want to end up in vendor jobs and those that do tend to do so not because the vendor jobs are better for their IT aspirations, but because they saw an opportunity to leverage their expertise while <a id="_idIndexMarker950"/>leaving IT as a field. Many vendors have no real IT support at all.</p>
			<p>Remember that vendors make products, they do not <em class="italic">do IT</em>. So, what a vendor is typically prepared to help support is quite different than what an IT department should be doing for their organizations. Even when they have great IT resources on staff, those resources are unlikely to be allowed to provide true IT support to a customer. Vendors typically only want to, and often only can, provide support within the very tight confines of <em class="italic">operating their product as intended</em>. It is not uncommon at all for IT organizations to know as much or more about vendor products and how to implement them best than the vendor does. The vendor simply does not have any reason to have that knowledge. A hammer maker is not likely to know as much about driving nails using their hammers as carpenters do. Engineers at Toyota are unlikely to be able to drive their own cars as well as professional, full time race car drivers. Canonical is unlikely to know the best way to deploy Ubuntu in your organization compared to your internal system administrators.</p>
			<p>The skills of vendors, mainly hardware engineering or software engineer (or both) are different than IT skills and even the most amazing of vendors have little reason to be especially good at IT tasks. It just is not their wheelhouse, why would they have those skills, they are not a part of their business. That is our business for those of us in IT.</p>
			<p>What vendors should be good at, and generally are, is in knowing their products. They know when there is a flaw to be fixed, they know how companies are tending to use their products in most cases, they know what changes are coming in the future (but may not be free to disclose this to customers.) The vendor is a valuable resource, but only if kept in a logical context. </p>
			<p>Because vendors do not sell IT, but rather products, it makes no sense for a vendor to maintain highly skilled IT resources on any scale, if at all. The idea that they would even be able to do what any internal IT can do is pretty absurd, it just does not make any sense. It is most common for vendors to employ mid-career and junior resources when they do offer some amount of IT assistance because there is simply no value to having more expensive resources on staff. Customers coming to a vendor to attempt to get IT resources cannot be treating IT as a priority or logically and therefore there is no reason to provide them expensive resources that they are not prepared to leverage: the profits on selling lower cost services are simply far greater. And customers who are sophisticated enough<a id="_idIndexMarker951"/> to need high end IT resources would know not to engage a vendor for that need. </p>
			<p>So, there is no magic. Vendors do not know things that we do not know. Generally, they know far less, at least of what is important in our environment. They have little to no access to the necessary business knowledge to make reasonable IT decisions. They are not at liberty to approach problems or solutions with the breadth of skill and products across the industry but only the products that they provide, complimentary products, and processes that encourage greater use of their products. They have no alignment with the values of IT and are financially encouraged to work in their interest, not yours. There are no shortcuts, the level of support that should be available from IT resources is second to none, no other organization has the knowledge and mandate to support your organization the way that your IT (which should almost always include external IT resources from non-vendors) does.</p>
			<p>There are essentially three ways that we can get support for our systems. First, we have internal IT staff. Presumably given the context of this book, that is the systems administration person or team. Second, we have external IT resources from paid IT firms that provide external IT, rather than sales, resources and get paid to be an extension of the internal IT team either ad hoc or perhaps all of the time. And then third, there are vendors and value added resellers (vendor representatives.)</p>
			<p>We have to remember that IT is not a special case and the sellers of products are not our business advisors. Just as sellers' agents in real estate cannot represent buyers, and buyers' agents cannot represent sellers we have the same conflict of interest and opposing representation in IT. The internal IT department and any external IT service providers are paid for representing the needs of the business and are legally, as well as ethically, required to do so. On the other side vendors and resellers are paid through sales profits or commissions and are financially renumerated for representing the needs of the vendor and are legally required to do so and ethically they are bound to the vendors, not to the customer. That does not mean that a vendor and reseller cannot be friendly, useful, important, or professional, of course they can be and should be. It simply means that they are representatives of product sellers and we, as IT professionals, are representatives of our constituent businesses. </p>
			<p>We can work together, and we can do so best by understanding each others roles and obligations. In other walks of life we rarely feel that sales people or product representatives are looking out for our best interests rather than trying to promote their wares, yet in IT this is a common point of confusion.</p>
			<p>Of course vendors as well as their reseller representatives (correctly called VARs but often presenting themselves as MSPs, but be careful not to confuse a true IT service firm with a reseller just hoping that their customers do not question the name) can be valuable allies and we should not completely discount them. They can be part of our solution process, especially<a id="_idIndexMarker952"/> when it comes to getting access to special tools, beta components, patches, release information, bug fixes, replacement hardware, and other components that come from the engineering group, rather than aspects handled by IT.</p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor236"/>Visualizing what IT handles and what engineering handles</h2>
			<p>Even as IT professionals it is sometimes confusing to understand what falls under IT and what falls under the engineering (of a vendor or a software engineering department.) But the answers here should be quite simple.</p>
			<p>As IT professionals, we use products made by others and assemble those products into complete solutions. We do not make Linux, we do not create a new database engine, we do not write the applications that our businesses run. We also do not form sheet metal, run chip fabs, or otherwise build computer hardware components, but we might buy parts and assemble them into a computer in rare circumstances (like how a mechanic might assemble car parts, but not actually pour metal to make them.)</p>
			<p>Vendors are responsible for making the tools and products that IT is responsible for then using and operating. Vendors write the operating systems, we in IT install them. Vendors make the applications, we install them. Vendors are involved in making products, not solutions. IT solves organizational challenges, it does not make products.</p>
			<p>If we were talking cars, perhaps it is more clear. Car vendors build cars. Customers ride in cars. The two obviously have an important relationship with each other, but it is pretty obvious that designing and building cars is a very different task than plotting a course and driving to a destination. We can obviously see the vendor making tools for us to use, and we are car buyers using those tools to solve transportation challenges. Apply this logic to IT and voila.</p>
			<p>Real support, the most important support, is always going to come from our own IT team (which always includes external IT staff as well.) Our own IT staff not only has the broad range of business knowledge necessary to make key support decisions, but also has the range of potential solutions to work around vendor limitations. It is common that solutions are bigger than can be addressed by a single product vendor in isolation. </p>
			<p>To give an <a id="_idIndexMarker953"/>analogy, if you were a logistics firm and you needed to get a shipping container from New York to Los Angeles and your truck broke down, of course you might ask the truck manufacturer for information on repairing the truck, but you would not stop there. You would look at replacement trucks, renting a truck, other truck vendors, consider the cost of using another logistics partner to ship on your behalf while you are down, or consider switching to rail or sea transport! Of all of those things, only repairing or maybe replacing your initial truck is within the potential scope of support of the truck manufacturer, and even repairing it from them is much more limited in scope than you would likely get with a mechanic. A skilled mechanic might be able to propose partial functionality, third party parts, or alternative fixes that are not possible from, or approved by, the original vendor. The original vendor has value here, important value, but only a tiny fraction of the value that the overall department would have.</p>
			<p>It takes a much broader scope to properly deal with most disasters. Rarely do we want to just sit on our hands waiting for a vendor to determine if an issue belongs to them or not, and then decide if you have valid support or not, and then decide how they are going to deal with it. Even a great vendor, with great support has their hands dramatically tied compared to what IT staff should be doing. IT has, or should have, the scope to do whatever is necessary to protect the company. That might involve engaging the preexisting vendor or it might involve working around a vendor, or perhaps it just involves coordinating multiple vendors. Even when a vendor does need to be involved, IT should be overseeing that vendor.</p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor237"/>IT vendor managements</h2>
			<p>More than any other IT department, system administration often has to interact with vendors and has a greater level of need to oversee them directly. At the same time, system administration is also the department most likely to not have any vendors, at least not in the traditional sense.</p>
			<p>Vendors should not be thought of as a department, but rather more like a specific tool. A tool that needs to be overseen and used when appropriate, but at the direction and discretion of IT. A vendor on their own lack's direction and control. </p>
			<p>Managing the vendors for the hardware, software, and services of an IT department should be an every day task of that department. The vendor relationship at that level is important as this is the level at which technical know-how should be exchanged. For us, as Linux system administrators, this means direct contact with our operating system vendor counterparts at<a id="_idIndexMarker954"/> vendors such as IBM Red Hat and Canonical who can keep us apprised of patches, upcoming changes, release dates, security alerts, and so forth and, in some cases, may be focused technical resources for us to lean on.</p>
			<p>Systems administration may have many other types of vendors as well. We may have to work with server hardware, storage, database, and even application vendors at times. That there are so many potential vendors highlights how critical IT management of the vendors are. Without IT oversight, there is no coordination between those vendors and no mandate for them to collaborate or to work towards a common goal. The mandate to work towards the good of the business lies solely with IT in this case. It is for IT and IT alone to ensure that the vendor resources at its disposal are used for the good of the business when appropriate, rather than being sales efforts for the vendors.</p>
			<p>The best practice is that support should come from within. Fundamentally, at the core of it all, we should see our IT team (inclusive of internal staff and external staff) as our solutions team both to design our solutions up front and to deal with them when something goes wrong. When vendors are required to be part of the solution process they should be engaged, managed, and overseen by the IT team and managed as just another resource.</p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor238"/>Triage skills and staff</h1>
			<p>Most companies<a id="_idIndexMarker955"/> fail dramatically when disaster strikes because triage processes either do not exist or are too poor. The skills to run the business day to day are different than the decision-making processes needed in real time in a crisis: there is little time for meetings, almost no ability to consult with different parties, and planning is out of the question. When in this mode we need someone leading who is trusted, handles stress well, and is a perceiver rather than a planner - someone who thrives running with rapid decision making and does not need to have planned their events ahead of time. Planning is excellent and as much as is reasonable should be done ahead of time, but everyone involved from junior IT staff to senior executive staff should understand that true emergencies cannot be adequately planned for, and real life will involve many unknowns that have to be evaluated on the fly.</p>
			<p>Our first process when there is a disaster is to head into triage mode. We need to know what exactly is not working, what has happened, what is the impact - basically we need to know the status, of everything. Is this something we think that we can fix in minutes? Is this going to require some investigation? How are people impacted? Are we losing money, productivity, customers?</p>
			<p>There are so many ways that we can be impacted by a disaster. Being able to quickly get a grasp of the business effects, how does each department play into the big picture, which teams can work just fine, which teams are dead in the water, are there teams that are functional but limping, and so forth is absolutely critical and can mean all of the difference between everyone just standing around being unsure what to do and a triage manager getting things fixed right away. We need status, and we need a lot of it, very quickly.</p>
			<p>In most cases a surprisingly small amount of our time is actually focused on solving a technical problem. This may be because it is simply a hardware failure and we just have to wait for replacement hardware to arrive. Or maybe it is complete software failure and we just have to rebuild our systems. Sometimes deep technical investigation has value, and sometimes it requires a lot of know-how to the cause of an issue, but this is not the majority case. We are much more likely to have a relatively quick fix, or at least quick in terms of the amount of administrator time is necessary to use during the process. When fixes take a long time, it is most typically because there is a third party that needs to be waited on.</p>
			<p>Typically, we are going to mentally focus on the technical aspects of an outage. Other aspects of most outages are more important. Some organizations have operational triage experts who step in and handle these aspects of an outage allowing us as system administrators to focus purely on the technical aspects under our auspices. For most businesses, though, dealing with an IT disaster is going to require IT oversight from beginning to end. In the majority of cases, the very team that we would hope would step in to assist IT in solving issues and managing the triage of the operational environment gets in the way of finding solutions rather than being part of the solution.</p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor239"/>I can give status, or I can fix things</h2>
			<p>Everyone who has ever dealt with any kind of outage, disaster, or what have you impacting a business knows that the expected behavior is not for executives and management to jump in and start to find ways to protect the business or to run interference to assist you in dealing with the problem that you are suddenly tasked with; but instead the very people we depend on to create an environment to minimize impact and to make us effective almost universally turn on us and begin demanding explanations, status reports, updates, estimates, promises, and miracles all of which are pointless at best and <a id="_idIndexMarker956"/>completely undermine the business at worst.</p>
			<p>Even under ideal conditions reports, status, updates, measurements and the like have a cost and come at the expense, even if only a small degree, from productivity. During a disaster, it is rare for anyone except those in the most critical positions of attempting to mitigate and fix the disaster to have any ability to provide status. So, the most expected event is that everyone will descend on those few, critical positions and demand status updates.</p>
			<p>There are several problems here that are all really obvious to us, working in these positions, but often lost on those who are now experiencing an impact to their productivity. We have two tools at our disposal to help with this. One, show this book to those in management and ask them to take the time to understand the situation. The second is planning. Make sure that as a part of your disaster planning and preparation process that there is training for management, and a policies and procedural plan, as to how status will be given, who may ask for it, and from whom. Consider designating and official spokesperson for IT (and other departments) who can spend all of their time giving updates as they are not involved in any other aspect of the disaster recovery efforts. Perhaps they will run a war room in person, or maintain an email messaging group, they could manage a chat room, or head a conference call that others can dial into as needed.</p>
			<p>Then any updates that exist can be passed, proactively, to this mandated reporter(s) and they can maintain that status for the organization. The entire organization should understand the critical nature of having a mandated point of reporting so that the team actually attempting to solve the issues and get the company back to full functionality can spend their time saving the company rather than reporting on its failures. Obviously the business has some business needs to know as much as possible about what is happening. More impactful is political problems internally as managers feel that they, too, have to provide status that they cannot have and many layers of organizations will have people acting emotionally and potentially willing to cause significant financial damage in the hopes of appearing to be concerned or just to satisfy their desire to know more than anyone can really know.</p>
			<p>When training management as to why IT (and other departments) cannot provide extensive updates we need them to understand why we, as the people attempting to remediate the issue, cannot be spending time giving status updates. </p>
			<p>We don't have any information to give. Fundamentally, this is the biggest piece of the puzzle. While we might have a simple answer like <em class="italic">the replacement part is scheduled to get here tomorrow</em> typically we know nothing about how long something is going to take to fix. Most<a id="_idIndexMarker957"/> things in IT are fixed essentially the moment that we know what the issue is. Until it is completely fixed we normally are only working from hypothesis. Pushing us for information really just ends up being analogous to demanding that we feed intentionally false information because have nothing else to provide. It is like torturing a prisoner that does not know any information, but if you torture them they are likely to just make something up in the hopes of the torture stopping.</p>
			<p>We are busy. If things have not already been resolved, chances are we are completely engaged in trying to get them resolved. All of the time used to give status has to come from the time working on resolving the issue. It is more than simply wasting time, it is also causing interruptions and demoralizing those trying to resolve things. It sends a huge organizational message that the issue does not matter very much and that the efforts to get it fixed are not appreciated as they should be. It makes IT wonder <em class="italic">if management is not prioritizing getting things fixed, why would we?</em></p>
			<p><em class="italic">Political Risk</em>. In attempting to get everyone who wants to be able to <em class="italic">plan</em> a disaster information to work from those working to resolve the issue are generally in a very tough position of having to guess quite heavily as to when things will be finally resolved. Most organizations handle this uncertainty, which cannot be helped, quite negatively. Intentional bad information is often rewarded, honesty is often punished. Putting your information providers into a position of potentially having to <em class="italic">just tell people what they want to hear</em> or <em class="italic">providing inaccurate information for political protection</em> means that the business may then operate with bad data causing unnecessary additional financial loss. It is a terrible time to be pushing for bad data over no data, yet it is when it is most likely to happen.</p>
			<p>Priorities. If the organization starts to prioritize, from management, the perception that reports, status meetings, calls, and other things that do nothing to resolve the issue to get the company back running at full speed are more important than finding a solution, this will naturally, and absolutely should, change how IT or any other department tackles the problem. If any issue is so trivial that we have meetings to discuss timelines instead of fixing the system, then overtime, rescheduling family events, even skipping lunch all become absurd IT sacrifices that obviously have no value to the company. We would never do any of those things just for some meeting, and if that meeting is more important than finding a solution, we have a relative value assessment that gives us a lot to work with.</p>
			<p>So how should management act? Management needs to do all the opposite. If status even matters, and <a id="_idIndexMarker958"/>we understand that it generally does, then have someone that is not involved in the remediation handle those communications. Keep priorities clear. Assign teams to run interference and keep all interruptions away from the team attempting to fix broken systems. Have people bring them food, drinks, coffee, run errands, whatever is needed - show that they matter, a lot, instead of suggesting that they do not matter. Do not punish messengers for delivering bad news, reward them for honesty. Empathize and think about the best results for the business.</p>
			<p>All things that are easy to say and hard to do when disaster has struck. Plan ahead, have these discussions, make a plan, get executive sign off before things happen. Have an action plan to put in place that says who is in charge, how things happen, and so on. </p>
			<p>Our first stage of triage is an assessment. Do we have a plan to get back online quickly? We need to know the situation as it stands, and we need to then relay that information somehow to management. From here, things get tricky. There are so many variables that teaching triage is anything but easy. Someone who excels at triage needs to be able to take the situation as it is and gauge a range of issues from ways to fix the existing problem, potential options to work around it, and in many cases, how to modify the organization to best react to it.</p>
			<p>This is very much a <em class="italic">thinking outside of the box</em> scenario. We need, at this point, to look at the big picture and figure out how to best keep the finances of the business running. This might seem like a management task, and again, ideally it is, but IT should play a role as we have certain types of insight that might be lacking elsewhere.</p>
			<p>Outside of technical fixes, mitigation strategies will vary broadly based on the type of business, type of impact, and so on. Should we send everyone to get coffee? Maybe plan for a long lunch? Get everyone home on vacation now to save on insurance because it is going to be a while before anyone is productive again? Perhaps moving people to paper or from email to instant messaging? Use downtime from one type of task to focus on others. Perhaps we do a deep office cleaning while people have the time - unplug those cables and really get the place clean.</p>
			<p>Big emergencies can present big opportunities as well. I have seen several times when companies have used catastrophic outages as chances to enact sweeping updates and changes that would require big time approval or large downtimes normally, but which could be slipped in during an outage that is happening anyway. I once even had an ISP based outage that was predicted to last so long that a team ran from New York to Washington, D.C. with a truck, put a rack of servers into the truck, and ran to a new location that was waiting and ready in New York and pulled of a datacenter migration that had been <em class="italic">indefinitely on hold</em> because of the necessary downtime and was able to bring workloads <a id="_idIndexMarker959"/>back online from New York before the ISP was able to restore service in Washington, D.C. A somewhat minor outage was turned into a huge <em class="italic">win</em> by the department. A large project that was struggling to get scheduled and approved was pulled off purely as a bonus while the team was able to simultaneously enact a significant <em class="italic">fix</em> by going to an alternative datacenter to overcome the limitations caused by the ISPs leased line at the original location.</p>
			<p><strong class="bold">Triage</strong> is hard because it requires that we be creative, open to alternative ideas, able to avoid panic, think broadly and outside of the box, and do so with little planning or preparation. If your organization does not have someone suited to this role, and relatively few do, then this is something you should be outsourcing, but your system administrator is one of the most likely candidates for it as the skillsets and aptitude of administration tend strongly towards triage and disaster recovery as compared to engineering and planning skillsets.</p>
			<p>In many cases outages result in far more than a single workload being inaccessible and prioritization within the technology space is also required. Your triage person or team needs a deep understanding of how workloads interrelate to one another, which ones depend on others, what can be worked around, what can be skipped, and how all of these workloads relate to the business itself. Only by knowing the scope of the technology as well as the business can anyone provide valuable insight into what to fix, in what order, and how the business can potentially work around things. </p>
			<p>I wish that we were able to provide concrete guidance as to how best to survive a disaster, but we cannot. Disasters come in so many shapes and sizes, and the ways that we can deal with <a id="_idIndexMarker960"/>them are more numerous still. You are best served by learning how to think, how to react, and being as prepared as possible for any eventuality.</p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor240"/>Staffing for triage: The perceiver</h2>
			<p>I had the great benefit of once working for a large IT department where deep psychological analysis was part and parcel in the general managerial processes. This might sound like a terrible thing, but the approach was excellent and the company used proper psychoanalysis to learn how people work, how they would work together, who was expected to be strong or weak in different areas, and how best to combine people to achieve the best results.</p>
			<p>I do not want to go in to detail in all of the ways that these techniques were or could be helpful, but one specific thing that I want to touch on is the idea of the Judger and Perceiver scale of the Myers-Briggs test. I am not a psychologist, so I recommend that you research the test and its interpretation on your own, and understand that like with all psychology it is both greatly accepted as well as heavily disputed as to its efficacy. I will not argue for or against here, but only say that understanding the judging to perceiving preference pair is highly valuable.</p>
			<p>I generally describe the Judge as a <em class="italic">planner</em>, one who likes to organize and put things in their place before an event occurs. The Perceiver is more a <em class="italic">reactor</em> or a <em class="italic">responder</em>, someone who wants to take the world as it comes and react to it in the moment.</p>
			<p>In our world, engineers and most managers are judgers. Their role, their value comes from thinking ahead and organizing the business or the technology to do what it needs to do. Perceivers tend to excel at being administrators, rather than engineers, and are exactly who you need to have at the ready when there is a disaster. Your perceiver personalities are your natural candidates for those who are likely to be good at handling triage operations and thinking on their feet. Humanity is naturally diverse to complement one another to handle multiple aspects of life and this is a great example of that.</p>
			<p>There is far more to being the right person or team member for the job than just fitting an aptitude on a personality test. The Myers-Briggs assessment is simply a tool for identifying who might be strong or weak for different positions, and for explaining how people tend to think and feel. For me, discovering that I was a strong perceiver and a weak Judger was influential in helping me to understand myself and how to communicate things about me to other people. It also gave me tools to help me to understand other people in my life so that I could communicate better and set expectations better when they operate differently from me.</p>
			<p>Whether your company uses a formal process, or you just take an online survey to learn about yourself, I recommend the Myers-Briggs and similar tools for simply helping you to understand yourself better, if nothing else. The better you know yourself, the better you can be prepared to succeed where you are strong and to ask for help where you are weak. If you are a team leader or manager, this kind of information can be useful in helping to understand <a id="_idIndexMarker961"/>your team better and how they can work together to be stronger.</p>
			<p>Do not be tempted to read too much into psychology tools or try to apply them too broadly. By and large these tools are best when applied to yourself and when you approach them openly and honestly with a desire to learn not about your own strengths, but about your weaknesses and use them for self-improvement. Remember that a test of this nature is not about comparative results, one result is not better or worse than another; all people live on scales and neither end of the scale, nor the middle, is a good or bad result. Strong teams, however, are generally built from a variety of different combinations of aptitudes and personalities to cover many different needs.</p>
			<p>I hope that I have just filled you with confidence and ideas that you will use to take your triage process to the next level rather than causing a panic attack about the complexities and uncertainties of dealing with disasters - that should not be the takeaway here. Use any panic that you are feeling now as motivation to immediately start your planning and documentation, and to kick off conversations with management to get stakeholder buy in now. Make it a priority and you can quickly move from being unprepared to being at the forefront of businesses ready to respond best to almost any circumstance.</p>
			<p>Do not feel that the person who has to handle triage has to be you. Maybe you are the best person for the job, maybe you are not. Very few people have the right personality and triage is a very special aptitude to have. What is important is identifying your triage person or team, whoever they are. Everyone has a role to play, find your place, and find the right people for the roles you have to fill.</p>
			<p>Our best practice in regards to staffing is to identify your triage people before there is a disaster and have them documented and in place to take over, and empowered to take over, when the time comes. Do not let the decision process of finding someone with a triage aptitude <a id="_idIndexMarker962"/>wait until the clock is ticking on your downtime, and do not let politics become the focus, rather than solutions, when time is of the essence.</p>
			<h1 id="_idParaDest-234"><a id="_idTextAnchor241"/>Logical approaches to troubleshooting</h1>
			<p>Possibly the hardest <a id="_idIndexMarker963"/>thing that we have to do as system administrators is troubleshoot problems. It is one thing to be able to deploy a system initially, but a very different thing to be able to troubleshoot it when things start to go wrong. With systems administration there are so many places where things can go wrong for us; we sit at the nexus of so many technologies and so many possibilities that tracking down the source of issues can be very challenging.</p>
			<p>Not surprisingly, experience makes this far easier than anything. The more you get experienced with maintaining and managing systems the more likely you are to be able to quickly <em class="italic">feel</em> your way around a system and often just sense what might be wrong when things get tricky. Nothing really trumps just knowing how a system will react when things are healthy and being able to sense what is wrong based on its behavior. Senior diagnosticians are often brought in for exactly this reason. With enough experience often you can just feel when an index, a cache, a disk, or lack of RAM is the issue.</p>
			<p>Short of sheer experience, our next best tools are a deep understanding of our own systems and how they interact, deep knowledge of technology fundamentals as they apply to our situation, and logical troubleshooting.</p>
			<p>Of course, in many situations failures are going to be quick and obvious. The power is out, a hard drive has failed, a key database has been deleted. There is nothing to track down, only things to be solved. Other times, though, we get complicated issues that could be caused by almost anything and we may have to track down something truly difficult to pinpoint.</p>
			<p>It amazes me how often I am brought in to assist with troubleshooting only to find that the work that has already been done is haphazard and is often redundant. Of course, at times, some guesses at easy to test failure points or early tests of known common fail points can speed discovery, but we have to be careful not to lose track of what we are doing and learn <a id="_idIndexMarker964"/>systematically from what we test.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor242"/>Stories of troubleshooting</h2>
			<p>A benefit of being an <a id="_idIndexMarker965"/>author is getting to regale you with tales of my own historic troubleshooting and there is no one to roll their eyes or cut me off and you cannot just walk away when you are bored. So here we go.</p>
			<p>One time I was called in to work on a system that was used for an extremely low latency application and the team had discovered intermittent problems with the application receiving responses from other systems. The issue would arise every so many minutes that a response would be received several nanoseconds later than it was expected to have arrived. Yes, nanoseconds! Nothing broke, no results were wrong, but there was just this tiny delay, and not very often.</p>
			<p>After much research, we finally found the issue through a combination of research and system understanding. Identifying what was happening was eventually done by hours of staring at a top monitor and looking for processes that were active around the time that the delay would occur.</p>
			<p>Eventually the process possibilities were whittled down it was discovered that a memory garbage collection process that was soon thereafter discontinued in the kernel was using excessive system resources in its default settings and causing the system to halt for just a few nanoseconds while it processed memory for cleaning.</p>
			<p>I was able to address the issue by setting the garbage collection process to only clean a portion of RAM on each cycle allowing it to work much faster. We ran into the issue only because we had so much physical RAM in the server that the garbage collection process took measurable time, something generally not expected.</p>
			<p>In this case, good research and patience were certainly important. Being able to <em class="italic">feel</em> the delay in the system based on the measuring tools (no human can actually feel a delay that short), and then using logic to determine how a process doing memory garbage collection could, and would, impact a process of this nature had to come together to make troubleshooting possible. Without a deep understanding of how the system runs, it would be out of the question.</p>
			<p>When troubleshooting anything I find that there are two key techniques that I am telling people to use over and over again. The first is to be systematic and to work from one end or the other. Avoid hopping around and testing at random. If you are testing network connectivity, for example, start at the near end of the stack and start building up a base of knowledge based on testing. </p>
			<p>In the networking example we can start with checking if our network connection is plumbed? Does it have an IP address? Can it ping the gateway? The ISP? A public IP? Is it able to resolve DNS? Can it reach the system to which it is supposed to connect? Can it reach the right port? Does it get a proper response?</p>
			<p>Instead of jumping around and testing different pieces, working from the nearest point and exploring helps us to understand exactly when things fail, and it tells us quickly.</p>
			<p>The other key technique that I always teach is <em class="italic">work from what you know</em>. Basically, establish your facts. There might be many things that you do not know, but you cannot worry about those things. The unknown will always exist. There are always things that you can know, though, and these we have to establish and work from. Use the facts that we have to build up<a id="_idIndexMarker966"/> a larger body of knowledge by finding more and more things that we know for certain.</p>
			<p>For example, if you can ping a remote server then you know that you have working plumbing, working routing, and that all of the equipment between you and that remote server is all working. Or if you know that a specific database is up and running and working properly, then you know that its operating system is also up and running, and that the hypervisor that it is on is up and running, and that the bare metal server that that hypervisor is installed on is up and running. </p>
			<p>It is always surprising to me how often people who are troubleshooting will, take the time to establish the facts, but then question them again. In the example above, they might decide that because they are seeing an issue that maybe the hypervisor has failed and go to check it again, even knowing that they just proved that it was still working. Or in the networking example, convincing themselves that they need to check on the status of a router that they just used to prove that networking could pass through it correctly.</p>
			<p>Going down the proverbial rabbit hole and making yourself (or your team) prove over and over again that something is or is not working that you already know the status of is a waste of time at best and can be extremely frustrating for those working with you. Once you start the pattern of reestablishing what you already solidly know, rather than trying to determine something new, you will likely continue doing so. It is very tempting to focus on those things and lose sight of growing the body of knowledge pertaining to the issue at hand.</p>
			<p>I find that writing down what we know to be true, whether as a starting point or from investigation, is a good tool. If we feel that we have to test again something that we already proven, then we have a problem. Why do we not trust what we have already proven? Why did we feel that it was proven if we now doubt it? What is the point of testing if we are not going to trust the results of the testing? </p>
			<p>If we test, prove, and then we are going to spin our wheels endlessly. This is a needless waste of time, time that we cannot afford during an outage. Either we need to approach what we consider to be fact differently, or we need to trust the assessment. By establishing trusted facts, we can use them to narrow down the possible issues.</p>
			<p>Troubleshooting is hard both because it is very technical when many factors are probably unknown, and because it is emotional being done when there is stress and sometimes even panic. This is generally compounded by additional needs and pressure from our <a id="_idIndexMarker967"/>organizations as well. Keeping a clear head is key. Breath, focus, get caffeine, talk out the issue, post on technical peer review communities and forums, and maybe even engage the vendor. Have your resources written down as it is easy to forget steps when stressed. </p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor243"/>Technical social media in problem solving</h2>
			<p>For more than<a id="_idIndexMarker968"/> two decades one of my strongest resources for dealing with serious issues has surprisingly been technical social media. I do not mean traditional social media outlets, but forums built solely for technical exchange of ideas. When faced with design challenges or, far more importantly, broken systems that need to be fixed, I have found that for me and my team that posting those problems to a forum to be absolutely invaluable.</p>
			<p>The reasons why this is so important are not always evident. The obvious benefit is that there are many seasoned professionals happy to provide a fresh set of eyes on your problem and may easily spot something that you have missed, or they may provide insights or suggest tools of which you were not aware. </p>
			<p>The bigger benefit, however, is in the process of requesting help. More times than not the act itself of having to write out what is wrong, the need to express it clearly in writing, and documenting the steps that I have followed will reveal to me, even before anyone has a chance to respond to me, what might be wrong. Writing down my steps encourages me to also be more methodical and to think about what obvious questions others will ask me causing me to attempt to fill in the gaps, follow good processes, and document far more than I would normally do for myself.</p>
			<p>I use this process of posting for public review for my own team, as well; and I make others do it. Of course there are details that cannot be posted publicly, and sometimes the entire process is too sensitive to be public at all even without identifiable details, but generally at least some degree of the disaster can be reviewed publicly for assistance. Using these kinds of forums for communicating amongst my own team works wonderfully and encourages the same good behavior of thinking through what has been done, approaching how to explain the problem differently, and forces clearer documentation of the troubleshooting process between team members because even the documentation process is being reviewed publicly in real time. </p>
			<p>This same process<a id="_idIndexMarker969"/> then provides documentation and automatic timeline of troubleshooting to use for a post mortem process. I often also invite post mortem review, generally informally, via the same mechanism. People are always happy to critique decisions. You have to be prepared to accept a bit of a brutal review.</p>
			<p>No one is a bigger proponent of the value of public peer review in focus environments than me. I have been championing this movement since the late 1990s and spent a great deal of my career working in the public eye through these communities. It has taught me many things that I would have never been exposed to and it has forced me to work differently knowing that everything that I do will be examined, reviewed, and questioned. Being prepared to explain every decision, to defend every outcome, to resort to logic and math because your arguments for or against a decision themselves are permanently recorded for review makes you rethink what you say and, I believe, pushes you to be better at everything that you do. It is easy to make an irrational argument when you think that no one is going to question you or that no one will notice.</p>
			<p>Troubleshooting best practices are simple: be methodical, document everything, and line up your resources before you need to rely on them.</p>
			<p>Next, we are going to look at when it even makes sense to go through this process or if we should <a id="_idIndexMarker970"/>simply start over.</p>
			<h1 id="_idParaDest-237"><a id="_idTextAnchor244"/>Investigating versus fixing</h1>
			<p>When we start <a id="_idIndexMarker971"/>working to deal with an outage, data loss, or other disaster, the natural inclination is to focus on finding a root cause, fixing that root cause, and then getting systems back into a working state. It makes sense, it is the obvious course of events, and it is emotionally satisfying to work through the process.</p>
			<p>The problem with this process is that it is based on a few flawed beliefs. It is a method derived from things like getting your car or house repaired after there is damage or an accident. The underlying principle being that the object or system in question is very expensive to acquire and in relative terms, cheap to repair. </p>
			<p>It also focuses on the value of determining why something has occurred over the value of getting systems up and running again. The assumption is that if something has happened once that it is expected to happen again and that by knowing what has failed and why that we will be able to avoid the almost inevitable recurring failures in the future.</p>
			<p>Of course, in IT and business systems, typically the cost to build is less than the cost to fix. More importantly the cost to build is more predictable than fixing. We should, if we have planned well and documented, be able to implement a new system in a known about of time with extreme reliability. Fixing an issue may or may not happen quickly, it represents a lot of unknowns. A fix may take a very long time, and the fix may not be reliable. Root cause analysis can be time consuming and unreliable.</p>
			<p>In most cases getting a system back up and running as quickly as possible carries great value, and while determining why a problem has occurred and finding a potential means of avoiding it in the future has little value. Business infrastructure experiences extraordinary change rates in everything from hardware to software to system design. A hardware failure that has happened today is unlikely to repeat in the same way. Software will likely be patched, updated, and modified quickly and worrying that old bugs will return is possible, but not a scenario worth a large amount of concern.</p>
			<p>If we were dealing with a car, house, road, bridge, or other large object of this nature failures are likely to recur as the system faces small, if any, changes, over time. We need to determine the point of failure, determine the risk of recurrence, and find a way to protect against it. It is hard to separate ourselves from this mindset. </p>
			<p>We have to evaluate the value to the business. What is the value of getting the system back up and running? What is the value to finding the root cause? We have to compare these values and, most of the time, we will find that solutions triumph over investigation. </p>
			<p>The fix versus investigate decision gets more and more weighted towards fixing when we have more modern infrastructure with imaged systems, state machines, and infrastructure as code. The greater the quality of our automation, the faster and less costly it is to recreate systems and the lower the value to investigating an issue.</p>
			<p>We also have to consider the possibility that with the right infrastructure that we can recreate a mirror system to use for diagnostics when it is deemed necessary. We can create an initial rebuild to get systems back up and running and build a mirror, if it makes sense, to <a id="_idIndexMarker972"/>use in order to attempt to recreate the failure and determine if there is a way or reason to protect against it in the future. Spending time attempting to identify the cause of and fixing an issue during an outage may not be the best way to accomplish that goal, even if it is deemed to be necessary.</p>
			<p>It is all cost analysis, but one that has to be done very quickly. The unknowns are very difficult sticking points here because the time to determine the root cause is completely unknown and may take minutes or days.</p>
			<p>This logic of just starting over applies to desktops as well as to servers. Even end user workstations have every opportunity to be interchangeable and designed in such a way to allow for rapid redeployment. If we are using images, automated software installation, and other automation it is quite standard for desktops, laptops, or whatever we are using for end users to be able to be deployed new often in a matter of minutes. A fresh deployment is more than just getting these systems back up and running with maximum efficiency, it also provides an opportunity for a completely clean installation as a bonus. Any cruft, malware, corruption or similar that might have happened on the machine will be wiped away and the end user will start as fresh as if their machine had just rolled off of the assembly line. In this way, we get a silver lining: a fresh rebuild process that we often would struggle to schedule but, ideally, would be doing on a semi-regular basis anyway.</p>
			<p>Rebuildable systems, whether desktops, servers, or cloud instances, all mean that we only need available hardware to be able to recover and move on from most disasters. That also means, assuming that our backups are stored somewhere online or offsite, that we have the ability to walk into a new site and rebuild our entire company there. That flexible and level of comfort is a game changer - something very few companies were able to consider even just a few years ago. Knowing that starting over <em class="italic">from scratch</em> is always a possibility makes us think about everything that we do in disaster recovery completely differently.</p>
			<p>In my own experience, even fifteen years ago, long before we had the automation and complexity of today's environments, we were moved almost entirely to <em class="italic">restore fast, only examine what we can after things are back online</em> and the ability to do so has only increased since then. Today we should, in almost all cases, be thinking of rebuilding from scratch as the default assumed starting point and we should only resort to more complicated forms of recovery when the situation demands it; and we should carefully evaluate why a situation today would demand it. That does not mean that rebuilds should be the only tool on our toolbelt, being the majority case in no way implies that they are the only correct solution, just that they are the most likely to be correct.</p>
			<p>Traditionally there was a stigma to rebuilding, as if it meant that we had given up or were in over our heads. We have to fight this incorrect emotional response. The right way to recover an <a id="_idIndexMarker973"/>environment is whatever creates the best situation for the organization, as a whole. As with everything that we do, emotion plays no role here. This is a financial and risk calculation only. We do what is best for the company, and that is all. </p>
			<p><strong class="bold">Best practice</strong>: </p>
			<ul>
				<li>Evaluate each situation, but when in down err on the side of a clean rebuild. No one size fits all, but rebuilding should be the better option in most cases.</li>
				<li>Design systems to be able to be rebuilt quickly, easily, and automatically.</li>
			</ul>
			<h1 id="_idParaDest-238"><a id="_idTextAnchor245"/>Summary</h1>
			<p>Disaster recovery, triage, proper staffing for emergencies, organizational preparedness, managerial oversight of processes during disaster situations, and every other aspect of a critical failure scenario is hard, scary, and stressful. How companies decide to handle these times often determines which companies survive, and which ones fail. We have to have the right people in place, as many organizational processes and procedures as possible, great documentation, deep knowledge of our systems, and the flexibility to do whatever it takes to make the business successful through hard times to truly succeed.</p>
			<p>Every company struggles with these same things. These are not simple tactics that we can apply overnight. It requires buy in from organizational stakeholders, it requires professionalism and planning not just before events transpire, but maintaining those processes and professionalism during times of panic when stress causes almost anyone to act irrationally. On one side we can view this as stressful and difficult, but on the other we can recognize it for what it is: a place where nearly all organizations struggle, most fail, and our greatest opportunity shine.</p>
			<p>Disaster planning and disaster recovery are easily your best chances to take the system administration role and grow it to something larger than the role and, often, larger than the IT organization itself. You cannot effectively isolate disaster preparedness to solely the IT department; it requires cooperation across all departments. Systems administration can lead, rather than follow, and make IT the core business unit that it always should have been.</p>
			<p>I realize that for many, the capabilities and scope of the IT department are deeply mired in politics and cannot easily be made fungible. Challenge yourself to at least evaluate, to consider, what it would take to push your organization in new directions. No organization should force IT to wear the sales hat just to convince the organization to do what is right for itself, but here in the real world our ability to sell ourselves, our department, and our ideas is often the difference between being heralded as the savior of the company, or just ignored.</p>
			<p>Remember that solutions come from you and not from vendors. Keep vendors and their scope in mind and remember that while disasters represent a great chance for you to save your organization, they also represent a huge opportunity for vendors to find new sales opportunities at a time when fear and emotions make well considered planning all but nonexistent. Rethink how you view vendors, make the context of the support relationship utmost in your mind. Always know who represents your interest and who is looking for where you can best serve them.</p>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor246"/>The postmortem</h2>
			<p>Putting a sidebar in the summary may seem out of place, but I think postmortems are much like a summary of a disaster themselves. So why not discuss them here?</p>
			<p>Most companies skip the all-important task of performing a post mortem. Postmortems are not about placing blame, and if that is what your company wants to use them for then it is probably better to avoid them, but in a healthy organization they serve as a critical learning tool on many levels. </p>
			<p>A good postmortem is going to expose mistakes in system design, documentation, planning, policies, procedures, and just about any other aspect of our systems. It should also aid us in identifying people who are strong or weak during a crisis. We should be using our postmortem processes to discover where we were weak and how we can improve, or potentially to determine not to change at all.</p>
			<p>A postmortem should also allow us to evaluate our decision processes that led to where we are today. This is almost universally overlooked and is actually where our true value of a postmortem exists. Changing the outcomes of individual plans or decisions is good, but generally minor, but finding entire decisionmaking processes that are failing gives us an opportunity to make changes that impact all decisions going forward.</p>
			<p>Learning how to make decisions is important and few organizations or people ever focus on the quality of the decision-making process and even fewer companies track it and attempt to improve it over time. This is a huge lost opportunity. Decision making is something that happens over and over again. Making better decisions on a regular basis is vastly more important than fixing individual decisions.</p>
			<p>Postmortems need to dig into <em class="italic">why did we decide to do what we did</em> and then examine if that was a good decision, but we must not fall into the trap of applying current knowledge to old decisions. The <em class="italic">if we had only known</em> game is a dangerous one. We have to evaluate what we could have known at the time and determine if we researched enough, thought it through properly, applied true business goals and so forth. </p>
			<p>It is easy to project knowledge after the fact and say <em class="italic">see, things failed, we lost money, that is someone's fault</em>. Emotionally, that feels like it must be true, but it is not. It could be true, but that is not likely a productive thing to determine. Bad things happen, risk is part of business, there is not always someone at fault causing these bad things to happen. Working in IT we deal with calculated risk every day. What we need to know in a post-mortem is if we calculated it correctly and took the right chances. </p>
			<p>A great example of good risk is when we have to travel from New York to Los Angeles. We can take a plane or we can drive. If we look primarily at our safety during the trip, flying feels scary and driving does not seem scary at all. Yet the chances that we die in a car crash over such a long distance is many times higher than the risk of dying in a plane crash. If we took a plane and the plane did indeed crash, it would be tempting to use the new knowledge that that particular flight was going to crash to say that the right decision would have been to drive, but that is wrong. The flight was still the right decision. Both approaches had their risks and the flight was the vastly lower of the two risks. We use knowledge that that particular flight would crash because there is no way to have known that ahead of time. We were playing it safe, we made the right call; but no option is without risks and punishing people for making good choices is a terrible outcome.</p>
			<p>People need to be rewarded for making tough choices, especially when they make the right rough choices. If we look to place blame, we risk punishing people for simply having made any decision at all, and if we do that we push them towards avoiding the decisions that protect the business to avoid being in the line of fire for false blame. Of course, if truly bad decisions were made, we want to discover that. It is just a very difficult task to maintain a focus on organizational and personal improvement rather than using postmortems to find scapegoats or deflect culpability.</p>
			<p>Used correctly, postmortems are a powerful tool. Used incorrectly, they are a waste of time or potentially worse. Even if the organization lacks the capability of performing good postmortems, do so just within IT. If IT itself has politics that make this impossible, do so just within systems administration. Even if no one else participates, do it for yourself.</p>
			<p>Document your post-mortems. People have a tendency to remember disasters negatively and to emotionally assign fault where none existed or where it is not deserved. Keep post-mortem documentation on hand as it is often useful for defending people, teams, or processes later. Good documentation, even after the fact, is a powerful tool. </p>
			<p>Remember that a post-mortem does not just need to ask <em class="italic">could we have avoided this disaster</em> but also <em class="italic">even if we could have, should we have?</em> Often the cost of avoiding disaster is greater than the risk cost of having the disaster. A post-mortem should cover the decision-making process, the decision itself, as well as the response to the disaster.</p>
			<p>I hope that the ideas and concepts in this chapter will help you to break out of the mindset of traditional roles and to tear apart <em class="italic">the box</em> and let your approach to disaster recovery reflect the best of what you and your organization can muster. </p>
			<p>There is an obvious lack of discussion around Docker and other modern container technologies for a book on Linux. This is not an accident, it's actually by design. The reason for this is that Docker and its kin are application container technologies that leverage other technologies that we have already addressed, and their practices are their own concern. At the system administration level, application containers are simply another workload - one that happens to use Type-C virtualization and manage its own dependencies and updates. Docker or other application container management is beyond the scope of this book as well as general system administration.</p>
			<p>In most cases, the system administrator is responsible for managing these technologies, but they're not special cases. Workloads are the same as we have discussed throughout this book. Even though they may have their own names, their own mechanisms, and their own management tools, all of these things are still governed by general case guidelines and rules that we should already know as system administrators. You will need to learn, if you're going to work with these technologies, and you very likely will if you're a system administrator today, many things that pertain specifically to the application container platform and management tools that you'll be using and apply that knowledge to what we've already learned.</p>
			<p>Best practices focus on learning the general cases, the rules that always apply, then figuring out how different techniques, technologies, and products are covered by the general cases.</p>
		</div>
	</div></body></html>