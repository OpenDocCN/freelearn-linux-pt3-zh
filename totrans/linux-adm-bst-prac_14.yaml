- en: '*Chapter 11*: Troubleshooting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Few things are as challenging in systems administration as **troubleshooting**
    when problems have risen. Troubleshooting is hard at the best of times, but as
    system administrators our job is almost always to troubleshoot a system that is
    either currently running in production and has to remain functional while we attempt
    to fix some aspect of it or is currently down and we have to get it back up and
    running in production as quickly as possible. The ability to work at a reasonable
    pace without the business losing money actively as we do so typically does not
    exist for us or when it does, is the exception rather than the rule. Troubleshooting
    is hard, critical, and stressful.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting involves more than just fixing an obvious technical problem,
    applying business logic is critical as well. We have to understand our troubleshooting
    in the greater context of the workload and the business and apply more than simple
    technical know-how. There is fixing a problem, and there is fixing the workload,
    and there is evaluating the needs of the workflow, and in the end there is maintaining
    the viability of the business.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The high cost of disaster avoidance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triage skills and staff
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logical approaches to troubleshooting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigating versus fixing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The high cost of disaster avoidance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we are going to talk extensively about what to do after there
    has been a disaster. Throughout this book we consider ways to avoid disaster.
    Something that is easy to overlook is that there is a cost to protecting our workloads
    against failures and that we have to weigh that against the cost of the failure
    itself combined with the likeliness that that disaster will even happen.
  prefs: []
  type: TYPE_NORMAL
- en: Too often we are told, or it is implied that disasters are to be avoided at
    all costs. This is crazy and should never be the case. Disaster avoidance has
    a cost, and that cost can be quite high. The disaster itself will have a cost
    and while that cost might be quite high, it is not always.
  prefs: []
  type: TYPE_NORMAL
- en: 'The risk that we take is that the cost of avoiding a disaster is sometimes
    greater than the cost of the disaster itself. There was a time when it was common
    for companies to spend tens of thousands of dollars on fault tolerant solutions
    to protect workloads whose common failure scenarios would only cause a fraction
    of that cost in losses. The disaster was literally less of a disaster than the
    disaster avoidance was! And the disaster avoidance is a certain cost, a disaster
    is only a potential cost. If we equate both to costs we could simplify the evaluation
    to a simple question such as this: *Is it better to lose $50,000 today, or to
    maybe lose $10,000 tomorrow?* That makes it far easier than it would otherwise
    be and removes most emotional response.'
  prefs: []
  type: TYPE_NORMAL
- en: The phrase I like to use about overspending on disaster avoidance is that it
    is like shooting yourself in the face today, to avoid maybe getting a headache
    tomorrow.
  prefs: []
  type: TYPE_NORMAL
- en: Never treat the disaster in a disaster prevention plan as a certainty, it is
    not. There is only a possibility that it will happen. Evaluate and use math.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we are going to look more deeply into how support should work
    in a business, improving support posture for your IT organization, learn about
    triage needs, discuss finding the right people for your troubleshooting team,
    and then delve into when to investigate, and when to fix our issues.
  prefs: []
  type: TYPE_NORMAL
- en: This is some of the hardest, most ambiguous, and ultimately most important material
    for us to cover.
  prefs: []
  type: TYPE_NORMAL
- en: Sources of solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Where do we get the solutions to problems that arise when we are system administrators?
    I want to start this conversation with my own career anecdote, because I think
    that everyone gets very different perspectives on how IT support works in the
    broadest of senses and understanding different perspectives is important before
    we start to define what good looks like.
  prefs: []
  type: TYPE_NORMAL
- en: When I first started working in IT, and for the first nearly two decades, it
    was an assumption that any and all issues would be resolved by the IT department.
    Of course, situations existed where applying patches, updated, or fixes from a
    vendor would be part of the process, but acquiring those patches, testing them,
    applying them, and so forth were always completely handled by the IT staff. Even
    the idea that you could ask a vendor to assist, guide, or advise was foreign let
    alone attempting to actually do so. Reaching out to a vendor for support was assumed
    to be an absolutely last resort situation and reserved only for those moments
    when it was believed that there was an unknown and as of yet unaddressed flaw
    in the hardware or software that had been identified and that it would be passed
    over to the vendor to fix that before handing it back to IT to apply said updates
    or fixes, if they ever became available.
  prefs: []
  type: TYPE_NORMAL
- en: During this era IT, and especially system administrators, were expected to know
    systems inside and out, be able to address reasonably any issue that could arise,
    and quite frankly figure out what needed to be done. No ifs, no ands, and certainly
    no buts. If you did not know how a system worked or what might be wrong you were
    expected to use knowledge and logic and get to the bottom of it. A thorough understanding
    of how systems worked, even if specific details were sometimes lacking, and good
    logic essentially always would allow you to resolve an issue.
  prefs: []
  type: TYPE_NORMAL
- en: It was not until the very end of the 2000s that I first encountered IT shops
    that would rely on vendors and resellers for some aspect of their support. The
    idea that systems were being run that the company and its IT organization (internal
    or external, does not matter) could not install, configure, and support was totally
    foreign to me and to most people I had been in the industry with for years. If
    you required support from the vendor sometimes, how did you not need it all of
    the time? What purpose was the IT team fulfilling if they were not the ones who
    possessed the requisite knowledge to implement and operate the systems that were
    under their purview? You need far more knowledge to plan and consider all solution
    options than it takes to fix the singular one that you finally deploy. If you
    need your vendor for day-to-day tasks, then you obviously lack the necessary skill
    sets and experience for the more critical high level decision making and that
    is something that a vendor cannot help with. Sadly, many organizations end up
    simply lacking all capability around solution planning and this explains why so
    many horrendous solutions that should obviously have been known to not meet the
    business needs get deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Today is seems that the world is focused on support coming from sales organizations
    and vendors, rather than from IT, but this creates two critical problems. First,
    what is IT even there to do if they are not in the critical support path. Are
    they even needed? And second, how is a vendor supposed to have the range of skills
    necessary to do internal IT needs when all they do is support products that the
    make? The disconnect here is significant.
  prefs: []
  type: TYPE_NORMAL
- en: There is no magic support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There tends to be this sort of unspoken belief, often amongst managers but sometimes
    even from IT people, that there is a group of magic companies out there, in the
    technological arena that can provide more or less unlimited support in a way that
    internal IT cannot. It is imagined, I suppose, that these companies are not comprised
    of human beings or perhaps it is thought that the vendors of servers, storage,
    and operating systems have secret manuals full of information not released to
    the public that include secret codes that tell misbehaving systems to start working
    again.
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, hardware and software vendors know essentially nothing that
    their customers do not know, at least not when it comes to the operations of their
    devices. It is no different than if you called up Ford or Toyota and asked them
    how to drive a race car, instead of asking race car drivers. Sure, the car companies
    will have some people on staff with a good idea of how a car will be driven under
    performance conditions, but none of them can possess more knowledge than actual
    race car drivers and they certainly will not have as much expertise on the ground
    as the driver currently going around the track.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware and software vendors are just groups of people, made up from and hired
    from the same pool of IT talent that any other firm has access to, in fact it
    is likely if you have worked in the industry for any length of time that someone
    you know will go from working in the field to working for a vendor, or vice versa,
    and there is a decent chance that this will happen to you yourself. I myself have
    been an engineer at at least half a dozen of the big ten vendors in the industry
    and at none of them was there any special sauce knowledge doled out in secret.
    All of our customer support or even internal support knowledge was acquired the
    same way that it was by customers. Sometimes customers actually had more access
    to our own documentation than we did!
  prefs: []
  type: TYPE_NORMAL
- en: If vendors knew how to make their products work so much better than they were
    working in the field, they would do anything that they could to get that information
    out there. Almost all major vendors have extensive documentation, training, certification,
    and other programs in the hopes that IT workers in the field will be able to do
    work on their own without anything going wrong at all and being able to fix it
    themselves whenever possible. It is not in their interest to have people say that
    the products do not work properly and that the vendor has to step in to fix things.
    The vendors desperately want IT to be able to properly deploy, configure, and
    maintain in the field without vendor involvement. That looks good for marketing
    and it generates the maximum profits. It also makes for the best relationship
    with IT which, by and large, is the biggest promoter and the biggest gatekeeper
    to hardware and software being purchased.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps there is a mistaken belief that the best of the best IT staff will automatically
    be gobbled up by vendors leaving the rest of the field with only those that could
    not cut it. That might sound plausible, but it is anything but reality. First,
    few vendors have the deep pockets that people imagine and there are often customers
    far more willing and able to pay and attract the top tier talent. Second, vendor
    support work is often very different from normal IT work and few people drawn
    into IT in general enjoy it as much simply because the technical and business
    aspects are different, while highly related the two jobs differ in significant
    ways so there is no automatic flow back and forth. Third, extremely little support
    from any vendor is as technical as an IT role would be. Vendor roles tend to involve
    sales and account management, following detailed rules and scripts, and implementing
    things in a pre-defined way designed not to accommodate IT processes, but sales
    ones. Bottom line, the best IT people rarely want to end up in vendor jobs and
    those that do tend to do so not because the vendor jobs are better for their IT
    aspirations, but because they saw an opportunity to leverage their expertise while
    leaving IT as a field. Many vendors have no real IT support at all.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that vendors make products, they do not *do IT*. So, what a vendor
    is typically prepared to help support is quite different than what an IT department
    should be doing for their organizations. Even when they have great IT resources
    on staff, those resources are unlikely to be allowed to provide true IT support
    to a customer. Vendors typically only want to, and often only can, provide support
    within the very tight confines of *operating their product as intended*. It is
    not uncommon at all for IT organizations to know as much or more about vendor
    products and how to implement them best than the vendor does. The vendor simply
    does not have any reason to have that knowledge. A hammer maker is not likely
    to know as much about driving nails using their hammers as carpenters do. Engineers
    at Toyota are unlikely to be able to drive their own cars as well as professional,
    full time race car drivers. Canonical is unlikely to know the best way to deploy
    Ubuntu in your organization compared to your internal system administrators.
  prefs: []
  type: TYPE_NORMAL
- en: The skills of vendors, mainly hardware engineering or software engineer (or
    both) are different than IT skills and even the most amazing of vendors have little
    reason to be especially good at IT tasks. It just is not their wheelhouse, why
    would they have those skills, they are not a part of their business. That is our
    business for those of us in IT.
  prefs: []
  type: TYPE_NORMAL
- en: What vendors should be good at, and generally are, is in knowing their products.
    They know when there is a flaw to be fixed, they know how companies are tending
    to use their products in most cases, they know what changes are coming in the
    future (but may not be free to disclose this to customers.) The vendor is a valuable
    resource, but only if kept in a logical context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because vendors do not sell IT, but rather products, it makes no sense for
    a vendor to maintain highly skilled IT resources on any scale, if at all. The
    idea that they would even be able to do what any internal IT can do is pretty
    absurd, it just does not make any sense. It is most common for vendors to employ
    mid-career and junior resources when they do offer some amount of IT assistance
    because there is simply no value to having more expensive resources on staff.
    Customers coming to a vendor to attempt to get IT resources cannot be treating
    IT as a priority or logically and therefore there is no reason to provide them
    expensive resources that they are not prepared to leverage: the profits on selling
    lower cost services are simply far greater. And customers who are sophisticated
    enough to need high end IT resources would know not to engage a vendor for that
    need.'
  prefs: []
  type: TYPE_NORMAL
- en: So, there is no magic. Vendors do not know things that we do not know. Generally,
    they know far less, at least of what is important in our environment. They have
    little to no access to the necessary business knowledge to make reasonable IT
    decisions. They are not at liberty to approach problems or solutions with the
    breadth of skill and products across the industry but only the products that they
    provide, complimentary products, and processes that encourage greater use of their
    products. They have no alignment with the values of IT and are financially encouraged
    to work in their interest, not yours. There are no shortcuts, the level of support
    that should be available from IT resources is second to none, no other organization
    has the knowledge and mandate to support your organization the way that your IT
    (which should almost always include external IT resources from non-vendors) does.
  prefs: []
  type: TYPE_NORMAL
- en: There are essentially three ways that we can get support for our systems. First,
    we have internal IT staff. Presumably given the context of this book, that is
    the systems administration person or team. Second, we have external IT resources
    from paid IT firms that provide external IT, rather than sales, resources and
    get paid to be an extension of the internal IT team either ad hoc or perhaps all
    of the time. And then third, there are vendors and value added resellers (vendor
    representatives.)
  prefs: []
  type: TYPE_NORMAL
- en: We have to remember that IT is not a special case and the sellers of products
    are not our business advisors. Just as sellers' agents in real estate cannot represent
    buyers, and buyers' agents cannot represent sellers we have the same conflict
    of interest and opposing representation in IT. The internal IT department and
    any external IT service providers are paid for representing the needs of the business
    and are legally, as well as ethically, required to do so. On the other side vendors
    and resellers are paid through sales profits or commissions and are financially
    renumerated for representing the needs of the vendor and are legally required
    to do so and ethically they are bound to the vendors, not to the customer. That
    does not mean that a vendor and reseller cannot be friendly, useful, important,
    or professional, of course they can be and should be. It simply means that they
    are representatives of product sellers and we, as IT professionals, are representatives
    of our constituent businesses.
  prefs: []
  type: TYPE_NORMAL
- en: We can work together, and we can do so best by understanding each others roles
    and obligations. In other walks of life we rarely feel that sales people or product
    representatives are looking out for our best interests rather than trying to promote
    their wares, yet in IT this is a common point of confusion.
  prefs: []
  type: TYPE_NORMAL
- en: Of course vendors as well as their reseller representatives (correctly called
    VARs but often presenting themselves as MSPs, but be careful not to confuse a
    true IT service firm with a reseller just hoping that their customers do not question
    the name) can be valuable allies and we should not completely discount them. They
    can be part of our solution process, especially when it comes to getting access
    to special tools, beta components, patches, release information, bug fixes, replacement
    hardware, and other components that come from the engineering group, rather than
    aspects handled by IT.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing what IT handles and what engineering handles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even as IT professionals it is sometimes confusing to understand what falls
    under IT and what falls under the engineering (of a vendor or a software engineering
    department.) But the answers here should be quite simple.
  prefs: []
  type: TYPE_NORMAL
- en: As IT professionals, we use products made by others and assemble those products
    into complete solutions. We do not make Linux, we do not create a new database
    engine, we do not write the applications that our businesses run. We also do not
    form sheet metal, run chip fabs, or otherwise build computer hardware components,
    but we might buy parts and assemble them into a computer in rare circumstances
    (like how a mechanic might assemble car parts, but not actually pour metal to
    make them.)
  prefs: []
  type: TYPE_NORMAL
- en: Vendors are responsible for making the tools and products that IT is responsible
    for then using and operating. Vendors write the operating systems, we in IT install
    them. Vendors make the applications, we install them. Vendors are involved in
    making products, not solutions. IT solves organizational challenges, it does not
    make products.
  prefs: []
  type: TYPE_NORMAL
- en: If we were talking cars, perhaps it is more clear. Car vendors build cars. Customers
    ride in cars. The two obviously have an important relationship with each other,
    but it is pretty obvious that designing and building cars is a very different
    task than plotting a course and driving to a destination. We can obviously see
    the vendor making tools for us to use, and we are car buyers using those tools
    to solve transportation challenges. Apply this logic to IT and voila.
  prefs: []
  type: TYPE_NORMAL
- en: Real support, the most important support, is always going to come from our own
    IT team (which always includes external IT staff as well.) Our own IT staff not
    only has the broad range of business knowledge necessary to make key support decisions,
    but also has the range of potential solutions to work around vendor limitations.
    It is common that solutions are bigger than can be addressed by a single product
    vendor in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: To give an analogy, if you were a logistics firm and you needed to get a shipping
    container from New York to Los Angeles and your truck broke down, of course you
    might ask the truck manufacturer for information on repairing the truck, but you
    would not stop there. You would look at replacement trucks, renting a truck, other
    truck vendors, consider the cost of using another logistics partner to ship on
    your behalf while you are down, or consider switching to rail or sea transport!
    Of all of those things, only repairing or maybe replacing your initial truck is
    within the potential scope of support of the truck manufacturer, and even repairing
    it from them is much more limited in scope than you would likely get with a mechanic.
    A skilled mechanic might be able to propose partial functionality, third party
    parts, or alternative fixes that are not possible from, or approved by, the original
    vendor. The original vendor has value here, important value, but only a tiny fraction
    of the value that the overall department would have.
  prefs: []
  type: TYPE_NORMAL
- en: It takes a much broader scope to properly deal with most disasters. Rarely do
    we want to just sit on our hands waiting for a vendor to determine if an issue
    belongs to them or not, and then decide if you have valid support or not, and
    then decide how they are going to deal with it. Even a great vendor, with great
    support has their hands dramatically tied compared to what IT staff should be
    doing. IT has, or should have, the scope to do whatever is necessary to protect
    the company. That might involve engaging the preexisting vendor or it might involve
    working around a vendor, or perhaps it just involves coordinating multiple vendors.
    Even when a vendor does need to be involved, IT should be overseeing that vendor.
  prefs: []
  type: TYPE_NORMAL
- en: IT vendor managements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More than any other IT department, system administration often has to interact
    with vendors and has a greater level of need to oversee them directly. At the
    same time, system administration is also the department most likely to not have
    any vendors, at least not in the traditional sense.
  prefs: []
  type: TYPE_NORMAL
- en: Vendors should not be thought of as a department, but rather more like a specific
    tool. A tool that needs to be overseen and used when appropriate, but at the direction
    and discretion of IT. A vendor on their own lack's direction and control.
  prefs: []
  type: TYPE_NORMAL
- en: Managing the vendors for the hardware, software, and services of an IT department
    should be an every day task of that department. The vendor relationship at that
    level is important as this is the level at which technical know-how should be
    exchanged. For us, as Linux system administrators, this means direct contact with
    our operating system vendor counterparts at vendors such as IBM Red Hat and Canonical
    who can keep us apprised of patches, upcoming changes, release dates, security
    alerts, and so forth and, in some cases, may be focused technical resources for
    us to lean on.
  prefs: []
  type: TYPE_NORMAL
- en: Systems administration may have many other types of vendors as well. We may
    have to work with server hardware, storage, database, and even application vendors
    at times. That there are so many potential vendors highlights how critical IT
    management of the vendors are. Without IT oversight, there is no coordination
    between those vendors and no mandate for them to collaborate or to work towards
    a common goal. The mandate to work towards the good of the business lies solely
    with IT in this case. It is for IT and IT alone to ensure that the vendor resources
    at its disposal are used for the good of the business when appropriate, rather
    than being sales efforts for the vendors.
  prefs: []
  type: TYPE_NORMAL
- en: The best practice is that support should come from within. Fundamentally, at
    the core of it all, we should see our IT team (inclusive of internal staff and
    external staff) as our solutions team both to design our solutions up front and
    to deal with them when something goes wrong. When vendors are required to be part
    of the solution process they should be engaged, managed, and overseen by the IT
    team and managed as just another resource.
  prefs: []
  type: TYPE_NORMAL
- en: Triage skills and staff
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most companies fail dramatically when disaster strikes because triage processes
    either do not exist or are too poor. The skills to run the business day to day
    are different than the decision-making processes needed in real time in a crisis:
    there is little time for meetings, almost no ability to consult with different
    parties, and planning is out of the question. When in this mode we need someone
    leading who is trusted, handles stress well, and is a perceiver rather than a
    planner - someone who thrives running with rapid decision making and does not
    need to have planned their events ahead of time. Planning is excellent and as
    much as is reasonable should be done ahead of time, but everyone involved from
    junior IT staff to senior executive staff should understand that true emergencies
    cannot be adequately planned for, and real life will involve many unknowns that
    have to be evaluated on the fly.'
  prefs: []
  type: TYPE_NORMAL
- en: Our first process when there is a disaster is to head into triage mode. We need
    to know what exactly is not working, what has happened, what is the impact - basically
    we need to know the status, of everything. Is this something we think that we
    can fix in minutes? Is this going to require some investigation? How are people
    impacted? Are we losing money, productivity, customers?
  prefs: []
  type: TYPE_NORMAL
- en: There are so many ways that we can be impacted by a disaster. Being able to
    quickly get a grasp of the business effects, how does each department play into
    the big picture, which teams can work just fine, which teams are dead in the water,
    are there teams that are functional but limping, and so forth is absolutely critical
    and can mean all of the difference between everyone just standing around being
    unsure what to do and a triage manager getting things fixed right away. We need
    status, and we need a lot of it, very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases a surprisingly small amount of our time is actually focused on
    solving a technical problem. This may be because it is simply a hardware failure
    and we just have to wait for replacement hardware to arrive. Or maybe it is complete
    software failure and we just have to rebuild our systems. Sometimes deep technical
    investigation has value, and sometimes it requires a lot of know-how to the cause
    of an issue, but this is not the majority case. We are much more likely to have
    a relatively quick fix, or at least quick in terms of the amount of administrator
    time is necessary to use during the process. When fixes take a long time, it is
    most typically because there is a third party that needs to be waited on.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, we are going to mentally focus on the technical aspects of an outage.
    Other aspects of most outages are more important. Some organizations have operational
    triage experts who step in and handle these aspects of an outage allowing us as
    system administrators to focus purely on the technical aspects under our auspices.
    For most businesses, though, dealing with an IT disaster is going to require IT
    oversight from beginning to end. In the majority of cases, the very team that
    we would hope would step in to assist IT in solving issues and managing the triage
    of the operational environment gets in the way of finding solutions rather than
    being part of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: I can give status, or I can fix things
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Everyone who has ever dealt with any kind of outage, disaster, or what have
    you impacting a business knows that the expected behavior is not for executives
    and management to jump in and start to find ways to protect the business or to
    run interference to assist you in dealing with the problem that you are suddenly
    tasked with; but instead the very people we depend on to create an environment
    to minimize impact and to make us effective almost universally turn on us and
    begin demanding explanations, status reports, updates, estimates, promises, and
    miracles all of which are pointless at best and completely undermine the business
    at worst.
  prefs: []
  type: TYPE_NORMAL
- en: Even under ideal conditions reports, status, updates, measurements and the like
    have a cost and come at the expense, even if only a small degree, from productivity.
    During a disaster, it is rare for anyone except those in the most critical positions
    of attempting to mitigate and fix the disaster to have any ability to provide
    status. So, the most expected event is that everyone will descend on those few,
    critical positions and demand status updates.
  prefs: []
  type: TYPE_NORMAL
- en: There are several problems here that are all really obvious to us, working in
    these positions, but often lost on those who are now experiencing an impact to
    their productivity. We have two tools at our disposal to help with this. One,
    show this book to those in management and ask them to take the time to understand
    the situation. The second is planning. Make sure that as a part of your disaster
    planning and preparation process that there is training for management, and a
    policies and procedural plan, as to how status will be given, who may ask for
    it, and from whom. Consider designating and official spokesperson for IT (and
    other departments) who can spend all of their time giving updates as they are
    not involved in any other aspect of the disaster recovery efforts. Perhaps they
    will run a war room in person, or maintain an email messaging group, they could
    manage a chat room, or head a conference call that others can dial into as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Then any updates that exist can be passed, proactively, to this mandated reporter(s)
    and they can maintain that status for the organization. The entire organization
    should understand the critical nature of having a mandated point of reporting
    so that the team actually attempting to solve the issues and get the company back
    to full functionality can spend their time saving the company rather than reporting
    on its failures. Obviously the business has some business needs to know as much
    as possible about what is happening. More impactful is political problems internally
    as managers feel that they, too, have to provide status that they cannot have
    and many layers of organizations will have people acting emotionally and potentially
    willing to cause significant financial damage in the hopes of appearing to be
    concerned or just to satisfy their desire to know more than anyone can really
    know.
  prefs: []
  type: TYPE_NORMAL
- en: When training management as to why IT (and other departments) cannot provide
    extensive updates we need them to understand why we, as the people attempting
    to remediate the issue, cannot be spending time giving status updates.
  prefs: []
  type: TYPE_NORMAL
- en: We don't have any information to give. Fundamentally, this is the biggest piece
    of the puzzle. While we might have a simple answer like *the replacement part
    is scheduled to get here tomorrow* typically we know nothing about how long something
    is going to take to fix. Most things in IT are fixed essentially the moment that
    we know what the issue is. Until it is completely fixed we normally are only working
    from hypothesis. Pushing us for information really just ends up being analogous
    to demanding that we feed intentionally false information because have nothing
    else to provide. It is like torturing a prisoner that does not know any information,
    but if you torture them they are likely to just make something up in the hopes
    of the torture stopping.
  prefs: []
  type: TYPE_NORMAL
- en: We are busy. If things have not already been resolved, chances are we are completely
    engaged in trying to get them resolved. All of the time used to give status has
    to come from the time working on resolving the issue. It is more than simply wasting
    time, it is also causing interruptions and demoralizing those trying to resolve
    things. It sends a huge organizational message that the issue does not matter
    very much and that the efforts to get it fixed are not appreciated as they should
    be. It makes IT wonder *if management is not prioritizing getting things fixed,
    why would we?*
  prefs: []
  type: TYPE_NORMAL
- en: '*Political Risk*. In attempting to get everyone who wants to be able to *plan*
    a disaster information to work from those working to resolve the issue are generally
    in a very tough position of having to guess quite heavily as to when things will
    be finally resolved. Most organizations handle this uncertainty, which cannot
    be helped, quite negatively. Intentional bad information is often rewarded, honesty
    is often punished. Putting your information providers into a position of potentially
    having to *just tell people what they want to hear* or *providing inaccurate information
    for political protection* means that the business may then operate with bad data
    causing unnecessary additional financial loss. It is a terrible time to be pushing
    for bad data over no data, yet it is when it is most likely to happen.'
  prefs: []
  type: TYPE_NORMAL
- en: Priorities. If the organization starts to prioritize, from management, the perception
    that reports, status meetings, calls, and other things that do nothing to resolve
    the issue to get the company back running at full speed are more important than
    finding a solution, this will naturally, and absolutely should, change how IT
    or any other department tackles the problem. If any issue is so trivial that we
    have meetings to discuss timelines instead of fixing the system, then overtime,
    rescheduling family events, even skipping lunch all become absurd IT sacrifices
    that obviously have no value to the company. We would never do any of those things
    just for some meeting, and if that meeting is more important than finding a solution,
    we have a relative value assessment that gives us a lot to work with.
  prefs: []
  type: TYPE_NORMAL
- en: So how should management act? Management needs to do all the opposite. If status
    even matters, and we understand that it generally does, then have someone that
    is not involved in the remediation handle those communications. Keep priorities
    clear. Assign teams to run interference and keep all interruptions away from the
    team attempting to fix broken systems. Have people bring them food, drinks, coffee,
    run errands, whatever is needed - show that they matter, a lot, instead of suggesting
    that they do not matter. Do not punish messengers for delivering bad news, reward
    them for honesty. Empathize and think about the best results for the business.
  prefs: []
  type: TYPE_NORMAL
- en: All things that are easy to say and hard to do when disaster has struck. Plan
    ahead, have these discussions, make a plan, get executive sign off before things
    happen. Have an action plan to put in place that says who is in charge, how things
    happen, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Our first stage of triage is an assessment. Do we have a plan to get back online
    quickly? We need to know the situation as it stands, and we need to then relay
    that information somehow to management. From here, things get tricky. There are
    so many variables that teaching triage is anything but easy. Someone who excels
    at triage needs to be able to take the situation as it is and gauge a range of
    issues from ways to fix the existing problem, potential options to work around
    it, and in many cases, how to modify the organization to best react to it.
  prefs: []
  type: TYPE_NORMAL
- en: This is very much a *thinking outside of the box* scenario. We need, at this
    point, to look at the big picture and figure out how to best keep the finances
    of the business running. This might seem like a management task, and again, ideally
    it is, but IT should play a role as we have certain types of insight that might
    be lacking elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Outside of technical fixes, mitigation strategies will vary broadly based on
    the type of business, type of impact, and so on. Should we send everyone to get
    coffee? Maybe plan for a long lunch? Get everyone home on vacation now to save
    on insurance because it is going to be a while before anyone is productive again?
    Perhaps moving people to paper or from email to instant messaging? Use downtime
    from one type of task to focus on others. Perhaps we do a deep office cleaning
    while people have the time - unplug those cables and really get the place clean.
  prefs: []
  type: TYPE_NORMAL
- en: Big emergencies can present big opportunities as well. I have seen several times
    when companies have used catastrophic outages as chances to enact sweeping updates
    and changes that would require big time approval or large downtimes normally,
    but which could be slipped in during an outage that is happening anyway. I once
    even had an ISP based outage that was predicted to last so long that a team ran
    from New York to Washington, D.C. with a truck, put a rack of servers into the
    truck, and ran to a new location that was waiting and ready in New York and pulled
    of a datacenter migration that had been *indefinitely on hold* because of the
    necessary downtime and was able to bring workloads back online from New York before
    the ISP was able to restore service in Washington, D.C. A somewhat minor outage
    was turned into a huge *win* by the department. A large project that was struggling
    to get scheduled and approved was pulled off purely as a bonus while the team
    was able to simultaneously enact a significant *fix* by going to an alternative
    datacenter to overcome the limitations caused by the ISPs leased line at the original
    location.
  prefs: []
  type: TYPE_NORMAL
- en: '**Triage** is hard because it requires that we be creative, open to alternative
    ideas, able to avoid panic, think broadly and outside of the box, and do so with
    little planning or preparation. If your organization does not have someone suited
    to this role, and relatively few do, then this is something you should be outsourcing,
    but your system administrator is one of the most likely candidates for it as the
    skillsets and aptitude of administration tend strongly towards triage and disaster
    recovery as compared to engineering and planning skillsets.'
  prefs: []
  type: TYPE_NORMAL
- en: In many cases outages result in far more than a single workload being inaccessible
    and prioritization within the technology space is also required. Your triage person
    or team needs a deep understanding of how workloads interrelate to one another,
    which ones depend on others, what can be worked around, what can be skipped, and
    how all of these workloads relate to the business itself. Only by knowing the
    scope of the technology as well as the business can anyone provide valuable insight
    into what to fix, in what order, and how the business can potentially work around
    things.
  prefs: []
  type: TYPE_NORMAL
- en: I wish that we were able to provide concrete guidance as to how best to survive
    a disaster, but we cannot. Disasters come in so many shapes and sizes, and the
    ways that we can deal with them are more numerous still. You are best served by
    learning how to think, how to react, and being as prepared as possible for any
    eventuality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Staffing for triage: The perceiver'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I had the great benefit of once working for a large IT department where deep
    psychological analysis was part and parcel in the general managerial processes.
    This might sound like a terrible thing, but the approach was excellent and the
    company used proper psychoanalysis to learn how people work, how they would work
    together, who was expected to be strong or weak in different areas, and how best
    to combine people to achieve the best results.
  prefs: []
  type: TYPE_NORMAL
- en: I do not want to go in to detail in all of the ways that these techniques were
    or could be helpful, but one specific thing that I want to touch on is the idea
    of the Judger and Perceiver scale of the Myers-Briggs test. I am not a psychologist,
    so I recommend that you research the test and its interpretation on your own,
    and understand that like with all psychology it is both greatly accepted as well
    as heavily disputed as to its efficacy. I will not argue for or against here,
    but only say that understanding the judging to perceiving preference pair is highly
    valuable.
  prefs: []
  type: TYPE_NORMAL
- en: I generally describe the Judge as a *planner*, one who likes to organize and
    put things in their place before an event occurs. The Perceiver is more a *reactor*
    or a *responder*, someone who wants to take the world as it comes and react to
    it in the moment.
  prefs: []
  type: TYPE_NORMAL
- en: In our world, engineers and most managers are judgers. Their role, their value
    comes from thinking ahead and organizing the business or the technology to do
    what it needs to do. Perceivers tend to excel at being administrators, rather
    than engineers, and are exactly who you need to have at the ready when there is
    a disaster. Your perceiver personalities are your natural candidates for those
    who are likely to be good at handling triage operations and thinking on their
    feet. Humanity is naturally diverse to complement one another to handle multiple
    aspects of life and this is a great example of that.
  prefs: []
  type: TYPE_NORMAL
- en: There is far more to being the right person or team member for the job than
    just fitting an aptitude on a personality test. The Myers-Briggs assessment is
    simply a tool for identifying who might be strong or weak for different positions,
    and for explaining how people tend to think and feel. For me, discovering that
    I was a strong perceiver and a weak Judger was influential in helping me to understand
    myself and how to communicate things about me to other people. It also gave me
    tools to help me to understand other people in my life so that I could communicate
    better and set expectations better when they operate differently from me.
  prefs: []
  type: TYPE_NORMAL
- en: Whether your company uses a formal process, or you just take an online survey
    to learn about yourself, I recommend the Myers-Briggs and similar tools for simply
    helping you to understand yourself better, if nothing else. The better you know
    yourself, the better you can be prepared to succeed where you are strong and to
    ask for help where you are weak. If you are a team leader or manager, this kind
    of information can be useful in helping to understand your team better and how
    they can work together to be stronger.
  prefs: []
  type: TYPE_NORMAL
- en: Do not be tempted to read too much into psychology tools or try to apply them
    too broadly. By and large these tools are best when applied to yourself and when
    you approach them openly and honestly with a desire to learn not about your own
    strengths, but about your weaknesses and use them for self-improvement. Remember
    that a test of this nature is not about comparative results, one result is not
    better or worse than another; all people live on scales and neither end of the
    scale, nor the middle, is a good or bad result. Strong teams, however, are generally
    built from a variety of different combinations of aptitudes and personalities
    to cover many different needs.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that I have just filled you with confidence and ideas that you will use
    to take your triage process to the next level rather than causing a panic attack
    about the complexities and uncertainties of dealing with disasters - that should
    not be the takeaway here. Use any panic that you are feeling now as motivation
    to immediately start your planning and documentation, and to kick off conversations
    with management to get stakeholder buy in now. Make it a priority and you can
    quickly move from being unprepared to being at the forefront of businesses ready
    to respond best to almost any circumstance.
  prefs: []
  type: TYPE_NORMAL
- en: Do not feel that the person who has to handle triage has to be you. Maybe you
    are the best person for the job, maybe you are not. Very few people have the right
    personality and triage is a very special aptitude to have. What is important is
    identifying your triage person or team, whoever they are. Everyone has a role
    to play, find your place, and find the right people for the roles you have to
    fill.
  prefs: []
  type: TYPE_NORMAL
- en: Our best practice in regards to staffing is to identify your triage people before
    there is a disaster and have them documented and in place to take over, and empowered
    to take over, when the time comes. Do not let the decision process of finding
    someone with a triage aptitude wait until the clock is ticking on your downtime,
    and do not let politics become the focus, rather than solutions, when time is
    of the essence.
  prefs: []
  type: TYPE_NORMAL
- en: Logical approaches to troubleshooting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Possibly the hardest thing that we have to do as system administrators is troubleshoot
    problems. It is one thing to be able to deploy a system initially, but a very
    different thing to be able to troubleshoot it when things start to go wrong. With
    systems administration there are so many places where things can go wrong for
    us; we sit at the nexus of so many technologies and so many possibilities that
    tracking down the source of issues can be very challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Not surprisingly, experience makes this far easier than anything. The more you
    get experienced with maintaining and managing systems the more likely you are
    to be able to quickly *feel* your way around a system and often just sense what
    might be wrong when things get tricky. Nothing really trumps just knowing how
    a system will react when things are healthy and being able to sense what is wrong
    based on its behavior. Senior diagnosticians are often brought in for exactly
    this reason. With enough experience often you can just feel when an index, a cache,
    a disk, or lack of RAM is the issue.
  prefs: []
  type: TYPE_NORMAL
- en: Short of sheer experience, our next best tools are a deep understanding of our
    own systems and how they interact, deep knowledge of technology fundamentals as
    they apply to our situation, and logical troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in many situations failures are going to be quick and obvious. The
    power is out, a hard drive has failed, a key database has been deleted. There
    is nothing to track down, only things to be solved. Other times, though, we get
    complicated issues that could be caused by almost anything and we may have to
    track down something truly difficult to pinpoint.
  prefs: []
  type: TYPE_NORMAL
- en: It amazes me how often I am brought in to assist with troubleshooting only to
    find that the work that has already been done is haphazard and is often redundant.
    Of course, at times, some guesses at easy to test failure points or early tests
    of known common fail points can speed discovery, but we have to be careful not
    to lose track of what we are doing and learn systematically from what we test.
  prefs: []
  type: TYPE_NORMAL
- en: Stories of troubleshooting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A benefit of being an author is getting to regale you with tales of my own historic
    troubleshooting and there is no one to roll their eyes or cut me off and you cannot
    just walk away when you are bored. So here we go.
  prefs: []
  type: TYPE_NORMAL
- en: One time I was called in to work on a system that was used for an extremely
    low latency application and the team had discovered intermittent problems with
    the application receiving responses from other systems. The issue would arise
    every so many minutes that a response would be received several nanoseconds later
    than it was expected to have arrived. Yes, nanoseconds! Nothing broke, no results
    were wrong, but there was just this tiny delay, and not very often.
  prefs: []
  type: TYPE_NORMAL
- en: After much research, we finally found the issue through a combination of research
    and system understanding. Identifying what was happening was eventually done by
    hours of staring at a top monitor and looking for processes that were active around
    the time that the delay would occur.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually the process possibilities were whittled down it was discovered that
    a memory garbage collection process that was soon thereafter discontinued in the
    kernel was using excessive system resources in its default settings and causing
    the system to halt for just a few nanoseconds while it processed memory for cleaning.
  prefs: []
  type: TYPE_NORMAL
- en: I was able to address the issue by setting the garbage collection process to
    only clean a portion of RAM on each cycle allowing it to work much faster. We
    ran into the issue only because we had so much physical RAM in the server that
    the garbage collection process took measurable time, something generally not expected.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, good research and patience were certainly important. Being able
    to *feel* the delay in the system based on the measuring tools (no human can actually
    feel a delay that short), and then using logic to determine how a process doing
    memory garbage collection could, and would, impact a process of this nature had
    to come together to make troubleshooting possible. Without a deep understanding
    of how the system runs, it would be out of the question.
  prefs: []
  type: TYPE_NORMAL
- en: When troubleshooting anything I find that there are two key techniques that
    I am telling people to use over and over again. The first is to be systematic
    and to work from one end or the other. Avoid hopping around and testing at random.
    If you are testing network connectivity, for example, start at the near end of
    the stack and start building up a base of knowledge based on testing.
  prefs: []
  type: TYPE_NORMAL
- en: In the networking example we can start with checking if our network connection
    is plumbed? Does it have an IP address? Can it ping the gateway? The ISP? A public
    IP? Is it able to resolve DNS? Can it reach the system to which it is supposed
    to connect? Can it reach the right port? Does it get a proper response?
  prefs: []
  type: TYPE_NORMAL
- en: Instead of jumping around and testing different pieces, working from the nearest
    point and exploring helps us to understand exactly when things fail, and it tells
    us quickly.
  prefs: []
  type: TYPE_NORMAL
- en: The other key technique that I always teach is *work from what you know*. Basically,
    establish your facts. There might be many things that you do not know, but you
    cannot worry about those things. The unknown will always exist. There are always
    things that you can know, though, and these we have to establish and work from.
    Use the facts that we have to build up a larger body of knowledge by finding more
    and more things that we know for certain.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you can ping a remote server then you know that you have working
    plumbing, working routing, and that all of the equipment between you and that
    remote server is all working. Or if you know that a specific database is up and
    running and working properly, then you know that its operating system is also
    up and running, and that the hypervisor that it is on is up and running, and that
    the bare metal server that that hypervisor is installed on is up and running.
  prefs: []
  type: TYPE_NORMAL
- en: It is always surprising to me how often people who are troubleshooting will,
    take the time to establish the facts, but then question them again. In the example
    above, they might decide that because they are seeing an issue that maybe the
    hypervisor has failed and go to check it again, even knowing that they just proved
    that it was still working. Or in the networking example, convincing themselves
    that they need to check on the status of a router that they just used to prove
    that networking could pass through it correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Going down the proverbial rabbit hole and making yourself (or your team) prove
    over and over again that something is or is not working that you already know
    the status of is a waste of time at best and can be extremely frustrating for
    those working with you. Once you start the pattern of reestablishing what you
    already solidly know, rather than trying to determine something new, you will
    likely continue doing so. It is very tempting to focus on those things and lose
    sight of growing the body of knowledge pertaining to the issue at hand.
  prefs: []
  type: TYPE_NORMAL
- en: I find that writing down what we know to be true, whether as a starting point
    or from investigation, is a good tool. If we feel that we have to test again something
    that we already proven, then we have a problem. Why do we not trust what we have
    already proven? Why did we feel that it was proven if we now doubt it? What is
    the point of testing if we are not going to trust the results of the testing?
  prefs: []
  type: TYPE_NORMAL
- en: If we test, prove, and then we are going to spin our wheels endlessly. This
    is a needless waste of time, time that we cannot afford during an outage. Either
    we need to approach what we consider to be fact differently, or we need to trust
    the assessment. By establishing trusted facts, we can use them to narrow down
    the possible issues.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting is hard both because it is very technical when many factors
    are probably unknown, and because it is emotional being done when there is stress
    and sometimes even panic. This is generally compounded by additional needs and
    pressure from our organizations as well. Keeping a clear head is key. Breath,
    focus, get caffeine, talk out the issue, post on technical peer review communities
    and forums, and maybe even engage the vendor. Have your resources written down
    as it is easy to forget steps when stressed.
  prefs: []
  type: TYPE_NORMAL
- en: Technical social media in problem solving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more than two decades one of my strongest resources for dealing with serious
    issues has surprisingly been technical social media. I do not mean traditional
    social media outlets, but forums built solely for technical exchange of ideas.
    When faced with design challenges or, far more importantly, broken systems that
    need to be fixed, I have found that for me and my team that posting those problems
    to a forum to be absolutely invaluable.
  prefs: []
  type: TYPE_NORMAL
- en: The reasons why this is so important are not always evident. The obvious benefit
    is that there are many seasoned professionals happy to provide a fresh set of
    eyes on your problem and may easily spot something that you have missed, or they
    may provide insights or suggest tools of which you were not aware.
  prefs: []
  type: TYPE_NORMAL
- en: The bigger benefit, however, is in the process of requesting help. More times
    than not the act itself of having to write out what is wrong, the need to express
    it clearly in writing, and documenting the steps that I have followed will reveal
    to me, even before anyone has a chance to respond to me, what might be wrong.
    Writing down my steps encourages me to also be more methodical and to think about
    what obvious questions others will ask me causing me to attempt to fill in the
    gaps, follow good processes, and document far more than I would normally do for
    myself.
  prefs: []
  type: TYPE_NORMAL
- en: I use this process of posting for public review for my own team, as well; and
    I make others do it. Of course there are details that cannot be posted publicly,
    and sometimes the entire process is too sensitive to be public at all even without
    identifiable details, but generally at least some degree of the disaster can be
    reviewed publicly for assistance. Using these kinds of forums for communicating
    amongst my own team works wonderfully and encourages the same good behavior of
    thinking through what has been done, approaching how to explain the problem differently,
    and forces clearer documentation of the troubleshooting process between team members
    because even the documentation process is being reviewed publicly in real time.
  prefs: []
  type: TYPE_NORMAL
- en: This same process then provides documentation and automatic timeline of troubleshooting
    to use for a post mortem process. I often also invite post mortem review, generally
    informally, via the same mechanism. People are always happy to critique decisions.
    You have to be prepared to accept a bit of a brutal review.
  prefs: []
  type: TYPE_NORMAL
- en: No one is a bigger proponent of the value of public peer review in focus environments
    than me. I have been championing this movement since the late 1990s and spent
    a great deal of my career working in the public eye through these communities.
    It has taught me many things that I would have never been exposed to and it has
    forced me to work differently knowing that everything that I do will be examined,
    reviewed, and questioned. Being prepared to explain every decision, to defend
    every outcome, to resort to logic and math because your arguments for or against
    a decision themselves are permanently recorded for review makes you rethink what
    you say and, I believe, pushes you to be better at everything that you do. It
    is easy to make an irrational argument when you think that no one is going to
    question you or that no one will notice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Troubleshooting best practices are simple: be methodical, document everything,
    and line up your resources before you need to rely on them.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to look at when it even makes sense to go through this process
    or if we should simply start over.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating versus fixing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we start working to deal with an outage, data loss, or other disaster,
    the natural inclination is to focus on finding a root cause, fixing that root
    cause, and then getting systems back into a working state. It makes sense, it
    is the obvious course of events, and it is emotionally satisfying to work through
    the process.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this process is that it is based on a few flawed beliefs. It
    is a method derived from things like getting your car or house repaired after
    there is damage or an accident. The underlying principle being that the object
    or system in question is very expensive to acquire and in relative terms, cheap
    to repair.
  prefs: []
  type: TYPE_NORMAL
- en: It also focuses on the value of determining why something has occurred over
    the value of getting systems up and running again. The assumption is that if something
    has happened once that it is expected to happen again and that by knowing what
    has failed and why that we will be able to avoid the almost inevitable recurring
    failures in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in IT and business systems, typically the cost to build is less than
    the cost to fix. More importantly the cost to build is more predictable than fixing.
    We should, if we have planned well and documented, be able to implement a new
    system in a known about of time with extreme reliability. Fixing an issue may
    or may not happen quickly, it represents a lot of unknowns. A fix may take a very
    long time, and the fix may not be reliable. Root cause analysis can be time consuming
    and unreliable.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases getting a system back up and running as quickly as possible carries
    great value, and while determining why a problem has occurred and finding a potential
    means of avoiding it in the future has little value. Business infrastructure experiences
    extraordinary change rates in everything from hardware to software to system design.
    A hardware failure that has happened today is unlikely to repeat in the same way.
    Software will likely be patched, updated, and modified quickly and worrying that
    old bugs will return is possible, but not a scenario worth a large amount of concern.
  prefs: []
  type: TYPE_NORMAL
- en: If we were dealing with a car, house, road, bridge, or other large object of
    this nature failures are likely to recur as the system faces small, if any, changes,
    over time. We need to determine the point of failure, determine the risk of recurrence,
    and find a way to protect against it. It is hard to separate ourselves from this
    mindset.
  prefs: []
  type: TYPE_NORMAL
- en: We have to evaluate the value to the business. What is the value of getting
    the system back up and running? What is the value to finding the root cause? We
    have to compare these values and, most of the time, we will find that solutions
    triumph over investigation.
  prefs: []
  type: TYPE_NORMAL
- en: The fix versus investigate decision gets more and more weighted towards fixing
    when we have more modern infrastructure with imaged systems, state machines, and
    infrastructure as code. The greater the quality of our automation, the faster
    and less costly it is to recreate systems and the lower the value to investigating
    an issue.
  prefs: []
  type: TYPE_NORMAL
- en: We also have to consider the possibility that with the right infrastructure
    that we can recreate a mirror system to use for diagnostics when it is deemed
    necessary. We can create an initial rebuild to get systems back up and running
    and build a mirror, if it makes sense, to use in order to attempt to recreate
    the failure and determine if there is a way or reason to protect against it in
    the future. Spending time attempting to identify the cause of and fixing an issue
    during an outage may not be the best way to accomplish that goal, even if it is
    deemed to be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: It is all cost analysis, but one that has to be done very quickly. The unknowns
    are very difficult sticking points here because the time to determine the root
    cause is completely unknown and may take minutes or days.
  prefs: []
  type: TYPE_NORMAL
- en: 'This logic of just starting over applies to desktops as well as to servers.
    Even end user workstations have every opportunity to be interchangeable and designed
    in such a way to allow for rapid redeployment. If we are using images, automated
    software installation, and other automation it is quite standard for desktops,
    laptops, or whatever we are using for end users to be able to be deployed new
    often in a matter of minutes. A fresh deployment is more than just getting these
    systems back up and running with maximum efficiency, it also provides an opportunity
    for a completely clean installation as a bonus. Any cruft, malware, corruption
    or similar that might have happened on the machine will be wiped away and the
    end user will start as fresh as if their machine had just rolled off of the assembly
    line. In this way, we get a silver lining: a fresh rebuild process that we often
    would struggle to schedule but, ideally, would be doing on a semi-regular basis
    anyway.'
  prefs: []
  type: TYPE_NORMAL
- en: Rebuildable systems, whether desktops, servers, or cloud instances, all mean
    that we only need available hardware to be able to recover and move on from most
    disasters. That also means, assuming that our backups are stored somewhere online
    or offsite, that we have the ability to walk into a new site and rebuild our entire
    company there. That flexible and level of comfort is a game changer - something
    very few companies were able to consider even just a few years ago. Knowing that
    starting over *from scratch* is always a possibility makes us think about everything
    that we do in disaster recovery completely differently.
  prefs: []
  type: TYPE_NORMAL
- en: In my own experience, even fifteen years ago, long before we had the automation
    and complexity of today's environments, we were moved almost entirely to *restore
    fast, only examine what we can after things are back online* and the ability to
    do so has only increased since then. Today we should, in almost all cases, be
    thinking of rebuilding from scratch as the default assumed starting point and
    we should only resort to more complicated forms of recovery when the situation
    demands it; and we should carefully evaluate why a situation today would demand
    it. That does not mean that rebuilds should be the only tool on our toolbelt,
    being the majority case in no way implies that they are the only correct solution,
    just that they are the most likely to be correct.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally there was a stigma to rebuilding, as if it meant that we had given
    up or were in over our heads. We have to fight this incorrect emotional response.
    The right way to recover an environment is whatever creates the best situation
    for the organization, as a whole. As with everything that we do, emotion plays
    no role here. This is a financial and risk calculation only. We do what is best
    for the company, and that is all.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate each situation, but when in down err on the side of a clean rebuild.
    No one size fits all, but rebuilding should be the better option in most cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design systems to be able to be rebuilt quickly, easily, and automatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Disaster recovery, triage, proper staffing for emergencies, organizational preparedness,
    managerial oversight of processes during disaster situations, and every other
    aspect of a critical failure scenario is hard, scary, and stressful. How companies
    decide to handle these times often determines which companies survive, and which
    ones fail. We have to have the right people in place, as many organizational processes
    and procedures as possible, great documentation, deep knowledge of our systems,
    and the flexibility to do whatever it takes to make the business successful through
    hard times to truly succeed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every company struggles with these same things. These are not simple tactics
    that we can apply overnight. It requires buy in from organizational stakeholders,
    it requires professionalism and planning not just before events transpire, but
    maintaining those processes and professionalism during times of panic when stress
    causes almost anyone to act irrationally. On one side we can view this as stressful
    and difficult, but on the other we can recognize it for what it is: a place where
    nearly all organizations struggle, most fail, and our greatest opportunity shine.'
  prefs: []
  type: TYPE_NORMAL
- en: Disaster planning and disaster recovery are easily your best chances to take
    the system administration role and grow it to something larger than the role and,
    often, larger than the IT organization itself. You cannot effectively isolate
    disaster preparedness to solely the IT department; it requires cooperation across
    all departments. Systems administration can lead, rather than follow, and make
    IT the core business unit that it always should have been.
  prefs: []
  type: TYPE_NORMAL
- en: I realize that for many, the capabilities and scope of the IT department are
    deeply mired in politics and cannot easily be made fungible. Challenge yourself
    to at least evaluate, to consider, what it would take to push your organization
    in new directions. No organization should force IT to wear the sales hat just
    to convince the organization to do what is right for itself, but here in the real
    world our ability to sell ourselves, our department, and our ideas is often the
    difference between being heralded as the savior of the company, or just ignored.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that solutions come from you and not from vendors. Keep vendors and
    their scope in mind and remember that while disasters represent a great chance
    for you to save your organization, they also represent a huge opportunity for
    vendors to find new sales opportunities at a time when fear and emotions make
    well considered planning all but nonexistent. Rethink how you view vendors, make
    the context of the support relationship utmost in your mind. Always know who represents
    your interest and who is looking for where you can best serve them.
  prefs: []
  type: TYPE_NORMAL
- en: The postmortem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Putting a sidebar in the summary may seem out of place, but I think postmortems
    are much like a summary of a disaster themselves. So why not discuss them here?
  prefs: []
  type: TYPE_NORMAL
- en: Most companies skip the all-important task of performing a post mortem. Postmortems
    are not about placing blame, and if that is what your company wants to use them
    for then it is probably better to avoid them, but in a healthy organization they
    serve as a critical learning tool on many levels.
  prefs: []
  type: TYPE_NORMAL
- en: A good postmortem is going to expose mistakes in system design, documentation,
    planning, policies, procedures, and just about any other aspect of our systems.
    It should also aid us in identifying people who are strong or weak during a crisis.
    We should be using our postmortem processes to discover where we were weak and
    how we can improve, or potentially to determine not to change at all.
  prefs: []
  type: TYPE_NORMAL
- en: A postmortem should also allow us to evaluate our decision processes that led
    to where we are today. This is almost universally overlooked and is actually where
    our true value of a postmortem exists. Changing the outcomes of individual plans
    or decisions is good, but generally minor, but finding entire decisionmaking processes
    that are failing gives us an opportunity to make changes that impact all decisions
    going forward.
  prefs: []
  type: TYPE_NORMAL
- en: Learning how to make decisions is important and few organizations or people
    ever focus on the quality of the decision-making process and even fewer companies
    track it and attempt to improve it over time. This is a huge lost opportunity.
    Decision making is something that happens over and over again. Making better decisions
    on a regular basis is vastly more important than fixing individual decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Postmortems need to dig into *why did we decide to do what we did* and then
    examine if that was a good decision, but we must not fall into the trap of applying
    current knowledge to old decisions. The *if we had only known* game is a dangerous
    one. We have to evaluate what we could have known at the time and determine if
    we researched enough, thought it through properly, applied true business goals
    and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to project knowledge after the fact and say *see, things failed,
    we lost money, that is someone's fault*. Emotionally, that feels like it must
    be true, but it is not. It could be true, but that is not likely a productive
    thing to determine. Bad things happen, risk is part of business, there is not
    always someone at fault causing these bad things to happen. Working in IT we deal
    with calculated risk every day. What we need to know in a post-mortem is if we
    calculated it correctly and took the right chances.
  prefs: []
  type: TYPE_NORMAL
- en: A great example of good risk is when we have to travel from New York to Los
    Angeles. We can take a plane or we can drive. If we look primarily at our safety
    during the trip, flying feels scary and driving does not seem scary at all. Yet
    the chances that we die in a car crash over such a long distance is many times
    higher than the risk of dying in a plane crash. If we took a plane and the plane
    did indeed crash, it would be tempting to use the new knowledge that that particular
    flight was going to crash to say that the right decision would have been to drive,
    but that is wrong. The flight was still the right decision. Both approaches had
    their risks and the flight was the vastly lower of the two risks. We use knowledge
    that that particular flight would crash because there is no way to have known
    that ahead of time. We were playing it safe, we made the right call; but no option
    is without risks and punishing people for making good choices is a terrible outcome.
  prefs: []
  type: TYPE_NORMAL
- en: People need to be rewarded for making tough choices, especially when they make
    the right rough choices. If we look to place blame, we risk punishing people for
    simply having made any decision at all, and if we do that we push them towards
    avoiding the decisions that protect the business to avoid being in the line of
    fire for false blame. Of course, if truly bad decisions were made, we want to
    discover that. It is just a very difficult task to maintain a focus on organizational
    and personal improvement rather than using postmortems to find scapegoats or deflect
    culpability.
  prefs: []
  type: TYPE_NORMAL
- en: Used correctly, postmortems are a powerful tool. Used incorrectly, they are
    a waste of time or potentially worse. Even if the organization lacks the capability
    of performing good postmortems, do so just within IT. If IT itself has politics
    that make this impossible, do so just within systems administration. Even if no
    one else participates, do it for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Document your post-mortems. People have a tendency to remember disasters negatively
    and to emotionally assign fault where none existed or where it is not deserved.
    Keep post-mortem documentation on hand as it is often useful for defending people,
    teams, or processes later. Good documentation, even after the fact, is a powerful
    tool.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that a post-mortem does not just need to ask *could we have avoided
    this disaster* but also *even if we could have, should we have?* Often the cost
    of avoiding disaster is greater than the risk cost of having the disaster. A post-mortem
    should cover the decision-making process, the decision itself, as well as the
    response to the disaster.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that the ideas and concepts in this chapter will help you to break out
    of the mindset of traditional roles and to tear apart *the box* and let your approach
    to disaster recovery reflect the best of what you and your organization can muster.
  prefs: []
  type: TYPE_NORMAL
- en: There is an obvious lack of discussion around Docker and other modern container
    technologies for a book on Linux. This is not an accident, it's actually by design.
    The reason for this is that Docker and its kin are application container technologies
    that leverage other technologies that we have already addressed, and their practices
    are their own concern. At the system administration level, application containers
    are simply another workload - one that happens to use Type-C virtualization and
    manage its own dependencies and updates. Docker or other application container
    management is beyond the scope of this book as well as general system administration.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, the system administrator is responsible for managing these technologies,
    but they're not special cases. Workloads are the same as we have discussed throughout
    this book. Even though they may have their own names, their own mechanisms, and
    their own management tools, all of these things are still governed by general
    case guidelines and rules that we should already know as system administrators.
    You will need to learn, if you're going to work with these technologies, and you
    very likely will if you're a system administrator today, many things that pertain
    specifically to the application container platform and management tools that you'll
    be using and apply that knowledge to what we've already learned.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices focus on learning the general cases, the rules that always apply,
    then figuring out how different techniques, technologies, and products are covered
    by the general cases.
  prefs: []
  type: TYPE_NORMAL
