

# 管道和重定向

在本章中，你将学习如何利用现有的最强大的计算概念之一：管道！管道可以用来连接命令，构建复杂的定制化流程，完成特定的任务。在本章结束时，你将能够理解（或编写）像这样的内容：

```
history | awk '{print $2}' | sort | uniq -c | sort -rn | head -n 10 
```

如果你有兴趣，这会打印出你最常用的十大 shell 命令列表；在我的机器上，它会输出以下内容：

```
1000 git
 115 ls
 102 go
  83 gpo (an alias I've set up for pushing a local git branch to the origin)
  68 make
  65 cd
  59 docker
  42 vagrant
  35 GOOS=linux
  30 echo 
```

要真正理解管道，你需要首先了解文件描述符和输入/输出重定向，这也是我们将要开始的内容。本章的一些信息可能比较密集，慢慢来，尝试所有示例，确保你理解每一个概念。现在投入的时间学习这些概念，将在你的职业生涯中为你节省大量时间。

在本章中，我们将涵盖以下主题：

+   文件描述符

+   使用管道（`|`）将命令连接在一起

+   你需要了解的 CLI 工具

+   实用的管道模式

+   检查文件描述符

# 文件描述符

你可能熟悉文件句柄（也称为*文件描述符*），这来自你在软件工程方面的经验。如果不熟悉，我们建议你查阅《第五章，介绍文件》。简而言之，如果你的程序需要读取或写入操作系统上的文件，打开该文件会给你一个“文件句柄”——它是该文件对象的指针或引用。

因为操作系统会调度对系统资源（如文件）的所有访问，它会追踪你的程序当前正在引用哪些文件句柄或描述符。

但即便是一个进程没有触碰操作系统上的任何文件，它也会打开一些文件句柄。在类似 Unix 的操作系统中，每个进程至少有三个文件描述符：

+   `stdin`：标准输入 - 或，`fd 0`（“文件描述符零”）

+   `stdout`：标准输出 - 或，`fd 1`（“文件描述符一”）

+   `stderr`：标准错误 - 或，`fd 2`（“文件描述符二”）

这三个文件描述符作为进程的标准通信通道，因此，它们对于系统上创建的每个进程来说，顺序都是一样的。第一个始终指向一个用于读取输入的文件。第二个指向一个用于写出输出的文件。第三个则指向一个用于接收错误输出的文件。

可选地，在前面提到的三个标准文件描述符之后，可能会有任意数量的其他文件描述符/句柄，具体取决于程序的操作。你的进程可能会有：

+   它正在操作的文件

+   它正在读取或写入的套接字（考虑到 Unix 或 TCP 套接字用于网络通信时的写入）

+   设备，如键盘或磁盘，它需要使用的设备

## 这些文件描述符引用了什么？

现在，你从一个进程的角度，已经知道了这些文件描述符的作用：

+   `0`（`STDIN`）：从这里获取输入

+   `1`（`STDOUT`）：将常规输出放到这里

+   `2`（`STDERR`）：将错误输出放到这里

但是，如果我们跳出单个进程来看，这些文件描述符到底指向哪些文件呢？输入来自哪里，输出和错误又写入哪里？

让我们以 Bash shell 进程为例：默认情况下，它从你的终端（在文件系统中表示为一个文件）获取输入（STDIN）。Bash 将输出和错误打印到相同的终端。实际上，你的整个 shell 会话都是通过对一个文件进行读写操作来进行的。在下一章*使用 Shell 脚本自动化任务*中，你将学到更多关于 Bash 的知识。

让我们更详细地看看这种输入和输出重定向。

# 输入输出重定向（或者说，玩转文件描述符以获得乐趣和收益）

这些知识在实际开发任务中非常有用：每当你想避免输入大量内容而改为从文件中获取，或者当你想记录程序输出时，都会用到这种技巧，还有许多其他情况。当你创建一个进程时，可以控制它的三个标准文件描述符指向哪里，从而产生强大的效果。

## 输入重定向：<

`<`（小于）符号让你控制一个进程的输入来源。例如，你通常是通过键盘逐个命令给 Bash 提供输入。让我们尝试从文件给 Bash 提供输入吧！

假设我有一个名为`commands.txt`的文件，内容如下（我在这里用`cat`来打印我的示例文件）：

```
# cat commands.txt
pwd
echo "hello there, friends"
echo $SHELL
cd /tmp
pwd 
```

这些都是 Bash 认为有效的 Shell 命令，所以我要启动一个新的 Bash 进程，并将此文件作为标准输入使用：

```
# bash < commands.txt
/tmp/gopsinspect
hello there, friends
/bin/bash
/tmp 
```

Bash 不会提示我输入并等待我输入，而是一次读取并执行一行：它从文件中读取输入，直到遇到换行符（`\n`）为止，就像你按下*RETURN*键一样，它执行命令。

在这个例子中，程序的标准输出仍然回到我们的终端，我们可以读取它。现在让我们来改变它。

## 输出重定向：>

我们想将`STDOUT`（文件描述符`1`）重定向到一个文件，而不是终端，将每个命令的输出记录下来，而不是实时地打印到终端：

```
# bash < commands.txt > output.log 
```

注意，现在终端中没有可见的输出——因为`>`字符将输出重定向到了`output.log`。使用`cat`打印出日志文件，确认它包含了预期的输出：

```
# cat output.log
/tmp/gopsinspect
hello there, friends
/bin/bash
/tmp 
```

有趣的是，你会注意到，由于文件描述符`1`是标准输出，写`>`和写`1>`是一样的。你很少看到使用`1`，因为通常会假设标准输出被重定向了。换句话说：

```
date > mydate.log
is equivalent to writing
date 1> mydate.log 
```

### 使用`>>`追加输出而不覆盖

在之前的例子中，我们通过重定向命令输出到`>`创建了一个日志文件。如果你多次运行这个例子，你会注意到日志文件一点也没有增长。每次用`> filename`重定向输出到文件时，文件中的任何内容都会被覆盖。

为了避免这种情况——就像处理一个长期存在的日志文件，这个文件收集来自多个进程或命令的输出——可以使用`>>`（追加）。这样，每次运行时，它会将内容追加到输出文件中，而不是覆盖文件的整个内容。

我们将在后面的章节中更详细地介绍 Bash 脚本，但现在，这里有一个简单的脚本，它每秒将当前时间的时间戳写入日志文件：

```
while true; do
    date >> /tmp/date.log
    sleep 1
done 
```

在这个示例脚本中，我们创建了一个无限循环（while true; do [ ... ] done），它运行`date`命令。它将此命令的输出重定向到`/tmp/date.log`文件中，使用`>>`，这会将输出追加到文件中（`>`每次都会覆盖文件）。然后，脚本会休眠一秒钟，再从头开始。

运行一次`date`命令会产生以下输出：

```
→  ~ date
Sat Jan  6 16:39:37 EST 2024 
```

另一方面，运行这个脚本最初不会看到任何可见的输出，因为输出被重定向到文件中。下面是我将这个小脚本粘贴到终端中，运行一会儿，按*Ctrl* + *C*终止它，然后打印出它创建的文件内容时的情况：

```
→  ~ while true; do
    date >> /tmp/date.log
    sleep 1
done
^C%
→  ~ cat /tmp/date.log
Sat Jan  6 16:44:01 EST 2024
Sat Jan  6 16:44:02 EST 2024
Sat Jan  6 16:44:03 EST 2024
[ ... ]
Sat Jan  6 16:44:08 EST 2024 
```

在所有日常情况下，你都会使用这种简单的输出重定向，比如为快速调试脚本创建一个临时的日志文件。

## 错误重定向使用 2>

许多命令行程序都有大量预期的输出，同时也会偶尔输出错误——想想一个`find`命令，它会遇到“权限拒绝”的错误，因为你没有权限查看某些目录。

尽管这些错误是微小且可以预见的，但你不希望它们与其他内容混在一起，污染你的输出。当你不是交互式地使用命令行工具，而是编写小脚本或更大的程序来处理你运行的命令的输出时，这一点尤其重要。

你已经看到如何重定向标准输入（`fd 0`）和标准输出（`fd 1`）。现在我们来看看如何使用`2>`（重定向文件描述符 2）语法来重定向标准错误（`fd 2`）：

```
find /etc/ -name php.ini > /tmp/phpinis.log 2>/dev/null 
```

这个命令在`/etc`目录树内查找任何名为`php.ini`的文件。它找到的文件（`find`的`STDOUT`）会写入`/tmp/phpinis.log`，而遇到的任何错误都会通过将其发送到一个特殊的文件`/dev/null`来忽略。

**提示**

`/dev/null`是一个特殊的类文件对象，当你尝试从中读取时，它返回零，写入任何内容时会被忽略——它作为一种垃圾桶，用来存放工程师希望静默或忽略的输出。你将在脚本中经常看到它的使用。

现在你已经了解了输入和输出的重定向，我们来看一下管道，它将这两个概念结合在一起：它们将一个命令的输出重定向到另一个命令的输入。

# 使用管道（|）将命令连接在一起

你已经学会了如何将三个标准文件描述符重定向到不同的位置，并了解了为什么这样做通常很有用。但如果你不仅仅是将输入输出重定向到不同的文件，而是想将 *多个程序* 连接在一起怎么办？

在命令行中，你可以使用管道符（`|`）将一个程序的输出连接到另一个程序的输入。这是一个非常强大的范式，在 Unix 和 Linux 中被广泛使用来创建自定义的排序、过滤和处理命令：

```
echo -e "some text \n treasure found \n some more text" | grep treasure 
```

如果你将这段代码粘贴到你的 shell 中，你会看到打印出 `treasure found`。下面是发生了什么：

1.  第一个命令 `echo` 执行并生成你看到的输出（这些输出被双引号包围，换行符使其成为一个三行字符串）。

1.  管道符将该输出（文件描述符 1）流向下一个命令（文件描述符 0）的输入，即 `grep`。`grep` 的输入现在与前一个命令的输出连接在一起。

1.  `grep` 命令依次查看每一行以换行符分隔的内容，并在第二行找到 `treasure` 的匹配项。`grep` 将第二行打印到其标准输出中。

## 多重管道命令

这是你在本章开头看到的—相当极端—示例：

```
history | awk '{print $2}' | sort | uniq -c | sort -rn | head -n 10 > /tmp/top10commands 
```

在这个复杂的命令中，每个管道只是将前一个命令的输出（`STDOUT`）作为输入（`STDIN`）传递给下一个命令。

将一个命令的输出传递到另一个命令的输入中，就是实现这些流程的方式，它能在不编写任何自定义软件的情况下，过滤和排序这些命令之间流动的数据。仅仅因为没有名为 `top10commands` 的程序，并不意味着你不能快速利用现有的标准命令来拼凑一个类似的程序。

### 阅读（并构建）复杂的多管道命令

无论你遇到的管道连接命令看起来多么复杂或神奇，它们都是通过相同的方式构建的：一次一个命令。无论你是在尝试理解像这样的复杂命令序列，还是自己创建一个，过程都是一样的：

1.  先看第一个命令，确保你基本理解它的作用。如果你不熟悉它，可以查看 man 页面或其他文档。

1.  运行命令并检查其输出。

1.  添加管道符及其后面的命令。

1.  从 *步骤 1* 开始重复，直到你完成所有命令。

你会看到，即使是最复杂的 shell/pipe 命令怪物，当你应用这个过程时也变得可以管理。始终记住，你所处理的只是一个数据流，它通过命令之间的管道流动，在这个过程中被塑形、修改、过滤、重定向和转换。

我们将在 *第十二章*，《使用 Shell 脚本自动化任务》中进一步讨论这个话题，但请尽量尊重其他需要阅读你代码的程序员：将你的语句限制为两到三个管道，并使用有意义的变量来存储中间结果，以便于阅读，如果你的内存限制允许的话。

现在你已经看到文件描述符的原始操作如何作为易于使用的输入和输出重定向暴露出来，让我们来看一些依赖于 Unix 内置可组合性的实际程序组合的示例。

# 你需要了解的 CLI 工具

在我们跳入章节开始时看到的那些复杂组合之前，先让我们看一些最常用的 Unix 辅助工具，这些工具用于过滤、排序和组合你将在命令行上创建的数据流。

## cut

`cut` 接收一个分隔符（`-d`）并根据该分隔符拆分输入，类似于许多编程语言中的 `String.Split()` 或 `String.Fields()`。然后，你可以使用 `-f` 选择要输出的字段（列表元素），例如，`f1` 表示第一个字段。

如果你输入多行给 `cut`，它会在所有行上重复相同的操作：

```
echo "this is a space-delimited line" | cut -d " " -f4
space-delimited 
```

你也可以看到使用不同分隔符的 `cut` 是如何工作的；在以下示例中，我们按连字符而不是空格进行切割：

```
→  ~ echo "this is a space-delimited line" | cut -d "-" -f1
this is a space
→  ~ echo "this is a space-delimited line" | cut -d "-" -f2
delimited line 
```

你可以看到这也改变了可用字段的数量——在这种情况下是两个，因为文本中只有一个连字符。尝试使用 `–f4` 打印第四个字段，如前面的示例所示，将只返回一个空行。

要获取所有用户名中包含 root 字符串的 macOS 用户的友好名称，可以使用以下命令：

```
# grep root /etc/passwd | cut -d ":" -f5
System Administrator
System Services
CVMS Root 
```

## sort

`sort` 执行逐行排序，可以是字母顺序或数字顺序。

使用 `-r` 进行反向排序通常在处理数字数据（`-n`）时很有用。你通常会一起使用 `-rn`（参见本章的实践管道模式中的 Top X 部分）。

`-h` 标志对于排序许多其他命令的可读输出非常有用，像这样：

```
# du -h | sort -rh
1.6M    .
1.3M    ./.git
1.2M    ./.git/objects
 60K    ./.git/hooks
 28K    ./.git/objects/d8 
```

## uniq

删除重复行。此命令需要排序数据才能按预期工作，否则它只会检查每行是否为前一行的重复：

```
# cat /tmp/uniq
one
two
one
one
one
seven
one 
```

默认行为；可能不是你想要的：

```
# uniq /tmp/uniq
one
two
one
seven
one 
```

`uniq` 会跳过相邻的重复项，但如果它们被其他文本分隔开，仍然会保留它们。现在，使用已排序的数据，效果如下：

```
# sort /tmp/uniq | uniq
one
seven
two 
```

### 计数

`uniq` 还提供了一个有用的“计数”选项，通过 `–c` 来访问。关于输入必须排序的警告值得在这里重申——例如，文件包含以下内容：

```
arch
alpine
arch
arch 
```

运行 `uniq -c` 后将产生以下输出：

```
$ uniq -c /tmp/sort1.txt
   1 arch
   1 alpine
   2 arch 
```

这不是大多数用户期望的结果：文件中有 3 次出现 `arch`，但 `uniq` 显示两个分别的计数。为了得到你期望的行为（`uniq` 应该返回没有重复行的输出），输入必须是已排序的。

这对初学者来说可能有些麻烦，但这正符合 Unix 的哲学：工具应该是小巧且锋利的，不应该重复彼此的功能。如果你编写一个排序工具，它应该只负责排序；如果你编写一个去重工具，它可以依赖于另一个工具的排序功能，以确保极其保守（且一致）的内存使用。

在这里，我们在使用`uniq`之前进行排序，这样可以得到我们预期的输出：

```
$ sort /tmp/sort1.txt | uniq -c
   1 alpine
   3 arch 
```

你会注意到，这个排序是按升序进行的，而这并不是你想要的，对于开头部分看到的命令的 top-X 列表。为了解决这个问题，我们对这个编号列表进行“反向数字”排序（`-rn`）（因为每行现在都以数字开头，感谢`uniq -c`，这变得很容易）。下面是一个在包含更多重复项的文件上执行的例子：

```
$ sort /tmp/sortme.txt | uniq -c | sort -rn
   6 ubuntu
   4 alpine
   3 gentoo
   2 yellow dog
   2 arch
   1 suse
   1 mandrake 
```

## wc

使用这个命令，你可以测量单词、行、字符和字节的输入计数。你还可以使用`-w`来计数字符串中用空格分隔的单词：

```
# echo "foo bar baz" | wc -w
       3 
```

行计数在以下格式中非常常见：

```
# wc -l < /etc/passwd
     123 
```

## head

`Head`返回流或文件的前几行——默认情况下是 10 行。使用`-n`指定你想要的行数：

```
# head -n 2 /etc/passwd
##
# User Database 
```

## tail

这与`head`相反：它返回文件或流的末尾几行。它像`head`一样接受`-n`参数。

`tail`也可以用于交互式地跟踪日志文件，即使该文件正在被写入新数据。你会在故障排除时经常看到它这样使用：

```
tail -f /var/log/ngnix/access.log 
```

## tee

有时候，标准输入的一个副本是不够的。`tee`将标准输入复制到标准输出的同时，还将其复制到一个文件中。作为软件开发人员，我特别喜欢在两种情况下使用`tee`。

首先，为了调试和记录日志：当我运行生成输出的脚本或程序时，`tee`可以同时将输出显示在屏幕上，并将其记录到文件中以供后续分析。我们这里使用的是`echo`命令，但你可能会在第一个管道前调用自己的程序：

```
# echo "Hello" | tee /tmp/greetings.txt
Hello
# cat /tmp/greetings.txt
Hello 
```

`tee`的第二个用途是从管道中复制数据，像我们在本章中学习构建的管道。你可以使用`tee`在管道的任何点拦截数据流，并保存/检查中间结果而不干扰管道的运行。

这是之前提到的“前 10 个命令”的例子，但是在将结果限制为前 10 个之前，插入了`tee`。这样会在我们截断结果之前将完整的结果保存在临时文件中：

```
history | awk '{print $2}' | sort | uniq -c | sort -rn | tee /tmp/all_commands.txt | head -n 10 
```

如果你想查看所有命令，而不仅仅是前 10 个，你可以使用`cat`或`less`来查看`/tmp/all_commands.txt`文件。

## awk

`awk`通常用于处理数据的列，但它实际上是一种完整的语言。

例如，你可以通过以下方式提取每行的第二列：

```
# echo "two columns" | awk '{print $2}'
columns 
```

## sed

`sed`是一个流编辑器，具有许多选项。最常用的是用于在流或文件中进行字符替换。

假设我们有一个这样的文件：

```
# cat /tmp/sensitive.txt
Nopasswords
not_a_password_either
sillypasswordtimes
password
ok this works 
```

如果我们只想编辑包含`password`的那一行，且不修改其他内容：

```
sed 's/^password$/REDACTED/' /tmp/sensitive.txt
nopasswords
not_a_password_either
sillypasswordtimes
REDACTED
ok this works 
```

这个例子使用的是一个文件，而不是来自其他命令的输入流。默认情况下，这不会修改原始文件。如果你*确实*想修改输入文件，可以使用`-i`（就地）选项。

现在你已经了解了管道符并且看到了常见的命令行工具，接下来我们将把这些基础知识结合起来，学习几种实用的模式，帮助你使日常的命令行工作变得更轻松。

# 实用的管道模式

如前所述，较长的多管道命令是逐步构建的——一次一个命令。然而，有一些常用的模式，你会经常看到它们被重复使用。

## “前 X 名”，并附上计数

这个模式根据出现的次数对输入进行排序，按降序排列。你在本章的原始示例中看到了这个，它展示了从 Bash 历史文件中提取的最常用的 shell 命令。

下面是这个模式的示例：

```
some_input | sort | uniq -c | sort -rn | head -n 3 
```

我们可以注意到这个模式的一些细节：

+   输入按字母顺序排序，然后通过 `uniq -c` 进行处理，`uniq -c` 需要已排序的输入才能正常工作。

+   `uniq -c` 消除重复项，但会添加一个计数（`-c`），表示每个条目出现的次数。

+   `sort` 会再次运行，这次是按逆数字（`-r` 和 `-n`）排序，按输入中的唯一计数排序，并输出逆序（从高到低）的排序结果。

+   `head` 会获取排名靠前的部分，并将其缩减为三行（`-n 3`），展示原始输入中的前三个字符串，以及它们出现的频率。

当你需要了解最常见的浏览器用户代理访问你的网站、那些试图探测并利用你网站的最恶劣的 IP 地址，或者任何其他排序和排名列表有用的情况时，这个模式会非常有用。

## curl | bash

`curl | bash` 模式是 Linux 中常用的快捷方式，用于直接从互联网下载并执行脚本。这个方法结合了两个强大的命令行工具：`curl`，用于从 URL 获取内容；以及 `bash`，用于执行下载的脚本。这个模式节省了大量时间，使开发人员可以快速部署应用程序或运行脚本，而无需手动下载然后执行它们。

例如，让我们使用这个模式安装 Pi-hole 广告拦截 DNS 服务器：

```
curl -sSL https://install.pi-hole.net | bash 
```

让我们一步步地分解这个模式：

1.  `curl -sSL https://install.pi-hole.net`: 该命令会获取 Pi-hole 安装脚本，该脚本托管在此 URL 上。我们传递了两个选项：

    +   `-sS`: 静默模式会从服务器获取原始响应，但如果发生错误，则会显示错误信息。

    +   `-L`: 跟随重定向。

1.  `|`: 管道符将前一个命令（`curl`）的输出作为输入传递给下一个命令（`bash`）。

1.  `bash`: 执行通过 `curl` 获取的脚本。

这是一个非常有用的模式，适用于自动化如代码部署或本地环境安装/配置等任务。然而，特别需要小心的是，下载并执行的脚本不能是恶意的。从互联网随便运行脚本是极其不推荐的做法。

### curl | sudo | bash 的安全性考虑

每当你信任第三方在你的机器上运行代码时，你就已经在安全性和便利性之间做出了权衡。从这个角度来看，使用`curl | sudo | bash`通过托管在受信任服务器上的脚本安装软件，与使用包管理器并没有太大区别。大多数包管理器（除了`nix`）的安全设计也不算特别出色，但它们通常会提供合理的安全功能。你在执行`curl | sudo | bash`安装脚本时，实际上放弃了所有这些安全特性：

+   没有可以进行校验和和加密签名的包来确保你获得了正确和官方的版本。

+   下载文件时没有任何限制或强制措施，你也无法知道这些服务器的安全性：你无法识别是否有被破坏的服务器托管恶意安装脚本。

+   这些脚本本身就是在你的机器上以 root 用户身份运行的代码，因此它们可以做任何*你*能在机器上做的事情，无论好坏。公平地说，许多流行的包管理器也存在这个问题。

基于以上所有原因，请注意我们的警告：将`curl`命令拆分成单独的步骤，并在运行`sudo` `bash`执行之前仔细阅读下载的安装脚本。需要注意的主要事项有：

+   确保你下载脚本的服务器/域名是值得信赖的；它应该是一个有声誉的开发者网站或受信任的第三方代码托管平台。

+   确保使用 HTTPS 下载`curl`（即 URL 应该以`https://`开头）。

+   仔细阅读脚本，查看它运行了哪些命令，以及从哪里拉取了额外的代码或可执行文件。如果它下载了额外的脚本或可执行文件，也要查看这些内容。

我们已经确定，`curl | sudo | bash`并不是一种特别安全的软件安装方法。如果你像大多数人一样，某一天禁不住诱惑，使用这种安装方法来安装某个特定的软件（例如 macOS 上的`homebrew`），遵循这些指南可以帮助你在一定程度上提高安全性。

现在我们来看一个常见的操作模式：使用`grep`进行过滤和搜索。

## 使用`grep`进行过滤和搜索

当你运行产生大量输出的命令时，通常最好将输出过滤到仅需要的内容。最常见的工具是`grep`，你可以将其视为一个高度可配置的文本搜索或字符串匹配功能。以下是过滤操作可能的示例。

假设你需要找到一个 Linux 进程的工作目录。`lsof`工具可以实现这个功能：

```
➜  ~ lsof -p 3243 | grep cwd
vagrant 3243 dcohen  cwd    DIR                1,4      192            51689680 /Users/dcohen/code/my_vagrant_testenv 
```

这里是发生情况的简要说明：

1.  我正在使用`lsof`获取一个特定进程（PID 3243）的打开文件句柄列表。

1.  我将结果（`|`）传递给 `grep` 工具，并用它在结果中搜索字符串 `cwd`。只有一行结果包含字符串 `cwd`，因此 `grep` 只会将该行输出到终端。

该模式在输入数据量*很大*时特别有用，但你只需要从中筛选出可以通过特定字符串识别的子集。`grep` 作用于输入文本的每一行，因此在提取像以下数据时非常有用：

+   包含你正在跟踪的 IP 地址的日志行

+   在管道数据流中出现的用户名

+   匹配某个模式的行（`grep` 支持正则表达式，并且可以接受字符串模式，除了字面搜索字符串之外）

`grep` 是一个功能强大且广泛使用的工具，你几乎每天都会用到它。有关更多信息，请通过输入 `man grep` 查看 `grep` 的手册页。

你在本书中已经看到过 `grep` 在文件中的使用（例如，`grep searchstring hello.txt`），但它也是管道命令中的一个宝贵过滤组件。现在让我们来看一个实际的例子。

## 使用 grep 和 tail 进行日志监控

当你查看生产日志时，试图找出问题所在，通常你只想查看包含特定关键字或搜索字符串的日志行。为了实现这一点，你可以运行类似这样的命令：

```
tail -f /var/log/webapp/too_many_logs.log | grep "yourSearchRegex" 
```

该模式持续监控日志文件中的新条目，条目的内容符合“yourSearchRegex”正则表达式，从而让你仅看到当前任务需要的日志。

## 使用 find 和 xargs 执行批量文件操作

`xargs` 是一个强大的工具，它赋予你迭代的能力（换句话说，就是一个“for”循环），并且可以在单个命令中使用。默认情况下，`xargs` 会接受每个（空格、制表符、换行符和文件结尾符分隔的）输入块，并使用该块作为输入执行指定的程序。例如，如果你需要在由某个 `find` 查询返回的特定文件中搜索文件内容，可以运行以下命令：

```
find . -type f -name "*\.txt" | xargs grep "search_term" 
```

此命令会查找所有以 `.txt` 结尾的文件，然后使用 `xargs` 对每个文件分别应用 `grep` 命令。此模式非常适合一次搜索或修改多个文件。请注意，`xargs` 是一个强大且*庞大的*工具，能够做许多事情（包括将字符串插入到它执行的命令中）。我们无法在这里涵盖所有内容，如果你遇到这种情况，请阅读手册页并在互联网上查找示例，看看这种功能如何帮你解决问题。

## 对数据分析进行排序、去重和反向数字排序

这是一个有用的模式，在本章开头你曾看到过应用，我用它来过滤大量命令历史记录，获取“在该系统上执行次数最多的前 X 条命令”列表。核心模式是这样的：

```
(input stream) | sort | uniq –c | sort -rn 
```

这种模式非常适合分析数据，它会对输入流中的数据进行排序，去重并统计唯一的出现次数，然后执行反向数字排序，最终输出去重后的数据，并将最常见的行排在前面。

这通常通过 `| head -n $NUMBER` 截断，以仅获取前 `$NUMBER` 条结果：

```
history | awk '{print $2}' | sort | uniq -c | sort -rn | head -n 10 
```

在这里，我们使用 history 获取整个 Shell 命令历史记录。这样会给出一系列像这样的行：

```
 12  brew install --cask emacs 
```

我们只关心顶级命令（在这个例子中是 `brew`），所以我们使用 `awk` 来获取第二列。

然后我们对数据进行排序，使得相同命令的重复项在流中紧挨着。接着，我们用 `uniq` 去除这些重复项，并为每个剩余的命令添加出现次数的计数。接下来，我们再次排序，这次使用 `-rn` 进行反向数字排序，这样就实现了“前 X 名”的效果。最后，我们用 head 取出前 10 行。

这会打印出你最常用的 10 个 Shell 命令列表；在我的机器上，它会输出：

```
1000 git
 115 ls
 102 go
  83 gpo (an alias I've set up for pushing a local git branch to the origin)
  68 make
  65 cd
  59 docker
  42 vagrant
  35 GOOS=linux
  30 echo 
```

## awk 和 sort 用于数据重新格式化和基于字段的处理

`awk` 不仅是一个程序，它还是一种流处理语言。如果你在 Unix 系统上处理数据流，那么花几天时间学习基础知识可以为你未来的职业节省数周时间。也就是说，使用 `$#` 语法引用每行数据流中的空白分隔列是一个很好的起点。

让我们看一个示例，假设有如下数据流：

```
Foo bar baz
Some data is nice 
```

当 awk 解释器看到 `$1` 时，它将其解释为“第一列”，在这种情况下，第 1 行是 `Foo`，第 2 行是 `Some`。`$2` 是第二列（`bar`，`data`），依此类推。当处理稍微复杂一些的数据时，这是一个非常常用的功能，比简单的 `cut` 命令更有效：

```
cat file.txt | awk '{print $2, $1}' 
```

这将产生类似如下的输出：

```
bar Foo
data Some 
```

在这个例子中，它会将每个文件的第 2 列打印在第 1 列之前，并忽略每行中的其他数据。这通常用于基于特定字段重新格式化和整理数据。

## sed 和 tee 用于编辑和备份

**sed** 代表 **Stream EDitor**，用于在你想要转换数据流时使用。当你在文本编辑器中进行查找/替换符号时，你每天都会执行这项操作十次。以下命令本质上是该功能的命令行版本：它将 `file.txt` 中所有的 `old` 替换为字符串 `new`，并将结果流写入新文件 `file.txt.changed`。这样做不会修改原始的 `file.txt` `file`：

```
sed 's/old/new/g' file.txt | tee file.txt.changed 
```

虽然编辑文件内容是此概念的简单演示，但 `sed` 对于转换数据流非常有用，因为它可以在一个命令的输出和下一个命令的输入之间快速传递数据流：

```
(input stream) | sed 's/old/new/g' | (next command) 
```

### ps、grep、awk、xargs 和 kill 用于进程管理

虽然 `pgrep` 是一个很好的工具，可以向所有匹配模式的进程发送信号，但有时它在你的系统上不可用。你可以通过以下一组管道命令组合来拼凑出类似的功能（并且能更具体地指定你想要定位的目标，而不仅仅是名称）：

```
ps aux | grep "process_name" | awk '{print $2}' | xargs kill 
```

`ps` 为你提供一个运行中进程的列表，`grep` 过滤出只包含你正在搜索的模式的进程。`awk` 获取每一行的第二列（进程 ID），然后将所有匹配的行传递给 `xargs`（我们的类 for 循环），它在每个 PID 上执行 `kill`。这会向每个匹配的进程发送 `SIGTERM` 信号，并（希望）将其停止。

### tar 和 gzip 用于备份和压缩

尽管许多工具有让你同时执行归档和压缩的标志，但将归档和压缩连接起来也是一种有意义的使用案例。这为你提供了额外的灵活性，可以添加更多的链式命令。例如，如果你想添加加密，那只需再加一个管道命令：

```
tar cvf - /path/to/directory | gzip > backup.tar.gz 
```

这将创建一个目录的压缩归档，通常用于文件备份和存储。你可以看到使用这种模式的更大的命令：

```
ssh user@mysql-server "mysqldump --add-drop-table database_name | gzip -9c" | gzip –d | mysql 
```

这是一个特别有趣的示例，它通过 SSH 登录到数据库服务器，导出数据库，压缩该数据流，通过 SSH 将其传回本地机器，再次解压缩，最后将其导入本地 MySQL 服务器。

你的目标不一定是写出像这个（或你在这里看到的其他一些）那么复杂的命令，但如果你知道如何在关键时刻拼凑出这样的命令，它可以帮助你在开发过程中脱离一些极其紧迫的困境。我们希望这一部分能展示出，理解 Unix 系统为你提供的输入输出重定向原语——通过 `<`、`>`、`>>`、`|` 以及一般的文件描述符——实际上是一种超级能力。明智地使用它。

# 高级：检查文件描述符

在 Linux 上，你可以轻松地 *查看* 一个进程的文件描述符指向哪里。我们将使用稍微神奇的 `/proc` 虚拟文件系统来完成这一操作。

Procfs（`proc` 虚拟文件系统）是一个仅限 Linux 的抽象，它将内核和进程状态表示为文件。这些文件中的数据来自操作系统内核，并且只在你读取时存在。仅列出 `/proc` 目录，你就能看到很多文件；以下是一些更为重要的文件，摘自 Arch Linux 的维基：

```
/proc/cpuinfo - information about CPU
/proc/meminfo - information about the physical memory
/proc/vmstats - information about the virtual memory
/proc/mounts - information about the mounts
/proc/filesystems - information about filesystems that have been compiled into the kernel and whose kernel modules are currently loaded
/proc/uptime - current system uptime
/proc/cmdline - kernel command line 
```

关于文件描述符，我们最感兴趣的是上面列出的清单中没有显示的内容：`/proc` 包含一个针对每个正在运行的进程的目录，目录名是每个 **进程 ID**（**PID**）的名称。

在一个进程的`/proc`目录中，该进程的文件描述符会以符号链接的形式表示，存放在名为`fd`的目录中。当你对这个`/proc/$PID/fd`目录进行长列表显示时，你会发现`l`是长列表中的第一个字符，它表示一个特殊的`link`文件，正如你在*第五章*《文件介绍》中回顾到的那样。

从实际角度看，`/proc/1/`是`init`进程的 proc 目录，你可以通过对`/proc/1/fd`进行长列表显示，查看 init 的文件描述符。

让我们看看我机器上运行的交互式 Bash shell 进程的文件描述符，`ps aux | grep bash`告诉我它的 PID 是 9：

```
root@server:/# ls -alh /proc/9/fd
total 0
dr-x------ 2 root root  0 Sep  1 19:16 .
dr-xr-xr-x 9 root root  0 Sep  1 19:16 ..
lrwx------ 1 root root 64 Sep  1 19:16 0 -> /dev/pts/1
lrwx------ 1 root root 64 Sep  1 19:16 1 -> /dev/pts/1
lrwx------ 1 root root 64 Sep  1 19:16 2 -> /dev/pts/1
lrwx------ 1 root root 64 Sep  5 00:46 255 -> /dev/pts/1 
```

你会注意到这是一个交互式 shell 会话：它的标准输入来自一个虚拟终端（`/dev/pts/1`），其标准错误和输出也回到这个终端。这是正确的。

让我们看看像 vim 这样的文本编辑器，它的行为与终端非常相似——输入和输出都是通过终端进行的。然而，这里有一个附加的复杂性，那就是文本编辑器通常会保持一个或多个文件处于打开写入状态。这是什么样子的呢？

在这个例子中，我正在运行 vim 文本编辑器，并编辑`/tmp`目录中的一个文件。让我们找到 vim 的进程 ID，这样我们就知道应该查看哪个`/proc`目录：

```
root@server:/# ps aux | grep vim
root       453  0.0  0.1  17232  9216 pts/1    S+   15:57   0:00 vim /tmp/hello.txt
root       458  0.0  0.0   2884  1536 pts/0    S+   15:58   0:00 grep --color=auto vim 
```

就是它；进程 453。不要被`grep`命令误导，后者也在命令参数中包括了`vim`。现在我们已经得到了 PID，接下来看看 vim 的文件描述符：

```
root@server:/# ls -l /proc/453/fd
total 0
lrwx------ 1 root root 64 Jan  7 15:58 0 -> /dev/pts/1
lrwx------ 1 root root 64 Jan  7 15:58 1 -> /dev/pts/1
lrwx------ 1 root root 64 Jan  7 15:58 2 -> /dev/pts/1
lrwx------ 1 root root 64 Jan  7 15:58 3 -> /tmp/.hello.txt.swp 
```

我们可以看到，stdin（`0`）、stdout（`1`）和 stderr（`2`）都指向一个终端设备，就像 shell 一样。我们还看到编辑器打开了一个文件，文件描述符`3`链接到 vim 正在编辑的文件。当一个进程打开额外的文件时，会创建新的文件描述符，你可以在这里查看它们。

除了本身很有趣外，这对于调试程序行为异常时（比如程序出现 bug 时），或者你在追踪一个可能有恶意行为的程序时非常有用。如果你花点时间学习它，`procfs`会变得非常有趣和有用：只需要输入`man proc`开始，或者阅读 Arch Linux Wiki 页面，了解更温和的介绍：[`wiki.archlinux.org/title/Procfs`](https://wiki.archlinux.org/title/Procfs)。

# 结论

在本章中，我们将前面所学的所有技能和理论整合起来，解锁 Unix 和 Linux 系统最强大的功能之一：通过管道和输入/输出重定向将数据流经多个命令。

我们从展示操作系统如何暴露像文件描述符这样的原始操作开始，然后开始研究输入输出重定向的实际应用。接着，我们讲解了管道，这是 Linux 和其他 Unix 操作系统中最有用的功能之一。在介绍必要的理论并展示了一些实用示例后，我们深入探讨了人们用来切割和处理通过管道传输的数据流的最常见辅助工具。最后，我们向你展示了人们在实际工作中使用的一些最常见和最有用的模式和程序组合。

本章的内容是你将在日常工作中遇到并使用的许多高级命令行操作的基础。你已经接触到了一些基本理论、工具和模式，这将使你能够轻松地开始构建用于常见开发、故障排除和自动化用例的自定义命令。

要提升你的技能，将本章中所学应用到日常工作中！将其作为参考来尝试不同的模式，并持续学习新的工具和命令，将它们加入到自己的定制工作流中，用来过滤或处理命令行上的数据。你很快就会像个高手一样。

# 在 Discord 上了解更多

要加入本书的 Discord 社区——你可以在这里分享反馈、向作者提问，并了解新版本的发布——请扫描下面的二维码：

[`packt.link/SecNet`](https://packt.link/SecNet)

![](img/QR_Code1768422420210094187.png)
