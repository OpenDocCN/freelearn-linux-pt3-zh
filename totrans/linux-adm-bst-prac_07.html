<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer019">
			<h1 id="_idParaDest-192"><em class="italic"><a id="_idTextAnchor198"/>Chapter 9</em>: Backup and Disaster Recovery Approaches</h1>
			<p>I have said it already earlier in this book and it bears repeating no matter how many times it takes: nothing is as important in what we do as system administrators as maintaining good backups. This is our utmost priority. It is so important that many organizations maintain an independent system administration team that handles nothing but backups to make sure that it maintains constant attention.</p>
			<p>Backups are not glamorous, and they are rarely exciting. This does not just make them a challenge for us in the technical world to want to spend time thinking about them when we could be implementing new automation or something else admittedly more exciting, but it also means that management often does not prioritize budgets or prioritize around backups. This creates a potential danger for system administrators that our careers can be slowed if we focus on critical functions like backups instead of doing flashy, high-profile projects to keep the interest of management while at the same time often being at very high risk of being punished if the backups that they do not prioritize do not work flawlessly when needed.</p>
			<p>Backups are moving more and more to the forefront of our consciousness, however, as they have moved from primarily protecting against catastrophic hardware failure to being on the front lines of security concern. Backups have evolved in recent years and now present opportunities to shine politically within our organizations and opportunities for renewed technical interest. The boring backup and restore landscape of the 1990s and 2000s is, quite literally, a thing of the past and today we have so many approaches, options, and products that we really must approach backups as a broader concern than we have in the past.</p>
			<p>In this chapter we are going to cover a lot of different aspects of backups. We will start with a broad overview of how we take backups and what components exist within backup systems. Then we will look at similar technologies that are often used in conjunction with and might be sometimes confused with backups. Then we will cover in depth what we have mentioned throughout this book, the idea of a modern <em class="italic">DevOps centric backup approach</em> and talk extensively about that. Then we will explore agents and the issues with crash consistency of our data in backups. Finally, we tackle triage and what we can do to make restores better once a disaster has happened.</p>
			<p>Without further ado, let us begin what is assuredly our single most important chapter in this book. Of any book, I would assume.</p>
			<p>In this chapter we are going to learn about the following:</p>
			<ul>
				<li>Agents and crash consistency</li>
				<li>Backup strategies and mechanisms</li>
				<li>Snapshots, archives, backups, and disaster recovery</li>
				<li>Backups in a DevOps world</li>
				<li>Triage concepts</li>
			</ul>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor199"/>Agents and crash consistency</h1>
			<p>In the next <a id="_idIndexMarker779"/>session<a id="_idIndexMarker780"/> we are going to look at mechanisms for taking backups. Before we do that, I want to look at why backups are so hard to do well in the first place. In order to do that I will work with two examples. One is a text document that we create using our favorite text editor. I will assume that you are a normal person and love the vi editor as much as I do. And we will compare that to another common use case, the data file of an enterprise database system.</p>
			<p>When we talk about backups, we are talking about taking data that is stored on a physical medium and replicating that data somewhere that is separate from the original system in such a way that it is able to survive in many cases when the original system has failed. That is a very high-level view of the goal of backups. It serves our purposes. Therefore, in order to perform a backup we must be able to take the data that the system has, read it, move it, and write it.</p>
			<p>Of these steps it may seem like moving data to another location or writing the data somewhere would be the biggest challenges. This is incorrect. Being able to read the data is actually where the real problems tend to exist. Primarily because what data we need to read is not always clear.</p>
			<p>How do computers store data on disk in the first place? Not every computer system works the same way, but there are certain basics that exist for which we know no alternatives. While working with data, computers hold the data in their random-access memory. While there it is completely volatile, but it is very fast. When the computer is done with a file it takes the data that is in its memory and then writes it onto the storage device, presumable a hard disk or SSD.</p>
			<p>If we attempt to take a backup of a system while data is in the computer's memory and not yet written to disk our backup would contain none of the data in question. This may sound silly to say, but it is necessary to remember. It is common for people both technical and management to assume that data is always stored somewhere, even when it has only gotten into memory and not had time to be stored anywhere yet.</p>
			<p>Once data is written to the disk, and then a backup is taken, the data from the disk should be copied to the backup location. All is well with the world.</p>
			<p>This is all well <a id="_idIndexMarker781"/>and <a id="_idIndexMarker782"/>good, but computers deal with situations far more complicated than just receiving files, and saving them to disk. In the real world, computers nearly always are reading existing files off of their storage mechanism, holding it in memory while manipulating the data, and then saving it again back to disk with the new changes incorporated into the data set. This is where things start to become tricky.</p>
			<p>We should start using our examples now. First the case with a text file editing in <strong class="source-inline">vi</strong> (if you insist, you can edit in Atom or something else.) We will also assume that we are not creating a new file, but rather editing a configuration file that already exists on the disk. A great example is the <strong class="source-inline">hosts</strong> file (found at <strong class="source-inline">/etc/hosts</strong>.)</p>
			<p>If we take a backup while the file is being edited, we would get a copy of the old data before any edits because any edits that are happening exists only in memory, not on disk. The fact that the file is open at the time does not mean that data is being written to disk. The old data is still on the disk while the new data exists in memory.</p>
			<p>When the file is saved, the data is written to the disk. Once written, we can make our copy. Of course, there can be a short moment while the data is being written where what is on disk is neither the original version nor the new version but something in between as the data is not finished being written.</p>
			<p>We can attempt to lock the files, but even with a file being locked we have a complicated situation. The backup mechanism, whatever it is, has to decide if it is going to skip over a locked file, wait for the lock to be released, or ignore the lock and take the backup anyway. No situation is very good. We either fail to backup all of the files in any particular <a id="_idIndexMarker783"/>backup<a id="_idIndexMarker784"/> process, we risk a long wait for an event that might <em class="italic">never</em> happen, or we risk getting a file that is out of date or worse, corrupt. There is no completely simple answer.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor200"/>Locking mechanisms in Linux</h2>
			<p>When <a id="_idIndexMarker785"/>processes<a id="_idIndexMarker786"/> are using files on Linux, and more or less on any operating system, there are three essential strategies that we normally see in action. The first, and most obvious, is to do nothing. That is correct. In many cases there is no locking performed whatsoever. The contents of a file are read and the file itself is not marked as being open. It is just read. This approach is great in that any other process can continue to use the file in question in whatever manner they desire, but it also gives us no information as to whether or not the file is about to be updated in some way.</p>
			<p>The primary alternative mechanism on Linux is called an advisory lock. With an advisory lock, the operating system marks when a file has been opened by a process. The locks are so called advisory because the operating system advises other processes that the file is in use. Another process can opt to ignore the lock and continue to read, or even to overwrite, the file that is locked. This is handy in that we can lock a file and not worry that we are completely blocking others from accessing it. The risk is that our lock is not honored, and race conditions are encountered where data is changed out of order or data that is believed to be saved gets overwritten. Ignoring an advisory lock to peek at a file and check out some old data is pretty safe. Ignoring an advisory lock to overwrite the file with changes without the original process knowing that it has happened is dangerous. Processes that fully support and honor an advisory lock are called cooperative processes.</p>
			<p>The third option is a mandatory lock. As the name implies, processes have no option but to honor a mandatory lock. Mandatory locks are managed by the Linux kernel and only exist if a filesystem is specifically mounted with this form of locking enabled. Mandatory locks on Linux, however, suffer from implementation problems that make them unreliable and subject to some uncommon race conditions which effectively defeat the purpose of the lock. Because of this mandatory locking is almost always ignored on Linux systems.</p>
			<p>Locking is conceptually hard. It carries risk and system overhead. The best way to respond to a locked file is not always clear and may vary by many different use cases of which the operating system is not aware. Because the operating system does not know how or why a file is being accessed in the first place, it has little means of usefully enforcing limits on secondary access. There is nearly always a reason that we might want to read a locked file and sometimes a good reason to even write to a locked file. Trusting that we will only run processes that will behave as needed for our use cases is our best course of action in nearly all cases. When more complex access is required, such as is the case with most database files, it is necessary to move to single process access with that process handling additional data access from higher in the application stack where more knowledge of the intended use of the file is know.</p>
			<p>Other than locking, the major concept that we have to understand is quiescence. Quiescence simply refers to the storage reaching a state of dormancy, a lack of fluctuation. When we say that we have achieved quiescence in our storage we mean that all of the data that is currently <em class="italic">in flight</em> whether actively being used by an application or just being held in one of any number of different cache layers has been written to disk. In some ways, we can think of locks as being a mechanism meant to warn us (or a process) as to a system not being currently quiesced. </p>
			<p>Unfortunately, there <a id="_idIndexMarker787"/>is no<a id="_idIndexMarker788"/> universal mechanism to enforce or even ensure quiescence on Linux. There is none on Windows either, dangerously contrary to many claims that VSS (the Volume Shadow Service) does <a id="_idIndexMarker789"/>this. VSS is the standard LVM on Windows and as such is often used in many storage operations. It is commonly said that VSS guarantees that all files are quiesced, but this is false. VSS has special hooks into common applications, primarily from Microsoft, such as SQL Server, Exchange, and Active Directory so that they are able to communicate effective with the storage layer as to the state of their quiescence. This is an amazing feature and incredibly handy to be integrated with the operating system automatically. It does not address third party applications which rarely have VSS integration leaving them dangerously unquiesced while many Windows administrators believe that VSS magically manages data flushing from all application layers that it does not, and cannot. </p>
			<p>At the operating system level, where we typically have to work as system administrators, our primary tools for quiescence are snapshots from a logical volume manager (such as those provided by LVM or ZFS) or to freeze the filesystem itself as is possible with XFS. The higher up the stack (higher means closer to the application itself) we freeze the system, the less likely that there will be some artefact still in flight and not written to disk. At the end of the day, however, unless an application itself guarantees that it has flushed all data down to disk and left nothing in a cache or in process of some sort, the best that we can hope for is a best effort situation. This is true regardless of the operating system. It is a universal challenge. Computers are simply really complex things.</p>
			<p>In the majority of cases, nearly all cases in fact, one or more of these quiescence methods will work just fine to get data safely to disk. In fact, most backups and most businesses rarely even rely on this much protection and typically rely on best effort file locks and multiple backups to provide working copies of a sufficient number of files on any given system. Most workloads write to the disk infrequently making relatively few into high-risk scenarios. But we cannot overlook at all of these methods leave us with no ability to truly ensure that a system is completely quiesced and we are always taking some degree of chance that critical data will be in flight in some form. If we are talking about a web server that only serves out static websites the risk might be so low as to simply ignore it. If we are talking about a critical enterprise database used for financial and accounting data the risk might be enormous and one that could never be undertaken no<a id="_idIndexMarker790"/> matter<a id="_idIndexMarker791"/> what. So, if we want to take backups at a system level, we need to consider our workloads, their quiescence, and what risk that creates for us.</p>
			<p>When we take a backup using one of these mechanisms that does not coordinate end to end with the application workloads to ensure that the data on physical media is complete and quiet, we refer to our backup as being <em class="italic">crash consistent</em>. This is an important term and one that is used often. Crash consistency does not mean that the system has actually crashed, but rather it refers to the state that storage is in after the abrupt crash or loss of power in a computer system.</p>
			<p>Crash consistency is such a critical idea that we need to really understand what we mean when we say it. For some it is said as a scary warning of impending doom. Others use it to imply a reliable system state. So, what does it really mean?</p>
			<p>When any computer crashes completely, meaning that there is no ability for recovery or protection after the event has occurred such as when there is catastrophic hardware failure or power is suddenly unavailable to the system, there is absolutely no mechanism to ensure that the data that is on the disk is accurate or complete. As any computer user is aware, sudden loss of power to a computer is generally not a problem, except that any unsaved changes to a file or our latest progress in a video game or whatever will be lost. This feels obvious and we do not normally think of it as being a significant problem of system design. Once in a while, we will find that a program or data set has become corrupted. We all fear this when a system returns from a crash, but rarely is it actually a problem - at least as an end user.</p>
			<p>This state of a system after an actual crash is essentially identical to the state of a system where a backup is taken without any quiescence. Any data that is currently being modified will be lost. Once in a while data that we thought was already saved to disk will be corrupted. By and large, everything will be there. This is why we call backups or any copy in this state crash consistent, meaning in the same consistency as a catastrophic crash on a physical machine.</p>
			<p>When talking about desktops it is easier to illustrate the point. The same risks exist on servers, but to a much more serious degree. With a desktop we tend to have a single user who has a very good idea of all data that is likely to be currently in flight and storage is pretty straightforward. In a server environment not only do we likely have lots of storage complexity with different possible and possibly risky mechanisms at every turn, but we likely have larger and more numerous cache situations throughout the stack and one or <a id="_idIndexMarker792"/>more multi-user workloads sitting on top that will rarely <a id="_idIndexMarker793"/>be interacting with users meaningfully at the time of a crash. </p>
			<p>Consider an example web server, a common example, with data stored in a database. A user has some interaction with the web page maybe entering contact details for sales or uploading a form or processing some banking transaction or placing an order. The user believes that the transaction completes as they receive feedback saying that their submission is complete. The user will likely not refresh the page or return as to them, the transaction is completely. But is it?</p>
			<p>The web server may have queued the transaction in memory to be handled a few seconds later, or left part of the data in a cache. Or it may have written the transaction to a database, and that database might still be holding it in a cache. Maybe the database has written the data to its file, but the filesystem has the data in a cache. Maybe the filesystem has committed the data, but the logical volume manager has the data in a cache. If you have hardware RAID, there is likely a cache there and before data gets written out to physical disks it will sit in that cache. The physical disks themselves can have caches which, one would hope, would be disabled in this situation but may not be and represent yet another place where data might be cached before the drive head actually puts the data physically onto the disk.</p>
			<p>Of course, lots of mechanisms probably exist to protect against many of these failures. Disk caches should be disabled, RAID caches are often protected by being non-volatile RAM or have a battery to allow them to flush to physical disk even after the system experiences a power loss, databases often log transactions before doing their final write so have the ability to roll back if they cannot roll forward and applications generally do not report that all is well to the end user until at least some degree of certainty exists that the transaction has been recorded. All of those are situations that we hope exist, however, and nothing guarantees them except for a combination of application and system design end to end. The best designed application can still be fooled by a hypervisor that presents volatile RAM as if it were a hard drive and reports quiescence where there is no storage written at all. So, trust is required.</p>
			<p>Dropping data in flight is only one concern. The other is corruption. Many layers along this path have a potential to become corrupted by an incomplete write operation. An individual file is often where we see corruption occur, this is the most common place. Twenty five years ago, we saw filesystem corruption regularly as well. Today this is not a common worry, but it does exist as a risk, still. Different filesystems carry different levels of risk for this<a id="_idIndexMarker794"/> type of scenario. In theory a logical volume management layer<a id="_idIndexMarker795"/> could hold some corruption risk. Certainly a storage layer of RAID or RAIN can become corrupted and sometimes, such as parity RAID, have some potential for total data loss during a corruption event.</p>
			<p>All of these steps are unlikely, but all are possible. Missing or corrupt data will generally not happen during a normal crash. How likely a disaster is to happen depends on many factors. We should neither look at crash consistency level solutions as useless or total loss waiting to happen, but neither should we pretend that they do not ignore basic risks of not having any system in place to ensure consistency. In all cases we are simply rolling the dice and hoping for the best. I have seen a great many small businesses lose data with what they had thought were completely reliable backups because they were depending on crash consistency and a vendor had pretended that data corruption was not a concern. Because crash consistent mechanisms are cheap and simple, they get a lot of attention from vendors.</p>
			<p>The alternative<a id="_idIndexMarker796"/> approach is called <strong class="bold">application consistency</strong>. We call it this because it refers to the potential state of the system when the workload application(s) are able to confirm that they have flushed all of their data to disk and have nothing currently in-flight. Applications may do this simply by confirming that they are quiesced currently, or they may have mechanisms to force such a state to occur. In either case, they avoid the problems of crash consistency by applying intelligence from the very top of the application stack and verifying that data is ready to be backed up.</p>
			<p>This approach requires the application support providing this quiescence and if we want to coordinate this action with a backup mechanism, of any sort, we have to have some way for the backup mechanism to call the application to request quiescence, or for the application to call the backup mechanism. In the Windows world there is a standard interface in VSS as we discussed earlier than any application can support if they chose to do so. Linux lacks a similar standard at this time.</p>
			<p>Because all of these mechanisms are complex and non-standard it presents major challenges when working with anything outside of a limited scope of popular services that are well known and supported. In reality, Windows is also similarly limited but the use of a smaller set of very standard services is more common there. </p>
			<p>Many applications choose to tackle this problem by incorporating their own backup services rather than attempting to depend on some other mechanism. Some may build an extremely <a id="_idIndexMarker797"/>robust backup mechanism that directly supports many <a id="_idIndexMarker798"/>different options such as built-in schedules, multiple backup levels, and storing directly to many different types of media. This is not very common but certainly exists.</p>
			<p>The most common option used by everything from the simplest of applications to enterprise databases is to simple write a backup file from the application directly to the local filesystem. The is extremely common and very effective. This produces a simple file or set of files that can be designated as a backup and kept from being used other than for the purposes of another backup mechanism using these files to then copy or send to another location.</p>
			<p>A really common example of this is MySQL and MariaDB databases. Databases are our hardest type of application to safely backup and so nearly any database system will incorporate <a id="_idIndexMarker799"/>some means of safely protecting the data without having<a id="_idIndexMarker800"/> to resort to a drastic, brute-force step such as shutting down the databases, coping files, and starting it back up when completed.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor201"/>MySQL example with mysqldump utility</h2>
			<p>Almost certainly<a id="_idIndexMarker801"/> the most well-known application level backup<a id="_idIndexMarker802"/> tool in the Linux ecosystem is <strong class="source-inline">mysqldump</strong> that comes with MySQL and MariaDB. This simple command line tool connects to a running MySQL or MariaDB database server, locks it and quiesces all of its data in RAM (it has no need to flush to disk), and then saves a copy of this data to disk. It is as simple to use as any utility could be while also being outrageously power. It has almost no impact on a running system and because it does not need to do any complicated flush to disk before taking a fully application-consistent copy to disk it does not need to stop read operations to the database and only needs to halt write applications for the briefest of moments.</p>
			<p>I will show here the simple, single command to take an easy backup of every database managed by a single MySQL or MariaDB instance:</p>
			<p class="source-code">mysqldump --all-databases &gt; mysqlbackup-`date +%H%M`.sql</p>
			<p>This example is about as basic as you can get, and yet it is so effective. You can run it anytime and because it takes the current time during the process it will not overwrite another backup file, even one takes just a minute earlier. You can make it write anywhere on disk that you like. You can use standard utilities to compress the resulting backup file while it is being written or at a later time very easily. Keep in mind this is a backup file, a file for the purpose of making a backup, but not an actual backup yet at this point.</p>
			<p>Once this file is stored to disk, it is safe and any backup tool can now take this file without worry than an application is using it or modifying it and place it wherever it makes sense. Total flexibility and simple application-consistency.</p>
			<p>At this point I think we understand consistency, how locks and quiescence play a role, and all of the problems that we may face while trying to make a backup of even a single file or application (remember that not all backups involve data that is actually in file format on storage anywhere), let alone trying to back up entire filesystems or systems. Appreciating the challenge is key to understanding why we are concerned about different aspects of different types of backup tools which we will<a id="_idTextAnchor202"/> look at next together.</p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor203"/>Backup strategies &amp; mechanisms</h1>
			<p>Backups are actually<a id="_idIndexMarker803"/> far more complex animals than most people imagine. So often when dealing with backups we are simply told to <em class="italic">take a backup</em> as if this is a straightforward activity with few variables. In the real world we do have some stock approaches that meet the majority of needs, if only minimally. There are cases, however, where to do effective backups requires a lot more thought and deep understanding of our workloads and infrastructure to be able to get correct.</p>
			<p>In the good old days, you know like the 1980s and 1990s, backups were almost always the same. They involved a simplistic agent of some sort, like the standard Linux <strong class="source-inline">tar</strong> command, that would run on a schedule (that we probably had to set manually with something like <strong class="source-inline">cron</strong>) that would take all of the files in a directory or, more likely, the entire system and package them up as a single file and place that single, large file onto a tape device. That tape device would then require a human to remove the tape and transport somewhere for safe keeping.</p>
			<p>Over the years new technologies would come out and backups slowly became more robust and more complex to discuss. Tape became less popular and other backup targets ranging from swappable hard disks to constantly online storage emerged. With backup media evolving the mechanisms that took backups had an opportunity to move from discrete <em class="italic">one tape per backup</em> schedules to more flexible or complex designs. With all of this came complex backup applications and the backup world went from simple and basic to quite complex.</p>
			<p>Because there are so many variables today, simply taking a backup is no longer a straightforward concept. People all have different views and ideas of what a backup entails based on their desires and experience. It is so varied that we even risk tunnel vision and thinking that all people see and experience backups similarly when, in reality, there are so many different ways being used in the real world to handle backup needs.</p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor204"/>Types of backups</h2>
			<p>First, we will <a id="_idIndexMarker804"/>discuss the actual backup mechanisms that may be used. These generally fall into two categories: agent based and agentless (I really hate both of these terms) but people normally think of them as three categories with a third being ad-hoc scripts. All of these ideas around their identities are totally wrong, of course, as are so many things that people tend to say. The terms have made inroads and you will need to<a id="_idIndexMarker805"/> use them regardless of their inaccuracies, however.</p>
			<h3>System level backups</h3>
			<p>System level<a id="_idIndexMarker806"/> backups <a id="_idIndexMarker807"/>are the so-called agent-based backups. These get this name because typically a software agent that runs on top of the operating system is installed and is visible. It can be seen in an installed software list somewhere on the system. The agent then runs and grabs files from <em class="italic">inside </em>of the operating system context and send them off somewhere else to be stored. There is often additional processing steps such as packaging, compression, deduplication, and encryption, but all of those are technically optional.</p>
			<h3>Everything is an agent</h3>
			<p>When we <a id="_idIndexMarker808"/>say that we have an agent-based backup solution it <a id="_idIndexMarker809"/>brings to mind software that must be purchased, downloaded, and installed onto our computers. Certainly, this is a very common thing that people will do.</p>
			<p>The idea of an agent is, however, far broader. Backup agents may be complex backup utilities that are built into the operating system rather than obtained separately. In the Linux world this is just as likely to be the case as not because so many powerful backup options are included in the repository ecosystems of the typical business-class distributions.</p>
			<p>Agents get even broader still. Classic utilities like <strong class="source-inline">tar</strong> and <strong class="source-inline">rsync</strong> are, or at least can be, backup agents. They are installed software components that can be used to carry out a backup from inside of the operating system.</p>
			<p>We can keep going. If you write your own script to move blocks or files around that, too, is an agent. The idea of an agent is incredibly broad and, logically, some component is always needed to perform a backup job and that component is always an agent.</p>
			<p>By working from inside of the operating system, an agent located here has the ability to utilize obvious access methods to the data that we want to back up. It can access the block devices directly either by talking at a block level with a tool like <strong class="source-inline">dd</strong> or by using a slightly high-level logical volume tool like LVM which can provide robust block level handling; or it can use the filesystem layer to request files one by one; or it can use APIs to talk to running applications that can provide some type of higher level managed data access. In one or more of these ways an agent can access the data necessary for a backup.</p>
			<p>All of this is to say that agents have the task of talking to the operating system in an attempt to trigger some level of quiescence from the storage subsystem to increase the chances of getting a consistent backup. Some agents talk extensively to many applications and some talk to none. </p>
			<p>Agents do all of the work and offer us many ways to talk to our data. But they have the limitation that they can only access what the operating system can access and they can only be used when the operating system is up and running. This last point means that any resources that the operating system itself is using will be in use at the same time that the agent needs to access them. This makes for a real challenge in system-wide consistency. However, we rarely are truly concerned with consistency on a full system scale. Of course, we would prefer it, but the majority of system files are of little consequence and easily replaced if lost or corrupted. Typically, only critical application data or maybe installed applications themselves are of major concern and the rest is just useful in case of a need to restore quickly.</p>
			<p>Typically, but not universally, with system-level backups it is necessary when performing a restore operation to install an empty operating system and then install the restore agent and allow the agent to run against the stored backup and put files back into place inside the operating system. This often means that system-level backups are excellent at taking backups<a id="_idIndexMarker810"/> of individual files and being able to restore<a id="_idIndexMarker811"/> individual files but find it more difficult to deal with having to restore entire lost systems. </p>
			<p>There was a time when essentially all backups worked this way and the general assumption was that nearly all restore operations were to retrieve single lost files rather than to retrieve entire failed systems. </p>
			<h3>People just do not lose files like they used to</h3>
			<p>In writing this<a id="_idIndexMarker812"/> chapter I was struck by a massive, fundamental change in the computing experience for most people over the last thirty years. Through all of my younger years in the industry and even before simply as a computer user, the big concern that we always had was losing, deleting, or having an individual file become damaged in some way. We really all pictures the concept of backups as just being lots of individual files being copied somewhere, and we envisioned any restoration to be a process of copying those files back once we had a place to put them and more often than not, needing to do so when there had been no failure whatsoever.</p>
			<p>This last point is what is most interesting to me. For decades it was assumed that people were going to accidentally overwrite or just delete critical files that they needed with great regularity, and it really happened. Needed to track down and restore an individual file that an end user deleted was a completely common task that was done so often that most large companies had teams dedicated to doing nothing else. </p>
			<p>Computing has changed such that today, this is rarely the case and the idea that an important file would be lost on its own does not seem impossible, but certainly seems unlikely. I assume users have become better educated and far more diligent with how files are managed, and of course a huge percentage of work that used to be done as individual files has moved to some sort of database of data stored behind an application front end that protects end users from themselves (this could range from Google Docs to Flickr, but even end users working with their own file management has decreased significantly.) Most operating systems, and even some applications themselves, now implement <em class="italic">trash</em> features that hold deleted files until they are explicitly disposed of to give end users lots of time to change their minds or find what was deleted accidentally. And finally there are features like self-service file restores included in operating systems or from simple add-on applications that allow files to be brought back from other media without needing to engage traditional backups or backup teams as the data is still stored somewhere locally (not to mention online backup systems that can be used by end users without intervention.)</p>
			<p>It is simply interesting to note that something that had become such a common problem that it drove most of an industry not that many years ago is effectively gone today. The days of computers<a id="_idIndexMarker813"/> being seen as file management and manipulation devices are gone and have given way to being online data consumption devices where most users are not even file-aware any longer. Many younger users can even be confused at the concept of file and storage management today.</p>
			<h3>Platform level backups</h3>
			<p>The alternative <a id="_idIndexMarker814"/>approach is to take a backup from the<a id="_idIndexMarker815"/> platform level, that is from the layer underneath the operating system. In practical terms this means the virtualization layer which would entail typically the hypervisor, but in some cases can be limited to external (to the operating system) storage virtualization, this would be especially common if using a SAN device, for example.</p>
			<p>This approach became all the rage with the advent of virtualization. New backup vendors entered the market with new technology aimed at making extra-operating system backups easier and faster. Backups at this level, of course, do not have access to things like the filesystem or application APIs to communicate with those components or to have knowledge of the data layout. So backups taken at this level must do so by talking directly to the block devices or to hypervisor level storage abstraction layers, which are essentially block devices themselves, leaving us generally blind to what the data is that we are actually getting. At this layer we might know, but only maybe, about the physical separation of devices presented to the operating system. But we have no visibility into filesystems or deeper. So all of the concerns that we have with system-level backups and consistency are potentially magnified many times over. Not only do we have no way to know if an individual application workload has quiesced, but we cannot even tell inform the system-managed filesystem, file locks, or logical volume manager that we are attempting to read the block devices. So if we take a backup at this level alone, we are totally blind and simply reading blocks off of a virtual disk with no reason to believe that it is in any sort of a consistent state and nothing tells the operating system that anything is happening at all. If we do this alone, the operating system literally has no way to know that it is happening.</p>
			<p>It does not take very much thinking about how this mechanism has to work to realize that platform-level backups without an agent are all but impossible. We could shut down a virtual machine and take a backup of its storage layer while the machine is powered down, of course. The shutdown process of a total operating system is effectively the best way to ensure full quiescence and consistency. How do we shut down a virtual machine from the platform level without just pulling the virtual plug on it and putting ourselves into a state of crash consistency? Surprisingly, with an agent.</p>
			<p>This is the big secret of platform-level (aka <em class="italic">agentless</em>) backups: they use an agent! They have to. In the real world taking a backup directly from any lower level service would be completely unreliable. But I hear you murmuring <em class="italic">I've done this, everyone has, and we don't install any agents</em>.</p>
			<p>What makes the backup agents in virtualization environments hard to identify is because we install them automatically and universally so that we do not think about it. In some cases, these agents are built right into the operating system. They have different names depending on the platform that you are using. The most famous is VMware Tools as VMware uses this and the installation process is well known. KVM, the primary hypervisor in the Linux world, has the guest agent built not just into most Linux-based operating systems, but actually baked right into the Linux kernel itself! So you never actually need to see the agent, even though it is almost always there. Many Linux distributions do the same with Hyper-V's equivalent agent called Linux Integration Services, which are not built into the kernel but are often included in the operating system automatically.</p>
			<p>In these cases, the agents from the hypervisor vendors that I have used as examples are not strictly backup agents, they are general purpose agents used to coordinate activity between the hypervisor and the operating system, but in reality, that is simply a more advanced agent function and nothing more. They are agents in every possible sense of the word, they are software that has to be installed into the operating system to work, and just like more traditional backup agents they are sometimes included with the operating system and are sometimes third-party add-on packages.</p>
			<p>These agents <a id="_idIndexMarker816"/>are<a id="_idIndexMarker817"/> generally part of a set of high-performance drivers needed to make full virtual machines work efficiently and so are rarely left missing. They provide a critical channel that allows the underlying platform layer to communicate up to the operating system layer or even higher. This communications channel is what is used to issue graceful shutdown commands to a virtual machine as well as to inform it that it needs to quiesce a filesystem or take a snapshot of a volume. In theory this agent could even hook into a specific application, although this is mostly only theoretical in the Linux world.</p>
			<p>There are important advantages to taking platform level backups. If we ponder the workings of these backup systems, we easily see that the expectation is going to be a block-level image taken of an entire file system container or block device. The real advantage to this is in its completeness. If we take an image level backup, rather than a file backup, we have an opportunity to have the entire device, not just portions of it, that will presumably allow us to restore a system completely rather than having to rebuild and then restore partially.</p>
			<h3>Disk images</h3>
			<p>When we talk about<a id="_idIndexMarker818"/> file level backups the essential mechanism, we are discussing is ultimately a file copy of some sort. File X exists on the filesystem that we want to protect and in order to protect the data that is in that file we copy that file to another location so that the data can exist in more than one place at the same time. Easy.</p>
			<p>With full block device copies at the platform level, we refer to the resulting copy as an image or a disk image. In theory we could be copying from one physical block device to another or, at the very least, from a virtual block device to a physical one. In nearly all real-world cases, especially those involving backups, what we actually do is copy the block device to a file. That file is called a disk image and is sometimes referred to oddly as an ISO file.</p>
			<p>We call this an image because it is essentially a picture of the entire disk as it was in any given moment. We also use the term snapshot to refer to this same operation. In regular English a snapshot is an image. The words are nearly interchangeable. The same is true in computing. Sometimes one is used to imply something different from the other, but there is no accepted definition that makes them not completely overlap.</p>
			<p>In every day usage, images are used to refer to images stored as regular files, such as ISO files and contain a complete copy of the original filesystem. Snapshots are used to refer to a nearly identical scenario but where a logical volume manager creates a partial image file that is in some way linked to the original file and may contain only the differences between the two. But that file is often used to make a standalone image file, which is indistinguishable from an image file otherwise, but is still called a snapshot. Which leads to the inappropriate situation where two identical files can have different designations based solely on untraceable and unknowable histories. Obviously, there are common misconceptions in what these files are that lead to these different uses of names for the same thing. Nevertheless, images and snapshots are overlapping concepts.</p>
			<p>Because a disk image contains the entire contents of a block device it can be used to directly restore the entire block device, even onto hardware that has no knowledge of what that block device should contain. If you backup a Linux server and restore it, the entire system is restored, even if it was encrypted. An image will not bypass the encryption, but the encryption will do nothing to alter the imaging process.</p>
			<p>Most virtualization platforms, even when using external storage devices like a SAN device, will storage the disk(s) belonging to a virtual machine as one or more disk image files (often in an advanced format rather than a raw ISO file.) This demonstrates that the image file is a full block device (virtually) and so can be used anywhere that any other block device is used. This is very important when we want to talk about restoring our data. If we have a complete image file, no restoration is needed if it exists in a place where we can access it. We can mount it directly under the right circumstances.</p>
			<p>Image backups have some big advantages because they can be taken <em class="italic">closer to the hardware</em> when it comes to performance. There are also big disadvantages, so it is not all roses. </p>
			<p>Advanced snapshot technologies that allow the system to essentially freeze a moment in time and work with it while the system is still under active use is a big deal for allowing major backups of active systems to have effectively. Taking full system images is by far the easiest process for handling backups because we do not have to think about what we are backing up, we just grab everything. Not only can we grab an entire operating system, but we can grab all of every operating system that exists on a single hypervisor. We can even take backups of virtual machines that are powered down as well as those that are actively running. System level backups have to happen per system, can only be <a id="_idIndexMarker819"/>done when the system is running, and only in very limited situations can they effectively take a complete backup and even more rarely can they find a way to do it that provides for a complete block level recreation of the system that can be restored in the same way.</p>
			<p>Platform level backups have been all the rage for over a decade now for good reasons. They are efficient, they protect against failing to select the right data to be backed up, they can easily be handled by a different team than the standard system administrators if necessary, and they can be done at large scale with essentially no deployments needing to be done to work (other than the deployments of agents that are needed otherwise, and the deployment of the backup software to the hypervisor layer.) Platform level backups fit perfectly into how most organizations want to be able to work, and they are exceptionally easy for support vendors to provide blindly as a service without needed to do any due diligence.</p>
			<p>Platform level backups come with plenty of caveats too, however, and we need to be aware of why we might not want to be focused on them most of the time. Being blind they generally require the most storage capacity to hold the backups as they tend to contain a lot of data that is unnecessary such as system files. This bloat also means that moving the resulting images around whether for archival purposes or to get the file(s) where they need to be for a restore will potentially take longer than would otherwise be required. Agents still need to be deployed. Mistakes are easily made because the system is almost universally misunderstood and, like so many things that are too complicated for the average user to grasp, they are seen as a panacea rather than just another tool to consider.</p>
			<p>Quiescence is a real struggle for platform level backup. Even when all agents are properly in place, the nature of those agents is that they expose fewer options and have fewer hooks into the application layer meaning that there is a higher risk that backups taken from this layer will only be crash consistent. Operating system files will be protected because there is nearly always proper communications made to the operating system itself via the agent, but almost no application workload will be protected by that.</p>
			<p>Platform level backup is flashy and cool. It is an easy way for vendors to capitalize on backup fears while doing minimal work; it is an easy path to big margins with any actual risks being pushed off to the customers who rarely do their homework. It makes customers feel safe because the risks are too complex for even more IT practitioners to grok, and it lets them tell themselves that they are protected. Ignorance makes people sleep well at night. </p>
			<p>Platform level backup is a power tool and a modern marvel, but it is still just one mechanism, just one approach and certainly not one that fits the bill every time.</p>
			<p>That was a lot of <a id="_idIndexMarker820"/>information about two very simple backup mechanisms. We needed that solid understanding so that we can move on to talking about many concepts that exist in and around the backup and recovery world.</p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor205"/>Snapshots, archives, backups, and disaster recovery</h1>
			<p>There are many technologies that are either confused with backups, or may be a component of a backup. In this section I want to break down these basics and make sure that we understand what they are, why they matter, how they are used, and when we should leverage them ourselves.</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor206"/>Snapshots</h2>
			<p>In our last section <a id="_idIndexMarker821"/>we talked about taking block device images, how images and snapshots are truly the same thing, and what people tend to mean when using the term snapshot instead of the term images. Now we are going to really delve into snapshots.</p>
			<p>Snapshots, as people tend to use the term, are amazing tools for doing some extraordinary things with storage. Snapshots are typically used to grab a momentary image of the state of a block storage device. The term snapshot is very descriptive in this case. </p>
			<p>For most snapshot systems, and as it is intended by most people using the term, the snapshot that is taken is kept on local storage along with the original data and the two are intrinsically linked. The assumption is that the snapshot contains only changes, or the differential, between it and the original data. There are multiple ways to achieve this, but essentially the end results are the same: a file much smaller than the original data that is quick to create but is able to perfectly recreate the state, or <em class="italic">snapshot</em>, of the block device at the time that the snapshot was taken.</p>
			<p>The obvious risk to this process is that the snapshot, presumably, contains <em class="italic">only</em> the differences between the time that it was taken and the original data. So, there is no protection here against system failure. If the original file becomes damaged or lost, the snapshot is useless. A snapshot might be an effective protection mechanism against accidentally deleting a file, malware, or ransomware at the system level because it allows a system to revert to its pre-compromised state easily and quickly. This is an important value and early snapshot mechanisms were often used expressly as a means of protecting against file deletion and overwrite mistakes. But since they are tightly coupled to the original data, the most important protections afforded us by true backups are completely missing here.</p>
			<p>This risk has led to the mantra of <em class="italic">Snapshots are not backups!</em> You hear this everywhere. And it is true, they are not, on their own. Snapshots are a really critical component of backups, though. Overstating the mantra has caused many people to incorrectly believe that backups created using a snapshot as a source are not real backups.</p>
			<h3>Types of Snapshots</h3>
			<p>A snapshot is a <a id="_idIndexMarker822"/>generic idea that can be executed in many ways. Two mechanisms popular for snapshots are copy on write and redirect on write. Nearly any production system that you encounter will utilize one of these two mechanisms. Understanding these two give a good insight into the thinking and design of snapshots and explain why they can be so powerful.</p>
			<p>First, copy on write (sometimes called COW Snapshots.) When a copy on write snapshot is initiated, a block device is essentially frozen in time. Mostly this is theoretical because nothing actually happens, and until there is an attempt to make a change to the storage, nothing will happen.</p>
			<p>When someone attempts to write new data to the block device, at that time the storage system takes any block that is about to be written and copies it to a new location and then overwrites the original block with the new data. There is a performance hit while all this copying is going on, of course, and as changes start to add up the size of the copied data will grow. So while the snapshot is literally of zero size when we first initiate it, it keeps growing as long as we keep writing to the block device.</p>
			<p>Copy on write snapshots are popular because they have so little penalty to being destroyed. To delete the snapshot all that has to be done is the extra data be deleted. The working block storage is untouched in all of this. Even a process monitoring the original block device would never know that there was a snapshot somewhere because everything related to the snapshot is external to the original block device. So there is basically no penalty to cleaning up the snapshot when it is no longer needed.</p>
			<p>The alternative approach, redirect on write, works a bit differently by manipulating points. These points point to all of the block locations of a storage device. When a block is changed by writing new data to it, the system does not modify the original block at all, but rather writes the new data in a new location and simply points that blocks pointer at the new location instead. In this way there is essentially no impact to any given write operation. The impact is not zero, but it is extremely low, especially compared to copy on write which requires moving existing data around and rewriting it during normal operations.</p>
			<p>With redirect on write we get some great features like the ability to maintain essentially indefinite versions of our storage device. In fact there are storage devices that simply use redirect on write for all operations and do not think of changes as snapshots but treat the entire storage system as an eternal snapshotting mechanism so that any portion of the storage can be rolled back to literally any point in time.</p>
			<p>The caveat of redirect on write, because it sounds pretty perfect at first glance, is that you still have a growing amount of data over time and if you keep interim versions of data, rather than just a single point in time, the degree of growth can end up rather staggering. If you then need to clean that up the process of cleaning up the system of pointers can get somewhat complex and has a performance penalty.</p>
			<p>Copy on write tends to be the best choice when we are talking about short term snapshots that are being taken, for example, just before a major system update is performed and the need to roll back to the time just before the update is critical, or a snapshot is taken in order to send data externally in a backup operation. At the end of either of these tasks, the snapshot would be destroyed and forgotten about. Copy on write is all about creating and destroying the entire snapshot quickly, not keeping it around, because the penalties of copy on write will exist only for a short time and the destruction process is painless.</p>
			<p>Redirect on write is really powerful when we intend to keep the snapshot around for a long time and when we want multiple snapshots that build off of one another. Because there is almost no penalty during use and only a real penalty during the destruction of the snapshot(s) it plays the opposite role of copy on write.</p>
			<p>So: redirect on <a id="_idIndexMarker823"/>write is probably the best choice for using snapshot as ongoing data protection and copy on write is typically what is used under the hood for things like backup software to originate a dataset.</p>
			<p>Even if we do not have any special backup software, we can use a snapshot taken by our storage system and use it as the building block of a meaningful, manual backup. This is far simpler than it sounds. In the simplest of examples, assuming that we have something like a mounted tape drive on the same system to write to, we simply use the storage systems ability to mount the snapshot as an immutable version of the block device and copy that to the tape. </p>
			<p>When we do this, the source snapshot file might be absolutely tiny, or theoretically even zero bytes, but what is sent to the tape (or any other storage media) is a complete copy of the entire block storage device in the state that it was at the time that the snapshot was taken. This is the miracle of snapshot technology. The automatic and generally completely transparent recreation or rehydration of the original block device state while using minimal, often trivial, additional storage space.</p>
			<p>When we do this, we have the semantic challenge of what do we call the copy that is no longer on the original media. It is an image, of course, but typically we still refer to it as a snapshot as it is identical to the snapshot in every way and represents a snapshot of the block device at a point in time. So, when we do this, the statement that snapshots are not backups becomes false because we can have a true backup that is also a snapshot. The correct statement would be that a snapshot is not typically a backup. Snapshots do often get used as backups, however, and even more often as the basis of backups.</p>
			<p>Because snapshots provide the ability to freeze the block device in a moment (or more than one moment) in time and then provide a way to utilize that frozen moment without having to interrupt the block device for future options it is perfect as a means of creating a source from which to take a backup while allowing the running system to continue on its merry way. It must be noted, though, that traditional snapshots all happen on top of the same underlying physical block device. So, while we do not need to halt the storage device in order to perform snapshot operations, we <a id="_idIndexMarker824"/>do use IOPS (input/output operations per second) from the original device. Our operations are not zero overhead, magically, but they are much lower overhead than other backup mechanisms will tend to be.</p>
			<p>Because of all of this, snapshots are a popular tool to use under the hood and behind the scenes to make many modern backups possible. Essentially every hypervisor or storage device<a id="_idIndexMarker825"/> level backup (platform backup) is powered by snapshots. In many cases, even system level (so called agent based backups inside of the operating system) use snapshots today, across all operating systems, not only Linux-based ones.</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor207"/>Archives</h2>
			<p>Closely related to, but <a id="_idIndexMarker826"/>importantly different from, backups are archives. These two concepts are very often confused for each other, even in very enterprise circles. In theory, just by saying the names and asking someone, anyone, to describe what they think of a backup and an archive is enough to get someone to self-describe why they are not the same thing. But taking <em class="italic">I can define it</em> and moving that to <em class="italic">I can clearly articulate and internalize that the two are different</em> is not always automatic.</p>
			<p>A backup, which we will define more in a moment, is a copy at a minimum. That much should be clear. Our English use of the term denotes this. We refer to things as our <em class="italic">backup copy</em>. If you articulate it well, everyone always agrees that if it is not a copy, then it is not a backup.</p>
			<p>An archive is different. Archiving something does not suggest that there is not a copy, we sometimes even say things like <em class="italic">archival copy</em>, but nothing in the definition of archive is it suggested that a copy is required. An archive refers to long term storage, often assumed to be either lower cost, harder to access, or otherwise <em class="italic">archival</em>. Long term storage, but not necessarily close at hand. </p>
			<p>Archival storage could simply be a second hard drive where data does not change. Maybe it is offline tape. Maybe it is cold cloud storage. Archival storage does not mean, necessarily, that the performance or accessibility of the storage is less than regular storage would be, but it is very common for it to be. </p>
			<p>Most organizations use archival storage as a lower cost means of maintaining emergency or occasional access to data that is no longer needed on a regular basis. It is easy to talk about potential scenarios for this. Old data could be last year's financial records, copies of old receipts, video footage from material already processed, old invoices, old meeting minutes, blueprints from completed projects, old project artefacts, you get the picture. Businesses produce enormous amounts of data that they never anticipate needing to use again, but cannot necessarily delete entirely.</p>
			<p>The assumption is <a id="_idIndexMarker827"/>that an archive, if it contains data of any importance, also needs to be backed up just like any other storage. The rule of thumb is that anything worth storing is worth backing up, if you feel that the cost or effort of taking a backup is too much then you should carefully reevaluate the desire to continue to store the data at all. Why continue to pay for its storage?</p>
			<p>In a properly planned storage infrastructure this is exactly what happens and archives are a powerful mechanism for data retention and protection when properly combined with a backup. As the term archive means that the data is not changing it means that there should never be a lock on the data and backups are as simple as can be. None of the complexities that face the backup process exist when dealing with an archive. The need to take backups often also does not exist; a single backup might be all that is needed if the archive <a id="_idIndexMarker828"/>is static. Or if the archive is only rarely changed, maybe a monthly or annual backup might be adequate.</p>
			<h3>Converting backups to archives</h3>
			<p>A not <a id="_idIndexMarker829"/>uncommon<a id="_idIndexMarker830"/> thing to have happen is for a backup to become an archive. This happens far more frequently than you might imagine and happens for reasons that most of us can identify with. I have seen it happen in the largest of organizations with no policy to avoid it.</p>
			<p>Under normal circumstances, all data that you want to store is live and in its proper location for use and available normally. A backup is taken of this data and, in case of disaster, the system can be restored. until there is a disaster all of the data exists both in the original location as well as in one (or more) backup location(s). This redundancy of locations is what makes the copies into backups. Technically, if the original storage location fails, the primary backup (which might be the only backup) stops being a backup and turns into the temporarily primary source location of the data. We never speak of it in this way as we all know what we mean by <em class="italic">restoring from backup</em>, but for that period of time when the original storage is gone, the first backup is now the master of that data and not a backup any more. It may have backups of itself, and generally we would, but you can only have a backup when there is data that is not a backup for the backup to be a backup of! Hard to explain, but a critical concept.</p>
			<p>The reason that this matters is because once you take a backup of data, it can be very easy to feel that our backup protects us and that we can then delete the original source data. It feels like we can, because there is a backup already. The problem here is that if we delete the original data, the backup (or at least the first backup) stops being a backup and instead turns into an archive! And in most cases, if the assumed master source location is no longer there, there may be no mechanism to take any further backups of this <a id="_idIndexMarker831"/>archived data. There may not even be any <a id="_idIndexMarker832"/>means of locating it.</p>
			<p>I have seen this myself in the real world. A daily backup task would run and end users just assumed that they did not need to store any data that they needed, even though it was a legal requirement to keep it and to keep it backed up, on the primary systems and so would delete the data immediately upon creation assuming that their data was protected by the backup system. There were some critical flaws in this plan, however. </p>
			<p>First, by deleting the original, any backups that existed became the source location rather than a backup and were simply a tape-based archive. Given that there was a legal requirement for the data to be retained and backed up, this violated the legal retention requirements. </p>
			<p>Second, the backup mechanism was staggered where some backups are kept for years, some for months, and some for weeks. So if the data happen to exist during a run that was kept for years there was a good chance that the one, singular tape that contained the files in question might be readable to able to retrieve the data, but if the data existed only during a backup run for a weekly job, even the archival version would be deleted in just a few weeks when that tape was overwritten.</p>
			<p>Third, and unrelated to the archival situation, often the data was deleted before the daily backup job ran at all meaning that the data was deleted with no backup or archive ever existing and so was lost instantly. The result was that no backups ever existed, and once in a while a file would get lucky and be retained for a few years without backup, but most files were either never archived at all or were archived only briefly and deleted in a few weeks when tapes were reused.</p>
			<p>Given that the requirement was seven years of retention, plus a backup of that retention, it is easy to see how far off the mark the process was simply because the end users thought that they could intentionally delete the original files because they thought a magic backup process was somehow protecting them. The backup team thought that all data was being kept live for seven years and that if any file was lost or corrupt that the end users would alert the backup team to restore it almost immediately. That the end users would themselves intentionally destroy the data and never alert the backup team that the data had been destroyed was never considered in the workflow, because why would it be?</p>
			<p>It is easy to see <a id="_idIndexMarker833"/>how we can, through actions of the end users in <a id="_idIndexMarker834"/>many cases, accidentally convert our backup location into an archival location and potentially lose data because we do not understand the complexities of the backup processes. If data is not going to be retained permanently in a primary location, then very complex processes are often needed to ensure a safe workflow for deletion.</p>
			<p>Archives are a powerful mechanism to lower cost and keep our mainline storage lean. Maybe we use archives only to reduce cost. In many cases, by keeping excess data away from our top tier of storage, it will allow us to invest in faster, but smaller systems for the data that we use every day. </p>
			<p>Archives can apply to portions of a file as well, of course. Sometimes even databases use a mix of storage locations to allow them to be able to store tables or portions of tables that are accessed regularly on the fastest storage, while rarely touched tables or parts of tables can be kept on slower storage that costs less. This could be automatic inside of a database engine, or an application might use multiple database systems and move data between them at the application layer, for example.</p>
			<p>Archives are a useful tool, but in no way a substitute for backups. </p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor208"/>Backups</h2>
			<p>It is hard to believe <a id="_idIndexMarker835"/>that we have gone this far without actually digging into exactly what we mean when we say <em class="italic">backup</em>. As with many things, it is hard to sometimes jump directly into a definition as there is just so much that we have to consider.</p>
			<p>At the most basic, a backup is a copy of an original set of data. We talked about this above. If the data does not become redundant, then it cannot be a backup. This is the most obvious piece of the puzzle.</p>
			<p>Next, the data must be stored twice. Meaning the hardware must exist more than once.</p>
			<p>In many cases we can do a comparison with paper or some other physical form of data storage. If we have a piece of paper with an important code on it, we feel a sense of urgency to copy that data somewhere - to make a backup. We know how easy it is to smudge, burn, lose, or send a piece of paper through the wash.</p>
			<p>Probably the most confusing piece of a backup requirement is that it be strongly decoupled from the original material. Being strongly decoupled is a term that involves some amount of opinion that makes determining appropriate levels of decoupling a little bit hard. What is decoupled enough for one organization may also not be enough for another.</p>
			<p>When we use the term tightly coupled, we are referring to situations where the original data, and the copy of the data, have a connection between them. At the most tightly coupled is something like the snapshot concept where anything that happens to the original file will ruin the snapshot a well, in all cases. That is fully coupled. Slightly less coupled would be something like storing the copy on the same physical device. The farther separated the copy becomes from the original data, the less coupled it is.</p>
			<p>Coupling can involve more than physical connections. It can also include concepts such as being stored on systems that share credentials like usernames and passwords. Coupling can be complex and there is no single, clear-cut way to describe it. When we talk about keeping backups heavily decoupled, though, we generally means a few things such as: completely disparate hardware and media, physically separate location, and disconnected authentication.</p>
			<p>The idea behind every additional step of decoupling is to keep any event that might happen to the original data to have little to no chance of also happening to the backup data. This could be an accidental file deletion, a failing hardware storage device, a ransomware attack, flood, or fire. There are so many ways to lose our original data, or to lose access to it. We have to consider these possibilities as broadly as possible and assess how much decoupling, and what type, make sense. No two systems are ever truly decoupled completely, but we can decouple to a practical level quite easily.</p>
			<p>The first step is easy and very few organizations make the mistake of attempting to put a backup file onto the same media as the original data. It does happen, however. Using the same drive happens very rarely, but attempting to use a second drive inside of the same device is sadly somewhat common. Of course, as you can imagine, many events that would cause data to be lost on one drive in a computer can cause data on another drive to be lost. A fire inside the chassis, extreme shock, flooding, loss of power, theft, data corruption caused by many types of events, or malicious attack are all likely to destroy a backup simultaneously with the original data. This defeats the purpose of the backup almost entirely.</p>
			<p>To compare to the physical world, we can compare to paper. You have a piece of paper on which you are storing critical data. You keep that piece of paper in a filing cabinet. You are worried about protecting that paper from something bad happening. You can use a single piece of<a id="_idIndexMarker836"/> paper and write the information on it twice. You can have a second piece of paper in the same folder in the same filing cabinet with the data duplicated. You can put a copy on a separate piece of paper in a second filing cabinet sitting next to the first one. Or you can put a copy of the paper in a separate filing cabinet in a different building. </p>
			<p>In these examples, it is easy to see the progression from tightly coupled to highly decoupled. When sharing a single piece of paper, or two papers in the same filing cabinet, the risk is obvious. Almost anything that would hurt the first copy would damage the second. Having paper in a second filing cabinet at least gives a modicum of protection that there might be two different keys for the two cabinets, a cabinet falling into water <em class="italic">might</em> not affect the other, file damage to one <em class="italic">might </em>not affect the other, and so forth, but it does not take much to see that their risks are still coupled, just not as closely.</p>
			<p>By putting a filing cabinet into a completely different building, possibly on a different floor or even in a different town, then having a flood, fire, or theft that is able to destroy both the original and the backup at the same time is extremely unlikely. There is also the chance of a volcanic event, nuclear war, or meteor strike that could still destroy both copies even if located miles apart. This is why we say strong decoupled, but never totally decoupled. </p>
			<p>With our backups we have to consider just how decoupled our data is, and how much it costs to decouple it further. We also have to consider the efficacy of decoupling and how that impacts our business. It is a complex question. For most businesses, though, we will want our backups to have physical distance, a variety of media, a separation of authentication, and multiple copies. What our backups contain and how they will be used play into this heavily.</p>
			<p>There is no simple best practice here, even common rules of thumb rarely apply outside of the extreme basics. Best practices dictate:</p>
			<p>Best Practice: If data is worth storing, it is worth backing up.</p>
			<p>Best Practice: A backup needs to be highly decoupled from the original data.</p>
			<p>The range of possibilities with backups are just so broad that we really must evaluate the needs, across the board, for every business and, in many cases, individual workloads. In some cases, such as backups of entire operating systems, our primary concern may be around rapid recovery in case of hardware failure and the actual contents of the<a id="_idIndexMarker837"/> backup may be trivial. In another case our backup may contain large amounts of highly proprietary data and keeping that backup safe and secure, ensuring that it cannot fall into the wrong hands, takes precedence.</p>
			<h3>Why tape still matters</h3>
			<p>Inevitably when talking <a id="_idIndexMarker838"/>about backups, the discussion of tape media surfaces. Generally, you get one of two responses: tape is amazing and I always use it or tape is dead. Wildly disparate opinions. </p>
			<p>Once upon a time, tape was essentially the only backup media option. Solid state drives did not exist yet and hard drives were outrageously expensive per megabyte and had a terrible shelf life and shock capabilities. Tape was the only affordable and durable media option, and even it was not very good at the time.</p>
			<p>In the decades since, everything has changed, as it often does. Hard drives, solid state drives, and services that host these for you have all become very standard. The biggest factors driving us to tape are gone, we have many options that are all viable. This has led many people to focus on these newer options and not keep up with the advancements in tape, but just like all of the other technologies, tape has advanced too and quite significantly so. </p>
			<p>Tape is a media almost purpose built for backups. It can move linear data at an extremely high rate, is very low cost per megabyte stored, and has incredible shelf durability and shock handling. Tape naturally, unless you leave the tape in the drive, becomes decoupled from the original system as simply as by just ejecting the tape from the drive. You can even have a remote tape system with automated tape ejection to make both technical and physical decoupling a completely automated process!</p>
			<p>Tape carries the benefit of allowing each tape to be at least partially decoupled from each other. Tapes can be stored in different locations and multiple tapes can be used for multiple copies of data. In some cases, groups of tapes might be stored together in a single box, effectively <em class="italic">re-coupling</em> the tapes to some degree. If tapes are stored apart from each other, though, dramatic decoupling is possible not just between the source data and the backup, but within the backup(s) itself!</p>
			<p>Tape is not perfect. It requires physically mounting the tape before being able to restore and it is terrible at locating single files buried deep within a backup. Tape shines at backing up or restoring large quantities of continuous data, but once you start to search for specific data the performance declines quickly. Some companies address the human component of tape<a id="_idIndexMarker839"/> management by employing robots and tape libraries, but this effectively puts the tapes back <em class="italic">online</em> and potentially re-couples them to the original data, at least partially, taking away one of their layers of protection. Straight tape, without a robot or library, has the benefit of needing to hack a human, on top of hacking computers, to be able to destroy the original data and the backup at the same time.</p>
			<p>Tape is useful enough that even some online cloud backup providers use tape as a component of their storage solutions. Tape has an important place not just in the modern world, but in the foreseeable one.</p>
			<p>Of course, backups may not be a single solution for a workload. A single workload or system might need multiple backups taken of it. One backup that is kept locally and moderately coupled that can be used for rapid restores in the event of an accident or hard drive failure. Remote backup to tape or immutable cloud storage for long term, highly decoupled data retention in the event of ransomware or total site loss.</p>
			<p>Decoupling is so critical to a backup being functional or useful that we must include it in our definition of backups, even if we cannot absolutely clearly define what constitutes a significant enough level of decoupling. This is because what is adequate for one organization or situation may not be for another. For me, this means that our definition of decoupling has to be subjective. Stakeholders need to define what a backup is to protect against and define the necessary decoupling from there.</p>
			<p>It should not be glossed over that modern ransomware has become a driving force in organizations beginning to analyze their traditional levels of backup coupling because suddenly the reach and threat of backups having any real level of coupling is dramatic. Ransomware techniques, at the time of writing, aggressively include strategies to ransom backups themselves whenever possible and techniques to hide their activities to thwart the ability of backups to protect against system encryptions. Backups remain the best defense<a id="_idIndexMarker840"/> against such threats, but ensuring extreme levels of decoupling, often requiring techniques like immutable storage and physically taking backups offline so that no computer can reach them without human intervention.</p>
			<h3>If you cannot recover, it was not a backup</h3>
			<p>I have heard this<a id="_idIndexMarker841"/> said so many times and I still love it. Simply, if your backup does not work when there is a disaster, was it really a backup at all? To a small degree this is overstating the case. A backup can fail and that failure can coincide with the moment when an original workload has failed. But this should be statistically so unlikely and if it were to happen there should be a coincidence that is outrageously obvious.</p>
			<p>What many want to express is a need for testing backups. Backups are complex and knowing the speed, process, and effectiveness of using the available mechanisms is a very critical component to any backup process. If you have never tested your systems, you have to assume that they will not work. And even if you have tested them, it is best to have tested them recently. Some backup systems even do automatic restoration tests to demonstrate the efficacy of the backup every, single time.</p>
			<p>Backup tests can be misleading. Like many other data protection mechanisms like RAID, redundancy power supplies, or failover SAN controllers the way that we tend to test backups in a predictable, pristine environment rarely reflects how a restore would be required under emergency conditions. It is common for backup tests to be performed when systems are idle or slower than usual and to reflect best case scenarios resulting in nearly always passing tests even on systems that would almost always fail in a real-world scenario. Consider adverse conditions such as actively failing, rather than having completely failed, systems, heavy load during operations, or an inconsistent data state (crash consistency) that will not be reflective during a test scenario.</p>
			<p>Backups may seem like a simple thing and every backup vendor is going to present their product as eliminating the need for you to understand your data. Vendors hope for your blind trust that they will do the impossible and offer you a chance to open your wallet and simply hope for the best. As system administrators it is our task to understand our data, know how our backup mechanisms work, determine the consistency and coupling needs of our <a id="_idIndexMarker842"/>organization and workloads, and to assemble a backup solution that meets or beats those needs.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor209"/>Disaster recovery</h2>
			<p>Ultimately the<a id="_idIndexMarker843"/> purpose of any backup mechanism is to enable disaster recovery. Disaster recovery is something that we all hope that we will never have to do, and yet is the most important moment in most of our careers. Disaster scenarios are when we earn our keep more than at any other time. Your ability to perform calmly and coolly when a disaster is striking, to be ready with the knowledge of how to get your workloads back online, and being able to adjust to whatever twist or surprise is thrown at you is key. Performance during a disaster can mean salary differences of hundreds of percents.</p>
			<p>As we have dug through the many types and approaches to backups, it should be natural that there are now many ways to recover as well. When planning for disaster recovery we really must take all of this into consideration as our planning process will very so greatly.</p>
			<p>If we use image-based backup methods, then the assumption is that we will approach restore processes by restoring an entire block device image (or images) all at once. This has some significant advantages during disaster recovery because we only have a single step: restore the entire system. In some cases, our restore mechanisms will automatically restore all systems at once, not just a single one! This is a very alluring prospect. The caveats to this method are that restores are typically slower and generally only crash consistent. Doing high speed restores with the utmost of reliability is the most challenging for this method.</p>
			<p>Using file-based backups we need to, in almost all cases, first restore a blank operating system, either a template or a vanilla build, and then restores individual files back to it in the place where they originally went. This method is theoretically faster than the image-base complete restore, but in practice rarely is because the time to build the base operating system from traditional methods. While theoretically you could build the base system from modern methods, this generally does not happen because if you were to do so you would naturally move on to DevOps style backups. However, these start to overlap conceptually here and you can use a file-based backup mechanism in conjunction with traditional file restores. The change would, of course, be that the file backups would be isolated to the meaningful system data rather than a blind backup of anything and everything.</p>
			<p>And lastly, a full DevOps style restore. This is more complex as there is no clear single definition. The assumption here is that rebuilds of the base operating system will be nearly instant because of any number of automated build mechanisms. And then that the data restore will be heavily minimized so that it, too, can happen at great speed.</p>
			<p>Of course, all of this will be going on in conjunction with triage operations that we will discuss shortly. Planning and timing these operations so that they are well known, that processes are tested, and restore times are predictable under different scenarios provides invaluable information that will be needed during a disaster scenario.</p>
			<p>Disaster recovery planning should be more than just individual workloads being tested in isolation, and it should be more than testing only data loss. Different organizations have different risks and testing multiple types of scenarios is important. We think of data restores when we think of disaster recovery, but that might not be what we are facing. If we have workloads running in multiple locations, we might be looking at how to work from a slower connection to a less than ideal data center location. A disaster recovery test might involve testing the ability to spin up workloads on backup systems, to failover clusters, or to run from a secondary data center.</p>
			<p>The best practice in disaster recovery is to always test your scenarios and to test them regularly. Never <a id="_idIndexMarker844"/>blindly trust that our backups, our planning, our networking or data centers will work. We need to test not only the technology behind our plans, but the procedures of those plans as well.</p>
			<p>Now that we have discussed mechanisms and terms in and around backups, it is time to really look at how the modern world of DevOps can redefine how our backups can work.</p>
			<h1 id="_idParaDest-203"><a id="_idTextAnchor210"/>Backups in a DevOps world</h1>
			<p>In earlier <a id="_idIndexMarker845"/>sections<a id="_idIndexMarker846"/> of this book, we have talked about modern concepts impacting the world of system administration such as DevOps and infrastructure as code. You may be wondering if these modern concepts have a potential impact on the worlds of backups and disaster recovery. Good question! And if the section title has not given away the answer, I will clue you in now: yes, yes they do!</p>
			<p>Traditionally we think of restoring data as either the very old fashioned way of just restoring individual files, or the more modern (think last two decades) way of restoring entire systems including the operating system and all of the files that go with it. We are so accustomed to thinking of restoring systems in this way that it is often very hard to think about the problem in any other context.</p>
			<p>In the ultra-modern DevOps style world where systems are built via automation and defined in code or configuration files we have to start to think about nearly everything in new contexts. When systems can be automatically built easily and rapidly through standard, non-disaster processes, the need to restore those systems rather than starting fresh completely vanishes. Imagine if instead of fixing a car after an accident if for less money, and less time, and more reliably you could have a brand new, but identical, copy delivered to your door - you would never waste time trying to restore something broken when a pristine, perfect copy can be had. </p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor211"/>Version control systems</h2>
			<p>When talking about<a id="_idIndexMarker847"/> backups, especially as they relate to DevOps, we should also talk about version (or revision) control systems. Version control systems, like GIT, Mercurial, or Subversion, are not themselves backup systems, but act as mechanisms to sync some of the most important data on a system to another location, where backups will often occur.</p>
			<p>When talking about version control in the context of backups, it can be a bit confusion as in some cases we might be looking at our operating system as being a master location and a version control system can be used to replicate configuration files to another location and from there, they can be simply backed up using any number of normal mechanisms. </p>
			<p>Because version control systems do not only store current data but also historical versions and changes to files, they become essentially immutable and therefore useful in a backup style situation since the backup mechanism does not need to deal with versions over time. The version control system holds all versions and changes to the files over their history. So a backup of the entire version control system automatically includes all of the changes to the files. Because of this, end users accidentally deleting files, bad changes to files, ransomware attacks on data all become moot as the ability to roll back to just before a bad change was made is built in.</p>
			<p>Version control systems typically can be used to replicate files (and their version histories) not only between one end point and a version control server, but also to many additional end points. For example, a system administration workstation or jump box (which we will describe in the following chapter) might contain a full copy of the configuration files for every Linux system in a company separate from any server or backup system. Even if every official copy of the data was lost due to enormous catastrophe, a<a id="_idIndexMarker848"/> single workstation may be able to recreate the entire set of documentation.</p>
			<p>These types of systems are already in wide use for non-backup related reasons and are considered absolute <em class="italic">must have</em> tools in the DevOps world. We may already be using them in places for system administration tasks. Even in systems that do not have a DevOps process, these tools can potential be used as if they were as far as data protection is concerned.</p>
			<p>Even the most traditional (meaning as far from DevOps as you can get) Linux system can benefit from the user of version control for its configuration files. Organizations do not even need to stand up their own infrastructures for version control if it does not make sense for them to do so. Vendors such as Gitlab and Microsoft via GitHub provide enterprise hosted version control systems for free with extensive features and access control systems.</p>
			<p>A small company with legacy style systems that wanted to embrace version control protection could, in a contrived example, simply got under the <strong class="source-inline">/etc</strong> file system and add it to a remote GIT repository and do a push. Voila, all of the data is protected, that quickly and easily. Set a <strong class="source-inline">cron</strong> job to run hours to push any additional changes and you have automated a robust backup system with essentially zero effort and definitely zero cost.</p>
			<p>Version control systems are one of those poorly kept secrets of the development world that have leaked into and become embraced by many other professions. Any system that works heavily with text files should be jumping onto the version control bandwagon as quickly as possible. Version control is one of the simplest ways to take data protection to the next level. This approach carries far more advantages than that simple example might imply, however, restoring a system carries a non-trivial amount of risk that there is corruption or worse, an infection or root kit, that gets restored. A clean start removes those risks giving you the peace of mind of starting fresh. Likely the fastest path to a working system, performed in the most repeatable and predictable way, with the lowest risk. This approach is also the easiest to test. You naturally test at least part of this approach every time you build a new server whether for production, testing, staging, and so on.</p>
			<p>When we build our base operating systems, install applications, and deploy configurations via DevOps-style automation we leave ourselves with only our system specific data that needs to be restored from backup systems, and even that only part of the time. If you consider a typical multi-tiered application, data that is unique and cannot be pulled <a id="_idIndexMarker849"/>from the initial build process is generally limited to database files or limited file storage. In an application running on multiple operating system instances, we often expect this data to only exist on one layer of those nodes, generally the database. If that node is replicated, we generally only need to take a backup of the data from a single node in the cluster (because we often have all of the data replicated between all nodes.) Through this process we eliminate many points of backup, many types of backups, and identify only the actual data that may or may not be at risk.</p>
			<p>Every environment varies and in one we may find that data that requires specific protection to account for only one percent of all data, in another it might be ninety nine. We cannot say with any certainty what you will be expected to find in the real world as every organization and workload is so different. Universally the ability to limit the scale of backups, and therefore restores, brings advantages. The smaller the backup means the faster and less impactful the backup will be. The smaller the backup size the lower the cost to store it. Smaller means less to verify against corruption. The same smaller size that reduces backup time also reduces restore time in the same way.</p>
			<p>In our multi-tiered example, we may need to rebuild three nodes to get our application back up and running. The top-level load balancing and proxy layer will be assumed to have no unique data and can be <em class="italic">restored</em> via the already tested build process that built it initially. The application layer should likewise have no unique data and be able to be automatically restored using the already tested build process.</p>
			<p>Our last standard layer, the database, should again, be built with all configuration and applications being deployed completely using the build process. At this point, all of the layers of our application, all of the configuration for it, have all been restored and our backup and restore mechanisms have not even come into play. The only piece missing at this stage is the restoration of the latest data in the database. The database is in place, but not the data that goes inside of it. It is now that our restore process kicks off and puts that data back where it belongs. This might be a simple file copy from a remote location to the database location with a restart of the database after the files are in place. Or maybe a database-specific restore operation has to happen on the data to ingest it again. In any case, the restore is of a minimal amount of data, of very limited types.</p>
			<p>This approach changes everything that we traditionally think about backups. It changes how quickly and<a id="_idIndexMarker850"/> how impactful backup operations are, it changes the tools that we need to use and potentially eliminates them altogether, it makes restores fast and even, potentially, fully automated! </p>
			<p>If we go back to our example case and assume we are using the popular MySQL application as our database platform, and that all of our necessary data is stored in this one spot which is a reasonable assumption for many common workload designs, our need to use special backup tools likely does not exist. Nor do we need to rely on complicated or risky mechanisms like snapshots. We can use built-in tools to the database platform to reliably get a complete dataset from the workload layer (we hesitate to say application layer as that is a software engineering term that would be different here from the systems term) where we know that the workload, with all of the necessary intelligence to do so, was able to stabilize and lock the data during the backup operation so that our backup is safely <em class="italic">application</em> <em class="italic">consistent</em>.</p>
			<p>When we step back and start ignoring convention, and we stop focusing on simply <em class="italic">buying a solution</em> but instead put on our IT hats and determine how best to protect our environment we can often find ways to both protect our organizations and to save money all at the same time.</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor212"/>IT provides solutions, vendors sell components</h2>
			<p>Broader than<a id="_idIndexMarker851"/> backups, and even broader than <a id="_idIndexMarker852"/>systems administration, the concept of where solutions come from is fundamentally something that we need to understand to do our jobs well at any level in IT. At its core, it is ITs job, and no one else's, to produce solutions for our business. In many cases we will need to turn to vendors to supply one or more components of any solution. IT is hardly going to fabricate their own CPUs and assemble parts to make servers or write their own operating systems and so vendors (which could, in this context, include those who do not get paid but provide things like open-source software including your Linux based operating system itself) are a necessary part of the solution process. </p>
			<p>Vendors are not solution providers (although a great many will call themselves that as a marketing name, of course) but rather sales organizations. Their role is to provide access to tools that will hopefully benefit us in the pursuit of a solution. It is IT that determines which tools are right to use, selects them, and uses them to assemble the final solution to meet the needs of the business. IT, which is a department that provides solutions, is something we do, not something we buy. We cannot simply depend on a vendor to sell us a product that will protect us; it is not that simple. We have to understand the business need, the workload, the backup products, the approach and put all of this information together to make a cohesive solution that works for us. Every business is unique, every solution should be as well.</p>
			<p>No vendor-supplied tool can take into account all of our unique backup needs today, let alone be adaptable for any changes in the future. If you can imagine it, many companies are so addicted to the vendor buying process that they attempt to adapt their workloads and internal processes (and needs) to fit the needs of a product that the vendor wants to sell! This would be like relocating to an undesirable house so that your boat salesman can excuse having pushed to sell you a boat when a car would have gotten you to work easily from your existing house.</p>
			<p>DevOps and similar infrastructure have really exposed the extent to which traditional <em class="italic">just buy what a salesman wants you to buy</em> processes have required businesses to adapt to the purchase, rather than choosing to buy what makes sense for the business. In previous technology generations the options were so much less broad, and the differences so much smaller that it was easy to hide or dismiss the inefficiencies. Today that is not possible.</p>
			<p>Enhanced backup and recovery processes turn out to be one of the best reasons to consider investing in DevOps and infrastructure as code engineering efforts. Typically, we can find many reasons to make DevOps attractive, but better backups is easily the best benefit.</p>
			<p>Now that we understand how backups can be taken with a truly modern infrastructure, we are on to <a id="_idIndexMarker853"/>our last topic here: triage. Time to move on<a id="_idIndexMarker854"/> from taking backups, to using them!</p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor213"/>Triage concepts</h1>
			<p>Planning is important<a id="_idIndexMarker855"/> and prepares you for many eventualities. When disaster finally strikes, though, most planning is going to go straight out the window. All of your assurances that your backups are good are not going to make you relax, end users are going to be panicking, management is going to forget that you have to fix things and pull you into meetings, stress is high, and nothing is quite as expected from the planning process.</p>
			<p>Triage is hard because every workload, time of day, current situation has so many dynamic elements. We have to be ready to adjust to anything, and we have to get our systems back online as quickly as possible.</p>
			<p>At the moment of a disaster is when things matter most and this is where system administrators really prove their mettle. Being prepared for a disaster is relatively easy, but staying cool and logical, evaluating the situation in real time, and managing the people around you all become unpredictable and very emotional challenges.</p>
			<p>There is no simple guide to triage and not everyone is going to be good at it. The more we are prepared ahead of time, the more we understand the entire environment, and the better we know the business environment that we are working in the more adaptable we are going to be.</p>
			<p>Triage is a skill best handled by a perceiver, in the Myers-Briggs chart. As such this is where administration, over engineering, really shines to its most extreme.</p>
			<p>When analyzing an outage our first step is to evaluate the situation and determine the extent of the disaster. Are we only missing services? Is it all services? Is there data loss? What is the current, ongoing, and future expected impact from the outage. If we do not have the data at hand, keep people busy gathering that data for you.</p>
			<p>Triage is especially needed to assist the business in understanding what it can do during the time of an outage. Many businesses panic or have no plan (or if they have a plan fail to action it properly.) But business behavior needs to be coordinated with technology recovery efforts. As we begin to approach our recovery we need to be working to also keep the business working as best as possible. Of course, we hope that corporate management will step in and guide operations, based on ITs assessments of what impact is and what recovery is likely to look like, to make them as efficient as possible. Often IT needs to be ready to provide guidance as well.</p>
			<p>In any triage operation with have to determine the criticality to our workloads as well as the potential for restoration. The most critical workload will still take a backseat if it will take weeks to restore<a id="_idIndexMarker856"/> when many minimal services can be up and running in short order. We need to consider loss of productivity, loss of customer confidence, failure to meet contracts, and similar concerns when deciding how to approach our disaster recovery.</p>
			<p>Recovery planning, even in the moment, needs to be coordinated with the business. What can the business do to assist in recovery, what can technology do to enable that? With good cooperation different businesses may find many different paths at their disposal to make the damage of an outage minimized. </p>
			<p>Businesses can take actions as simple as sending staff home to relax to get people out of the way and allow IT more freedom to undertake restoration. A staff with a surprise vacation day or two might be refreshed and excited to return to the office and attempt to at least partially make up for lost time. Instead of making the team frustrated that they cannot be productive, why not reward them for their hard work?</p>
			<p>Shifting communications to systems that are not down is important as well. Can staff move to phones if email is down? Email if phones are down? Instant messaging? Voice chat through some other platform? Maybe this is a good time to visit customers in person!</p>
			<p>There is no way to list the countless ways that businesses can leverage an outage. What we need is creativity and the freedom to work with the business to help them see how they can keep working as best as they can, and allow them to direct us in how we can recover in the best way for them. </p>
			<p>Thinking about triage operations makes it evidence the importance of concepts that we have already discussed such as self-recovering systems, minimized restore sizes, and careful planning. It also highlights how important it is to have operations and other departments engaged, involved, informed of available plans, and ready to assist and coordinate when things fall apart. Good planning makes triaging better, but you cannot plan your triage operations. There are too many variables to think that we can truly plan for every contingency.</p>
			<p>I wish that triage and disaster recovery oversight was something we could teach concretely. It is a scary situation and all we can do it make sure that the right people with triage and <a id="_idIndexMarker857"/>perception mindsets are empowered and at the ready when the time comes, have good backups, good restore processes, and as much planning as makes sense while having an organization ready to work together to minimize impact as a coherent team.</p>
			<h1 id="_idParaDest-207"><a id="_idTextAnchor214"/>Summary</h1>
			<p>Nothing matters like backups. I feel like that is at least the fifth time that I have written that in this book, and it is certainly not enough. Today backups are more important than they have ever been. We face more disaster scenarios and more advanced data loss situations than ever before in our industry. Backups have always been and will likely always be our strongest defense against complete failure.</p>
			<p>Backups have been changing, quite a lot, in the last several years. The assumptions as to how we would approach backups even ten years ago do not readily apply today, and yet many organizations still use legacy applications, legacy designs, and need to still use legacy backups. So, our job is a complex one and our desire for modern backups may be needed to drive towards more modern application designs so that we can protect them in a better way.</p>
			<p>But now we understand the mechanisms underlying different approaches to backups, why we want to consider backing up in different ways, and how we can advance our backup practices into the future. Backups are probably the best place for you to set yourself apart; nothing matters more and rarely is anything as forgotten as much as backups.</p>
			<p>In our next chapter we are going to look at how users exist and interact in Linux systems and how we can approach authentication, remote access, and security.</p>
		</div>
	</div></body></html>