<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer017">
			<h1 id="_idParaDest-158"><em class="italic"><a id="_idTextAnchor164"/>Chapter 7</em>: Documentation, Monitoring, and Logging Techniques</h1>
			<p>Now we are getting into the real <em class="italic">meat </em>of system administration work, although I think that most people would feel like this is the <em class="italic">potatoes</em>. In this chapter we are going to be dealing with all of the parts of system administration that no one can see on our servers. These are those nearly invisible components of our jobs that are so critical and can do so much to separate the juniors from the seniors; the extra steps that make all of the difference when things start to go wrong.</p>
			<p>In this chapter we are going to learn about the following:</p>
			<ul>
				<li>Modern Documentation: Wiki, Live Docs, Repos</li>
				<li>Tooling and Impact</li>
				<li>Capacity Planning</li>
				<li>Log Management and Security</li>
				<li>Alerts and Troubleshooting</li>
			</ul>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor165"/>Modern documentation: Wiki, live docs, repos</h1>
			<p>The thing about<a id="_idIndexMarker593"/> documentation<a id="_idIndexMarker594"/> is that everyone admits that it <a id="_idIndexMarker595"/>is important, everyone talks about it, and almost no one does it or if they do, they don't keep it up to date. Documentation is boring, often harder than it seems to do well, and because almost no management will ever follow up and verify it, extremely easy to ignore. No one ever gets promoted because of excellent documentation, no one throws documentation parties, and no one talks about it on their curriculum vitae. Documentation just is not cool enough for people to want to spend time talking about. </p>
			<p>Documentation is, however uncool it might feel, amazingly important for so many reasons. It can go far for moving someone from being an acceptable system administrator to being a great one.</p>
			<p>Documentation does some interesting things. Of course, it allows us to recall how systems work and what tasks need to be done to them. It allows us to hand off tasks to others. It protects the business should we go on vacation, get sick, or move on to greener pastures or even retire. But beyond these obvious points, documentation allows us, forces us in fact, to think differently about our systems that we maintain. </p>
			<p>In the software engineering world, a new technique of writing tests before writing the functions that they test has become popular and has shown that it can force people to think differently about how they approach problem solving and can lead to greater efficiency. We have very similar benefits in system administration. Approaching documentation more aggressively can lead to faster processes, better planning, less wasted time, and fewer mistakes. Taking a <a id="_idIndexMarker596"/>documentation-first approach, that is writing documentation before systems are <a id="_idIndexMarker597"/>built or configured, can help us think differently about our system designs and to document <a id="_idIndexMarker598"/>thoroughly: An intentional process of documenting what should be, rather than attempting to document what we did. This provides a wholly different way of thinking, and a way to double verify veracity, and an actual process to encourage completeness of documentation.</p>
			<p>If we force ourselves to document everything before we enter it into a system, we can improve our chances of having accurate and complete data. Avoiding the need to go back and attempt to remember everything that was needed or done on a system is important. Using a document first process we have an opportunity to catch missed documentation at the time that we go to use it, which may only be a few minutes later. If we work first and then document it, it is very easy to forget small details and there is no triggering event to remind us to verify that something is written. Alternatively, if we document first, we have the moment when we need to put data into a system or a configuration to make. We have a triggering event, the actual moment of entering configuration data to remind us that we should have pulled that out of a document. It is not foolproof, only more reliable than typical processes.</p>
			<p>No one really disputes, at least not in polite company, that documentation is needed or that it is one of the most important things that we can do working as system administrators, or really working in information technology at all. It is practically a mantra that we repeat, yet few of us really internalize this decision. Instead, we pay lip service to the ideology of documenting everything and still push off documentation as a secondary concern that we might do tomorrow if, and only if, we get bored during some mythical free time. This is where things break down. We cannot simply claim to believe that documentation is all important, we have to truly believe it and act accordingly.</p>
			<p>We can talk all day about the importance of documentation, but all that really matters is taking that knowledge and putting it into action, however that works for you and your organization. To make that more likely to be successful we need to use good documentation tools. Having a high barrier to documentation encourages us to avoid it or to see it as too time consuming to do at the correct time. If we make documentation fast and easy, we are much more likely to find ourselves just doing it, perhaps even enjoying it to some degree. I know that I am personally very satisfied finding my documentation to be complete and up to date - having that satisfaction of knowing that I could show it to someone, at any time, and feel good about what is there.</p>
			<p>In choosing a platform for documentation we have many considerations. How will the documentation be <a id="_idIndexMarker599"/>stored, backed<a id="_idIndexMarker600"/> up, protected, secured, and accessed? What<a id="_idIndexMarker601"/> kind of data will be stored: text, audio, images, video, or code? How many people will use it? Do you need to make it accessible inside an office? In multiple offices? Globally? Will third parties need to access it?</p>
			<h3>Wikis</h3>
			<p>Over the last two<a id="_idIndexMarker602"/> decades, the wiki has arisen to be the de facto tool for documentation of all sorts. Wikis are designed around being fast and easy to edit and at this they excel. Wikis also traditionally use simple markup languages, like the MarkDown language, that make it easy to store exact text and technical data without it being manipulated by a formatting system. This creates a minor learning curve but rewards a small amount of very standard learning with the ability to make very accurate, well formatted documents quickly.</p>
			<p>The wiki format is all about simplicity - the simplest possible system that still allows for enough formatting to be able to be used for nearly any documentation task. This simple format makes it easy to have a variety of wiki products on the market that satisfy nearly any specific need. From small, light, and free open-source products that you can run yourself to large, hosted commercial offerings that you simply sign up for and use. It covers nearly all bases. Organizations of any size can use it effectively and there is nearly always a wiki option that integrates with other systems that you have.</p>
			<p>A wiki will generally suffer from a need for some degree of organization which is not native to the platform. The strength of a wiki, that it is fast and flexible, is also a great weakness: it is just far too easy to start to throw data someplace that it does not belong and to leave no trail as to how to find the information again. Some wikis will go above and beyond the basics and include meta data tagging options or structured data organization options. These are the exception, not the norm.</p>
			<p>Wikis have, for a number of years, been used as a component of or even the basis for larger products. A great example of this is Microsoft's SharePoint which uses a wiki engine as its core rendering engine and all of its interface details are simply advanced components being rendered on top of a wiki.</p>
			<p>An issue typical to wikis is that they are rarely able to have the same data modified by multiple people at the same time. Their simplistic design often assumes that they will be treated quite simply - a single author, as the only reader, during the time of edits. This makes a wiki more useful in single user environments, or environments where users rarely use the documentation platform at the same time, or in organizations where different users <a id="_idIndexMarker603"/>tend to be segmented off from one another so that they will use different documentation pages at different times. If your team needs to have multiple people making active edits, or viewing updating information, in real time of the same data then other documentation options are likely going to be better. </p>
			<h3>Live docs</h3>
			<p>The newest way to <a id="_idIndexMarker604"/>approach documentation feels much like a step backward: word processor documents. Yes, you read that correctly. Hear me out. </p>
			<p>Traditionally, that is in the late 1990s and early 2000s, the idea that you would use a word processor document as documentation seemed ridiculous unless you were such a tiny company that you only had one person who would ever need to use and access these documents and then it was reasonable as it made storing everything as a file relatively easy. This has changed heavily in the last several years as new technologies have turned nearly all mainstream word processors into online, web-based, multi-user tools that only resemble early word processors in their superficial capacity, but not in usability or technology.</p>
			<p>These next generation document systems provide a surprisingly powerful and robust mechanism to use for documentation purposes. While there is no single universal standard for how these systems should behave, a set of conventions have arisen that are sensible and are followed by all major systems and are available both commercially and in free, open-source packages as well as in hosted or self-hosting modes. Of the greatest importance to us are the ideas that these live documents are able to be edited by multiple users at the same time, show changes as they are made in real time, have secure access controls, track changes, and use web interfaces that are easily published online or anywhere that they are needed, as well as, being able to output documentation to an easily portable or transferable set of formats.</p>
			<p>Modern document handling systems like these will often times use a database behind the scenes, rather than resorting to sets of individual documents, and only expose individual documents as views into a single, large data set rather than truly individual sets of data. These systems are becoming increasingly powerful and can fit easily into other document management or replacement workflows. Nearly all organizations today are already tackling the need for modern document systems in other parts of the business, and these will easily be systems into which system documentation can be added without incurring any additional cost or effort. Doing more with the systems you have to maintain already is a great way to get high value at low cost.</p>
			<p>Because these modern<a id="_idIndexMarker605"/> document systems allow for multiple users on the same document at the same time, they are especially useful for times when you have a multi-person team working on a single customer or system at the same time and the documentation needs to be shared. That way one person making a change keeps the data on everyone's screens constantly updated. The documentation system itself becomes a mechanism for team collaboration instead of being a risk of using outdated data because someone did not know to refresh their view.</p>
			<p class="callout-heading">Alternative interfaces to similar data</p>
			<p class="callout">As these kinds of<a id="_idIndexMarker606"/> tools have become more and more popular, alternative interfaces to similar database drive document data have started to arise. Popular alternative formats like notepad applications are beginning to become more popular. These formats are less well known than traditional word processing and spreadsheet tools but can be very good for system documentation.</p>
			<p class="callout">Because of the multi-media and often changing ad hoc nature of documentation, journal-style applications can be ideal. Over time I expect to see more and more applications designed around flexible documentation to become more mainstream.</p>
			<p>Personally, I have become a large fan of these systems. They utilize standard tools that nearly all staff know already and tools that are likely to be already <a id="_idIndexMarker607"/>being used for many other purposes and repurposes them in a way that is surprisingly well suited for them. Less retraining, fewer special case tools to manage, and easy access and usability by teams that may use the documentation less often.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor166"/>Repos</h2>
			<p>A new standard for<a id="_idIndexMarker608"/> documentation is to use online code repository systems. These systems generally work from a collection of text files that are loosely formatted but they get updated and version controlled centrally. This is a very different approach than what is taken with the examples given previously. This system does not address live collaboration between team members, but does allow for offline usage quite easily using standard tools used in the development space.</p>
			<p>The real reason that the use of version-controlled code repositories has become an area of interest for documentation is that it is already being used heavily in the development and DevOps spaces and so is a natural system to adapt for use in IT documentation. The ease of using documentation offline using local reading and writing tools, and the ability to also have online copies makes it very flexible. </p>
			<p>There are beginning to be ways to even use this type of documentation in a live, shared manner with some of the newer editing tools. Likely we will see this advance significantly in the near future as more focus is put on expanding the robustness of this process.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor167"/>Ticketing systems</h2>
			<p>Traditional documentation <a id="_idIndexMarker609"/>as we have been discussing is really state documentation: documenting the way that systems <em class="italic">are</em> or, at least, <em class="italic">should be</em>. There is much more to document. The other system that we should use is a ticketing system or, to think of it another way, <em class="italic">change documentation</em>.</p>
			<p>Tickets are a form of documentation just like your wiki might be. Unlike a wiki, tickets are focused on recording events in time. They track errors, problems, issues, requests, observations, reactions, changes, decisions, and so forth. Unlike traditional documentation that is a final document showing the results of all decisions and changes made until the current time, your ticket system should reflect the history of your systems and workloads to allow you to, theoretically, <em class="italic">play back</em> events as they happened to not only know what changes were made to a system but also who made them, who requested them, who approved them, and why.</p>
			<p>Tickets, when used properly, play a huge role in the lives of a system administrator. While possible to function without a good ticketing system, this will add so much unnecessary work. Using tickets to track tasks as they get assigned, the process of completing the work, and the final disposition provides the missing half of the documentation puzzle. </p>
			<p>If your business does not have or is unsupportive of getting a ticketing system, consider implementing a <a id="_idIndexMarker610"/>private one just for yourself. Ticket software comes in many shapes and forms and free software and services are available that work quite well if spending money on upgraded products or more extensive features is not an option. You can think of your ticket system as a personal work journaling mechanism if that makes more sense.</p>
			<p>You do not have to go overboard attempting to integrate tickets into your company's greater workflow if you cannot get top level buy in or it does not make sense for how the organization should work, but it is hard to imagine any IT department that would not benefit dramatically for being able to track IT change events, including denied events, to be able to demonstrate the history and activity of the department and to be able to trace potential issues caused by changes in our systems.</p>
			<p>Like traditional documentation, ticketing systems come in all shapes and sizes. Play with a few, give them a try, do not be afraid to change to something else. Find something that works for you and allows you to effectively document the changes that you make, when you made <a id="_idIndexMarker611"/>them, how long it took you, why you did it in the first place; and all with a minimum of effort.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor168"/>Approaching documentation</h2>
			<p>Chances are you have <a id="_idIndexMarker612"/>some amount of documentation in your job today. Chances are even better that what you have is incomplete, out of date, and essentially useless. It is okay, nearly all companies suffer from bad documentation. But this is a tremendous opportunity for improvement.</p>
			<p>If you have documentation like most businesses, the best thing is often to literally start over. Look over your options, think about how your company will need to approach documentation and collaboration and pick an approach. It does not even have to be the right one. Any documentation process is good, even if you risk having to do it again. Pick a system and try it out. See if it fits the style of the data that you need to store, if it is comfortable for you to use, and if it allows for the style of collaboration (if any) that your business needs.</p>
			<p>Do not try to document absolutely everything right away. Take a single system or workload and try documenting that one item in a very good way. Format it to look really good. Organize the data to make the data that you need quickly be clearly visible and available near the top so that someone trying to address a problem does not have to search far to find what they need. Remove redundancy and ensure that data exists only one time, in a single, predictable place. Think of your documentation like a relational database that needs some normalization, and the first major step is organization and the second is removing redundancy. Documentation is always hard to maintain, and redundancy of data makes it all but impossible. Attempting to change unknown occurrences of the same information gives no clue to how to find it all and what needs to be updated.</p>
			<p>When you find a<a id="_idIndexMarker613"/> system and process that works for you, stick with that. Start documenting everything. Make it a huge priority, do nothing without documentation. Add tickets and start making everything get tracked.</p>
			<p>Maybe you work for a company where documentation is already good. Chances are, though, you do not. And if you do, chances are you will never work in a place like that again. Good documentation is a rarity even before we consider the importance of tickets in the overall documentation equation.</p>
			<p>There are no best practices as to what tools to use or in what format to put your documentation, but there are some high-level best practices to consider:</p>
			<ul>
				<li>Use both state and change documentation systems to track all aspects of your systems.</li>
				<li>Avoid data redundancy in state documentation systems (it is fine in change systems.)</li>
				<li>Keep all documentation up to date and secure.</li>
				<li>Do not document data that can be recreated reliably from other data.</li>
			</ul>
			<p>It is easy to say that we need to be religious about our documentation, and everyone agrees that it is of the utmost importance. Yet actually moving from saying it, to doing it, is understandably hard. Management rarely verifies documentation as we are working, but they do reward getting other work done and frown upon delays. It is unfortunate that often the most important aspects of our careers are not seen as important enough or interesting enough for those outside of our field and they get deprioritized by people with no knowledge <a id="_idIndexMarker614"/>of how they play into what we do.</p>
			<p>In our next section, we will move on from purely manual system tracking to beginning to use tools on our systems to measure and track them in a more automated fashion.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor169"/>Tooling and impact</h1>
			<p>One of the<a id="_idIndexMarker615"/> fundamental <a id="_idIndexMarker616"/>natures of physics, as well as a rule that you learn straight away in industrial engineering, is that you cannot observe or measure events without in some way impacting them. In computing, we face the same problem. If anything, we face it far more than in most other places.</p>
			<p>The more that we measure, log, or put metrics on our systems the more of the system resources needed for our workloads is taken up by the measurement processes. As computers have gotten faster over the years the ability to measure without completely crippling our workloads has become more common and now, we often even track checkpoints inside of applications in addition to operating system metrics. But we always have to maintain an awareness of what this impact is. </p>
			<p>At some point there is more value to just letting the systems that we have run as fast as they can rather than trying to measure them to see how fast they are going. A sprinter running flat out is faster than a sprinter running while carrying measurement devices to determine their speed. The measurement process works against them. However the sprinter getting more feedback might be able to improve with the additional knowledge over time. But you will never see someone attempting to outrun a charging hippopotamus (they are one of the fastest and most dangerous land mammals, you know) first stop to turn on measuring devices. They will just run as fast as they can. Knowing how to run faster is only useful if it gives you both potentially useful data that by using you can enact improvements and you get the chance to implement those improvements. If the hungry hippopotamus catches you, all those measurements will be for naught.</p>
			<p>Different tools will have very different levels of impact. Some simple everyday tools that we use on our systems may have almost no impact at all, but will generally give us only an extremely high level view of what the computer is doing. Other tools, like log collection, can require a great many resources and can even put noticeable strains on networking and storage resources.</p>
			<p>Collecting data from a system is not the only activity that uses system resources. Collating that data and presenting it in a form useful for humans also requires resources, as would shipping that data off to an external system. Each step of the process requires that we use more and more resources. All of this is before we even consider how much human time may be involved in examining the data, as well. It is always tempting to simply opt for the most possible insight and monitoring into a system, but unless we can derive true value<a id="_idIndexMarker617"/> from that<a id="_idIndexMarker618"/> process it is actually a negative to do so.</p>
			<p>In general, we will use a variety of tooling on our systems to provide some degree of regular measurement. There are both data collection tools, like the sysstat SAR utility, and immediate, <em class="italic">on the spot</em> observation tools like <strong class="source-inline">top</strong>, <strong class="source-inline">htop</strong>, and <strong class="source-inline">glances</strong> that allow us to watch a system's behavior in real time. Both kinds of tools deliver a lot of value.</p>
			<p>Of course, there are a large variety of both free and paid, software and service tools that can take your monitoring to another level. Researching these will be very beneficial as even the open-source offerings have become amazingly powerful and robust. Performance tooling is typically handled locally as it is rarely used for alerting or security considerations and using it to perform postmortem investigations is often fruitless so incurring the cost of central <a id="_idIndexMarker619"/>data collection for performance data is not <a id="_idIndexMarker620"/>commonly worth it. Centralized tools do exist and can be quite useful. When used, these tend to be chosen for ease of use to humans rather than to serve a specific technical need. Decentralized tools that can optionally leverage a single pane of glass style interface to display data from many locations are quite popular for this specific need.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor170"/>Netdata</h2>
			<p>I typically do not want <a id="_idIndexMarker621"/>to delve into specific products, but I feel that <strong class="bold">Netdata</strong> makes for an exceptional use case as a way to demonstrate the variety and power of available tools on the market today for Linux system administrators.</p>
			<p>First, Netdata is free and open source. So as a system administrator who may have to justify any software purchases, this is one that can be downloaded and implemented to enhance our monitoring abilities without needing any approvals or notifications. Installation is also very quick, and very easy.</p>
			<p>Second, Netdata provides crazy gorgeous dashboards right out of the box. These make it just more fun to do our jobs, for one thing. If we need to show data to management or present it in a meeting, few things are going to look more impressive and polished than Netdata dashboards. This is a tool that makes it easier to sell the business on what we do.</p>
			<p>Third, Netdata uses surprisingly few resources. For the amazing graphical output that it generates you would never expect such a light utility to be able to pull it off.</p>
			<p>Fourth, Netdata is decentralized. It runs locally on each server and does not send its data off to a central location to be collected. You can make a combined view of many systems, but doing so is all handled in the web browser of the viewer actively pulling the individual dashboards from each system directly and simply displaying disparate systems on one screen. There is no central server used to aggregate before display.</p>
			<p>I love Netdata as an example of truly useful, free, open source, groundbreaking software that makes our everyday experience in system administration better. And it shows a pattern that is potentially useable for a great many other products and product types to make decentralization more viable than it may first appear.</p>
			<p>One of the more important things that you will do as a system administrator is learning what tools to use, when, and how to read them. One of the most valuable things that I have found over the years is becoming comfortable with what a healthy system will look like, both historically and in real time, and being able to look at a variety of tools and to get an innate sense of how the system is behaving. There is little way to teach this other than talking about the value of observing systems at idle, and at standard load and observing what they look like; and of course, the better you understand how system components, and software works, the more you are able to interpret what you are seeing in a meaningful way.</p>
			<p>With enough practice and understanding it can be possible to essentially <em class="italic">sense</em> the behavior of a system and gain a confidence into why a system is behaving as it is. This is not something that can be learned from a book and requires putting in a lot of time working with systems and paying close attention to what you observe from monitoring tools and combining that with what you observe from the system's performance and a solid understanding of the interaction of the physical components.</p>
			<p>I find that momentary tools, such as top which is included in nearly all systems by default, presents a perfect way to stare at running systems as they perform their duties and become accustomed to how CPU utilization will fluctuate under appropriate load, how processes will shift around, and how load will vary. Some of the most complicated system troubleshooting will sometimes be done with little more than staring at changing process lists over time (and performing really well-timed screenshots.)</p>
			<p>This is an area that can do quite a lot to separate junior from senior system administrators. It is far less about knowing the basics as much as truly internalizing them and being able to intuitively apply that knowledge on the fly when a system is behaving badly or possibly being able to do so based solely on someone describing the problem! How drives perform under different conditions, how the CPU is behaving under different loads, how caches are<a id="_idIndexMarker622"/> hit, how memory is tuned, when is paging good or bad, and so forth. </p>
			<p>Understanding all of these factors is an ever-changing target. Each system is unique and new CPU schedulers, NUMA technologies, drive technologies, and so forth regularly change how systems behave and what our expectations of them should be. There is really no substitute for experience, and only one way to get experience.</p>
			<p>Choosing tooling can be hard. I tend to stay very light unless I have a workload with a very specific need. You should play with many different measurement tools to have a good feel for what is available and be ready to choose the right tool for you and for the task at hand whenever needed. In many cases for me, the simplest tools like <strong class="bold">free</strong>, <strong class="bold">top</strong>, and the <strong class="bold">sysstat suite</strong> are more than adequate for almost everything that I do, and they are available on essentially every system that I have encountered for over a decade. But on my own systems in my own environment, you will often catch me using something a bit more graphical and fun like Netdata as well.</p>
			<p>Best Practices: </p>
			<ul>
				<li>Learn measurement <a id="_idIndexMarker623"/>tooling before you need it and learn to use it quickly and efficiently.</li>
				<li>Limit your usage of measurement tools to only that which is truly useful. Do not impact <a id="_idIndexMarker624"/>performance without a good reason.</li>
			</ul>
			<p>Now that we have talked about how we measure what our systems are doing, it our next section we <a id="_idIndexMarker625"/>start using these tools to plan for the future.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor171"/>Capacity planning</h1>
			<p>When we take our <a id="_idIndexMarker626"/>knowledge of system resource usage away from the being in the moment and begin to apply it over the long-term aspects of a system, we start to think about capacity planning. Capacity planning should be, at least in theory, a rather important aspect of system administration. Many organizations will treat capacity planning as a non-technical exercise, however, and take it out of system administration hands. It is amazing how often I am told by a system administrator that they have received hardware that they did not specify and now have to <em class="italic">make it work</em> even though it was designed by someone with no knowledge of how it would be used! So much training and knowledge of system design in system administration being ignored and critical purchasing being down with no rhyme or reason.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor172"/>It Is already designed when purchased</h2>
			<p>One of the strangest problems that I run into with great regularity is system administrators asking me how they should set up hardware which they have already specified, ordered, and received. Most critically, they ask how they should configure the RAID and division of logical disks or splitting physical arrays. I am always amazed by this.</p>
			<p>Obviously, different <a id="_idIndexMarker627"/>configurations of the server change how it would need to be configured at the hardware level. The software to be run on the server, the amount of software, the needed performance of that software, the amount of storage that will be needed, how backups will work, and nearly everything about how a server will be used over its anticipated lifespan is needed to be well understood to be able to even begin the process of specifying hardware to be purchased. How did they know how much RAM to buy, or how many cores, how fast the CPUs should be, which CPU models to start with, even which brands would work? In many cases people overspend and overbuy by such a degree that things work out and no one notices because the mistakes are made in the form of lost money that no one investigates. A server budget was given, no one follows up to determine if the server that was purchased was a good value, only if it was in budget. So overbuying is often a way to cover for failing to do capacity planning, and one that can be costing companies a significant amount of money. </p>
			<p>Most noticeable, though, is RAID configuration. When someone asks me what RAID level and configuration that I would recommend for hardware that was purposely purchased new for this project I have no idea how to respond. Any and all decisions about the RAID configuration surely had to have been made before the server was purchased. It is only by knowing the performance, reliability, and capacity artefacts of not only each RAID level but of different configuration options and applying that knowledge in combination with available physical drive and controller options that you could have even approached purchasing the storage portion of the server in the first place.</p>
			<p>In order to decide on storage needs you have to know what you need, first. Then you have to know how the hardware that you will specify will meet those needs. Some questions that come to my mind when someone says that they have hardware and bought it without having specified any design yet include:</p>
			<p>How did you know<a id="_idIndexMarker628"/> which hardware RAID card to buy? Or even that you needed a hardware controller at all?</p>
			<p>How did you determine how much cache to purchase?</p>
			<p>How did you know which types of drives would most useful?</p>
			<p>How did you know what speed of drives you would need in throughput and/or IOPS?</p>
			<p>How did you know what size of drives to get?</p>
			<p>How did you know the quantity of drives to get?</p>
			<p>How did you determine caching capacity?</p>
			<p>How did you determine tiering capacity?</p>
			<p>How did you determine hot spare needs?</p>
			<p>The decisions necessary to make any of these decisions require having made all of the decisions as a whole. The final outcome is a product of the whole and any change in RAID level, for example, would drastically change the usable capacity, the system performance, and the overall reliability. Every small change makes everything else change with it. No piece can be decided upon individually, let alone changed. The most innocuous change could result in a system that is not large enough in capacity, or fast enough for the workload to function; and more dangerously the reliability of the storage system could swing wildly between extremely safe and extremely dangerous.</p>
			<p>It is really hard to describe just how crazy this process is; and even crazier to realize that this might even be normal for how people buy servers! The best analogy that I can muster is to say that it is like buying a transmission for a 1978 Ferrari and expecting it to just work when you do not even know if you are getting a car, boat, or small plane yet, let alone what year or model of Ferrari!</p>
			<p>Capacity planning is about far more than saving money, at least indirectly. It is about ensuring that systems that we purchase can meet all of the <em class="italic">projected</em> needs of our business for the duration of time that makes sense to do so. This is obviously a difficult number to really nail down as what feels appropriate as a projection, what are the likely changes coming in the near future, and what is a reasonable time frame for your systems, are all rather fungible concepts.</p>
			<p>It is a common trend in businesses to want to project astronomic growth using <em class="italic">pie in the sky</em> numbers as hardware investment bases as well as using the maximum reasonable lifespan of the hardware to calculate over. While we cannot control the political processes<a id="_idIndexMarker629"/> that drive our businesses from our positions within IT, except in the rarest of cases, what we can control is the quality of our own numbers being provided to those making the decisions.</p>
			<h3>Buy late</h3>
			<p>Good business logic says that, with rare exceptions, the cost of everything in IT goes down over time. It goes down a lot. The cost of memory, compute cycles, or storage is fractional today compared to just a few years ago and this trend has really never stopped nor reversed, nor is it likely to. Momentary issues due to scarcity during times of manufacturing or logistical crisis can happen, but these are extremely rare and short lived events. Given any amount of time to make a purchase, the cost of systems in a few months will be better than they are today. Either the money that we spend is less, or the amount that we get for that money is greater. In either case, we benefit by investing later.</p>
			<p>A common example that we can use is what if we bought a server today with plans to use it for eight years and we have expected growth, so we buy a server that meets our eight-year projections. To acquire a server with that much power, maybe we will spend $20,000 today. Or to get a server that we project will last us for four years, we might spend $8,000 today. A big difference. Of course, this is a contrived example, but in the real world, these kinds of costs are typical in a lot of common scenarios.</p>
			<p>In this example, we then assume that in four years we can buy another new server for an additional $8000 that meets our needs for four more years. Again, contrived, but often true. The cost tends to work out similarly to this.</p>
			<p>The number of advantages to buying less to last an expected shorter amount of time is hard to overstate. First there is often hard cost savings because of the nature of server pricing means that buying less, more often simply costs less because of the price benefits that happen within the operational lifespan of a modern server. And then there is the time-value of money that says that spending the same amount of money, but delaying spending it, means that you have more money to make you money in the interim and that the same money that you spend in the future is worth less than that money today. Then there is newer technology - if we wait for years to buy a new server we potentially get a lot of newer technology in that server that can contribute not only to capacity advantages, but also lower power consumption, great durability, and so forth. </p>
			<p>Then the advantage <a id="_idIndexMarker630"/>of having two servers. We assume that the second server is going to replace the first, and maybe it will. But we might also use the purchase to simply expand capacity, the first might remain in production service. If we are replacing the first server, it may be redeployed in another role within the organization or could be used as a backup server for the new one. Almost always you will be able to find a highly effective use for the original investment.</p>
			<p>Most likely the biggest advantage is in delayed decision making. By holding off spending much of our initial budget by several years we get the flexibility to invest that money at any time, or never. Instead of doing an eight-year projection, which is wildly inaccurate to the point of being totally useless, we do two four-year projections, which are still pretty inaccurate, but the degree to which they are more accurate is pretty crazy. At our first four-year checkpoint we get to evaluate how good our last projection was and make a new one based on this new data and new starting point. We not only get to do a fresh evaluation of our own organization with four more years of insight, but also four more years of insight on the industry, and four more years of new technology. Very few businesses would make the same decisions in four years that they would make today. In business, delayed decision making of this nature can be astronomically beneficial.</p>
			<p>Sadly, for most businesses, projecting becomes an emotional exercise because there are political benefits to making people feel good and showing faith in the business or its leadership; and it just makes us feel good to think about all of the success that we are surely going to experience. And better projections normally means more clout, bigger budgets, more to work with for many years to come. Almost no business ever goes back and evaluates past projections to see if people did a good job, so there are rewards for being overly optimistic and generally zero risk of retribution if they are falsified for personal gain (emotional, financial, or political.) This system makes projections very dangerous and anything that we can do to reduce our dependency on them is important.</p>
			<p>At the end of the day, delaying purchasing of server resources until they are actually needed is one of the best practice strategies that we can have in technology purchasing. It is the best approach from a purely financial viewpoint, the best approach from a decision making and planning perspective, and the best way to allow technological and manufacturing advancements to work in our favor.</p>
			<p>There are three key numbers that will come from system administration during this process. The first is <a id="_idIndexMarker631"/>simply answering the question of <em class="italic">how many resources are we using currently?</em> The second question is <em class="italic">how many resources did we use in the past?</em> And third, <em class="italic">what resources do we think that we will use in the future?</em></p>
			<p>In reality, answering any of these questions is surprisingly hard. Just counting up the resources that we have purchased and own today tells us nothing. We need to really understand how our CPUs, RAM, and different aspects of storage are being used and how they affect workloads.</p>
			<p>For example, if we are running a large database instance the database might happily cache outrageous amounts of storage into memory to improve database performance or reduce storage wear and tear. But reducing available memory may not have an impact on database performance. Because of this, just measuring memory utilization can be very tricky. How much <em class="italic">useful </em>memory are we using? Storage is easier, but similar challenges can exist. CPU is the easiest, but nothing is ever completely straightforward. Just because we use system resources at a certain level does not tell us how well they are needed.</p>
			<p>With CPU we might have a system that is averaging fifty percent CPU utilization. For one company, or set of workloads, this might mean that we can put twice as many workloads on this system to get its utilization close to up to one hundred percent. For another company, or set of workloads, it might mean that we have overloaded the system and there is enough context switching and wait times that some applications are noticing latency. It is not uncommon for companies to target ninety percent utilization as a loose average, but for others targeting just ten percent can be required. </p>
			<p>The tradeoff, in this case, is about waiting for throughput or latency. Available CPU cycles means that the CPU could be doing more tasks, but if a CPU is tied up doing tasks all of the time then it is not necessarily available if a new task is suddenly presented to it. If you are working with low latency systems, having available system resources at the ready to process that task at the time that it is first presented can be a requirement. In order to assess capacity use and needs for systems requires us to deeply understand not just how much of the system is being used, but what that ultimately means for our workloads.</p>
			<p>As with so many aspects of system administration, the key is to understand our workloads inside and out. We have to know how they work, how they consume resources, how they will respond to more or fewer resources. Everything that we do in capacity planning depends on this.</p>
			<p>Of course we have <a id="_idIndexMarker632"/>tools that we mentioned previously to help us with determining how well a system is performing, and with application measurement tools and/or human user observation we can reasonably determine what kind of resources are necessary for where our workloads are today.</p>
			<p>Many of these tools that we use can also be used to collect historical records of system performance. Chances are that if we were to do this all of the time that we would produce a volume of data that we will never be prepared to utilize. Some organizations do collect this forever, but this is the exception, not the rule. More practical, in most circumstances, is to develop and track baselines over time. This generally means doing some sort of activity where you record measurements, as well as recording what you can about end user application performance, of the system so that you can look back and see what system utilization has been. This data should be collected over long periods of time. Weeks or months, at least sometimes, to find hot spots and cold spots. Common cold spots might be Sunday overnight when many applications are not used at all. A common hot spot is month-end financial processing times. Every organization has different utilization patterns. You need to learn yours.</p>
			<p>With this collected data we can then analyze to see how utilization has changed over time. Changes will normally come from increases, or decreases, in application level utilization. But this is far from the only aspect that might change. It is important to be cognizant that application updates, operating system patches, changes in system configuration and so forth should be recorded and noted against data recording to make the data more meaningful in evaluation.</p>
			<p>Long term data collection has to be considered against effort. Collecting data and collating that data on any scale can be extremely time consuming and potentially resource intensive. There is the possibility that after collecting all of that data that it will provide no useful insight or that the skills to read it back will be lacking. It is not unreasonable for a<a id="_idIndexMarker633"/> system administrator to track system performance data mentally if working with systems that are used constantly. In some cases, this will be more practical.</p>
			<h3>Risk of too much data overhead</h3>
			<p>In pursuing capacity planning we risk creating a situation where we generate more overhead for ourselves, which will generally equate to more cost, than if we had not collected the data in the first place. We have to find an appropriate balance.</p>
			<p>Large organizations will tend to find large value in using lots of data collection to save money on a large scale. At scale automating the data collection and analysis is often relatively simple. Small businesses will often find this impractical. To collect any reasonably thorough amount of system data for a single server could result in expenses as large as the cost of the systems themselves. Clearly that is unworkable. Common sense has to prevail.</p>
			<p>Many small organizations <a id="_idIndexMarker634"/>will have just a single primary workload and will potentially never fully utilize the smallest of servers and will, from a capacity perspective, always experience overkill until moving away from running their own hardware, if that ever becomes practical. Large organizations are operating farms of servers and have many avenues to improve overall cost from playing with different software options, using many smaller servers or fewer large ones, using different processor models or even architectures, and so forth.</p>
			<p>The effort to save money just has to be kept in check against the potential value in the data collection. This is true with all decision-making processes. Beware that data collection is part of the total that makes up the high cost of decision making and you always have to remember to keep that cost far below the expected value level of that decision.</p>
			<p>When evaluating the cost of the data collection, we have to consider the time to collect the data, the cost of storage, and the cost of analyzing that data. We cannot forget, we have to consider the cost of considering all of this!</p>
			<p>There are vendors that make tools specifically for tackling these difficult questions and they can be very good. Dell, famously, provides tools to customers that can be run over long periods of time and produce very detailed reports as to how systems are being used and, of course, also provide <em class="italic">recommendations</em>, which are actually sales pitches, to sell you more products. If used properly, these tools can be quite valuable.</p>
			<p>Of course the natural question will also be <em class="italic">but what about cloud computing, does that not change all of this? </em>And yes, considering cloud computing is important and plays into this process.</p>
			<p>As cloud computing enters into an organization's planning we have even more complexity to consider, in some ways. In other ways, cloud computing can make the process of capacity planning far simpler, if not moot.</p>
			<p>In cloud computing, or at least in nearly all of it, we buy our capacity as needed or very nearly as needed. This<a id="_idIndexMarker635"/> is the beauty of cloud computing. Use only what you need and let the system decide what it needs in real time. This is great, in theory. But just allowing the system to do this still leaves us with a need to predict what this approach will cost in order to compare financially against alternatives, and to predict what this will cost in order to budget properly for it.</p>
			<p>If your organization is using cloud computing currently, this can make our processes far easier. Generally your cloud platform itself will be able to tell you an awful lot about system utilization rates. Even if traditional reporting is not available, the billing for cloud computing can often tell you as much as you may need to know for many types of planning.</p>
			<p>Our capacity planning best practices are purely mathematical. Use reasonable measurements and understanding of our systems and workloads and our best understanding of business expectations and input from other teams to plan for capacity needs for tomorrow and into the future. Study and understand the <em class="italic">cone of uncertainty</em> and use that sense of increasingly unforeseeable future combined with a good understanding of financial concepts such as the time value of money and the increased value of technological purchasing over time to provide best effort evaluations of future capacity investment needs for your organization.</p>
			<p>Capacity planning is often boring and more political than technical, but it is a role that can rarely be handled by anyone except system administration so it is our lot in life (professional life, at least) to be integrally involved in system hardware purchasing projections. It is essentially the computing futures market on a tiny scale inside of our own business. In our <a id="_idIndexMarker636"/>next section we move from capacity and performance needs and look at tools used for security and troubleshooting starting with system and application logging.</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor173"/>Log management and security</h1>
			<p>If you ask system<a id="_idIndexMarker637"/> administrators<a id="_idIndexMarker638"/> in casual conversation at the bar, you might believe that it is a major task for system administrators to collect all of their system logs and to spend hours each day manually and skillfully going through them line by line looking for system errors and malicious actors. Reality is very different. No one is doing this, no one was ever doing this, and no company is interested in paying for people to do this. Log reading is a serious skill and an activity that is excessively boring. It is also a type of task at which humans are extremely poor.</p>
			<p>If you were to attempt to have humans doing your log management by actually reading logs when there is nothing known to be wrong with a system you would run into a few problems. First, realistically no human can read logs fast enough to be truly effective. Systems log a lot of data and attempting to keep up with that kind of flow of truly mindless information would make humans extremely error prone. And then there is the cost. Anyone skilled enough to be able to handle log reading like that would be at the top end of the pay scale, and that job being so painful would have to be a premium pay position, and since typically servers run around the clock you would likely need four or five full time people <em class="italic">per server</em> to even make the attempt. Costly beyond anyone's wildest imagination and completely impractical to the point of useless. And hence, no one does it.</p>
			<p>That does not mean that logs are not valuable. The opposite is true. Logs are very valuable and we need to collect, protect, and know how to use them.</p>
			<p>When it comes to logs we need to know how to read them. This might seem trivial, but when you have an emergency and need to read your logs is not the time to find out that you do not understand what is being logged or how to interpret it. The system logs on Linux are rather consistent. The challenges really begin when different applications are logging as well. These logs might be independent or consolidated into other logs. Each application is responsible for its own logging and so we can face quite a potential for log variety when we start running a number of disparate applications. Add to this mix any logging done by bespoke in-house applications and things can possibly get quite complicated.</p>
			<p>There is no need to teach log reading here. This is an activity that anyone reading this book should be well acquainted with. The exercise that you should perform now, though, is to go through your logs and determine which logs are of a format that is unfamiliar to you and make sure that you are ready to read through any of your system logs at any time without needing to do additional research before doing so. Fast and efficient log reading will do much to make you a better system administrator.</p>
			<p>Knowing how to read your logs is not enough. You should also be familiar with your logs enough to recognize what normal activity will look like. You will be far better at recognizing when something is wrong if you first know what it looks like when it is right. So many companies bring in specialists to deal with problems after they have arisen and in doing so, there are so many more challenges created because there is no <em class="italic">logging baseline</em> to use as <a id="_idIndexMarker639"/>a basis<a id="_idIndexMarker640"/> for comparison against what things are doing now.</p>
			<p>Without a solid baseline, errors that occur regularly can cause a lot of wasted time as diagnostic time is spent researching them to determine if they are normal log noise, a real problem, or an actual problem that is simply a component of the problem being diagnosed. When things are going wrong, we want to be extra efficient. That is the very last time when we want to be figuring out what good looks like.</p>
			<p>It is always good to be prepared to hop directly onto a server and use traditional tools like vi, view, cat, head, tail, and grep to look at logs. You never know what situation you are going to be thrown into in the future.</p>
			<p>With many modern systems today we expect to see extensive tooling around logs as there is much more that we can do with our logs than simply storing them on our local servers and poking around at them after something bad has happened.</p>
			<p>Today, logging is one of the areas that has seen massive changes and advancements in server systems. We are leaps and bounds beyond where logging typically was just twenty years ago. There are some very simple advancements, such as high performance, graphical log viewers, that can be used to make the observation of logs faster and easier. There is advanced central logging to move logs away from the servers themselves and there is automated log processing.</p>
			<p>Regardless of if logging is local or remote, modern log viewers have made a tremendous difference in how efficiently we can use our logs. Whether using local desktop GUIs, character-based sessions on the terminal, or a modern web interface, log viewing tools have been improving and for the last decade or more have made the act of reading logs pleasant and easy. Amazingly, very few organizations provide these kinds of tools or accommodate their usage and so it is far less common than it should be to find system administrators using them. If you talk to system administrators do not be surprised to find out that very few have actually had the pleasure of working in an environment with logging tools beyond the basic, included system log text files and the standard text file manipulation tools that are included in the operating system.</p>
			<p>Good log viewers are an important starting point in a log management journey. Make logs accessible to view quickly and make their viewing a pleasurable and as simple as possible experience. When things are going wrong you do not want to be spending any more time than is<a id="_idIndexMarker641"/> absolutely<a id="_idIndexMarker642"/> necessary getting to your logs or to digging through them. You certainly do not want to be working to install a log viewing tool at a time when something is already broken.</p>
			<p>Log viewing applications are only a starting point, we hope, for most organizations. The real leap in log management happens when we introduce central log collection. This is really where the logging revolution has taken place. Of course, even going back decades, the potential and tools for basic log aggregation existed. This can be as simple as using network copy commands or network mapped drives to store the same old text files on a central file server. So the fundamental idea of central logging is not new. </p>
			<p>Originally the central logging constraint was that servers were not networked. Later it was performance and storage issues. Centralizing logs used a lot of network bandwidth and required a lot of storage capacity and, in some cases, was a performance nightmare for the log server as well. Those days are long since in the past. Today all of those things are trivialized simply by the natural leaps in capacity and performance of all aspects of our systems with far smaller growth in log size and complexity. Logs today are not all that much larger than they were decades ago.</p>
			<p>Early log centralization systems were very basic and unable to scale gracefully. Large amounts of aggregated log data presents big challenges for most systems as they need to be able to continue to ingest large amounts of real time data from many sources while simultaneously being able to recall and display that data.</p>
			<p>Modern central logging applications all use new, modern databases designed around this time of data flow and storage. No one single type of database is used for this, but many newer databases excel at handling these needs allowing data traditionally stored as large, unwieldy text files to be reduced to much smaller and more efficient database items with metadata, caching, collation, and other features that allow for the ingestion of massively larger amounts of data than ever before while being able to continue to display data effectively. This change, along with the general improvements in system power, has made for effective centralized logging not only on the LAN, but in many cases, even for and to servers running hosted, in the cloud, or otherwise not sitting on a traditional LAN.</p>
			<p>By using this kind of system we have a few benefits. One is speed and efficiency in log reading. One (or at least fewer) places to go to read logs means that system administrators <a id="_idIndexMarker643"/>are <a id="_idIndexMarker644"/>looking at logs much faster than before and faster log reading means faster solutions. By having logs from many servers, systems, applications, and more all in a single place also means that we can correlate data between these systems without requiring humans to look at logs from multiple sources and make these connections manually.</p>
			<p>New tools, like volume graphs, also allow us to see patterns that we may have been unable to detect before. If multiple computers suddenly show a spike in log traffic maybe applications have suddenly become busy, or maybe there is a failure or attack underway. Centralized logging tools make it easier not only for us to understand what a baseline looks like for a single system, but what a baseline will look like for all of our systems combined! More layers of system understanding.</p>
			<p>Once we have these modern tools centralized the next logical step is using automation to read logs for us. <strong class="bold">Security Information and Event Management</strong> (<strong class="bold">SIEM</strong>) is the term generally applied <a id="_idIndexMarker645"/>for automatic log monitoring tools. Automation for logs is not new and even the United States government was putting rules in place for it by 2005. But for many businesses, log automation is far beyond their current plans or capacity.</p>
			<p>Of course, like any automation, the degree to which we use it can vary greatly. Light automation might<a id="_idIndexMarker646"/> simply <a id="_idIndexMarker647"/>send alerts in case of certain triggering events occurring in the logs or alerting on uncharacteristic activity patterns. Complex automation might use artificial intelligence or threat pattern databases to scour <a id="_idIndexMarker648"/>logs <a id="_idIndexMarker649"/>across many systems at once to look for malicious activity.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor174"/>Why central logging?</h2>
			<p>Given that centralized<a id="_idIndexMarker650"/> logging carries so much network overhead and because it tends to be so costly to implement, it is easy to question the value to centralizing logging to a single server, or group of servers, rather than simply leaving logs on individual hosts and finding better methods of examining logs from them. This is a very valid question. Of course, central logging is not going to be right for every organization. It is right for a large number of them, though.</p>
			<p>While many advancements have been made to make central logging more capable than ever before, there have also been many advancements in making decentralized logging better as well including on-device log viewers with reporting, alerting, and attractive user interfaces and aggregation tools that display the data from multiple sources in a single dashboard even though the data itself is disparately located.</p>
			<p>Central logging offers unique advantages, though. The biggest advantage is simply that the data is not tied to the device. If a device dies, goes offline, or that device is compromised we have isolation for our logs. It is not uncommon to be stuck trying to get a server back online and running just so that we can look at the logs from that server. </p>
			<p>If the server dies completely we may never want to bother bringing it back online. Or if the logs tells us that there is a catastrophic failure we might know from that, that we do not want to attempt to recover a failed device. Or perhaps we want one person to be working on getting a failed server back up and running simultaneously while another scours logs to determine what led up to the failure or possibly determine what is needed to restore services. </p>
			<p>If a server or application is compromised there is a risk that the logging mechanisms or storage systems will be compromised along with it. In fact this is generally quite likely. Modifying logs to cover up a compromise is very common with sophisticated attacks and simply deleting logs common in simpler ones. In most cases logs are attacked because they are the most likely place to easily identify that a compromise has happened or is happening and what to do to mitigate it. If the logs never show any signs of an attack, you may easily never discover that one has happened.</p>
			<p>If we instead send our logs, either in their entirety or at least a copy, directly to a remote logging server in real time then we have the logs stored separately from both the application and the server storage with an air gap so that, in order to modify those logs, a completely unrelated system has to also be compromised, without being detected. This is orders of magnitude more difficult to do, especially as the existence of and information about any external logging system will generally not be known until a compromise is already under way at which point it may already be too late to avoid detection.</p>
			<p>Because of the fact that logging is far more than just a way to look for bugs and primarily the key security recording system for auditing and tracking breaches, malicious activity, infiltration attempts, and others. it is critical that this data be kept safe from both accidental loss and intentional destruction. </p>
			<p>Possibly the most important purpose of log separation, at least in larger organizations big enough to have multiple IT teams, is the separation of duties. If the logs are completely controlled by the same system administrator (presumably you) that controls the server then it is trivial to make major modifications and hide the evidence of those changes. If the logs are <a id="_idIndexMarker651"/>sent to an external system to which we are not the administrator then it is much harder for us to hide those changes as we must prevent them from being logged in the first place while not causing the logging itself to fail. </p>
			<p>Having a strict separation of duties for this level of security may sound like something limited to large organizations, but even quite small companies, even those not large enough to have a single fill time system administrator, can take advantage of this aspect of a system like this by using an externally hosted logging platform rather than running their own. In this way all of the necessary system administration and security for the logging platform is encapsulated not only away from the individual system administrator but also away from the IT team and the entire corpus of the company itself!</p>
			<p>Because every business is different, there is a place for different levels of logging and log management depending on your needs. Our constant IT mantra has to be that one size does not fit all. </p>
			<p>So our best practice with logging is a difficult one. We need to evaluate logging needs. How can we use our logs efficiently to troubleshoot faster and better. How can we increase our security. Do we have factors that make local log storage outweigh the benefits of remote? Should we host our own log systems or use a third-party SaaS application that is managed for us? Will the security benefits of a SIEM or similar solution justify their cost and complexity?</p>
			<p>Our only true best practices are to ensure that you are prepared to read logs before you need to, and to look at logs from time to time to understand what a healthy system looks like for you.</p>
			<p>With logging we are forced to really look much more heavily at rules of thumb rather than best practices. In general, central log collection is a worthwhile endeavor for almost any environment with more than a single critical workload. This can mean a single company that has multiple workloads, or smaller firms should generally be using some form of<a id="_idIndexMarker652"/> external support vendor and that vendor would, in theory, have multiple customers and would generally benefit from a similar approach allowing them to centralize logs on behalf of their customers.</p>
			<h3>The smallest IT department</h3>
			<p>This is a topic beyond<a id="_idIndexMarker653"/> the scope of the role of system administration, but one that everyone in IT should really understand because nearly every IT role will, at some point, be put in a position of being part of an organization that is simply, too small.</p>
			<p>In most fields we can talk about a minimum size to any professional team. Doctors, lawyers, auto mechanics, veterinarians, software engineering, you name it. In all these examples we can talk about the staff and team necessary to make a position make sense. A doctor working with no nurses or assistants of any kind is going to be really inefficient and lack some vectors for healthcare training. Same with a veterinarian, if you are a lone vet and have no receptionist, cashier, vet tech, and so on, then you are forced to do roles at a fraction of your value. In software engineering it is more about the wide range of discrete tasks and roles that go into software design that cannot reasonably be done by a single person, even on a small project.</p>
			<p>IT is one of the more dramatic fields for this because IT is so broad and covers so many totally different knowledge areas. And every company has the need for a large scope of that IT skill set. Some skills are unique to certain types of environments, but the large base of foundational skills apply to essentially any and every company. </p>
			<p>One of the most challenging aspects of this is that there is little room for underdeveloped skills. Because IT is the decision making and guidance around core business functions, infrastructure, support, efficiency, and security there is really no time that you do not want expert and mature guidance. A seemingly simple mistake, made nearly anywhere in the entire infrastructure, carries the risk of being a point of breach, an over expense, a decision that starts small but leads to a domino line of other decisions that will all be based on that one.</p>
			<p>Most businesses do not need most, if indeed any, individual skill more than part time with some skills, like that of CIO, being needed potentially for just a few hours per year. Obviously if skills are not needed more than a few hours per year, or even if only a few per day, paying for the skill full time would not make sense.</p>
			<p>Then there is the issue of coverage. Many businesses need to only have coverage for forty hours per week with strict office hours and all systems capable of being taken offline when the office is close. This is, however, not at all normal. Most businesses need to operate six or seven days per week, and long hours per day and running twenty four by seven is totally reasonable. To have full coverage just for someone to answer support tickets, let alone make decisions or solve real problems, would require at least five people just to have shift coverage, ignoring any skills needed.</p>
			<p>When you consider <a id="_idIndexMarker654"/>all of the discrete roles that exist in even the most minimal IT environment: systems, networking, CIO, helpdesk, desktop support, end user support, application support and then any specialty roles like cloud applications, backup, disaster recovery, project management, and on and on. What many companies attempt to do is to find a single person who can fill all of these roles, a generalist. This is a great theory, there is one person with tons of skills who can do a little bit of each one adding up to one whole person.</p>
			<p>In the real world, this does not work, at all. First because the number of people who truly possess all those skills, keep them up to date, and are so good at all of them in the universe can probably be counted on one hand. Second, anyone with good CIO level skills or system administration skills has a huge per hour billable value from those skills alone. A worker is always worth the value of their maximum skill full time, not their minimum skill. And having additional skills raises your maximum. So a CIO with all these other skills would be, in theory, worth even more than if they only had the CIO skill. So even if you found such a person, either you would have to pay them an absorbent amount of money to do the job, or they would have to be willing to do the job for a tiny fraction of their value which from an employment standpoint makes no sense.</p>
			<p>Second because of coverage. A single person can only work so many hours leaving a business without support most of the time. And even if you have a business that only exists eight hours a day and is happy to do all support and even proactive maintenance during that time you still have the issue that generally many of the roles that one person is called on to perform will need to happen simultaneously.</p>
			<p>Amazingly, tons of companies of all sizes attempt this approach and universally end up with bad results, although many never measure their results or even understand what good performance from an IT department should look like or even what that department should be accomplishing so often ignore or even praise the failures in this area.</p>
			<p>Finding a theoretically useful lower limit to the size of an IT department is hard. As a rule of thumb, if you do not need at a minimum three full time IT staff who never do anything outside of IT, then you should not try to staff an IT department.</p>
			<p>Entire ranges of IT businesses like Management Service Providers and IT Service Providers provide IT skills, management, oversight, and tooling in small portions for businesses who only need a little of many different resources. Smaller companies should never feel badly turning to these kinds of companies, they are necessary to provide the level of scale and division of that scale necessary to do IT well. That said, just like employees, the average firm is not going to be very good. So just as you want to make sure that you are<a id="_idIndexMarker655"/> hiring employees who are good, you want to hire a service provider who is good. Service providers are very much like employees, but employees that are more likely aligned with your business needs and generally with far more potential longevity - a good service provider relationship could easily outlast the career length of an individual employee.</p>
			<p>Rethinking inappropriate IT departments from both sides can be a boost to the industry. So many employers are unhappy with IT results that are predictably bad based on the IT structures that they enforce. And so many IT practitioners are unhappy with their careers or at least their immediate jobs, because they feel that they have to, or are encouraged to, work in environments that simply do not make any sense.</p>
			<p>Considering service providers as part of the in house IT team can make it possible to get the IT team that you need, at a price that is actually plausible. IT is not really outside of the budget of any company. If it seems like IT is going to be too expensive, something is wrong. The job of IT is to make the business money. </p>
			<p>The most common mistakes that I see when companies engage service providers is either assuming many incorrect rules of engagement such as assuming that local resources are better or that the service provider has to match the technology that you plan to use - if you were doing this, how would you ever determine the service provider to hire as only they would have the expertise to determine the technology to be used! A Catch-22 for sure. And the other key mistake is confusing service providers (companies that provide IT services) with value added resellers (vendor sales representatives). The latter will often market themselves as the former, but it is easy to tell them apart. The first one's business is to provide IT as a service. The second one's business includes selling hardware, software, and third party services, potentially in addition to layering on some IT.</p>
			<p>Best practices:</p>
			<p>Avoid running IT departments that are too small to support the necessary roles and division of labour.</p>
			<p>Never hire a reseller to do the job of an IT service provider.</p>
			<p>Never allow your IT staff (internal or external) to have a conflict of interest and also sell the hardware, software, and services that it is their job to recommend and choose.</p>
			<p>Now we have a <a id="_idIndexMarker656"/>good idea as to why logging is so critical to our organization. Good use of and understanding of logs and putting in place a proper, well thought out infrastructure for logs is one of the areas in which we truly see a separation between struggling and truly excelling IT departments.</p>
			<p>Not a heavily technical discussion at this point. It is really all about sitting down and putting in the effort to develop and roll out a logging plan. Making logging happen for your organization. Centralized, decentralized, automated, whatever works for you. Getting started with something, turn your logs into a robust tool that makes your life easier.</p>
			<p>In the next section we will continue on from logging to look at more general monitoring. Two highly related<a id="_idIndexMarker657"/> concepts that together really take our administration to another level.</p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor175"/>Alerts and troubleshooting</h1>
			<p>Having just<a id="_idIndexMarker658"/> discussed logs we <a id="_idIndexMarker659"/>now have to consider the highly related concept of system alerting. I have to mention that of course logging systems themselves are also a potential source of alerts. If we use automation in our logging systems, that automation will generally be expected to either send alerts directly, or add alerts to an alerting system.</p>
			<p>Alerts are, fundamentally, a way for our monitoring systems to reach out and tell us humans that they are in trouble and it is time for us to step in and work our human-intelligence magic. While we hope that our systems will have automation and can repair many problems themselves, the reality is that for the foreseeable future nearly all companies will have to keep working in a reality where human intervention is needed on a regular basis in systems administration. Whether it is to log in and clear a full disk or stop a broken process or identify a corrupt file or even to trigger a failover to a different application or notify the business of expected impact humans have a large role to play in systems still.</p>
			<p>Having good mechanisms for discovering serious issues and alerting humans is critical to quality support. In<a id="_idIndexMarker660"/> order to<a id="_idIndexMarker661"/> understand good alerting, we have to talk about both how we discover that something is wrong, and how we are notified of it.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor176"/>On-device and centralized alerting systems</h2>
			<p>We can start by <a id="_idIndexMarker662"/>looking<a id="_idIndexMarker663"/> at on device and centralized alerts. Traditionally, going back more than a few years, it was common for systems to handle their own alerts individually. Systems were already set to log issues into a central log, it was a natural extension to have them also send out emails or similar notifications should something bad be detected. Alerting was very simple, and each system would handle its own detection and its own alerting individually. While centralized monitoring and alerting has long existed, the popularity of external monitoring really did not become highly mainstreamed until it was necessary to monitor hosted Internet resources, such as websites, where an outage would often be seen by customers first, rather than by employees. When outages are first noticed by internal staff, the decision to delay discovery can be more flexible.</p>
			<p>The simplicity of on-device alerting is enticing, and for smaller organizations or those that can risk slower error detection it can serve well. The key issues with on-device alerting are that many types of serious outages or attacks may disable alerting completely, or at least delay it. A simple example is the server that loses power or whose CPU melts, the system going offline is what we want to receive an alert about, but the system going offline suddenly precludes the possibility of the system telling anyone that something bad has happened. The same happens when there is a sudden loss of network connectivity. In the event of a system compromise, a hacker may take alerting capabilities offline before being detected leaving a system running, but unable to call out for help.</p>
			<p>On-device alerting is generally inexpensive and simple. It is often built in and only needs a small amount of configuration. If using simple mechanisms such as email to send alerts, even simple scripts can add a lot of alerting functionality. This approach uses very few system resources and for small businesses or those that simply do not have to worry about potential downtime without employees reporting a loss of functionality, it can be adequate.</p>
			<p>For the majority of businesses or workloads, the caveats of on-device alerting are too great. Whether a system is customer facing and you want to maximize customer confidence, or a system is internal and you want to move discovery of issues from employees to IT to improve performance, or a system has few, if any, end users that may every discover that it is not working and you want to make sure that work is continuing to be done (such as with a scanning security system or filter) then external monitoring is necessary.</p>
			<p>External monitoring allows us to disseminate alerts even when the system in question has completely failed. Because total failure is quite common in alerting events, this can be pretty important. Complete failure might mean that hardware has failed, power has been lost, software has crashed, or networking has been lost, as examples. These are all common failure cases, and all either certainly or likely will cause on-device alerting to fail. External alerts give us a level of confidence that we will be alerted when something fails that is otherwise lacking.</p>
			<p>Of course, external systems can fail as well, leading to a lack of alerts, but there are ways that we can effectively hedge against this. One option, of course, is external alerting on the external alerting system. Essentially backup alerting. And of course, by having only a single alert source, it <a id="_idIndexMarker664"/>is easy for us to simply check that source as<a id="_idIndexMarker665"/> humans to verify that it is working. An external alert mechanism, if decoupled from the systems that it monitors, is extremely unlikely to fail at the same time that another system fails and while not perfect, this will easily eliminate 99.99% or more of missed alerts which, for most organizations, is plenty. </p>
			<h3>Out of band on-device alerts</h3>
			<p>There can be a<a id="_idIndexMarker666"/> little bit of a middle ground in alert management. If we think of our systems as a stack, each device lower in the stack is able to monitor, in a very minor way, the services running above it. For example, an application can tell us if a database connection is working, the operating system can tell us if an application is still running, a hypervisor can tell us if the operating system is still running, and, at the bottom of the stack, an out of band management hardware device can tell us if the core system components have failed.</p>
			<p>This system is not foolproof and tends to be quite basic. An operating system knows very little about the workings of an application process and mostly can only tell us if the application has crashed completely causing an error code be returned to the operating system, or it can tell us if the application is using an inordinate amount of resources such as suddenly spiking in memory requests or using a large number of CPU cycles, but the operating system will have little idea if the application keeps running but is throwing errors or gibberish to end users.</p>
			<p>For those unfamiliar, out of band management is actually an external <em class="italic">computer</em> that is housed inside of the chassis with the server hardware but has its own tiny CPU, RAM, and networking. Because it is a nearly completely separate computer from the server itself, the OOB (out of band) management system can report, either directly or to some monitoring system, if there are critical hardware failures on the server itself such as a failed motherboard, CPU, memory, storage, or other component that would normally make the server itself unable to send out its own alerts. </p>
			<p>An OOB management system does share the chassis, location, and power with the server, though. This <a id="_idIndexMarker667"/>means that it still has limited ability to monitor a system for certain types of common failures. As with many things, for many businesses this might be adequate, for others it will not be enough.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor177"/>Pushed and pulled alerts</h2>
			<p>Alerting systems also<a id="_idIndexMarker668"/> have two <a id="_idIndexMarker669"/>basic ways of interacting with us, as the actors being alerted by the system. What we tend to think of is pushed alerts. That is, alerts that are sent to us with the intention of grabbing our attention when we are not thinking about alerts.</p>
			<p>Typically pushed alerts go out via email, text messages (SMS), telephone calls, WhatsApp, Telegram, Signal, RocketChat, Slack, Microsoft Teams, Zoho Cliq, MatterMost, or other, similar, real-time communications channel. Some alerting systems have their own applications that you install to desktops or smartphones so that they can push out alerts rapidly and reliably without having to integrate or depend on any additional infrastructure. You can easily imagine an organization running their own email and messaging platforms only to have those platforms be monitored by our alert system and also be the path by which we receive the alerts. Even if the alerting system itself does not fail, it is possible that it will be unable to tell us that something has failed because the systems that it monitors are also the systems that get the alerts to us. This is why having one fewer path to fail and one fewer dependency in alert delivery is sometimes approached. </p>
			<p>Alerting is a surprisingly complex animal. If alerting was being handled by humans, rather than computers, we would quickly find that call centers have complex, multi-branch decision trees to follow for what to do when we cannot tell someone that something is wrong. With humans, though, we know that in an extreme emergency someone will start pulling out their personal cell phone and texting someone to call someone and knock on a door and wake someone up or whatever. Computers can do all of this, too, but they need access to those tools, algorithms to make those decisions, and knowledge of how to reach people. Easier said that done.</p>
			<p>To solve this particular problem, many companies opt to include humans in the communications path. An expensive, but effective, tool. Sometimes human decision making, and flexibility wins out. Human call centers that are always staffed and receive alerts on behalf of technical teams and managers and then manage the contact path to whomever needs to receive the alert can be a great option. And, obviously, hybrid options where computer systems alert end recipients directly but humans are always involved in verifying that alerts go out, acknowledgements are received, or whatever is possible.</p>
			<p>The alternative to pushed alerts is pulled alerts. Pulled alerting refers to systems that display the status of any open or logged alerts when the end user logs in to look at them. These<a id="_idIndexMarker670"/> systems <a id="_idIndexMarker671"/>are vastly more reliable because the end user is looking at the system and knows if there is an inability to view the alert status or not. If the system has failed, then they can start working on the issue right away. If it has not failed, they see the alerts and know if action is needed or not.</p>
			<h3>Silence as success</h3>
			<p>The basic problem that arises with using purely pushed alerts is that we rely on silence to tell us that everything is okay. Rather than getting a confirmation that everything has been checked and that nothing is currently wrong, we depend on not having been reached to create an assumption of nothing being wrong. This is a dangerous approach.</p>
			<p>We all know the feeling of waking up or having been on a long car ride or maybe being at a party and not actively watching our phones and mostly feeling good that the office has not reached out to us, no one has called, so everything must be fine. Then you look at your phone some hours later and realize that the battery has died, there was no service, or you had your phone on silence. Panic sets in. You plug in your phone, get service back, and turn on the ringer and find that you have been missing call after call, voicemail after voicemail, text after text telling you that there is a huge emergency, you left the office having changed a critical password and not telling anyone, the system is down, no one can get in except for you and you are not responding!</p>
			<p>Trusting that no one was able to get my attention, therefore nothing can be wrong just does not work. But neither does staring at a console and never being offline. There has to be a balance. It is clear that simply hoping that you will be able to be reached is a recipe for disaster. Maybe a disaster that takes many, many years to finally happen, but a disaster that is almost certainly going to happen eventually. There are just too many variables that can go wrong.</p>
			<p>Pull monitoring and alert systems are therefore important as a means of verifying that pushed alerts are<a id="_idIndexMarker672"/> working <a id="_idIndexMarker673"/>currently or to work around known disconnects. Or to run an active monitoring site.</p>
			<p>Alerting's systems, too, struggle with defining a successful alert. Many mechanisms like email and SMS texting will confirm only that a message has been sent or possibly received by the recipient's infrastructure vendor, but they do not give any indication that the message has made it all of the way to the end user's device, or that the end user has been displayed the message. Even if a message does go end to end, does it get filtered into a spam folder and hidden? Unless we have a human actually acknowledge an alert there is very little, we can do to have confidence that an alert has truly been seen.</p>
			<p>Pulled alert systems, generally displayed as dashboards, are typically what we picture as a red light, green light system. It is common to display monitored systems or components graphically and to show systems believed to be healthy shown as green, those experiencing problems, but not yet indicating an outage as yellow, and those that have failed whatever sensor test we are performing as red. This does not just give the humans the ability to quickly eyeball the range of alerts, but also makes it trivial for the system to roll up large groups of alerts into single displays. As long as those are green, you know everything in the group is healthy. If it is red, you can dig in to see exactly what is wrong. At the highest level you can, in theory, even have a single big indicator that shows as green or red. Green if systems are one hundred percent good and red if any system has failed. Simply: is action needed or is it not. If you can be green most of the time, this might be exactly what you need to combine reliable monitoring with low overhead in verification.</p>
			<p>Most alert systems will offer both a dashboard to show pulled alerts along with push notifications to improve response times and reach people who are not actively checking alerts. Pulled alert systems are often used at the core of a call center where humans on shifts watch the pulled alerts around the clock and either enact the push alerts to the concerned parties or follow up to ensure delivery of automated alerts. It is less common for organizations to open pulled alerts to many staff which can be a mistake as it can lower stress and increase alert reliability.</p>
			<p>Similarly, the interaction between the monitoring system and the end points that are being monitored <a id="_idIndexMarker674"/>can <a id="_idIndexMarker675"/>work in either direction or both. The monitoring system may have remote access to the end points and actively reach out to them to request their status. Or an agent running on the end points may reach out to the monitoring server to push their status over to it.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor178"/>In house and hosted monitoring</h2>
			<p>Monitoring and <a id="_idIndexMarker676"/>alerting<a id="_idIndexMarker677"/> are, in some ways, two separate pieces and there is software and services that do either, and those that do both. The monitoring component determines if a series of sensors detects something that is wrong in our workloads. The alerting component takes the results of the monitoring and attempts to notify the correct parties. It used to be that the two components were always merged into single products, but in more recent years with more and more types of systems needing to be monitored (more than IT systems, that is) and as alerting needs have increased and have needed to become more robust, different vendors have started to build each independently in some cases. </p>
			<p>Today monitoring solutions come in a good variety of packages and styles. You easily may decide that you would benefit from using more than one. Choosing a good package might be the hardest part of your monitoring puzzle. Monitoring software is available commercial and free, closed and open source, and built to run on nearly any platform. </p>
			<p>Monitoring is a function that you will generally want to host externally to your primary infrastructure as you want it be less quick and reliable, and more totally independent of your other systems compared to other workloads. You can host it yourself on your own equipment, go with third party cloud or VPS hosted infrastructure, or get a SaaS application from a provider. You can tackle this in any manner than makes sense for your organization. But rarely do you want your monitoring solution to sit on the same hardware, let alone the same datacenter, as your other workloads or you risk losing monitoring when you lose everything else.</p>
			<p>Whether to run and maintain your own monitoring or to go with a hosted product will mostly be a question of cost and politics within your organization. Even free and open-source monitoring solutions can be robust enough for the most demanding of organizations. You will need to determine if the cost of building, maintaining, and <em class="italic">monitoring</em> a monitoring solution makes sense for your organization or if simply buying that functionality ready to go makes sense for you. In most cases this is determined by scale. If you monitor a very large number of workloads or your monitoring needs are highly unique you may benefit from building the expertise in house and having full time specialists dedicated to this project. Generally for a project like this to be cost effective to keep in house you will want to have either fully dedicated staff or heavily dedicated staff who have the time and resources to really learn the products and maintain them properly. Often monitoring and logging will be bundled together whether as a single product or under a single person or team as they overlap so heavily, and logging can be thought of as a specialty function of monitoring. The two may be operated separately or combined. Using both through the same alerting channels generally makes sense as they can leverage the same effort and infrastructure.</p>
			<p>Like most things in this chapter, the real struggle around best practices is finding what we can distill as being <em class="italic">best</em> as guidance here is very broad and mostly ambiguous. There is no one size<a id="_idIndexMarker678"/> fits all. It seems<a id="_idIndexMarker679"/> like cheating to say, but it is true that the real best practice here is to evaluate your business' needs based on cost, functionality, support, separation from your production environment and determine what monitoring and alerting mechanisms are right for you.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor179"/>RMMs and monitoring</h2>
			<p>If you work in <a id="_idIndexMarker680"/>internal IT <a id="_idIndexMarker681"/>then the term RMM might be something that you have never heard of, but if you have worked in the service provider sector then RMMs are the core tools expected to be used for customer support in that area.</p>
			<p>An RMM, which stands for Remote Monitoring and Management, is a tool category designed around the needs of service providers who almost always need to work remotely to their client sites and to be able to quickly monitor many disparate client systems at one time. This is generally quite different from internal IT needs where often they are not remote and even when they are, their systems are typically integrated.</p>
			<p>A rare few non-service providers still lean on RMM tools as a monitoring mechanism. Typically RMMs are very light and inflexible but are, at their core, monitoring systems much like what we are discussing here. So you can certainly consider using an RMM that you purchase or run yourself, or if you have a service provider, this might be part of the service that you are already paying for. RMMs are even available as free, open-source products. So no company, of any size can say that they do not have the resources to at least do the most basic levels of monitoring.</p>
			<p>In some cases, traditional monitoring tools designed for internal IT teams are so robust that they actually displace RMMs in service providers. Or the two could be used in tandem, as well.</p>
			<p>The rule of thumb is that more monitoring is better than less, hosting outside of your environment is generally best, make sure that alerts have many channels to find a way to get to a human, make sure that pull monitoring is available to at least verify that push is working, and consider having your monitoring system create actionable tickets for your support team automatically to track follow ups.</p>
			<p>Is there an actual best practice? Yes. The best practice here is simple and broad: if a workload has a purpose, then it should be monitored. Monitoring, because it does not directly stop production from running if it does not exist, can too easily be overlooked. Almost no one gets<a id="_idIndexMarker682"/> promoted<a id="_idIndexMarker683"/> for doing good monitoring or fired for lacking it, but implementing proper monitoring is effective in separating the good administrators from the run of the mill.</p>
			<p>Go set up some monitoring!</p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor180"/>Summary</h1>
			<p>In this chapter we have looked at the range of key non-systems components that surround the systems themselves. Documentation, system measurement, data collection and planning, log collection and management, and finally monitoring sensors and alerting based on them. These could almost be considered soft skills within the systems administration realm.</p>
			<p>Consistently in environments that I have taken over we have found documentation to be practical non-existent, measuring systems to be all but unheard of, capacity planning being a process no one has ever so much as discussed, monitoring often minimal and unreliable at best, and log collection while well understood, simply a pipe dream when it comes to real world implementation. Yet a single system administrator with almost no resources could, with just some time, pull together some free, open-source software and tackle each of these projects on their own with little to no budgetary constraints and could often hide the workloads somewhere within the system if it was necessary to do so.</p>
			<p>This chapter has not been about how to make your systems run better; it has been about everything else. How do we know that they have been running better? How do we know that they are running right now? How do we know that we can pass the proverbial baton on to someone else should we win the lottery? How do we confidently say that we are doing what needs to be done to make a best effort against a malicious attack? The topics in this chapter have made us look at how to be better at all of the things that we do and not just the ones that are most visible.</p>
			<p class="callout-heading">Up your visibility</p>
			<p class="callout">Too often what we do in IT is completely invisible to those on other teams, even those in management to whom we report. Maybe it is invisible because there is simply nothing to show. Or maybe what we do is too hard and complex for people outside of our realm to really understand. Or maybe we are invisible because we choose to accept being invisible.</p>
			<p class="callout">Most of the topics in this section provide perfect opportunities to step out of the IT dungeon or closet and get in management's face(s) to do a little IT team self-promotion. From monitoring dashboard to beautiful documentation, to capacity charts, to log drill down examples there is almost always something that we can print out or show on a big screen and look pretty impressive for having implemented.</p>
			<p class="callout">Getting the attention of management and showing that we are being proactive, that we are following best practices, this is where we can make a sales pitch for just how amazing our value is to the organization. Do not be afraid to do some self-promotion, you deserve it. Make some noise and show off how you are preparing the business for the greatest success.</p>
			<p>Go out and make sure that all of these systems mentioned in this chapter are implemented in your environment. Keep it simple to get started, but do not skip systems.</p>
			<p>In our next chapter we are going to move on to scripting and system automation including DevOps, which I know that you have been waiting for.</p>
		</div>
	</div></body></html>