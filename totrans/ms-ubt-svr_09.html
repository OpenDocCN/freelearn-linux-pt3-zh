<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer122">
<h1 class="chapterNumber">9</h1>
<h1 class="chapterTitle" id="_idParaDest-125">Managing Storage Volumes</h1>
<p class="normal">When it comes to storage on our servers, it seems as though we can never get enough. While hard disks are growing in capacity every year, and high-capacity disks are cheaper than ever, our servers gobble up available space quickly. As administrators of servers, we always do our best to order servers with ample storage, but business needs evolve over time, and no matter how well we plan, a successful business will always need more. While managing your servers, you’ll likely find yourself adding additional storage at some point. But managing storage is more than just adding new disks every time your current one gets full. Planning ahead is also important, and technologies such as <strong class="keyWord">Logical Volume Manager</strong> (<strong class="keyWord">LVM</strong>) will <a id="_idIndexMarker444"/>make your job much easier as long as you start using it as early as you possibly can.</p>
<p class="normal">LVM itself is just one of the concepts we’ll go over in this chapter that will give you more flexibility with how you handle servers. I’ll also walk you through additional concepts that will no doubt come in handy as you manage storage and volumes on your server. More specifically, this discussion will include:</p>
<ul>
<li class="bulletList">Adding additional storage volumes to the filesystem</li>
<li class="bulletList">Formatting and partitioning storage devices</li>
<li class="bulletList">Mounting and unmounting volumes</li>
<li class="bulletList">Understanding the <code class="inlineCode">/etc/fstab</code> file</li>
<li class="bulletList">Backing up and restoring volumes</li>
<li class="bulletList">Utilizing LVM</li>
</ul>
<p class="normal">One possible solution when your server is running out of disk space is to add an additional storage volume. So the first order of business for us, in this chapter, will be to look into how we can do exactly that.</p>
<h1 class="heading-1" id="_idParaDest-126">Adding additional storage volumes</h1>
<p class="normal">At some point <a id="_idIndexMarker445"/>or another, you’ll reach a situation where you’ll need to add additional storage to your server. On physical servers, we can add additional hard disks, and on virtual or cloud servers, we can add additional virtual disks. Either way, in order to take advantage of the extra storage, we’ll need to determine the name of the device, format it, and mount it. </p>
<p class="normal">In the case of LVM (which we’ll discuss later in this chapter), we’ll have the opportunity to expand an existing volume, often without a server reboot being necessary. There’s an overall process to follow when adding a new device, though. When adding additional storage to your system, you should ask yourself the following questions:</p>
<p class="normal"><strong class="keyWord">How much storage do you need?</strong> If you’re adding a virtual disk, you can usually make it any size you want, as long as you have enough space remaining in the pool of your hypervisor.</p>
<p class="normal"><strong class="keyWord">After you attached it, what device name did it receive?</strong> When a new disk is attached to our server, it will be detected by the system and given a device name. In most cases, the naming convention of <code class="inlineCode">/dev/sda</code>, <code class="inlineCode">/dev/sdb</code>, and so on will be used. In other cases (such as virtual disks), this will be different, such as <code class="inlineCode">/dev/vda</code>, <code class="inlineCode">/dev/xda</code>, and possibly others. The naming scheme usually ends with a letter, incrementing to the next letter with each additional disk.</p>
<p class="normal"><strong class="keyWord">How do you want the storage device formatted?</strong> At the time of writing, the ext4 filesystem is the most common. However, for different workloads, you may consider other options (such as XFS). When in doubt, use ext4, but definitely read up on the other options to see if they may benefit your use case. ZFS is another option that you can consider, though compared to the other choices, it’s relatively new. We’ll discuss formatting in the next section, <em class="italic">Formatting and partitioning storage devices</em>.</p>
<p class="normal">It may be common knowledge to you by now, but the word filesystem is a term that can have multiple meanings on a Linux system depending on its context and may confuse newcomers. Linux administrators like us will most often use the term filesystem to discuss the file and directory structure of a typical Linux system. However, the term is also used to describe how a disk is formatted for use with the distribution (for example, the ext4 filesystem). In this chapter, we’ll be primarily focusing on the latter.</p>
<p class="normal"><strong class="keyWord">Where do you want it mounted?</strong> The new disk needs to be accessible to the system and possibly users, so you would want to mount (attach) it to a directory on your filesystem where your users or your application will be able to use it. In the case of LVM, which we also discuss in this chapter, you’re probably going to want to attach it to an existing storage group. You can come up with your own directory for use with the new volume, but I’ll discuss a few common locations later on in this chapter. We’ll go over the process of mounting and unmounting in the <em class="italic">Mounting and unmounting volumes</em> section.</p>
<p class="normal">Let’s consider the answers to the first two questions. With regard to how much space you should add, you would want to research the needs of your application or organization and find a reasonable amount. In the case of physical disks, you don’t really get a choice beyond deciding which disk to purchase. In the case of virtual disks, you’re able to be more frugal, as you can add a small disk to meet your needs (you can always add more later).</p>
<p class="normal">The main <a id="_idIndexMarker446"/>benefit<a id="_idIndexMarker447"/> of LVM with virtual disks is being able to grow a filesystem without a server reboot. For example, you can start with a 30 GB volume and then expand it in increments of 10 GB by adding additional 10 GB virtual disks. This method is certainly better than adding a 200 GB volume all at once when you’re not completely sure all that space will ever be used. </p>
<p class="normal">LVM can also be used on physical servers as well, but would most likely require a reboot anyway since you’d have to open the case and physically attach a hard drive. Some servers allow for hot-plugging, which gives you the ability to add/remove physical hard drives without powering off the server first, which is a great benefit to have.</p>
<p class="normal">Next, the device name can be found with the <code class="inlineCode">fdisk -l</code> command. The <code class="inlineCode">fdisk</code> command is normally used for creating and deleting partitions, but it will also allow us to determine which device name our new disk received. Using the <code class="inlineCode">fdisk -l</code> command will give you the info, but you’ll need to run it as <code class="inlineCode">root</code> or with <code class="inlineCode">sudo</code>:</p>
<pre class="programlisting con"><code class="hljs-con">sudo fdisk -l
</code></pre>
<p class="normal">Executing this command produces output similar to the following:</p>
<figure class="mediaobject"><img alt="" height="449" src="../Images/B18425_09_01.png" width="876"/></figure>
<p class="packt_figref">Figure 9.1: Output of the fdisk -l command</p>
<p class="normal">I always<a id="_idIndexMarker448"/> recommend running <code class="inlineCode">fdisk -l</code> <em class="italic">before and after</em> attaching a new device. That way, it will be more obvious which device name represents the new device.</p>
<p class="normal">Another trick is to use the following command, with which the output will update automatically as you add the disk:</p>
<pre class="programlisting con"><code class="hljs-con">dmesg --follow
</code></pre>
<p class="normal">Just start the command, attach the disk, and watch the output. When done, press <em class="keystroke">Ctrl</em> + <em class="keystroke">c</em> on your keyboard to return to the prompt.</p>
<p class="normal">You can also find the device name of your new disk with the <code class="inlineCode">lsblk</code> command. The benefit of <code class="inlineCode">lsblk</code> is that you don’t need <code class="inlineCode">root</code> privileges, and the information it returns is simplified:</p>
<figure class="mediaobject"><img alt="" height="301" src="../Images/B18425_09_02.png" width="738"/></figure>
<p class="packt_figref">Figure 9.2: Output of the lsblk command</p>
<p class="normal">On a typical server, the first disk (basically, the one that you installed Ubuntu Server on) will be given a device name of <code class="inlineCode">/dev/sda</code> while additional disks will be given the next available name, such as <code class="inlineCode">/dev/sdb</code>, <code class="inlineCode">/dev/sdc</code>, and so on (depending on the type of hard disk you have). Nowadays, <strong class="keyWord">Non-Volatile Memory Express</strong> (<strong class="keyWord">NVMe</strong>) hard drives are becoming increasingly <a id="_idIndexMarker449"/>common, so you may see a device name similar to <code class="inlineCode">/dev/nvme0n1</code>. You’ll also need to<a id="_idIndexMarker450"/> know the partition number. Device names for disks will also have numbers at the end, representing individual partitions. For example, the first partition of <code class="inlineCode">/dev/sda</code> will be given <code class="inlineCode">/dev/sda1</code>, while the second partition of <code class="inlineCode">/dev/sdc</code> will be given <code class="inlineCode">/dev/sdc2</code>. These numbers increment and are often easy to predict. As I mentioned before, your device<a id="_idIndexMarker451"/> naming convention may vary from server to server, especially if you’re using a <strong class="keyWord">Redundant Array of Independent Disks</strong> (<strong class="keyWord">RAID</strong>) controller or a virtualization host such as VMware or XenServer. If you haven’t created a partition on your new disk yet, you will not see any partition numbers at the end of their names.</p>
<p class="normal">Now that you’ve added and named an additional storage volume, we can proceed through the process of setting it up. We need to decide where we’re going to mount it, and what purpose it will serve. But before we can even mount the storage device in the first place, we need to create at least one partition on it and then format it. We’ll take care of both in the next section.</p>
<h1 class="heading-1" id="_idParaDest-127">Formatting and partitioning storage devices</h1>
<p class="normal">Once you’ve<a id="_idIndexMarker452"/> installed<a id="_idIndexMarker453"/> a physical or virtual disk, you’re well on your way to benefiting from additional storage. But in order to utilize a disk, it must first be formatted. In order to ensure we’re formatting the correct disk, we need to find the name the device was given. As you already know from the previous section, there’s a specific naming scheme that is used in Linux distributions to name disks. So you should already know the device name of the new disk. As explained earlier, you can use the <code class="inlineCode">sudo fdisk -l</code> command to see details regarding the storage devices attached to your server:</p>
<pre class="programlisting con"><code class="hljs-con">sudo fdisk –l
</code></pre>
<p class="normal">This will produce an output that looks similar to the following:</p>
<figure class="mediaobject"><img alt="" height="409" src="../Images/B18425_09_03.png" width="826"/></figure>
<p class="packt_figref">Figure 9.3: Using fdisk -l to view a list of storage devices on the server</p>
<p class="normal">In my case, the device <code class="inlineCode">/dev/sdb</code> is brand-new—I just added it to the server. Since I’m using a virtual machine for the examples in this chapter, the new disk shows a model of <code class="inlineCode">QEMU HARDDISK</code>. It doesn’t have any partitions currently; notice how we see a few lines above it referring to a different hard disk and partitions, such as <code class="inlineCode">/dev/sda3</code>. We don’t have any lines like that in the description underneath for <code class="inlineCode">/dev/sdb</code>. If we did have one or more partitions on that device, they would show up in the output.</p>
<p class="normal">At this point, we know which storage device is new—there’s no doubt that in the preceding example, it’s <code class="inlineCode">/dev/sdb</code>. We always need to make sure we don’t attempt to format or repartition the wrong device, or we may lose data. In this case, we can see <code class="inlineCode">/dev/sdb</code> has no partitions (and this volume wasn’t present before I added it), so it’s obvious <a id="_idIndexMarker454"/>which <a id="_idIndexMarker455"/>disk we’ll want to work with. Now we can create one or more partitions on it, to continue preparing it for use.</p>
<h2 class="heading-2" id="_idParaDest-128">Creating a partition</h2>
<p class="normal">To create an<a id="_idIndexMarker456"/> actual partition on this device, we’ll use the <code class="inlineCode">fdisk</code> command with <code class="inlineCode">sudo</code>, using the device’s name as an option. In my case, I would execute the following to work with disk <code class="inlineCode">/dev/sdb</code>:</p>
<pre class="programlisting con"><code class="hljs-con">sudo fdisk /dev/sdb
</code></pre>
<p class="normal">Note that I didn’t include a partition number here, as <code class="inlineCode">fdisk</code> works with the disk directly (and we also have yet to create any partitions). In this section, I’m assuming you have a disk that has yet to be partitioned or one you won’t mind wiping. When executed correctly, <code class="inlineCode">fdisk</code> will show you an introductory message and give you a prompt:</p>
<figure class="mediaobject"><img alt="" height="315" src="../Images/B18425_09_04.png" width="825"/></figure>
<p class="packt_figref">Figure 9.4: Main prompt of fdisk</p>
<p class="normal">At this point, you can press <em class="keystroke">m</em> on your keyboard for a menu of possible commands you can execute. In this example, I’ll walk you through the commands required to set up a new disk for the first time.</p>
<p class="normal">I’m sure it goes without saying, but be aware of the destructive possibilities of <code class="inlineCode">fdisk</code>. If you run <code class="inlineCode">fdisk</code> against the wrong drive, irrecoverable data loss may result. It’s common for an administrator to memorize utilities such as <code class="inlineCode">fdisk</code> to the point where using it becomes muscle memory. But always take the time to ensure that you’re running such commands against the appropriate disk.</p>
<p class="normal">Before we continue with creating a new partition, some discussion is required with regard to <a id="_idIndexMarker457"/>the <strong class="keyWord">Master Boot Record</strong> (<strong class="keyWord">MBR</strong>) and <strong class="keyWord">GUID Partition Table</strong> (<strong class="keyWord">GPT</strong>) partition <a id="_idIndexMarker458"/>tables. When creating a partition table on a new disk, you’ll have the option to set it up to use an MBR or GPT partition table. GPT is the newer standard, while MBR has been around for quite some time and is probably what you’ve been using if you’ve been working with servers for a long time.</p>
<p class="normal">You may see MBR referred to as DOS when referring to the older partition structure. As you may already <a id="_idIndexMarker459"/>know, <strong class="keyWord">DOS</strong> is short for <strong class="keyWord">Disk Operating System</strong>, but we’re not referencing that operating<a id="_idIndexMarker460"/> system here in this chapter; we’re referencing the partitioning structure that IBM came up with decades ago. For example, while using <code class="inlineCode">fdisk</code>, it will refer to the MBR partition structure as DOS. In this chapter, we’ll use MBR to refer to the older style whenever possible to avoid confusion.</p>
<p class="normal">With MBR partition tables, you have some limitations to consider. First, MBR only allows you to create up to four primary partitions. In addition, it also limits you to using somewhere around 2 TB of a disk. If the capacity of your disk is 2 TB or less, this won’t be an issue. However, disks larger than 2 TB are becoming more and more common.</p>
<p class="normal">On the other hand, GPT doesn’t have a 2 TB restriction, so if you have a very large disk, the decision between MBR and GPT has pretty much been made for you. In addition, GPT doesn’t have a restriction of up to four primary partitions, as <code class="inlineCode">fdisk</code> with a GPT partition table will allow you to create up to 128 of them. It’s certainly no wonder GPT is fast becoming the new standard! It’s only a matter of time before GPT becomes the default, so unless you have a good reason not to, I recommend using it if you have a choice.</p>
<p class="normal">When you first enter the <code class="inlineCode">fdisk</code> prompt, you can press <em class="keystroke">o</em> to create an MBR-style partition layout, or you can press <em class="keystroke">g</em> to create the partition layout with the newer GPT style. As I’ve mentioned before, this is a potentially destructive process, so make sure you’re using this utility against the correct drive! Make sure you press the associated key for your chosen partition style and then press <em class="keystroke">Enter</em>, and then we can proceed. Once you press <em class="keystroke">g</em> or <em class="keystroke">o</em>, you should see a confirmation that you have created a new partition table.</p>
<p class="normal">Continuing on, after you’ve made your choice and created either an MBR or GPT partition table, we’re ready to proceed. Next, at the <code class="inlineCode">fdisk</code> prompt, type <em class="keystroke">n</em> to tell <code class="inlineCode">fdisk</code> that you would like to create a new partition. Then, you’ll be asked if you would like to create a primary or extended partition (if you’ve opted for MBR). With MBR, you would want to choose primary for the first partition, and then you can use extended for creating additional <a id="_idIndexMarker461"/>partitions. If you’ve opted for GPT, this prompt won’t appear, as it will create your partition as primary no matter what.</p>
<p class="normal">The next prompt that will come up will ask you for the partition number, defaulting to the next available number. Press <em class="keystroke">Enter</em> to accept the default. Afterward, you’ll be asked for the first sector of the partition to use (press <em class="keystroke">Enter</em> to accept the default of <code class="inlineCode">2,048</code>), and then the next prompt will ask you for the last sector to use. If you press <em class="keystroke">Enter</em> to accept the default last sector, your partition will consist of all the free space that was remaining on the device. If you’d like to create multiple partitions, don’t accept the default at the last sector prompt. Instead, you can clarify the size of your new partition by typing the <em class="keystroke">+</em> symbol followed by the number of mebibytes or gibibytes to use, and then <code class="inlineCode">M</code> for mebibytes or <code class="inlineCode">G</code> for gibibytes. For example, you can enter <code class="inlineCode">+20G</code> here to create a partition of 20 GiB. Note that there is no space after the <code class="inlineCode">+</code> symbol, nor is there a space between <code class="inlineCode">20</code> and <code class="inlineCode">G</code>.</p>
<p class="normal">At this point, you’ll be returned to the <code class="inlineCode">fdisk</code> prompt. To save your changes and exit <code class="inlineCode">fdisk</code>, press <em class="keystroke">w</em> and then <em class="keystroke">Enter</em>. Now if you run the <code class="inlineCode">fdisk -l</code> command as <code class="inlineCode">root</code>, you should see the new partition you created. Here is some example output from the <code class="inlineCode">fdisk</code> command from one of my servers, to give you an idea of what the entire process looks like:</p>
<figure class="mediaobject"><img alt="" height="646" src="../Images/B18425_09_05.png" width="877"/></figure>
<p class="packt_figref">Figure 9.5: Example run of the fdisk command</p>
<p class="normal">If you’ve made a mistake or you want to redo your partition layout, you can do so by entering the <code class="inlineCode">fdisk</code> prompt again and then pressing <em class="keystroke">g</em> to create a new GPT layout or <em class="keystroke">o</em> to create a new <a id="_idIndexMarker462"/>MBR layout. Then, continue through the steps again to partition your disk. Feel free to practice this a few times until you get the hang of the process.</p>
<h2 class="heading-2" id="_idParaDest-129">Formatting partitions</h2>
<p class="normal">After you <a id="_idIndexMarker463"/>create your partition layout for your new disk and you’re satisfied with it, you’re ready to format it. Now that I’ve created a partition layout on the new disk, the output of <code class="inlineCode">sudo fdisk -l</code> will be different:</p>
<figure class="mediaobject"><img alt="" height="311" src="../Images/B18425_09_06.png" width="873"/></figure>
<p class="packt_figref">Figure 9.6: Another example of sudo fdisk -l after creating a partition</p>
<p class="normal">Notice that now, we have the partition <code class="inlineCode">/dev/sdb1</code> added, which is visible in the output. Now, we can go ahead and format it. To do so, we take care of that with the <code class="inlineCode">mkfs</code> command. This command is run with a specific syntax that entails typing <code class="inlineCode">mkfs</code> along with a period (<code class="inlineCode">.</code>), followed by the type of filesystem you would like to format the target as. The following example will format <code class="inlineCode">/dev/sdb1</code> as ext4:</p>
<pre class="programlisting con"><code class="hljs-con">sudo mkfs.ext4 /dev/sdb1
</code></pre>
<p class="normal">Your output <a id="_idIndexMarker464"/>will look similar to mine in the following screenshot:</p>
<figure class="mediaobject"><img alt="" height="385" src="../Images/B18425_09_07.png" width="825"/></figure>
<p class="packt_figref">Figure 9.7: Formatting a volume using the ext4 filesystem</p>
<p class="normal">If you’ve opted for a filesystem type other than ext4, you can use that in place of ext4 when using <code class="inlineCode">mkfs</code>. The following example creates an XFS filesystem instead:</p>
<pre class="programlisting con"><code class="hljs-con">sudo mkfs.xfs /dev/sdb1
</code></pre>
<p class="normal">Some filesystems, such as XFS, are not supported by default and may need an additional package installed in order for them to be used. In the case of XFS, it requires the <code class="inlineCode">xfsprogs</code> package to be installed.</p>
<p class="normal">So, now that we’ve<a id="_idIndexMarker465"/> created one or more partitions and formatted them, we’re ready to mount the newly created partition(s) on our server. In the next section, I’ll walk you through mounting and unmounting storage volumes.</p>
<h1 class="heading-1" id="_idParaDest-130">Mounting and unmounting volumes</h1>
<p class="normal">Now that you’ve <a id="_idIndexMarker466"/>added <a id="_idIndexMarker467"/>a new storage volume to your server and formatted it, you can mount the new device so that you can start using it. To do this, we use the <code class="inlineCode">mount</code> command. This command allows you to attach a storage device (or even a network share) to a local directory on your server. Before mounting, the directory must be empty. The <code class="inlineCode">mount</code> command, which we’ll get to practice with an example very shortly, basically just requires you to designate a place (directory) for the device to be mounted to. But where should you mount the volume?</p>
<p class="normal">Normally, there are two directories, created by default, in your Ubuntu Server installation that exist for the purposes of mounting volumes: <code class="inlineCode">/mnt</code> and <code class="inlineCode">/media</code>. While there is no hard rule as far as where media needs to be mounted, these two directories exist as part of<a id="_idIndexMarker468"/> the <strong class="keyWord">Filesystem Hierarchy Standard</strong> (<strong class="keyWord">FHS</strong>) that was mentioned in <em class="chapterRef">Chapter 4</em>, <em class="italic">Navigating and Essential Commands</em>. The purposes of the <code class="inlineCode">/mnt</code> and <code class="inlineCode">/media</code> directories are defined within this specification. The FHS defines <code class="inlineCode">/mnt</code> as a mount point for a temporarily mounted filesystem, and <code class="inlineCode">/media</code> as a mount point for removable media.</p>
<p class="normal">In plain English, this means that the intended purpose of <code class="inlineCode">/mnt</code> is for storage volumes you generally keep mounted most of the time, such as additional hard drives, virtual hard disks, and network-attached storage. The FHS document uses the term <em class="italic">temporary</em> when describing <code class="inlineCode">/mnt</code>, but in practice, this is typically where things are mounted that you generally expect to be around for a while. In regard to <code class="inlineCode">/media</code>, the FHS is basically indicating that removable media (flash drives, CD-ROM media, external hard drives, and so on) are intended to be mounted there.</p>
<p class="normal">However, it’s <a id="_idIndexMarker469"/>important to point out that where the FHS indicates you<a id="_idIndexMarker470"/> should mount your extra volumes is only a suggestion. (Perhaps a strong suggestion, but a suggestion nonetheless.) No one is going to force you to follow it, and the fate of the world isn’t dependent on your choice. With the <code class="inlineCode">mount</code> command, you can literally mount your extra storage anywhere that isn’t already mounted or full of files. You could even create the directory <code class="inlineCode">/kittens</code> and mount your disks there and you won’t suffer any consequences other than a few chuckles from your colleagues.</p>
<p class="normal">Often, organizations will come up with their own scheme for where to mount extra disks. Although I personally follow the FHS designation, one example of a custom layout was with a company I worked with in the past. They used the <code class="inlineCode">/store</code> directory for mounting storage on their servers, a directory they created themselves on each server. Whatever scheme you use is up to you; the only suggestion I can make is to be as consistent as you can from one server to another, if only for the sake of sanity.</p>
<p class="normal">The <code class="inlineCode">mount</code> command generally needs to be run as <code class="inlineCode">root</code>. While there is a way around that (you can allow normal users to mount volumes, but we won’t get into that just yet), it’s usually the case that only <code class="inlineCode">root</code> can or should be mounting volumes. As I mentioned, you’ll need a place to mount these volumes, so to facilitate that, we can create a directory called <code class="inlineCode">/mnt/vol1</code> with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">sudo mkdir /mnt/vol1
</code></pre>
<p class="normal">When you’ve created a directory, like I have, or decided on an existing one, you can mount a volume with a command similar to the following:</p>
<pre class="programlisting con"><code class="hljs-con">sudo mount /dev/sdb1 /mnt/vol1
</code></pre>
<p class="normal">In that example, I’m mounting device <code class="inlineCode">/dev/sdb1</code> to directory <code class="inlineCode">/mnt/vol1</code>.</p>
<p class="normal">Of course, you’ll need to adjust the command to reference the device you want to mount and where you want to mount it. As a reminder, if you don’t remember which devices exist on your server, you can list them with <code class="inlineCode">fdisk –l</code>.</p>
<p class="normal">Normally, the <code class="inlineCode">mount</code> command wants you to issue the <code class="inlineCode">-t</code> option with a given type. In my case, the <code class="inlineCode">mount</code> command would’ve been the following had I used the <code class="inlineCode">-t</code> option, considering my disk is formatted with <code class="inlineCode">ext4</code>:</p>
<pre class="programlisting con"><code class="hljs-con">sudo mount /dev/sdb1 -t ext4 /mnt/vol1
</code></pre>
<p class="normal">A useful trick<a id="_idIndexMarker471"/> when<a id="_idIndexMarker472"/> mounting devices is to execute the <code class="inlineCode">df –h</code> command before and after mounting.</p>
<p class="normal">While that command is generally used to check how much free space you have on various mounts, it does show you a list of mounted devices, so you can simply compare the results after mounting the device to confirm that it is present.</p>
<p class="normal">In that example, I used the <code class="inlineCode">-t</code> option along with the type of filesystem I formatted the device with. In the first example, I didn’t. This is because, in most cases, the <code class="inlineCode">mount</code> command is able to determine which type of filesystem the device uses and adjust itself accordingly. Thus, most of the time, you won’t need the <code class="inlineCode">-t</code> option. In the past, you almost always needed it, but it’s easier nowadays. The reason I bring this up is that if you ever see an error when trying to mount a filesystem that indicates an invalid filesystem type, you may have to specify this. Feel free to check the man pages for the <code class="inlineCode">mount</code> command for more information regarding the different types of options you can use.</p>
<p class="normal">When you are finished using a volume, you can unmount it with the <code class="inlineCode">umount</code> command (the missing <em class="italic">n</em> in the word <em class="italic">unmount</em> is intentional):</p>
<pre class="programlisting con"><code class="hljs-con">sudo umount /mnt/vol1
</code></pre>
<p class="normal">The <code class="inlineCode">umount</code> command, which also needs to be run as <code class="inlineCode">root</code> or with <code class="inlineCode">sudo</code>, allows you to disconnect a storage device from your filesystem. In order for this command to be successful, the volume should not be in use. If it is, you may receive a device- or resource-busy error message. If you execute <code class="inlineCode">df -h</code> after unmounting, you should see that the filesystem is missing from the output and, thus, isn’t mounted anymore.</p>
<p class="normal">The downside to manually mounting devices is that they will not automatically remount themselves the<a id="_idIndexMarker473"/> next<a id="_idIndexMarker474"/> time your server boots. In order to ensure the mount is available anytime your server boots up, you’ll need to edit the <code class="inlineCode">/etc/fstab</code> file, which I’ll walk you through in the next section.</p>
<h1 class="heading-1" id="_idParaDest-131">Understanding the /etc/fstab file</h1>
<p class="normal">The <code class="inlineCode">/etc/fstab</code> file <a id="_idIndexMarker475"/>is a very critical file on your Linux system. You can edit this file to call out additional volumes you would like to automatically mount at boot time. However, the main purpose of this file is to also mount your main filesystem, so if you make a mistake while editing it, your server will not boot (at all). Definitely be careful here.</p>
<h2 class="heading-2" id="_idParaDest-132">Analyzing the contents of /etc/fstab</h2>
<p class="normal">When your <a id="_idIndexMarker476"/>system boots, it looks at the <code class="inlineCode">/etc/fstab</code> file to determine where the root filesystem is. In addition, the location of your <code class="inlineCode">swap</code> area is read from this file and mounted at boot time as well. Your system will also read any other mount points listed in this file, one per line, and mount them. Basically, just about any kind of storage you can think of can be added to this file and automatically mounted. Even network shares from Windows servers can be added here. It won’t judge you (unless you make a typo).</p>
<p class="normal">As an example, here are the contents of <code class="inlineCode">/etc/fstab</code> on one of my machines:</p>
<figure class="mediaobject"><img alt="" height="273" src="../Images/B18425_09_08.png" width="875"/></figure>
<p class="packt_figref">Figure 9.8: Viewing the contents of the /etc/fstab file</p>
<p class="normal">When you install Ubuntu Server, the <code class="inlineCode">/etc/fstab</code> file is created for you and populated with a line for each of the partitions the installer created during installation. On the server I used to grab the example <code class="inlineCode">fstab</code> content, I have a single partition for the root filesystem, and you can also see where the swap file is mentioned.</p>
<p class="normal">Each partition is<a id="_idIndexMarker477"/> typically designated with a <strong class="keyWord">Universally Unique Identifier</strong> (<strong class="keyWord">UUID</strong>) instead <a id="_idIndexMarker478"/>of the <code class="inlineCode">/dev/sdaX</code> naming convention you might be more accustomed to if you’ve worked with storage devices in the past. In my output, you can see UUID <code class="inlineCode">dm-uuid-LVM-H8VEs7qDbMgv...</code>, which refers to my root filesystem, and you can also see that I have a <code class="inlineCode">swap</code> file located at <code class="inlineCode">/swap.img</code>.</p>
<p class="normal">The concept of a UUID has been around for a while, but there’s nothing stopping you from replacing the UUID with the actual partition names (such as <code class="inlineCode">/dev/sda1</code> or similar). If you were to do that, the server would still boot, and you probably wouldn’t notice a difference (assuming you didn’t make a typo).</p>
<p class="normal">Nowadays, UUIDs are preferred over common device names due to the fact that the names of devices can change depending on where you place them physically (such as which <a id="_idIndexMarker479"/>particular <strong class="keyWord">Serial Advanced Technology Attachment</strong> (<strong class="keyWord">SATA</strong>) port a hard disk is plugged into, which of your USB ports an external drive is connected to, and so on) or how you order them (in the case of virtual disks).</p>
<p class="normal">Add to this the fact that removable media can be inserted or removed at any time, and you have a situation where you don’t really know what name each device is going to have at any one time. For example, your external hard drive may be named <code class="inlineCode">/dev/sdb1</code> on your system now, but it may not be the next time you mount it if something else you connect claims the name of <code class="inlineCode">/dev/sdb1</code>. This is one situation in which the concept of UUIDs comes in handy. The UUID of a device will not change if you reorder your disks (but it will change if you reformat the volume). As stated in <em class="italic">Figure 9.8</em>, you can easily list the UUIDs of your volumes with the <code class="inlineCode">blkid</code> command:</p>
<pre class="programlisting con"><code class="hljs-con">blkid
</code></pre>
<p class="normal">The output will show you the UUID of each device attached to your system, and you can use this command any time you add new volumes to your server to list your UUIDs. This is also the first step in adding a new volume to your <code class="inlineCode">/etc/fstab</code> file. While I did say that using UUIDs is not required, it’s definitely recommended and can save you from trouble later on.</p>
<p class="normal">Each line of an <code class="inlineCode">fstab</code> entry is broken down into several columns, each separated by spaces or tabs. There isn’t a set number of spaces necessary to separate each column; in most cases, spaces are only used to line up each column to make them easier to read. However, at least one space is required.</p>
<p class="normal">In the first column of the example <code class="inlineCode">fstab</code> file, we have the device identifier, which can be the UUID or <a id="_idIndexMarker480"/>label of each device that differentiates it from the others. (You can add a label to a device while formatting it with the <code class="inlineCode">-L</code> argument with <code class="inlineCode">mkfs</code> commands.) In the second column, we have the location we want the device to be mounted to. In the case of the root filesystem, this is <code class="inlineCode">/</code>, which (as you know) is the beginning of the Linux filesystem. The third entry in the screenshot (for <code class="inlineCode">swap</code>) has a mount point of <code class="inlineCode">none</code>, which means that a mount point is not necessary for this device. In the third column, we have the filesystem type, the first two being <code class="inlineCode">ext4</code>, and the third having a type of <code class="inlineCode">swap</code>.</p>
<p class="normal">In the fourth column, we have a list of options for each mount separated by a comma. In this case, we only have one option for each of the example lines. With the root filesystem, we have an option of <code class="inlineCode">errors=remount-ro</code>, which tells the system to remount the filesystem as read-only if an error occurs. Such an issue is rare but will keep your system running in read-only mode as best it can if something goes wrong. The <code class="inlineCode">swap</code> partition has a single option of <code class="inlineCode">sw</code>. There are many other options that can be used here, so feel free to consult the man pages for a list. We will go over some of these options in this section.</p>
<p class="normal">The fifth and sixth columns refer to <code class="inlineCode">dump</code> and <code class="inlineCode">pass</code> respectively, which on my system are <code class="inlineCode">0</code> and <code class="inlineCode">0</code> for each line. The <code class="inlineCode">dump</code> partition is almost always <code class="inlineCode">0</code> and can be used with a backup utility to determine whether the filesystem should be backed up (<code class="inlineCode">0</code> for no, and <code class="inlineCode">1</code> for yes). In most cases, just leave this at <code class="inlineCode">0</code> since this is rarely ever used by anything nowadays. The <code class="inlineCode">pass</code> field refers to the order in which <code class="inlineCode">fsck</code> will check the filesystems. The <code class="inlineCode">fsck</code> utility scans hard disks for filesystem errors in the case of a system failure or a scheduled scan. The possible options for <code class="inlineCode">pass</code> are <code class="inlineCode">0</code>, <code class="inlineCode">1</code>, or <code class="inlineCode">2</code>. With <code class="inlineCode">0</code>, the partition is never checked with <code class="inlineCode">fsck</code>. If set to <code class="inlineCode">1</code>, the partition is checked first. Partitions with a <code class="inlineCode">pass</code> of <code class="inlineCode">2</code> are considered a second priority and checked last. As a general rule of thumb, consider using <code class="inlineCode">1</code> for your main filesystem and <code class="inlineCode">2</code> for all others. It’s not uncommon for cloud server providers to use <code class="inlineCode">0</code> for both fields. This may be because if a disk does undergo a routine check, it would take considerably longer to boot up. In a cloud environment, you can’t always wait very long to get a server up and running.</p>
<p class="normal">Now that we<a id="_idIndexMarker481"/> understand all the columns of a typical <code class="inlineCode">fstab</code> entry, we can work through adding another volume to the <code class="inlineCode">fstab</code> file.</p>
<h2 class="heading-2" id="_idParaDest-133">Adding to the /etc/fstab file</h2>
<p class="normal">To add another <a id="_idIndexMarker482"/>volume to the <code class="inlineCode">fstab</code> file, we first need to know the <code class="inlineCode">UUID</code> of the volume we would like to add (assuming it’s a hard disk or virtual disk). Again, we do that with the <code class="inlineCode">blkid</code> command:</p>
<pre class="programlisting con"><code class="hljs-con">blkid /dev/sdb1
</code></pre>
<p class="normal">Notice that I used the device name of <code class="inlineCode">/dev/sdb1</code> as an argument. This is because I want to specifically fetch the UUID of the new device we added. The output of that command will give us the UUID of that device, so it can be added to the <code class="inlineCode">/etc/fstab</code> file. Copy that down somewhere, as we’ll need it shortly. Next, we need to know where we want to mount the volume. Go ahead and create the directory now, or use an existing directory if you wish. For example, you could create the directory <code class="inlineCode">/mnt/extra_storage</code> for this purpose:</p>
<pre class="programlisting con"><code class="hljs-con">sudo mkdir /mnt/extra_storage
</code></pre>
<p class="normal">At this point, we should have all we need in order to add a new entry to <code class="inlineCode">fstab</code>. To do so, we’ll need to open the file in a text editor and then create a new line after all the others. If you don’t have a preferred editor, you can use the <code class="inlineCode">nano</code> editor:</p>
<pre class="programlisting con"><code class="hljs-con">sudo nano /etc/fstab
</code></pre>
<p class="normal">For example, the <code class="inlineCode">/etc/fstab</code> file after adding an entry for <code class="inlineCode">/dev/sdb</code> would look similar to the following:</p>
<figure class="mediaobject"><img alt="" height="514" src="../Images/B18425_09_09.png" width="877"/></figure>
<p class="packt_figref">Figure 9.9: The /etc/fstab file after adding a new entry to it</p>
<p class="normal">In my example, I created<a id="_idIndexMarker483"/> a comment line with a little note about what the extra volume will be used for (<code class="inlineCode">Extra storage</code>). It’s always a good idea to leave comments, so other administrators will have a clue regarding the purpose of the extra storage. Then, I created a new line with the UUID of the volume, the mount point for the volume, the filesystem type, <code class="inlineCode">defaults</code> option, and a <code class="inlineCode">dump</code>/<code class="inlineCode">pass</code> of <code class="inlineCode">0</code> and <code class="inlineCode">0</code>.</p>
<p class="normal">The <code class="inlineCode">defaults</code> option is one I’ve not mentioned before. By using <code class="inlineCode">defaults</code> as your mount option in <code class="inlineCode">fstab</code>, your mount will be given several useful options in one shot, without having to list them individually. Among the options included with <code class="inlineCode">defaults</code> are the following, which are worth an explanation:</p>
<ul>
<li class="bulletList"><code class="inlineCode">rw</code>: Device will be mounted read/write</li>
<li class="bulletList"><code class="inlineCode">exec</code>: Allow files within this volume to be executed as programs</li>
<li class="bulletList"><code class="inlineCode">auto</code>: Automatically mount the device at boot time</li>
<li class="bulletList"><code class="inlineCode">nouser</code>: Only <code class="inlineCode">root</code> is able to mount the filesystem</li>
<li class="bulletList"><code class="inlineCode">async</code>: Output to the device should be asynchronous</li>
</ul>
<p class="normal">Depending on your needs, the options included with defaults may or may not be ideal. Instead, you can call the options out individually, separated by commas, choosing only the ones you need. For example, with regard to <code class="inlineCode">rw</code>, you may not want users to be allowed to change the content. In fact, I strongly recommend that you use <code class="inlineCode">ro</code> (read-only) instead unless your users have a very strong use case for needing to make changes to files. I’ve actually learned this the hard way, where I’ve experienced an entire volume getting completely wiped out (and no one admitted to clearing the contents). This volume included some very important company data. From that point on, I mandated <code class="inlineCode">ro</code> being used for everything, with a separate <code class="inlineCode">rw</code> mount created, with only a select few (very responsible) people having access to it.</p>
<p class="normal">The <code class="inlineCode">exec</code> option may also not be ideal. For example, if your volume is intended for storing files and backups, you may not want scripts to be run from that location. By using the inverse<a id="_idIndexMarker484"/> of <code class="inlineCode">exec</code> (<code class="inlineCode">noexec</code>), you can prevent scripts from running to create a situation where users are able to store files on the volume but not execute programs that are stored there.</p>
<p class="normal">Another option worth explaining is <code class="inlineCode">auto</code>. The <code class="inlineCode">auto</code> option basically tells your system to automatically mount that volume whenever the system boots or when you enter the following command:</p>
<pre class="programlisting con"><code class="hljs-con">sudo mount -a
</code></pre>
<p class="normal">When executed, <code class="inlineCode">sudo mount -a</code> will mount any entry in your <code class="inlineCode">/etc/fstab</code> file that has the <code class="inlineCode">auto</code> option set. If you’ve used <code class="inlineCode">defaults</code> as an option for the mount, those will be mounted as well since <code class="inlineCode">defaults</code> implies <code class="inlineCode">auto</code>. This way, you can mount all filesystems that are supposed to be mounted without rebooting your server (this command is safe to run whenever, as it will not disrupt anything that is already mounted).</p>
<p class="normal">The opposite of the <code class="inlineCode">auto</code> option is <code class="inlineCode">noauto</code>, which can be used instead. As you can probably guess by the name, an entry in <code class="inlineCode">fstab</code> with the <code class="inlineCode">noauto</code> option will not be automatically mounted and will not be mounted when you run <code class="inlineCode">mount -a</code>. Instead, entries with this option will need to be mounted manually.</p>
<p class="normal">You may be wondering, then, what the point is of including an entry in <code class="inlineCode">/etc/fstab</code> just to use <code class="inlineCode">noauto</code>, which kind of seems to defeat the purpose. To explain this better, here is an example <code class="inlineCode">fstab</code> entry with <code class="inlineCode">noauto</code> being used:</p>
<pre class="programlisting con"><code class="hljs-con">UUID=e51bcc9e-45dd-45c7 /mnt/ext_disk ext4 rw,noauto 0 0
</code></pre>
<p class="normal">Here, let’s say that I have an external disk that I only mount when I’m performing a backup. I wouldn’t want this device mounted automatically at boot time (I may not always have it connected to the server), so I use the <code class="inlineCode">noauto</code> option. But since I do have an entry for it in <code class="inlineCode">/etc/fstab</code>, I can easily mount it any time after I connect it with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">sudo mount /mnt/ext_disk
</code></pre>
<p class="normal">Notice that I didn’t have to include the device name or options; only the destination path for the mount. The <code class="inlineCode">mount</code> command knows what device I’m referring to since I have an entry in the <code class="inlineCode">/etc/fstab</code> file for a device to be mounted at <code class="inlineCode">/mnt/ext_disk</code>. This saves me from having to type the device name and options each time I want to mount the device. So, in addition to mounting devices at boot time, the <code class="inlineCode">/etc/fstab</code> file also becomes a convenient place to declare devices that may be used on an on-demand basis but aren’t always attached.</p>
<p class="normal">One final option I would like to cover before we move on is <code class="inlineCode">users</code>. When used with a mount in <code class="inlineCode">/etc/fstab</code>, this allows regular users (users other than <code class="inlineCode">root</code>) to mount and unmount the filesystem. This way, <code class="inlineCode">root</code> or <code class="inlineCode">sudo</code> will not be necessary at all for a mount used with this option. Use this with care, but it can be useful if you have a device with non-critical data that you don’t mind your users having full control over when mounting and unmounting.</p>
<p class="normal">While the concept of a text file controlling which devices are mounted on the system may seem odd at first, I think you’ll appreciate being able to view a single file in order to find out everything that should be mounted and where it should be mounted. As long as administrators add all on-demand devices to this file, it can be a convenient place to get an overview of<a id="_idIndexMarker485"/> the filesystems that are in use on the server. As a bonus, you can also use the <code class="inlineCode">mount</code> command (with no options) to have the system provide you with a list of everything that’s mounted. Go ahead and try that, and I’ll meet you in the next section.</p>
<h1 class="heading-1" id="_idParaDest-134">Backing up and restoring volumes</h1>
<p class="normal">Since we’re<a id="_idIndexMarker486"/> dealing<a id="_idIndexMarker487"/> with servers, the data that’s being stored on our storage devices is no doubt going to be extremely important. While it’s normal to have a few test servers for use as test subjects in a typical environment, our servers usually exist to carry out a very important task. I can tell you from first-hand experience, never put too much trust in storage devices. In fact, I recommend not trusting them at all. I consider all storage to be temporary, as hard drives can and do break. If your important data is only stored on one device, it’s not safe. In this section, I’m going to discuss some very important topics around backups.</p>
<p class="normal">First, consider RAID volumes. We haven’t discussed them in this chapter because while the technology can still be beneficial, it’s not as popular as it once was. Don’t get me wrong, there’s still a place for RAID, but it’s just not as popular as it used to be. RAID allows you to join multiple disks in various configurations, which can result in a lower chance of losing data. </p>
<p class="normal">For example, RAID level 1 ensures that two hard disks always have the same data. If one of the disks physically fails, then you haven’t actually lost anything. When you replace a failed disk in RAID, it will rebuild the array with the new disk and then you’ll again benefit from having some expandability. RAID 5 allows you to have multiple disks to benefit from more space, and RAID 6 is the same but it allows you to have two disks fail before you lose data, rather than just one. Generally, that’s the difference between one level of RAID and another; how many disks are allowed to fail before it becomes a problem.</p>
<p class="normal">However, RAID suffers from some serious problems. The worst is that it’s <em class="italic">not</em> a backup solution. It doesn’t advertise itself to be that, but many administrators mistakenly assume that their data is safe when utilizing RAID. The truth is, the level of protection RAID offers you is minimal. If there’s a lightning storm and a power surge gets past your surge protector and fries a hard disk, chances are the other one will fry too. Generally, the environmental factors that cause one hard disk to fail will likely cause other disks to fail too. Worse yet, if a criminal breaks into your server room, grabs your server, and runs away with it, then the crook got away with your server <em class="italic">and</em> all the disks in your RAID anyway, so there are various scenarios where it won’t save you. RAID can definitely be good to have, but it’s more of a convenience than anything else.</p>
<p class="normal">Backups that are actually good exist off of the server somewhere else. The further away the backup is from the source server, the better. If you store your backups in a drawer outside of your server room, then that’s certainly better than leaving an external backup disk connected all the time (which can also be susceptible to power surges just the same as an internal disk). But if a terrible storm takes out your entire building, then having the backup disk stored in the same physical location will work against you.</p>
<p class="normal">It may seem as though I’m being a bit overly dramatic here. But actually, I’m not. These situations can and do happen. Successful backups are resilient, and allow you to get your servers up and running quickly. Backups of your data are on a more important level than that, as some companies can go out of business if they lose their important files, which can include things like schematics that enable the company to be in business in the first place. As a system administrator, you’ll need to develop a backup scheme that will account for as many scenarios as possible.</p>
<p class="normal">An effective backup routine will include several layers. Having an external disk is a useful backup, but why not have more than one, just in case one of them fails? Perhaps you can store one off-site, and swap the off-site and on-site backup disks weekly. In addition, perhaps you may use a command such as <code class="inlineCode">rsync</code> to copy the files on your server to a remote server in another location periodically. You may even consider cloud backup solutions, which are another great addition.</p>
<p class="normal">In this section, I can’t give you a backup scheme for your organization because the layout of <a id="_idIndexMarker488"/>your <a id="_idIndexMarker489"/>backup system will depend on the needs of the organization, which are different from one company to the next. But what I can leave you with are some tips:</p>
<ul>
<li class="bulletList">Make sure to test your backups regularly. Simply having a backup isn’t enough—they have to actually work! Try to restore data from a backup periodically to test the effectiveness.</li>
<li class="bulletList">Have at least three layers in your backup scheme, with at least one being off-site. This can be a combination of external hard disks, network-attached storage, cloud storage, mirroring data to another server in another location, or whatever makes the most sense for your organization.</li>
<li class="bulletList">Consider encryption. Although it’s beyond the scope of this chapter, if your backups fall into the wrong hands, then protected data may leak and be readable by people you don’t want to have the information.</li>
<li class="bulletList">Check the policies of your organization, and ensure that your backup scheme is compliant. Not all companies have such a scheme, but if yours does, this is critical. Consider retention (how long backups must be kept for) and how frequently backups must be updated. If you don’t have organizational policies, consult a lawyer to determine whether there are legal retention requirements for the industry of your organization.</li>
</ul>
<p class="normal">Above all else, the point is to keep your data safe. So far in this book, we’ve looked at creating additional volumes and mounting them, and we even took a quick look at <code class="inlineCode">rsync</code> earlier on. You’ve already learned some of the tools that can be made a part of a backup scheme, and you’ll learn about more methods before the book comes to a close. For now, keep these points in mind as you proceed through the book, and consider how each new <a id="_idIndexMarker490"/>skill <a id="_idIndexMarker491"/>you learn can be implemented as part of a backup scheme, if applicable.</p>
<p class="normal"><strong class="keyWord">LVM</strong> is one of my favorite technologies, giving us additional flexibility with our storage. In fact, let’s take a look at that now.</p>
<h1 class="heading-1" id="_idParaDest-135">Utilizing LVM</h1>
<p class="normal">The needs of your <a id="_idIndexMarker492"/>organization will change with time. While we as server administrators always do our best to configure resources with long-term growth in mind, budgets and changes in policy always seem to get in our way. LVM is something that I’m sure you’ll come to appreciate. In fact, technologies such as LVM are those things that make Linux the champion when it comes to scalability and cloud deployments. With LVM, you are able to resize your filesystems online, without needing to reboot your server.</p>
<p class="normal">Take the following scenario for example. Say you have an application running on a virtualized production server—a server that’s so important that downtime would cost your organization serious money. When the server was first set up, perhaps you gave the application’s storage directory a 100 GB partition, thinking it would never need more than that. Now, with your business growing, it’s not only using a lot of space, but you’re about to run out! What do you do? If the server was initially set up with LVM, you could add an additional storage volume, add it to your LVM pool, and grow your partition, all without rebooting your server! On the other hand, if you didn’t use LVM, you’re forced to find a maintenance window for your server and add more storage the old-fashioned way, which would include having it be inaccessible for a time.</p>
<p class="normal">With physical servers, you can install additional hard drives and keep them on standby without utilizing them to still gain the benefit of growing your filesystem online, even though your server isn’t virtual. In addition, if your server supports hot-plugging, you can still add additional volumes without powering the server down.</p>
<p class="normal">It’s for this reason that I must stress that you should always use LVM on storage volumes in virtual servers whenever possible. Let me repeat myself: you should <em class="italic">always</em> use LVM on storage volumes when you are setting up a virtual server! If you don’t, this will eventually catch up with you when your available space starts to run out and you find yourself working over the weekend to add new disks. </p>
<p class="normal">This process might involve manually syncing data from one disk to another and then migrating your users to the new disk. This is not a fun experience, believe me. You might not think you’ll be needing LVM right now, but<a id="_idIndexMarker493"/> you never know.</p>
<h2 class="heading-2" id="_idParaDest-136">Getting started with LVM</h2>
<p class="normal">When setting up<a id="_idIndexMarker494"/> a new server via Ubuntu’s installer, you’re given the option to use LVM during installation. But it’s much more important for your storage volumes to use LVM, and by those, I mean the volumes where your users and applications will store their data. LVM is a good choice for your Ubuntu Server’s root filesystem if you’d like the root filesystem to also benefit from the features of LVM. In order to get started with LVM, there are a few concepts that we’ll need to understand, specifically <strong class="keyWord">volume groups</strong>, <strong class="keyWord">physical volumes</strong>, and <strong class="keyWord">logical volumes</strong>.</p>
<p class="normal">A volume group is a namespace given to all the physical and logical volumes that you wish to use with that implementation of LVM. Basically, a volume group is the highest name that encompasses your entire implementation of an LVM setup. Think of it as a kind of container that is able to contain disks. An example of this might be a volume group named <code class="inlineCode">vg-accounting</code>. This volume group would be used as a location for the accounting department to store their files. It will encompass the physical volumes and logical volumes that will be in use by these users. It’s important to note that you aren’t limited to just a single volume group; you can have several, each with its own disks and volumes.</p>
<p class="normal">A physical volume is a physical or virtual hard disk that is a member of a volume group. For example, the hypothetical <code class="inlineCode">vg-accounting</code> volume group may consist of three 100 GB hard disks, and each would be considered a physical volume. Keep in mind that these disks are still referred to as physical volumes in the context of LVM, even when the disks are virtual. Basically, any block device that is owned by a volume group is a physical volume.</p>
<p class="normal">Finally, logical volumes are similar in concept to partitions. Logical volumes can take up a portion, or the whole, of a disk, but unlike standard partitions, they may also span multiple disks. For example, a logical volume can include three 100 GB disks and be configured such that you would receive a cumulative total of 300 GB. When mounted, users will be able to store files there just as they would a single partition on a standard disk. When the volume gets full, you can add an additional disk and then grow the partition to increase its size. Your users would see it as a single storage area, even though it may consist of multiple disks.</p>
<p class="normal">The volume group can be named anything you’d like, but I always give mine names that begin with <code class="inlineCode">vg-</code> and end with a name detailing its purpose. As I mentioned, you can have multiple volume groups. Therefore, you can have <code class="inlineCode">vg-accounting</code>, <code class="inlineCode">vg-sales</code>, and <code class="inlineCode">vg-techsupport</code> (and so on) all on the same server. Then, you assign physical volumes to each. For example, you can add a 500 GB disk to your server and assign it to <code class="inlineCode">vg-sales</code>. From that point on, the <code class="inlineCode">vg-sales</code> volume group owns that disk. You’re able to split up your physical volumes in any way that makes sense to you. Then, you can create logical volumes utilizing these physical volumes, which is what your users will use.</p>
<p class="normal">I think it’s always <a id="_idIndexMarker495"/>best to work through an example when it comes to learning a new concept, so I’ll walk you through such a scenario. In my case, I just created a local Ubuntu Server VM on my machine via VirtualBox, and then I added four additional 20 GB disks after I installed the distribution. </p>
<p class="normal">Virtualization is a good way to play around with learning LVM if you don’t have a server available with multiple free physical disks.</p>
<p class="normal">To get started with LVM on a server that isn’t already using it, you’ll first need to have at least one additional (unused) volume, and install the required packages, which may or may not be present on your server. To find out if the required <code class="inlineCode">lvm2</code> package is installed on your server, execute the following command:</p>
<pre class="programlisting con"><code class="hljs-con">apt search lvm2 |grep installed
</code></pre>
<p class="normal">If it’s not present (the output of the previous command doesn’t include <code class="inlineCode">[installed,automatic]</code>), the following command will install the <code class="inlineCode">lvm2</code> package and its dependencies:</p>
<pre class="programlisting con"><code class="hljs-con">sudo apt install lvm2
</code></pre>
<p class="normal">Next, we’ll need to take an inventory of the disks we have available to work with. You can list them with the <code class="inlineCode">fdisk -l</code> command as we’ve done several times now. In my case, I’ve added a few new volumes to my server, so now I have <code class="inlineCode">/dev/sdb</code>, <code class="inlineCode">/dev/sdc</code>, <code class="inlineCode">/dev/sdd</code>, and <code class="inlineCode">/dev/sde</code> to work with. The names of your disks will be different depending on your hardware or virtualization platform, so make sure to adjust all of the following commands accordingly.</p>
<p class="normal">To begin, we’ll need to configure each disk to be used with LVM, by setting up each one as a physical volume. Note that we don’t need to format a storage device, or even use <code class="inlineCode">fdisk</code> to set it up before beginning the process of setting up LVM. Formatting actually comes later in this particular process. The <code class="inlineCode">pvcreate</code> command is the first command we run to configure our disks for use with LVM. Therefore, we’ll need to run the <code class="inlineCode">pvcreate</code> command against all of the drives we wish to use for this purpose. For example, if I had four disks I wanted to use with LVM, I would run the following to set them up:</p>
<pre class="programlisting con"><code class="hljs-con">sudo pvcreate /dev/sdb
sudo pvcreate /dev/sdc
sudo pvcreate /dev/sdd
sudo pvcreate /dev/sde
</code></pre>
<p class="normal">And so on, for however many disks you plan on using.</p>
<p class="normal">To confirm that<a id="_idIndexMarker496"/> you have followed the steps correctly, you can use the <code class="inlineCode">pvdisplay</code> command as <code class="inlineCode">root</code> to display the physical volumes you have available on your server:</p>
<figure class="mediaobject"><img alt="" height="412" src="../Images/B18425_09_10.png" width="769"/></figure>
<p class="packt_figref">Figure 9.10: Output of the pvdisplay command on a sample server</p>
<p class="normal">The screenshot shows only one volume, as it had to be formatted to fit this page. The <code class="inlineCode">pvdisplay</code> command will show more output if you scroll up. Although we have some physical volumes to work with, none of them are assigned to a volume group. In fact, we haven’t even created a volume group yet. We can now create our volume group with the <code class="inlineCode">vgcreate</code> command, where we’ll give our volume group a name and assign our first disk to it:</p>
<pre class="programlisting con"><code class="hljs-con">sudo vgcreate vg-test /dev/sdb
</code></pre>
<p class="normal">Here, I’m creating a volume group named <code class="inlineCode">vg-test</code> and I’m assigning it one of the physical volumes I prepared earlier <code class="inlineCode">(/dev/sdb</code>). Now that our volume group is created, we can use the <code class="inlineCode">vgdisplay</code> command with <code class="inlineCode">sudo</code> to view details about it, including the number of<a id="_idIndexMarker497"/> assigned disks (which should now be <code class="inlineCode">1</code>):</p>
<figure class="mediaobject"><img alt="" height="585" src="../Images/B18425_09_11.png" width="762"/></figure>
<p class="packt_figref">Figure 9.11: Output of the vgdisplay command on a sample server</p>
<p class="normal">At this point, if you created four virtual disks as I have, you have three more disks left that are not part of the volume group. Don’t worry, we’ll come back to them later. Let’s forget about them for now as there are other concepts to work on at the moment.</p>
<p class="normal">All we need to do at this point is to create a logical volume and format it. Our volume group can contain all of, or a portion of, the disk we’ve assigned to it. With the following command, I’ll create a logical volume of 5 GB from the virtual disk I added to the volume group:</p>
<pre class="programlisting con"><code class="hljs-con">sudo lvcreate -n myvol1 -L 5g vg-test
</code></pre>
<p class="normal">The command may look complicated, but it’s not. In this example, I’m giving my logical volume the <a id="_idIndexMarker498"/>name <code class="inlineCode">myvol1</code> with the <code class="inlineCode">-n</code> option. Since I only want to give it 5 GB of space, I use the <code class="inlineCode">-L</code> option and then <code class="inlineCode">5g</code> to represent 5 GB. Finally, I give the name of the volume group that this logical volume will be assigned to. You can run <code class="inlineCode">lvdisplay</code> with <code class="inlineCode">sudo</code> to see information regarding this volume:</p>
<figure class="mediaobject"><img alt="" height="466" src="../Images/B18425_09_12.png" width="714"/></figure>
<p class="packt_figref">Figure 9.12: Output of the lvdisplay command on a sample server</p>
<p class="normal">At this point, we<a id="_idIndexMarker499"/> should have everything we need as far as setting up LVM is concerned. But we still need to format a volume before we can use it, similar to a non-LVM disk.</p>
<h2 class="heading-2" id="_idParaDest-137">Formatting logical volumes</h2>
<p class="normal">Next, we need <a id="_idIndexMarker500"/>to format our<a id="_idIndexMarker501"/> logical volume so that it can be used. However, as always, we need to know the name of the device so that we know what it is we’re formatting. With LVM this is easy. The <code class="inlineCode">lvdisplay</code> command gave us this already; you can see it in the output (it’s the third line down in <em class="italic">Figure 9.12</em>, under <code class="inlineCode">LV Path</code>). Let’s format it with the <code class="inlineCode">ext4</code> filesystem:</p>
<pre class="programlisting con"><code class="hljs-con">sudo mkfs.ext4 /dev/vg-test/myvol1
</code></pre>
<p class="normal">And now this device can be mounted as any other hard disk. I’ll mount mine at <code class="inlineCode">/mnt/lvm/myvol1</code>, but you can use any directory name you wish:</p>
<pre class="programlisting con"><code class="hljs-con">sudo mount /dev/vg-test/myvol1 /mnt/lvm/myvol1
</code></pre>
<p class="normal">To check our work, execute <code class="inlineCode">df -h</code> to ensure that our volume is mounted and shows the correct size. We now have an LVM configuration containing just a single disk, so this isn’t very useful. The 5 GB I’ve given it will not likely last very long, but there is some remaining space we can use that we haven’t utilized yet. With the following <code class="inlineCode">lvextend</code> command, I can resize my logical volume to take up the remainder of the physical volume:</p>
<pre class="programlisting con"><code class="hljs-con">sudo lvextend -n /dev/vg-test/myvol1 -l +100%FREE
</code></pre>
<p class="normal">In this case, <code class="inlineCode">+100%FREE</code> is the argument that clarified that we are wanting to use the entirety of the remaining space for the logical volume. If done correctly, you should see output similar to the following:</p>
<pre class="programlisting con"><code class="hljs-con">Logical volume vg-test/myvol1 successfully resized.
</code></pre>
<p class="normal">Now my logical <a id="_idIndexMarker502"/>volume is using the<a id="_idIndexMarker503"/> entire physical volume I assigned to it. Be careful, though, because if I had multiple physical volumes assigned, that command would’ve claimed all the space on those as well, giving the logical volume a size that is the total of all the space it has available, across all its disks. You may not always want to do this, but since I only had one physical volume anyway, I don’t mind. Go ahead and check your free space again with the <code class="inlineCode">df -h</code> command:</p>
<pre class="programlisting con"><code class="hljs-con">df -h
</code></pre>
<p class="normal">Unfortunately, it’s not showing the extra space we’ve given the volume. The output of <code class="inlineCode">df</code> is still showing the size the volume was before. That’s because although we have a larger logical volume, and it has all the space assigned to it, we didn’t actually resize the <code class="inlineCode">ext4</code> filesystem that resides on this logical volume. To do that, we will use the <code class="inlineCode">resize2fs</code> command:</p>
<pre class="programlisting con"><code class="hljs-con">sudo resize2fs /dev/mapper/vg--test-myvol1
</code></pre>
<p class="normal">The double-hyphen in the previous command is intentional, so make sure you’re typing the command correctly.</p>
<p class="normal">If run correctly, you should see output similar to the following:</p>
<pre class="programlisting con"><code class="hljs-con">The filesystem on /dev/mapper/vg--test-myvol1 is now 5241856 (4k) blocks long.
</code></pre>
<p class="normal">Now you should see the added space as usable when you execute <code class="inlineCode">df -h</code>. The coolest part is that we resized an entire filesystem without having to restart the server. In this scenario, if our users have got to the point where they have utilized the majority of their free space, we will be able to give them more space without disrupting their work.</p>
<p class="normal">However, you may have additional physical volumes that have yet to be assigned to a volume group. In my example, I created four and have only used one in the LVM configuration so far. We can add additional physical volumes to our volume group with the <code class="inlineCode">vgextend</code> command. In my case, I’ll run this against the three remaining drives. If you have additional physical volumes, feel free to add yours with the same commands I use, but substitute my<a id="_idIndexMarker504"/> device names <a id="_idIndexMarker505"/>with yours:</p>
<pre class="programlisting con"><code class="hljs-con">sudo vgextend vg-test /dev/sdc
sudo vgextend vg-test /dev/sdd
sudo vgextend vg-test /dev/sde
</code></pre>
<p class="normal">You should see a confirmation similar to the following:</p>
<pre class="programlisting con"><code class="hljs-con">Volume group "vg-test" successfully extended
</code></pre>
<p class="normal">When you run <code class="inlineCode">pvdisplay</code> now, you should see the additional physical volumes attached that weren’t showing there before. Now that we have extra disks in our LVM configuration, we have some additional options. </p>
<p class="normal">We could give all the extra space to our logical volume right away and extend it as we did before. However, I think it’s better to withhold some of the space from our users. That way, if our users do use up all our available space again, we have an emergency reserve of space we could use at a pinch if we needed to while we figure out the long-term solution. In addition, LVM snapshots (which we will discuss soon) require you to have unallocated space in your LVM setup.</p>
<p class="normal">The following example command will add an additional 10 GB to the logical volume:</p>
<pre class="programlisting con"><code class="hljs-con">sudo lvextend -L+10g /dev/vg-test/myvol1
</code></pre>
<p class="normal">And finally, make the free space available to the filesystem:</p>
<pre class="programlisting con"><code class="hljs-con">sudo resize2fs /dev/vg-test/myvol1
</code></pre>
<p class="normal">With very large volumes, the resizing may take some time to complete. If you don’t see the additional space right away, you may see it gradually increase every few seconds until all the new<a id="_idIndexMarker506"/> space is<a id="_idIndexMarker507"/> completely allocated.</p>
<h2 class="heading-2" id="_idParaDest-138">Removing volumes with LVM</h2>
<p class="normal">Finally, you<a id="_idIndexMarker508"/> may be curious about how to remove a logical volume or volume group. For these purposes, you would use the <code class="inlineCode">lvremove</code> or <code class="inlineCode">vgremove</code> commands. It goes without saying that these commands are destructive, but they are useful in situations where you want to delete a logical volume or volume group. To remove a logical volume, the following syntax will do the trick:</p>
<pre class="programlisting con"><code class="hljs-con">sudo lvremove vg-test/myvol1
</code></pre>
<p class="normal">Basically, all you’re doing is giving the <code class="inlineCode">lvremove</code> command the name of your volume group, a forward slash, and then the name of the logical volume within that group that you would like to remove. To remove the entire volume group, the following command and syntax should be fairly self-explanatory:</p>
<pre class="programlisting con"><code class="hljs-con">sudo vgremove vg-test
</code></pre>
<p class="normal">You can only remove a logical volume if it’s not in use, and this may not be something you’ll do very often, but if you ever do need to decommission an LVM component, then there are commands that will enable you to do so.</p>
<p class="normal">Hopefully, you’re convinced by now how awesome LVM is. It allows you flexibility over your server’s storage that other platforms can only dream of. The flexibility of LVM is one of the many reasons why Linux excels in the cloud market. These concepts can be difficult to grasp at first if you haven’t worked with LVM before. But thanks to virtualization, playing around with LVM is easy. I recommend you practice creating, modifying, and destroying volume groups and logical volumes until you get the hang of it. If the concepts aren’t clear now, they will be with practice.</p>
<p class="normal">In this section, you saw some ways in which LVM can benefit you; it allows you to take the storage of your server to the next level, even expanding it and growing it on demand. However, LVM also has additional tricks up its sleeve. It even allows you to create snapshots as well. We’ll cover this useful ability next.</p>
<h2 class="heading-2" id="_idParaDest-139">Understanding LVM snapshots</h2>
<p class="normal"><strong class="keyWord">LVM snapshots</strong> allow<a id="_idIndexMarker509"/> you to capture a logical volume at a certain point in time and preserve it. After you create a snapshot, you can mount it as you would any other logical volume and even revert your volume group to the snapshot if something fails. In practice, this is useful if you want to test some potentially risky changes to files stored within a volume, but want the insurance that if something goes wrong, you can always undo your changes and go back to how things were. LVM snapshots allow you to do just that. LVM snapshots require you to have some unallocated space in your volume group.</p>
<p class="normal">However, LVM snapshots are definitely <em class="italic">not</em> a viable form of backup. For the most part, these snapshots are best when used as a temporary holding area when running tests or testing out experimental software before rolling out changes to production systems. During Ubuntu’s installation process, you were offered the option to create an LVM configuration. Therefore, you can use snapshots to test how security updates will affect your server if you used LVM for your root filesystem. If the new updates start to cause problems, you can always revert back. When you’re done testing, you should merge or remove your snapshot.</p>
<p class="normal">So, why did I refer to LVM snapshots as a temporary solution and not a backup? First, similar to our discussion earlier, backups aren’t secure if they are stored on the same server that’s being backed up. It’s always important to save backups of the server at least, preferably off-site. But what’s worse is that if your snapshot starts to use up all available space in your volume group, it can get corrupted and stop working. Therefore, this is a feature you would use with caution, just as a means of testing something, and then revert back or delete the snapshot when you’re done experimenting. Don’t leave an LVM snapshot hanging around for too long.</p>
<p class="normal">When you create a snapshot with LVM, what happens is a new logical volume is created that is a clone of the original. Initially, no space is consumed by this snapshot. But as you run your server and manipulate files in your volume group, the original blocks are copied to the snapshot as you change them, to preserve the original logical volume. If you don’t keep an eye on usage, you may lose data if you aren’t careful and the logical volume will fill up.</p>
<p class="normal">To show this in an example, the following command will create a snapshot (called <code class="inlineCode">mysnapshot</code>) of the <code class="inlineCode">myvol1</code> logical volume:</p>
<pre class="programlisting con"><code class="hljs-con">sudo lvcreate -s -n mysnapshot -L 4g vg-test/myvol1
</code></pre>
<p class="normal">You should see the following output:</p>
<pre class="programlisting con"><code class="hljs-con">Logical volume "mysnapshot" created.
</code></pre>
<p class="normal">With that example, we’re using the <code class="inlineCode">lvcreate</code> command, with the <code class="inlineCode">-s</code> option (snapshot) and the <code class="inlineCode">-n</code> option (which allows us to name the snapshot), where we declare a name of <code class="inlineCode">mysnapshot</code>. We’re also using the <code class="inlineCode">-L</code> option to designate a maximum size for the snapshot, which I set to 4 GB in this case. Finally, I give it the volume group and logical volume name, separated by a forward slash (<code class="inlineCode">/</code>). From here, we can use the <code class="inlineCode">lvs</code> command to monitor its size.</p>
<p class="normal">Since we’re creating a new logical volume when we create a snapshot, we can mount it as we would a normal logical volume. This is extremely useful if we want to pull a single file without having to restore the entire thing.</p>
<p class="normal">But what about restoring the snapshot? One of the major benefits of snapshots is the ability to “roll back” to when the snapshot was taken. Essentially, this allows you to test changes to the<a id="_idIndexMarker510"/> server and then undo those changes. To roll back to a snapshot, we can do so with the <code class="inlineCode">lvconvert</code> command:</p>
<pre class="programlisting con"><code class="hljs-con">sudo lvconvert --merge vg-test/mysnapshot
</code></pre>
<p class="normal">The output will look similar to the following:</p>
<pre class="programlisting con"><code class="hljs-con">Merging of volume mysnapshot started.
myvol1: Merged: 100.0%
</code></pre>
<p class="normal">However, it’s important to note, that, unlike being able to resize a logical volume online, we cannot merge a snapshot while it is in use. If you do, the changes will take effect the next time it is mounted. Therefore, you can either unmount the logical volume before merging or unmount and remount after merging. Afterward, you’ll see that the snapshot is removed the next time you run the <code class="inlineCode">lvs</code> command.</p>
<p class="normal">Since you cannot merge (roll back) a snapshot that is in use, if the snapshot is of the root filesystem, you’ll have to reboot the server for the rollback to finalize.</p>
<p class="normal">If you’d like to make the snapshot permanent, which finalizes all of the changes you’ve made since the snapshot was first taken, we can use the <code class="inlineCode">lvremove</code> command. For our example snapshot in this section, we can use the following command to make the snapshot permanent:</p>
<pre class="programlisting con"><code class="hljs-con">sudo lvremove vg-test/mysnapshot
</code></pre>
<p class="normal">As you can probably conclude based on the name of the command, <code class="inlineCode">lvremove</code> deletes the snapshot. The act of deleting the snapshot is actually what makes its changes final, while the <code class="inlineCode">lvconvert</code> command mentioned earlier rolls back to when the snapshot was taken.</p>
<p class="normal">LVM snapshots are definitely a useful feature, even if it’s not supposed to be considered a backup solution. My favorite use case for these snapshots is to take a snapshot of the root filesystem before installing all available updates. After I reboot, and the updates take effect, I can either delete the snapshot (if everything seems to be fine) or revert back to the<a id="_idIndexMarker511"/> snapshot if the updates seem to be causing a problem. If nothing else, LVM snapshots are yet another trick you can use when and if the need for it comes up.</p>
<h1 class="heading-1" id="_idParaDest-140">Summary</h1>
<p class="normal">Efficiently managing the storage of your servers will ensure that things continue to run smoothly, as a full filesystem can definitely cause your server to grind to a halt. Thankfully, Linux servers feature a very expansive toolset for managing your storage, some of which are a source of envy for other platforms. As Linux server administrators, we benefit from technologies such as LVM and utilities such as <code class="inlineCode">ncdu</code>, as well as many others. In this chapter, we explored these tools and how to manage our storage. We covered how to format, partition, mount, and unmount volumes, as well as manage the <code class="inlineCode">fstab</code> file, LVM, monitor disk usage, and more.</p>
<p class="normal">In the next episode of our Ubuntu Server saga, we’ll work through connecting to networks. We’ll configure our server’s hostname, work through examples of connecting to other servers via OpenSSH, and take a look at IP addressing.</p>
<h1 class="heading-1" id="_idParaDest-141">Relevant videos</h1>
<ul>
<li class="bulletList">Linux Crash Course – fstab: <a href="https://linux.video/fstab"><span class="url">https://linux.video/fstab</span></a></li>
<li class="bulletList">Linux EssentialsCrash Course – Formatting and mounting storage volumes: <a href="https://linux.video/format-mount"><span class="url">https://linux.video/format-mount</span></a></li>
<li class="bulletList">LVM deep dive: <a href="https://linux.video/lvm"><span class="url">https://linux.video/lvm</span></a></li>
</ul>
<h1 class="heading-1" id="_idParaDest-142">Further reading</h1>
<ul>
<li class="bulletList">Ubuntu LVM documentation: <a href="https://learnlinux.link/u-lvm"><span class="url">https://learnlinux.link/u-lvm</span></a></li>
</ul>
<h1 class="heading-1">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the author and other readers: </p>
<p class="normal"><a href="https://packt.link/LWaZ0"><span class="url">https://packt.link/LWaZ0</span></a></p>
<p class="normal"><img alt="" height="177" src="../Images/QR_Code50046724-1955875156.png" width="177"/></p>
</div>
</div></body></html>