- en: '*Chapter 3*: System Storage Best Practices'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第3章*：系统存储最佳实践'
- en: Probably the most complicated and least understood components of **System Administration**
    involve the area of **storage**. Storage tends to be poorly covered, rarely taught,
    and often treated as myth rather than science. Storage also involves the most
    fear because it is in storage that our mistakes risk losing data, and nothing
    tends to be a bigger failure than data loss.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**系统管理**中可能最复杂且最难理解的部分涉及**存储**领域。存储往往被忽视，极少教授，且常常被当作神话看待而非科学。存储也是最令人恐惧的领域，因为在存储中，我们的错误可能导致数据丢失，而没有什么比数据丢失更严重的失败了。'
- en: Storage decisions impact performance, capacity, longevity, and most importantly,
    *durability*. Storage is where we have the smallest margin of error as well as
    where we can make the biggest impact. In other areas of planning and design we
    often get the benefit of quite a bit of *fudge factor*, mistakes are often graceful
    such as a system that is not quite as fast as it needs to be or is somewhat more
    costly than necessary, but in storage overbuilding might double total costs and
    mistakes will quite easily result in non-functional systems. Failure tends to
    be anything but graceful.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 存储决策影响性能、容量、寿命，最重要的是*耐用性*。存储是我们误差最小的领域，也是我们能产生最大影响的地方。在其他规划和设计领域，我们通常能够获得相当大的*容错空间*，错误往往表现得比较优雅，比如系统没有达到预期的速度，或者成本略高于必要，但在存储领域，过度建设可能会使总成本翻倍，错误也很容易导致系统无法正常运行。失败通常远非优雅。
- en: We are going to address how we look at and understand storage in Linux systems
    and demystify storage so that you can approach it methodically and empirically.
    By the end of this chapter, you should be prepared to decide on the best storage
    products and designs for your workload taking into account all of the needs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论如何在Linux系统中查看和理解存储，并揭开存储的神秘面纱，让你能够以系统化和经验主义的方法进行操作。在本章结束时，你应该能够根据工作负载的所有需求，决定最适合的存储产品和设计。
- en: 'In this chapter we will look at the following key topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下关键主题：
- en: Exploring key factors in storage
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探讨存储中的关键因素
- en: 'Understanding block storage: Local and SAN'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解块存储：本地和SAN
- en: Surveying filesystems and network filesystems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调查文件系统和网络文件系统
- en: Getting to know **logical volume management** (**LVM**)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解**逻辑卷管理**（**LVM**）
- en: Utilizing RAID and RAIN
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用RAID和RAIN
- en: Learning about replicated local storage
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解本地存储的复制
- en: Analyzing storage architectures and risk
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析存储架构和风险
- en: Exploring key factors in storage
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探讨存储中的关键因素
- en: When thinking about storage for systems administration we are concerned with
    **cost**, **durability**, **availability**, **performance**, **scalability**,
    **accessibility**, and **capacity**. It is easy to get overwhelmed with so many
    moving parts when it comes to storage and that makes it a risk that we may lose
    track of what it is that we want to accomplish. In every storage decision, we
    need to remain focused on these factors. Most importantly, on all of these factors.
    It is extremely tempting to focus on just a few causing us to lose our grasp of
    the complete picture.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑系统管理中的存储时，我们关注的是**成本**、**耐用性**、**可用性**、**性能**、**可扩展性**、**可访问性**和**容量**。在面对如此多的因素时，存储会让人感到不知所措，这也让它成为一个风险点，我们可能会忽视我们真正想要实现的目标。在每个存储决策中，我们需要始终关注这些因素。最重要的是，关注所有这些因素。我们很容易只关注其中几个，从而失去对整个图景的把握。
- en: In most cases, if you study postmortems of storage systems that have failed
    to meet business needs, you will almost always find that one or more of these
    factors was forgotten during the design phase. It is very tempting to become focused
    on one or two key factors and ignore the others, but we really have to maintain
    a focus on all of them to ensure storage success.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，如果你研究那些未能满足业务需求的存储系统的事后分析，你几乎总是会发现，设计阶段忘记了一个或多个这些因素。很容易只专注于一两个关键因素，而忽视其他因素，但我们必须始终关注所有因素，才能确保存储的成功。
- en: We should begin by breaking down each factor individually.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该从单独分析每个因素开始。
- en: Cost
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本
- en: It might seem that no one could forget about cost as a factor in storage but
    believe me it happens, and it happens often. As a rule, IT is a business function
    and all businesses, by definition, are about making money, and the cost of providing
    an infrastructure need always must factor in profits. So, because of this, no
    decision in IT (or anywhere in a business) should happen without cost as a consideration.
    We should never allow cost to be forgotten, or just as bad allowing someone to
    state that *cost is no object* because that can never be true and makes absolutely
    no sense. Cost may not be the primary concern, and the budgetary limits may be
    flexibility, but cost always matters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你会觉得，存储的成本不可能被忽视，但相信我，这种情况确实存在，并且发生得很频繁。通常，IT是一个业务职能，所有的企业本质上都是为了赚钱，提供基础设施的成本必须始终考虑到利润。因此，基于这一点，IT中的任何决策（或任何企业中的决策）都不应该在没有考虑成本的情况下做出。我们绝不应该让成本被遗忘，或者更糟的是，允许有人说*成本不是问题*，因为那永远不可能成立，完全没有意义。成本可能不是最主要的关注点，预算限制可能具有一定灵活性，但成本始终是重要的。
- en: Storage is generally one of the costliest components of a production system
    so we tend to see costs be more sensitive when dealing with storage than when
    dealing with other parts of physical system design such as CPU and RAM. Storage
    is also often easiest to solve by simply throwing more money at it and so many
    people when planning for hardware err on the side of overbuilding because it is
    easy. Of course, we can always do this and as long as we understand enough of
    our storage needs and how storage works it will *work* outside of being overly
    expensive. But, of course, it is difficult to be effective system administrators
    if we are not cost effective - the two things go together.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 存储通常是生产系统中最昂贵的组成部分之一，因此，在处理存储时，我们往往比处理其他物理系统设计部分（如CPU和内存）时更加敏感成本。存储也通常是通过简单地投入更多资金来解决的，因此许多人在硬件规划时，倾向于过度建设，因为这比较容易。当然，我们可以始终这样做，只要我们足够了解自己的存储需求以及存储的工作原理，它就能*正常工作*，但不会过于昂贵。不过，当然，如果我们不具备成本效益，作为系统管理员就很难有效工作——这两者是相辅相成的。
- en: Durability
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 耐用性
- en: 'Nothing is more important when it comes to storage than durability: the ability
    of the storage mechanism to resist data loss. Durability is one of two aspects
    of reliability. For most workloads and most system scenarios, durability is what
    trumps all else. It is very rare that we want to store something that we cannot
    reliably retrieve even if that retrieval is slow, delayed, or expensive. Concepts
    such as data availability or performance mean nothing if the data is lost.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储方面，没有什么比耐用性更重要了：即存储机制抵抗数据丢失的能力。耐用性是可靠性的两个方面之一。对于大多数工作负载和大多数系统场景，耐用性是最为关键的。我们很少需要存储那些即使检索缓慢、延迟或昂贵，也无法可靠地恢复的数据。诸如数据可用性或性能等概念在数据丢失的情况下毫无意义。
- en: Durability also refers to data that resists corruption or decay. In storage
    we have to worry about the potential of a portion of our data set losing integrity
    which may or may not be something that we can detect. Just because we can retrieve
    data alone does not tell us that the data that we are retrieving is exactly what
    it is supposed to be. Data corruption can mean a file that we can no longer read,
    a database that we can no longer access, an operating system that no longer boots,
    or worse, it can even mean a number in an accounting application changing to a
    different, but valid, number which is all but impossible to detect.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 耐用性还指的是数据能够抵抗损坏或衰退。在存储中，我们必须担心数据集的某些部分可能会失去完整性，而这可能是我们能够或无法检测到的。仅仅因为我们能检索到数据，并不能告诉我们我们所检索的数据是否完全符合预期。数据损坏可能意味着文件无法读取，数据库无法访问，操作系统无法启动，甚至更糟糕，它可能意味着会计应用中的一个数字变成了另一个不同但有效的数字，而这种变化几乎不可能被检测到。
- en: Availability
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可用性
- en: Traditionally, we thought about data reliability mostly in terms of how available
    our data was when it came time to retrieve it. Availability is often referred
    to as *uptime* and if your storage is not available, neither is your workload.
    So, while availability generally takes a back seat to durability, it is still
    extremely important and one of the two key aspects of overall storage reliability.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，我们主要从数据可用性的角度考虑数据的可靠性，尤其是在检索数据时。可用性通常被称为*正常运行时间*，如果存储不可用，那么你的工作负载也无法使用。因此，尽管可用性通常会让位于耐用性，但它仍然极为重要，是整体存储可靠性的两个关键方面之一。
- en: There are times when availability and performance become intertwined. There
    can be situations where storage performance drops so significantly that data becomes
    effectively unavailable. Consider a shower that just drips every few seconds,
    technically there is still water, but it is not coming through the pipes fast
    enough to be able to use it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有时可用性和性能会交织在一起。有些情况下，存储性能会显著下降，导致数据实际上变得无法使用。可以想象一下淋浴水滴每隔几秒才滴落一次，从技术上讲，水仍然存在，但它流经管道的速度太慢，无法有效使用。
- en: We will be talking about RAID in depth in just a little bit, but availability
    and performance are good real-world examples. A famous situation can arise with
    large RAID 6 arrays when a drive or two have failed and have been replaced and
    the array is online and in the process of actively rebuilding (a process by which
    missing data is recalculated from metadata.) It is quite common for the RAID system
    to be overwhelmed due to the amount of data being processed and written that the
    resulting array, while technically online and available, is so slow that it cannot
    be used in any meaningful way and operating systems or applications attempting
    to use it will not just be useless from the extreme slowness but may even error
    out reporting that the storage is offline due to the overly long response times.
    *Available* can become a murky concept if we are not careful.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会深入讨论RAID，但可用性和性能是很好的现实例子。一个著名的情况是，当一个或两个硬盘出现故障并被替换后，RAID 6阵列仍然在线，并且正在进行主动重建（即通过元数据重新计算丢失的数据的过程）。RAID系统常常因处理和写入的数据量过大而不堪重负，虽然阵列从技术上讲是在线且可用的，但其速度非常慢，以至于无法以任何有意义的方式使用，操作系统或应用程序试图使用时，由于极端的慢速，不仅无用，甚至可能因响应时间过长而错误报告存储设备已离线。*可用性*可能变得模糊不清，如果我们不小心的话。
- en: Performance
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: When it comes to computers in the twenty first century storage is almost always
    the most significant performance bottleneck in our systems. CPU and RAM almost
    always have to wait on storage rather than the other way around. Modern storage
    using solid state technologies has done much to close the performance gap between
    storage systems and other components, but the gap remains rather large.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于21世纪的计算机来说，存储几乎总是我们系统中最重要的性能瓶颈。CPU和RAM几乎总是需要等待存储，而不是反过来。现代采用固态技术的存储设备在缩小存储系统与其他组件之间的性能差距方面做出了很大贡献，但这一差距依然相当大。
- en: Performance can be difficult to measure as there are many ways of looking at
    it, and different types of storage media tend to have very different performance
    characteristics. There are concepts such as latency (time before data retrieval
    begins), throughput (also known as *bandwidth*, measuring the rate at which data
    can be streamed), and input/output operations per second or IOPS (the number of
    storage related activities that can be performed in each amount of time.) Most
    people think of storage only in terms of throughput, but traditionally IOPS have
    been the most useful measurement of performance for most workloads.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 性能可能很难衡量，因为有许多不同的视角，而不同类型的存储介质通常具有截然不同的性能特性。比如延迟（数据检索开始前的时间）、吞吐量（也称为*带宽*，衡量数据流的速度）以及每秒输入输出操作数（IOPS，表示在特定时间内可以执行的存储相关活动次数）。大多数人仅从吞吐量的角度考虑存储，但传统上，IOPS是大多数工作负载下衡量性能的最有用指标。
- en: 'It is always tempting to reduce factors to something simple to understand and
    compare. But if we think about cars, we could compare three vehicles: one with
    a fast acceleration but a low top speed, one with slow acceleration and a high-top
    speed, and a tractor trailer that is slow to accelerate and has a low top speed
    but can haul a lot of stuff at once. The first car would shine if we only cared
    about latency: the time for the first packet to arrive. The second car would shine
    if we cared about how quickly a small workload could be taken from place to place.
    This is most like measuring IOPS. The tractor trailer will be unbeatable if our
    concern is how much total data can be hauled between systems over the duration
    of the system. That''s our throughput or bandwidth. With cars, most people think
    of a *fast car* as the one with the best top speed, but with storage most people
    think about the tractor trailer example as what they want to measure, but not
    what *feels* fast when they use it. In reality, performance is a matter of perspective.
    Different workloads perceive performance differently.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 总是很容易将因素简化为一些易于理解和比较的内容。但是如果我们拿汽车做比较，我们可以将三种车辆进行对比：一种加速快但最高速度低，一种加速慢但最高速度高，另一种是拖车，虽然加速慢且最高速度低，但一次可以搬运大量物品。如果我们只关心延迟：即第一个数据包到达的时间，那么第一辆车会脱颖而出。如果我们关心如何快速地将一个小的工作负载从一个地方移动到另一个地方，第二辆车则表现得最好，这最像是在测量
    IOPS。拖车则在我们关心的是在系统之间传输的数据量时无可匹敌，这就是我们的吞吐量或带宽。对于汽车，大多数人认为*快车*是指最高速度最快的车，但在存储领域，大多数人认为拖车的例子是他们想要衡量的目标，而不是*在使用时感觉*快的东西。实际上，性能是一个视角问题，不同的工作负载对性能的感知是不同的。
- en: For example, a backup deals in steady, linear data and will benefit most from
    storage systems designed around throughput. Therefore, tape works so well for
    backup performance and why old optical media such as CD and DVD were acceptable.
    But other workloads, like databases, depend heavily on IOPS and low latency and
    benefit little from total throughput and so really benefit from solid state storage.
    Other workloads like file servers often need a blend of performance and work just
    fine with spinning hard drives. You have to know your workload in order to design
    a proper storage system to support it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，备份涉及到稳定、线性的数据显示，并且最受益于以吞吐量为设计核心的存储系统。因此，磁带在备份性能方面表现优异，也解释了为什么旧的光学介质如 CD 和
    DVD 曾经可以接受。但其他工作负载，比如数据库，则非常依赖于 IOPS 和低延迟，并且很少受益于总体吞吐量，因此更适合使用固态存储。其他工作负载，如文件服务器，通常需要性能和容量的平衡，使用旋转硬盘就可以很好地满足需求。你必须了解你的工作负载，才能设计出合适的存储系统来支持它。
- en: Performance is even more complex when we start thinking in terms of burstable
    versus sustainable rates. There is just a lot to consider, and you cannot short
    circuit this process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始考虑突发性与持续性速率时，性能变得更加复杂。需要考虑的因素非常多，而且你无法绕过这个过程。
- en: Scalability
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展性
- en: A typical physical system deployment is expected to see four to eight years
    in production today and it is not uncommon to hear of systems staying in use far
    longer. Spend any amount of time working in IT and you are likely to encounter
    systems still powered on and completely critical to a company's success that have
    been in continuous use for twenty years or even more! Because a storage system
    is expected to have such a long lifespan, we have to consider how that system
    might be able to grow or change over that potential time period.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的物理系统部署如今预计在生产中使用四到八年，且听说有些系统的使用寿命远超这个范围也并不罕见。只要在 IT 行业工作一段时间，你很可能会遇到一些仍在运行并对公司成功至关重要的系统，这些系统已经连续使用了二十年甚至更久！由于存储系统预期拥有如此长的使用寿命，我们必须考虑该系统在这一潜在时间段内如何能够增长或变化。
- en: Most workloads experience capacity growth needs over time and a storage design
    that can expand capacity as needed can be beneficial both for just protecting
    against the unknown but also by allowing us to invest minimally up front and spending
    more only *if* and *when* additional capacity becomes needed. Some storage systems
    may also be able to scale in terms of performance as well. This is less common
    and less commonly considered critical, yet even if a workload only increases capacity
    needs and not performance needs *per se*, larger capacity alone can warrant a
    need for increases performance just to handle tasks such as backups since large
    capacities mean larger time to backup and restore.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数工作负载随着时间的推移会出现容量增长需求，设计一个可以根据需要扩展容量的存储系统，对于应对未知的挑战非常有益，同时也能让我们在前期投资较少，仅在*需要*和*当*额外容量变得必要时才增加支出。一些存储系统也可能在性能方面进行扩展。这种情况较少见，也较少被认为是关键需求，但即便是一个工作负载仅仅增加了容量需求，而不是性能需求*本身*，仅仅是更大的容量也可能要求提升性能，尤其是处理备份等任务时，因为大容量意味着更长的备份和恢复时间。
- en: In theory, you could also have a situation where the needs for reliability (durability,
    availability, or both) may need to increase over time. This, too, can be possible,
    but is likely to be much more complex.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，你也可能遇到一种情况，其中对可靠性的需求（持久性、可用性或两者）可能随着时间的推移而增加。这种情况是可能的，但往往会更加复杂。
- en: Storage is an area in which flexibility to adjust configuration over time is
    often the hardest, but also the most important. We cannot always foresee what
    future needs will be. We need to plan our best to allow for flexibility to adjust
    whenever possible.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 存储是一个在灵活调整配置方面通常最难、但也是最重要的领域。我们无法总是预见到未来的需求。我们需要尽力规划，以便在可能的情况下留有调整的灵活性。
- en: Capacity
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容量
- en: Finally, we look at capacity, the amount of data that can be storage on a system.
    Capacity might seem straightforward, but it can be confusing. Even in simple disk-based
    arrays we have to think in terms of raw capacity (the sum of the capacities of
    all devices) and in terms of the resultant capacity (the usable capacity of the
    system that can be accessed for storage purposes. Many storage systems have redundancies
    to provide for reliability and performance and this comes at the cost of consumed
    raw capacity. So, we have to be aware of how our configuration of our storage
    will affect the final outcome. Storage admins will talk in terms of both raw and
    usable capacity.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来看看容量，系统中可以存储的数据量。容量看似简单，但有时也会令人困惑。即使是简单的磁盘阵列，我们也必须考虑原始容量（所有设备容量的总和）和结果容量（可用于存储目的的可用容量）。许多存储系统有冗余设计，以提供可靠性和性能，而这会消耗原始容量。因此，我们必须了解存储配置如何影响最终结果。存储管理员会同时讨论原始容量和可用容量。
- en: Now that we have a good handle on the aspects of storage that we need to keep
    in mind we can dive into learning more about how storage components are put together
    to build **enterprise storage subsystems**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了存储方面的关键要素，接下来我们可以深入学习存储组件如何组合成**企业存储子系统**。
- en: 'Understanding block storage: Local and SAN'
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解块存储：本地存储和SAN
- en: At the root of any standard storage mechanism that we will encounter today is
    the concept of **block devices**. Block devices are storage devices that allow
    for non-volatile data storage that can be stored and retrieved in arbitrary order.
    In a practical sense, think of the *standard* block device as being the hard drive.
    Hard drives are the prototypical block device, and we can think of any other block
    device as behaving like a hard drive. We can also refer to this as implementing
    a drive interface or *appearance*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天会遇到的任何标准存储机制的根本概念是**块设备**。块设备是允许存储非易失性数据并可以按任意顺序存储和检索的存储设备。从实际意义上讲，*标准*块设备可以理解为硬盘。硬盘是典型的块设备，我们可以将任何其他块设备视为像硬盘一样工作。我们也可以将其称为实现驱动接口或*外观*。
- en: 'Many things are block devices. Traditional spinning hard drives, solid state
    drives (SSD), floppy disks, CD-ROM, DVD-ROM, tape drives, RAM disks, RAID arrays
    and more are all block devices. As far as a computer is concerned, all of these
    devices are the same. This makes things simple as a system administrator: everything
    is built on block devices.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 许多设备都是块设备。传统的旋转硬盘、固态硬盘（SSD）、软盘、光盘、DVD-ROM、磁带驱动器、RAM磁盘、RAID阵列等都是块设备。从计算机的角度来看，所有这些设备都是一样的。这使得作为系统管理员的工作变得简单：一切都建立在块设备之上。
- en: From a system administrator perspective, we often simple refer to block devices
    as *disks* because from the perspective of the operating system we cannot tell
    much about the devices and only know that we are getting block storage. That block
    storage might be a physical disk, a logical device built on top of multiple disks,
    an abstraction built on top of memory, a tape drive, or remote system, you name
    it. We cannot really tell. To us it is just a block device and since block devices
    generally represent disks, we call them disks. It is not necessarily accurate,
    but it is useful.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统管理员的角度来看，我们通常简单地将块设备称为*磁盘*，因为从操作系统的角度，我们无法准确识别设备，只知道我们正在使用块存储。这种块存储可能是物理磁盘，是建立在多个磁盘之上的逻辑设备，或者是建立在内存之上的抽象，磁带驱动器，或者远程系统，你可以随意猜测。我们其实无法真正分辨出来。对我们来说，它只是一个块设备，而由于块设备通常代表磁盘，我们称它们为磁盘。这虽然不一定准确，但却很有用。
- en: Locally attached block storage
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地附加块存储
- en: The simplest type of block storage devices is those that are physically attached
    to our system. We are familiar with this in the form of standard internal hard
    drives, for example. Local block devices commonly attach by way of SAS, SATA,
    and NVMe connections today. In the recent past, **Parallel SCSI** (just called
    **SCSI** at the time), and **Parallel ATA** (aka **PATA**) just called **ATA**
    or **IDE** at the time, were standards. All of these technologies, as well as
    some more obscure, allow physical block devices to attach directly to a computer
    system.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的块存储设备类型是那些物理附加到我们系统的设备。比如，我们对标准的内部硬盘就比较熟悉。如今，本地块设备通常通过SAS、SATA和NVMe连接。近年来，**并行SCSI**（当时仅称为**SCSI**）和**并行ATA**（即**PATA**，当时仅称为**ATA**或**IDE**）是标准。这些技术，以及一些较为冷门的技术，允许物理块设备直接附加到计算机系统上。
- en: It is locally attached storage that we will work with most of the time. And
    all block devices have to be locally attached somewhere in order to be used. So
    this technology is always relevant.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种本地附加存储，我们大部分时间都会使用它。所有块设备都必须以某种方式本地附加才能被使用。所以这种技术始终具有相关性。
- en: 'Locally attached block devices come with a lot of inherent advantages over
    alternatives. Being locally attached there is a natural performance and reliability
    advantage: the system is as simple as it gets and that means that there is less
    to go wrong. All other things being equal, simple trumps complex. Storage is a
    great example of this. Fewer moving parts and shorter connection paths means we
    get the lowest possible latency, highest possible throughput, and highest reliability
    at the lowest cost!'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本地附加的块设备相较于其他替代方案有很多固有的优势。由于本地附加，它具有天然的性能和可靠性优势：系统尽可能简单，这意味着出错的机会较少。其他条件相同的情况下，简单的总是优于复杂的。存储就是一个很好的例子。更少的运动部件和更短的连接路径意味着我们能够获得最低的延迟、最高的吞吐量和最高的可靠性，同时成本最低！
- en: Of course, locally attached storage comes with caveats or else no one would
    even make another option. The negative of locally attached storage is flexibility.
    There are simply some scenarios that locally attached storage cannot accommodate
    and so we must sometimes opt for alternative approaches.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，本地附加存储也有一些限制，否则没人会选择其他方案。本地附加存储的缺点是灵活性。确实有一些场景是本地附加存储无法满足的，因此我们有时必须选择其他替代方案。
- en: Storage Area Networks (SAN)
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储区域网络（SAN）
- en: The logical alternative to a locally attached device is a remotely attached
    device and while one would think that we would simply refer to these types of
    block devices in this manner, we do not. A remote attached device uses a network
    protocol to implement the concept of *remoteness* into the storage and the network
    on which a remote device is communicating is called a **Storage Area Network**
    and because of this, common vernacular simply refers to all remote block storage
    as being a **SAN**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本地附加设备的逻辑替代方案是远程附加设备，虽然我们可能会认为我们会简单地以这种方式称呼这些类型的块设备，但实际上并不是这样。远程附加设备使用网络协议将*远程性*的概念引入存储，而远程设备所使用的网络被称为**存储区域网络**，因此，常见的术语通常将所有远程块存储称为**SAN**。
- en: The terrible terminology of SAN
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SAN的可怕术语
- en: Technically speaking, a Storage Area Network should be a reference to a dedicated
    network that is used to carry block device traffic and in very technical circles
    this is how the term is used. Devices on a SAN can be direct block devices, disk
    arrays, and other similar *block over network* devices. The SAN is the network,
    not a *thing* that you can buy.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度讲，存储区域网络（SAN）应该是指一个专用网络，用于传输块设备流量，在非常专业的圈子里，这个术语是这样使用的。SAN上的设备可以是直接的块设备、磁盘阵列以及其他类似的*网络上的块设备*。SAN是网络，而不是一个你可以买到的*东西*。
- en: In the common parlance, however, it is standard to refer to any device that
    provides storage, implements a block device interface, and connects to a network
    rather than directly to a computer as a SAN. You hear this every day in phrases
    like *did you buy a SAN?*, *we need a SAN engineer*, *I spoke to our SAN vendor*,
    *should we upgrade the SAN?*, and *where is our SAN?* Go to your nearest IT hardware
    vendor and ask them to sell you a SAN and they will without hesitation, the terminology
    is so standard that dollars to donuts says that they will be completely confused
    if you try to act like a SAN is anything but a hardware device into which you
    place hard drives and is connected to a network via some sort of cable.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在日常用语中，通常将任何提供存储、实现块设备接口并连接到网络而非直接连接到计算机的设备称为SAN。你每天都会听到这样的表达，比如*你买了SAN吗？*、*我们需要一个SAN工程师*、*我和我们的SAN供应商谈过了*、*我们应该升级SAN吗？*以及*我们的SAN在哪里？*去最近的IT硬件供应商那里，问他们卖不卖SAN，他们会毫不犹豫地卖给你，这个术语已经如此标准，以至于可以打赌，如果你试图把SAN当成不是硬件设备的东西，他们会完全困惑，而这种设备是你放入硬盘并通过某种电缆连接到网络的设备。
- en: Because storage is complex, confusing, and scary and because storage area networks
    add additional layers of complexity on top of the basics this entire arena became
    treated as black boxes full of magic and terminology quickly deteriorated and
    most beliefs around SAN became based on misconceptions and myth. Common myths
    include impossible ideas such that SANs cannot fail, that SANs are faster than
    the same technology without the networking layer, that SANs are a requirement
    of other technologies, and others.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存储既复杂又令人困惑，而且令人害怕，存储区域网络又在基础之上增加了额外的复杂性，这整个领域迅速被视为充满魔法和术语的黑匣子，术语也迅速恶化，大多数关于SAN的认知都基于误解和神话。常见的误区包括一些不可能的想法，比如SAN不可能出现故障、SAN比没有网络层的相同技术更快、SAN是其他技术的必需品等等。
- en: We can only be effective system administrators if we understand how the technology
    works and avoid giving in to myths (and marketing.) For example, we cannot make
    meaningful risk analysis or performance decisions if we believe that a device
    is magic and do not consider its actual risk profile.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当我们理解技术如何工作并避免陷入神话（和市场营销）时，才能成为有效的系统管理员。例如，如果我们认为某个设备是魔法般的，而不考虑其实际的风险状况，我们就无法做出有意义的风险分析或性能决策。
- en: In theory, a SAN is an extremely simple concept. We take any block device whether
    it is a physical device like an actual hard drive, or some more complicated concept
    like an array of drives, and encapsulate the standard block device protocol (such
    as SCSI or ATA) and send that over a network protocol (such as TCP/IP, Ethernet,
    or *FiberChannel*.) The network protocol acts as a simple tunnel, in a way, to
    get the block protocol over a long distance. That's all that there is to it. At
    the end of the day, it is still just a SCSI or ATA based device, but now able
    to be used over a long distance.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，SAN是一个非常简单的概念。我们将任何块设备，无论是像实际硬盘这样的物理设备，还是像磁盘阵列这样更复杂的概念，都将其封装在标准块设备协议（如SCSI或ATA）中，并通过网络协议（如TCP/IP、以太网或*FiberChannel*）传输。网络协议充当了一个简单的隧道，将块协议传输到远距离。就这么简单。归根结底，它仍然只是一个基于SCSI或ATA的设备，但现在可以在远距离使用。
- en: Of course, what we just added is a bit of complexity, so SANs are automatically
    more fragile than local storage. Any and all risk and complication of local storage
    remains plus any complications and risk of the networking layer and equipment.
    The risk is cumulative. Plus, the extra networking layer, processing, and distance
    all must add additional latency to the storage transaction.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们刚刚增加了一些复杂性，因此SAN自然比本地存储更脆弱。所有本地存储的风险和复杂性仍然存在，并且还增加了网络层和设备带来的复杂性和风险。风险是累积的。而且，额外的网络层、处理和距离都必然会给存储事务增加额外的延迟。
- en: Because of these factors, SAN based storage is always slower and more fragile
    than otherwise identical local storage. The very factors that most myths have
    used to promote SAN are exactly their weaknesses.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正因为这些因素，基于SAN的存储总是比其他相同配置的本地存储要慢且更脆弱。大多数神话所用来宣传SAN的那些因素，正是它们的弱点。
- en: 'The SAN approach does have its strengths, of course, or else it would serve
    no purpose. A SAN allows for three critical features: distance, consolidation,
    and shared connections.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，SAN的做法是有其优势的，否则它就没有存在的意义了。SAN提供了三个关键特性：距离、整合和共享连接。
- en: Distance can mean anything from a few extra feet to across the world. Of course,
    with longer distances come higher latencies, and typically storage is very sensitive
    to latency, so it is pretty rare that remote block storage is useful from outside
    of the range of local connection technologies. If you have to pull block storage
    data over the WAN, you will likely experience at very least latencies that cause
    severe performance issues and will typically see untenable bandwidth constraints.
    Typical production block storage is assumed to be many GB/s (that big B, not little
    b) of throughput and sub-millisecond latency, but WAN connections rarely hit even
    a single Gb/s and even the best latencies are normally a few milliseconds if not
    scores or more!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 距离可以是从几英尺到跨越全球的任何距离。当然，距离越远，延迟越高，而存储通常对延迟非常敏感，因此远程块存储在超出本地连接技术范围的情况下很少有用。如果你必须通过广域网（WAN）获取块存储数据，你可能会遇到至少导致严重性能问题的延迟，并且通常会面临不可承受的带宽限制。典型的生产级块存储假定吞吐量是每秒多个GB（大写B，而不是小写b）且延迟在毫秒级别，但WAN连接通常连一个Gb/s都难以达到，即便是最好的延迟通常也会是几毫秒，甚至几十毫秒以上！
- en: Consolidation was traditionally the driving value of a SAN. Because many systems
    can physically connect to a single storage array over a single network it became
    easy, for the first time, to invest in a single, expensive storage system that
    could be used by many physically separate computer systems at once. The storage
    on the device would be *sliced* and every device that attaches to it sees its
    own unique portion of the storage.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 整合曾是SAN的传统驱动力。因为许多系统可以通过单一网络连接到一个存储阵列，这使得第一次可以投资一个昂贵的存储系统，让多个物理分离的计算机系统同时使用。设备上的存储会被*切分*，每个连接到它的设备都能看到属于自己的独特存储部分。
- en: When local storage isn't local
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当本地存储不再是本地存储
- en: With all of the interfaces, abstractions, and incorrect terminology that often
    exists in IT, it can be really easy to lose track of exactly what is happening
    much of the time. SANs are one of these places were getting confused is par for
    the course. It is the nature of a SAN to take a block device that is far away
    and make it seem, to the computer using it, as if it were local. But it can also
    take something that is local, and make it seem local, when it is really remote.
    Did I just say that?
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在IT领域，接口、抽象和不准确的术语层出不穷，这使得我们在很多时候很容易迷失，不清楚到底发生了什么。SAN就是这样一个很容易让人感到困惑的地方。SAN的特点是，它将远程的块设备呈现给计算机时，仿佛这个设备是本地的。但它也可以把本地设备表现为本地设备，尽管实际上它是远程的。我刚才说的是什么意思？
- en: The best example is that of the external USB hard drive. We all use them; they
    are super common. Go to any local department store and pick one up. Order one
    online. You probably have five on a shelf that you have forgotten about. A USB
    drive, while external, is obviously still local, right?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的例子就是外部USB硬盘。我们都在使用它们，它们非常常见。去任何本地的商场，你都可以买到。或者在线上订购。你大概在某个架子上放了五个，已经忘记了它们。虽然USB硬盘是外部的，但显然它仍然是本地的，对吧？
- en: Well, it isn't all that easy to say. Sure, it is physically close. But in technology
    terms remote means that something is *over a network* and local is *not over a
    network*. It does not matter how far away something is, it is the network aspect
    that determines local and remote devices. Otherwise, my desktop in Texas is physically
    attached to my dad's desktop in New York because there is a series of cables the
    entire way in between them.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其实，说起来并不那么简单。没错，它在物理上是很接近的。但在技术术语中，远程意味着某个东西是*通过网络连接*的，而本地则是*不通过网络连接*的。无论某个东西有多远，决定本地和远程的因素是网络方面。否则，我位于德克萨斯的桌面和位于纽约的爸爸的桌面之间就因为有一整条电缆连接而被认为是物理连接的。
- en: This presents an interesting challenge because, you see, USB is actually a very
    simple networking protocol, as are IEEE 1394 and Thunderbolt. If you physically
    dissect an external drive you can see this at work, to some degree. They are made
    from standard hard drives, generally with SATA interfaces, and a tiny network
    adapter that encapsulates the SATA protocol into the USB network protocol to be
    sent over the network (often just two feet total distance.)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这带来了一个有趣的挑战，因为你看，USB实际上是一个非常简单的网络协议，IEEE 1394和Thunderbolt也是如此。如果你物理地解剖一个外部硬盘，你可以在某种程度上看到这一点。它们由标准硬盘组成，通常带有SATA接口，以及一个微型网络适配器，将SATA协议封装成USB网络协议并通过网络传输（通常只需两英尺的总距离）。
- en: USB and its ilk might not feel like a network protocol, but it really is. It
    is a layer two network protocol that competes with Ethernet and can attach multiple
    devices, to multiple computers, and can even use things similar to switches. It
    is a real networking platform and that means that external hard drives attached
    via USB are, in fact, tiny SANs! Hard to believe, but it is true. Consider your
    mind blown.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: USB及其类似设备可能不会让你觉得它是一个网络协议，但它确实是。它是一个二层网络协议，与以太网竞争，能够将多个设备连接到多个计算机，并且甚至能使用类似交换机的设备。它是一个真正的网络平台，这意味着通过USB连接的外部硬盘实际上是微型SAN！这很难相信，但确实如此。考虑一下，你的思维可能会被震撼。
- en: Storage, being the largest cost of most systems, being able to be shared and
    sliced more efficiently lowered cost to deploy new physical computer systems.
    Hard drives, for example, might come in 1TB sizes, but a single system might need
    only 80GB or 300GB or whatever and with a shared SAN hundreds of computers systems
    might share a single storage array and each use only what they need. Today we
    gain most of this efficiency through local storage with virtualization, but before
    virtualization was broadly available only systems like a SAN were able to address
    this cost savings. So, in the early days of SAN, the focus was cost savings. Other
    features really came later. This value has mostly inverted today and so is generally
    more expensive than having overprovisioned local storage but can still exist in
    some cases.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 存储是大多数系统中最大的一笔开销，能够更高效地共享和切割存储降低了部署新物理计算机系统的成本。例如，硬盘可能有1TB的容量，但单个系统可能只需要80GB或300GB，或者其他容量，而通过共享SAN，数百台计算机系统可能共享一个存储阵列，并且每台计算机只使用它所需要的部分。今天，通过虚拟化，我们主要通过本地存储来实现这种效率，但在虚拟化广泛普及之前，只有像SAN这样的系统才能解决这个成本节省问题。因此，在SAN的早期，重点是成本节省，其他特性则是在后期才出现的。如今，这一价值大多已发生反转，因此通常比过度配置的本地存储更昂贵，但在某些情况下仍然存在。
- en: The last value is shared connections. This is where two or more computers access
    the same portion of the storage on the same device - seeing the same data. This
    might sound a bit like traditional file sharing, but it is anything but that.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个价值是共享连接。这是指两个或多个计算机访问同一设备上相同存储部分的情况——看到相同的数据。这可能听起来有点像传统的文件共享，但实际上完全不是那样。
- en: In file sharing we are used to computers having a *smart* gatekeeping device
    that arbitrates access to files. With a SAN, we must remember that this is a *dumb*
    block device that has no logic of its own. Using a SAN to attach two or more computer
    systems to a single logical block device means that each computer thinks of the
    storage as being its own, private, fully isolated system and has no knowledge
    of other systems that might be also attached to it. This can lead to all kinds
    of problems from lost changes to corrupt files, to destroyed file systems. Of
    course, there are mechanisms that can be used to make shared storage spaces possible,
    but by definition they are not implemented by the SAN and have to be provided
    at a higher level on the computer systems themselves.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件共享中，我们习惯于计算机拥有一个*智能*的门控设备，用于调解对文件的访问。而在SAN中，我们必须记住，这是一个*无脑*的块设备，没有自己的逻辑。使用SAN将两个或多个计算机系统连接到单个逻辑块设备，意味着每台计算机都把存储看作是自己的私有、完全隔离的系统，并且对可能也连接到它的其他系统一无所知。这可能导致各种问题，从丢失的更改到损坏的文件，再到摧毁的文件系统。当然，有一些机制可以用于实现共享存储空间，但按照定义，它们并非由SAN实现，必须在计算机系统的更高层次提供。
- en: Shared SCSI connections
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 共享SCSI连接
- en: 'In the days before SAN, or before SANs were popular and widely available, there
    was another technique allowing two computers to share a single pool of hard drives:
    shared SCSI.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在SAN之前，或者说在SAN尚未普及和广泛应用的日子里，还有另一种技术允许两台计算机共享同一硬盘池：共享SCSI。
- en: With this technique, a single SCSI ribbon cable (typically able to connect to
    eight, sixteen, or even thirty-two devices. One device would need to be a controller,
    presumably on the motherboard of a computer. The other connections were open for
    connecting hard drives. But another connector could be connected to another controller
    on a separate computer and the two computers could each see and access the same
    drives.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '使用这种技术，一根单独的SCSI带状电缆（通常可以连接八个、十六个，甚至三十二个设备。一个设备需要是控制器，通常在计算机的主板上。其他连接用于连接硬盘。但另一个连接器可以连接到另一台计算机上的控制器，两个计算机可以同时看到并访问相同的硬盘。  '
- en: Due to the limitations of needing to share a single ribbon cable between two
    physical computers made this technique outrageously limited and awkward, but feasible.
    The primary value to a setup of this nature was allowing one computer system to
    fail and the other to take over, or to double the CPU and RAM resources assigned
    to a single data set beyond what could fit in a single server chassis. But the
    reliability and performance limits of the storage component left the system generally
    less than practical and so this technique was rarely implemented in the real world.
    But historically it is very important because it is the foundation of modern shared
    block storage, it was standard knowledge expected in late 1990s systems training,
    and it helps to visualize how SAN works today - more elegant, more flexible, but
    fundamentally the same thing.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '由于需要在两台物理计算机之间共享一根带状电缆的限制，这种技术非常有限且笨拙，但仍然可行。这种设置的主要价值在于允许一台计算机系统故障时，另一台计算机可以接管，或者将分配给单一数据集的CPU和RAM资源加倍，超出单一服务器机箱的容量。但存储组件的可靠性和性能限制使得该系统通常不够实用，因此这种技术在实际中很少实施。但从历史角度来看，它非常重要，因为它是现代共享块存储的基础，它曾是1990年代末期系统培训中的标准知识，也有助于我们理解今天SAN的工作方式——更优雅、更灵活，但本质上仍然相同。  '
- en: Today, the biggest use cases for shared block storage connections is for clustered
    systems that are designed to use this kind of storage as shared backing for virtualization.
    This was the height of fashion around 2010 but has since given way to other approaches
    to tackle this kind of need. This would now be a rather special case system design.
    But the technologies that are used here will be co-opted for other storage models
    as we will soon see.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，共享块存储连接的最大使用场景是为集群系统提供支持，这些系统设计用来将这种存储作为虚拟化的共享后端存储。大约在2010年时，这一方式最为流行，但如今已经被其他方法取而代之，来解决这种需求。现在这会是一个相对特殊的系统设计。然而，我们很快会看到，这里使用的技术将被改造用于其他存储模型。
- en: The world of SAN has many popular connection technologies. There are super simple
    SAN transports that are so simple that no one recognizes them as being such including
    USB, Thunderbolt, and IEEE1394/Firewire. Then there are a range of common enterprise
    class SAN protocols such as iSCSI (SCSI over IP), FibreChannel, FCoE (Fibre Channel
    over Ethernet), *FC-NVMe* (NVMe over Fiber Channel), and so on. Each SAN protocol
    presents its own advantages and challenges, and typically vendors only offer a
    small selection from their own equipment so choosing a vendor will typically limit
    your SAN options and picking a SAN option will limit your vendor selection choices.
    Understanding all of these protocols moves us from the systems world into the
    networking one. It is rare that as a system administrator you will be in a position
    to choose or even influence the choices in SAN design, typically this will be
    chosen for you by the storage, networking, and/or platform teams. If you do get
    to have influence in this area then significant study of these technologies, their
    benefits, and their applicability to your workload(s) will be necessary but is
    far outside of the scope of this tome.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'SAN的世界有许多流行的连接技术。有些超简单的SAN传输，简单到没有人认为它们是SAN，包括USB、Thunderbolt和IEEE1394/Firewire。然后是一些常见的企业级SAN协议，如iSCSI（通过IP的SCSI）、光纤通道、FCoE（以太网光纤通道）、*FC-NVMe*（光纤通道上的NVMe）等。每种SAN协议都有其自身的优点和挑战，通常供应商只提供他们自己设备中的一小部分选择，因此选择供应商通常会限制你的SAN选项，而选择SAN选项则会限制你的供应商选择。理解所有这些协议将我们从系统世界带入网络世界。作为系统管理员，你很少有机会选择或影响SAN设计中的选择，通常这将由存储、网络和/或平台团队为你决定。如果你确实能够在这个领域产生影响，那么就需要对这些技术、它们的优点以及它们如何适应你的工作负载进行深入研究，但这远远超出了本书的范围。  '
- en: Block storage is not going anywhere. As much as we get excited about new storage
    technologies, such as object storage, block storage remains the underpinning of
    all other storage types. We have to understand block devices both physically and
    logically as we will use them in myriad ways as the building blocks of our storage
    platforms. Block storage is powerful and ubiquitous. It represents the majority
    of storage that we interact with during our engineering phases and is expected
    to remain at the core of everything that we do for decades to come.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 块存储并不会消失。尽管我们对新的存储技术（如对象存储）感到兴奋，但块存储仍然是所有其他存储类型的基础。我们必须在物理上和逻辑上了解块设备，因为在我们的存储平台的各种方式中，我们将使用它们作为构建块。块存储功能强大且普遍存在。它代表了我们在工程阶段中与之交互的大多数存储，并且预计将在未来几十年内继续保持核心地位。
- en: 'When deciding between local and remote block storage there is a useful rule
    of thumb: *You always want to use local storage until you have a need that local
    storage cannot fulfill. Or you never want to use remote storage until you have
    no other choice.*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当决定使用本地存储还是远程块存储时，有一个有用的经验法则：*在本地存储能够满足需求之前，您总是希望使用本地存储。或者在没有其他选择之前，您从不想使用远程存储。*
- en: Surveying filesystems and network filesystems
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调查文件系统和网络文件系统
- en: Sitting on top of block storage we typically find a **filesystem**. Filesystems
    are the primary (and by primary, I mean like they make up something like 99.999%
    or more of use cases) manner of final data storage on computer systems. Filesystems
    are what hold files, as we know them, on our computer storage.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在块存储的顶部，我们通常找到一个**文件系统**。文件系统是计算机系统上主要（我指的是，它们构成了99.999%或更多用例的）最终数据存储的方式。文件系统是我们所知的，在计算机存储中保存文件的工具。
- en: A filesystem is a data organization format that sits on top of block storage
    and provides a mechanism for organizing, identifying, storing, and retrieving
    data using the file analogy. You use filesystems every day on everything. They
    are used even when you cannot see them whether it is on your desktop, cell phone,
    or even on your VoIP phone, or microwave oven! Filesystems are everywhere.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统是一种位于块存储之上的数据组织格式，提供了一种使用文件类比来组织、识别、存储和检索数据的机制。您每天在所有设备上都在使用文件系统。即使在您看不到它们的情况下，无论是在您的桌面、手机甚至VoIP电话或微波炉上！文件系统无处不在。
- en: Filesystems are really databases
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文件系统实际上是数据库。
- en: If you want to get a little geeky with me for a moment and be honest you are
    reading a book on system administration best practices so we both know you are
    loving getting into some serious details, we can look at what a filesystem really
    is. At its core a filesystem is a NoSQL database, specifically a file database
    (essentially a specialized document database), that uses a raw block device as
    its storage mechanism and is only able to store and retrieve files.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想和我一起稍微“极客”一下，并且诚实地说，你正在阅读一本关于系统管理最佳实践的书，所以我们都知道你喜欢深入了解一些严肃的细节，我们可以看看文件系统的真正含义。在其核心，文件系统是一个NoSQL数据库，具体来说是一个文件数据库（本质上是一种专门的文档数据库），它使用原始块设备作为其存储机制，只能存储和检索文件。
- en: There are other specialty databases that use block devices directly (often called
    raw storage when dealing with database lingo), but they are rare. Filesystems
    are a database type that is so common, so much more common than all other database
    types combined, that no one ever talks about or thinks about them being databases
    at all. But under the hood, they are truly a database in every sense.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他专业数据库直接使用块设备（在处理数据库术语时通常称为原始存储），但它们很少见。文件系统是一种如此常见的数据库类型，比所有其他数据库类型加起来都要常见，以至于没有人谈论或认为它们实际上是数据库。但在幕后，从各个方面来看，它们都是真正的数据库。
- en: To show direct comparisons, a standard database regardless of type has a standardized
    storage format, a retrieval format, a database engine (driver), and in some cases
    a database management layer (that can often allow for the use of multiple database
    engineers within a single system interface), and a query interface for accessing
    the data. Whether you compare MongoDB or MS SQL Server you will find that filesystems
    behave identically. The chosen filesystem on disk format is the storage format,
    the retrieval format is the *file*, the database engine is the filesystem drive,
    the database management system in Linux is the Virtual File System (which we will
    discuss later), and the query language is a list of underlying POSIX commands
    implemented in C (with simple shell-based abstractions that we can use for convenience.)
    Compare to standard databases and there is no way to tell them apart! Very cool
    stuff.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行直接对比，无论类型如何，标准数据库都有标准化的存储格式、检索格式、数据库引擎（驱动程序），并且在某些情况下，还会有数据库管理层（它通常允许在同一系统接口中使用多个数据库工程师），以及用于访问数据的查询接口。无论你比较
    MongoDB 还是 MS SQL Server，你会发现它们的文件系统行为是一样的。磁盘上选择的文件系统格式就是存储格式，检索格式就是 *文件*，数据库引擎就是文件系统驱动，Linux
    中的数据库管理系统是虚拟文件系统（我们稍后会讨论），查询语言是实现于 C 语言中的一系列底层 POSIX 命令（同时我们还可以使用一些简单的基于 Shell
    的抽象命令来提高便利性）。与标准数据库相比，根本无法区分它们！非常酷的技术。
- en: After a computer system is deployed, nearly everything that we do with it from
    a storage perspective involves working on the filesystem. We tend to focus heavily
    on block storage during engineering phases, and filesystems during administration
    phases. But certainly, we have to plan our filesystems properly prior to deploying
    a system to production. Proper filesystem planning is actually something that
    is heavily overlooked with most people simply accepting defaults and rarely thinking
    filesystem design at all.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算机系统部署完成，几乎所有从存储角度做的操作都涉及到文件系统的操作。在工程阶段，我们往往会过多关注块存储，而在运维阶段，则会更多关注文件系统。但显然，在将系统部署到生产环境之前，我们必须妥善规划文件系统。文件系统的正确规划其实是一个常被忽视的环节，很多人只是接受默认设置，很少考虑文件系统的设计。
- en: Most operating systems have native support for a few different filesystems.
    In most cases an operating system has one obviously standard and primary filesystem
    and a handful of special case filesystems that are relegated to use on niche hardware
    devices or for compatibility with other systems. For example, Apple macOS uses
    APFS (Apple File System) for all normal functions but can use ISO 9660 when working
    with optical disks or FAT32 and **exFAT** used for compatibility with Windows
    storage devices (such as USB memory sticks or external hard drives.) Windows is
    similar but with NTFS instead of APFS. Windows recently has added one alternative
    filesystem, ReFS, for special needs, but it is not commonly used or understood.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数操作系统都原生支持几种不同的文件系统。在大多数情况下，一个操作系统有一个明显标准的主文件系统，并且还会有一些特殊的文件系统，这些文件系统通常用于特殊硬件设备，或者是为了兼容其他系统。例如，Apple
    macOS 在执行所有正常功能时使用 APFS（Apple 文件系统），但在处理光盘时使用 ISO 9660，在与 Windows 存储设备（如 USB 存储棒或外部硬盘）兼容时使用
    FAT32 和 **exFAT**。Windows 操作系统也类似，不过它使用的是 NTFS，而不是 APFS。最近，Windows 添加了一个替代文件系统
    ReFS，用于特殊需求，但它并不常见，且使用者不多。
- en: In Linux, however, we have several primary filesystem options and scores of
    specialty filesystem options. We have no way to go through them all here, but
    we will talk about several of the most important as understanding why we have
    them, and when to choose them is very important. Thankfully in production systems
    we really only have to concern ourselves with a few key products. If you find
    filesystems interesting, you can research the many Linux filesystem options to
    learn more about filesystem design and history and you might even find one that
    you want to use somewhere special!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在 Linux 中，我们有多个主要的文件系统选项和数十种专用文件系统选项。我们无法在此一一介绍它们，但我们将讨论几个最重要的文件系统，因为理解它们的存在意义以及选择合适文件系统的时机非常重要。幸运的是，在生产环境中，我们实际上只需要关注几个关键的文件系统。如果你对文件系统感兴趣，可以深入研究许多
    Linux 文件系统选项，以了解更多关于文件系统设计和历史的信息，或许你还会找到一个适合特殊用途的文件系统！
- en: 'These are the key Linux filesystems today with which we need to be concerned
    for everyday purposes: XFS, EXT4, ZFS, BtrFS. Nearly everything that we do will
    involve one of those four. There are loads of less popular filesystems that are
    well integrated and work perfectly well like JFS and ReiserFS but are almost never
    seen in production. There are older formats like EXT2 and EXT3 that have been
    superseded by more recent updates. There are loads and loads of filesystems that
    are standard on other systems that can be used on Linux like NTFS from Windows
    or UFS from the BSD family. There are the standard niche filesystems like ISO
    9660 and FAT32 that we mentioned earlier. Linux gives you options at every turn
    and filesystem selection is a great example of just how extreme it can get.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们需要关注的关键 Linux 文件系统有：XFS、EXT4、ZFS 和 BtrFS。这四种文件系统几乎涵盖了我们日常使用的所有操作。还有一些不那么流行，但已经很好集成并且工作非常稳定的文件系统，如
    JFS 和 ReiserFS，虽然它们几乎不会在生产环境中出现。还有一些较旧的文件系统，如 EXT2 和 EXT3，已被更新版本所取代。另有一些标准文件系统，如
    Windows 的 NTFS 或 BSD 系列的 UFS，可以在 Linux 上使用。还有像 ISO 9660 和 FAT32 这样的标准小众文件系统，我们之前提到过。Linux
    在每个方面都提供了选择，文件系统的选择便是其灵活性的一个典型例子。
- en: 'EXT: The Linux filesystem family'
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EXT：Linux 文件系统家族
- en: Nearly every operating system has its own, special-sauce filesystem that it
    uses natively or by default and is tightly associated with it and Linux is no
    exception.... just kidding, Linux is absolutely the exception which is amazing
    considering how much more robust Linux is in its filesystem options than any other
    operating system. Illumos has ZFS, FreeBSD has UFS, Windows has NTFS, macOS has
    APFS, AIX has JFS, IRIX had XFS and on, and on. Linux truly has no filesystem
    of its own, yet it has nearly everyone elses.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每个操作系统都有自己的、独特的文件系统，作为其本地或默认使用的文件系统，并且与操作系统紧密相关，Linux 也不例外……开个玩笑，实际上 Linux
    正是一个例外，这一点令人惊讶，因为 Linux 在文件系统选项上的强大远超其他任何操作系统。Illumos 有 ZFS，FreeBSD 有 UFS，Windows
    有 NTFS，macOS 有 APFS，AIX 有 JFS，IRIX 曾有 XFS，等等。Linux 确实没有属于自己的文件系统，但它几乎包含了所有其他系统的文件系统。
- en: Most people talk about the EXT filesystem family as being the Linux native filesystem
    and certainly nothing else comes close to matching that description. When Linux
    was first being developed, long before anyone had actually run it, the MINIX filesystem
    was ported to it and became the default filesystem as the new operating system
    began to take off. But as the name suggests, the MINIX Filesystem was native to
    MINIX and predated Linux altogether.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人都把 EXT 文件系统家族看作是 Linux 的本地文件系统，确实没有其他文件系统能与之相提并论。在 Linux 开发初期，早在任何人实际运行它之前，MINIX
    文件系统被移植到 Linux 上，成为默认的文件系统，随着这个新操作系统的崛起而广泛使用。但是，正如名字所示，MINIX 文件系统原本是 MINIX 的本地文件系统，早于
    Linux 出现。
- en: Just one year after Linux was first announced, the EXT filesystem (or MINIX
    Extended File System) was created taking the MINIX Filesystem and, you guessed
    it, extending it with new features mostly around timestamping.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 就在 Linux 发布的第一年，EXT 文件系统（或 MINIX 扩展文件系统）应运而生，它以 MINIX 文件系统为基础，并且如你所料，加入了许多新特性，主要集中在时间戳处理上。
- en: As Linux began to grow, EXT grew with it and just one year after EXT was first
    released its successor EXT2 was released as a dramatic upgrade taking the Linux
    filesystem ecosystem from a hobby system to a serious enterprise system. EXT2
    ruled the Linux ecosystem almost exclusively from its introduction in 1993 until
    2001 when Linux went through a bit of a filesystem revolution. EXT2 was such a
    major leap forward that it was backported to MINIX itself and had drivers appear
    on other operating systems like Windows and macOS. Possibly no filesystem is more
    *iconically* identified with Linux than EXT2.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Linux 的发展，EXT 文件系统也在不断发展，仅仅在 EXT 发布一年后，它的继任者 EXT2 就作为一次重要的升级发布，将 Linux 的文件系统生态系统从一个爱好型系统转变为一个严肃的企业级系统。EXT2
    从 1993 年推出开始，几乎独占了 Linux 生态系统，直到 2001 年 Linux 经历了一次文件系统革命。EXT2 是如此重要的进步，以至于它被回移植到
    MINIX 本身，并且出现了 Windows 和 macOS 等其他操作系统的驱动程序。可能没有任何文件系统比 EXT2 更加*标志性*地与 Linux 相关联了。
- en: By 2001 many operating systems were looking to more advanced filesystem technologies
    to give them a competitive advantage against the market and Linux did so both
    by introducing more filesystem options and by adding journaling functionality
    to EXT2 to increment its version to EXT3\. This gave the EXT family some much
    needed stability.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 到了 2001 年，许多操作系统开始寻求更先进的文件系统技术，以在市场中获得竞争优势，Linux 通过引入更多的文件系统选项，并且在 EXT2 上添加日志功能，将其版本更新为
    EXT3，从而实现了这一点。这为 EXT 文件系统家族带来了急需的稳定性。
- en: Seven more years and we received one additional major upgrade to Linux' quasi-native
    filesystem with EXT4\. Surprisingly, the primary developer on EXT3 and EXT4 stated
    that while EXT4 was a large step forward that it was essentially a stopgap measure
    of adding improvements to what is very much a 1980s technology. Filesystem design
    principals leaped forward especially in the early 2000s and the EXT family is
    likely at the end of the road, but still has a lot of useful life left in it.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 又过了七年，我们对 Linux 的准原生文件系统进行了一个重要的升级，推出了 EXT4。令人惊讶的是，EXT3 和 EXT4 的主要开发者表示，尽管 EXT4
    是一个巨大的进步，但它本质上仍然是 1980 年代技术的权宜之计，主要是在原有基础上进行改进。文件系统设计原则在 2000 年代初期有了飞跃发展，而 EXT
    系列很可能已经走到了发展的尽头，但它仍然具有很长的使用寿命。
- en: I am going to delve into a little bit of detail for each of the main filesystem
    options, but to be clear this is a cursory look. Filesystem details can change
    quickly and can vary between versions or implementations so for really specific
    details such as maximum file size, file count, filesystem size and so forth please
    look to Wikipedia or filesystem documentation. You will not need to memorize these
    details and rarely will you even need to know them. In the 1990s filesystem limitations
    were so dramatic that you had to be acutely aware of them and work around them
    at every turn. Today any filesystem we are going to use is able to handle almost
    anything that we throw at it, so we really want to understand where different
    products shine or falter and when to consider which ones at a high level.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我将深入探讨每种主要文件系统选项的一些细节，但需要明确的是，这只是一个粗略的了解。文件系统的细节会迅速变化，并且在不同版本或实现中有所不同，因此对于最大文件大小、文件数量、文件系统大小等非常具体的细节，请参阅
    Wikipedia 或文件系统的文档。你不需要记住这些细节，也很少需要了解它们。在 1990 年代，文件系统的限制非常明显，你必须时刻警惕并在每个环节绕过这些限制。如今，我们将使用的任何文件系统几乎都能应对我们遇到的任何挑战，因此我们真正需要理解的是不同产品的优缺点，以及何时在高层次上选择使用哪一个。
- en: EXT4
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: EXT4
- en: Linux, as a category, has no default filesystem in the way that other operating
    systems do, but if you were to attempt to make the claim that any filesystem deserves
    this title today that honour would have to go to **EXT4**. More deployed Linux-based
    operating systems today choose EXT4 as their default filesystem than any other.
    But this is beginning to change so it seems unlikely that EXT4 will remain dominant
    for more than a couple years yet.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 作为一个类别，并没有像其他操作系统那样有默认的文件系统，但如果你今天想要主张某个文件系统应当被称为默认文件系统，那么这个荣誉应该归于**EXT4**。目前，部署最广泛的
    Linux 操作系统选择 EXT4 作为默认文件系统，而不是其他任何文件系统。但这种情况正在开始发生变化，因此 EXT4 在未来几年内保持主导地位的可能性不大。
- en: EXT4 is reasonably fast and robust, quite flexible, well known, and meets the
    needs of nearly any deployment. It is the jack of all trades of Linux filesystems.
    For a typical deployment, it is going to work quite well.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: EXT4 的速度合理，可靠性强，灵活性高，广为人知，能够满足几乎任何部署的需求。它是 Linux 文件系统中的全能选手。对于典型的部署，EXT4 的表现相当不错。
- en: EXT4 is what we call a *pure filesystem*, which means it is just a filesystem
    and does not do anything else. This makes it easier to understand and use, but
    also makes it more limited.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: EXT4 是我们所说的*纯文件系统*，这意味着它仅仅是一个文件系统，不做其他任何事情。这使得它更易于理解和使用，但也使得它的功能更加有限。
- en: XFS
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XFS
- en: Like the EXT family, **XFS** dates back to the early 1990s, and comes from a
    Linux competitor, in this case SGI's IRIX UNIX system. It is venerable and robust
    and was ported to Linux in 2001, the same year that EXT3 released. For twenty
    years now EXT3/4 and XFS have competed for the hearts and souls of Linux Administrators
    (and Linux Distro creators to choose them as the default filesystem.)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与 EXT 系列一样，**XFS** 源自 1990 年代初期，来自 Linux 的竞争者——SGI 的 IRIX UNIX 系统。它具有悠久的历史和强大的稳定性，并且在
    2001 年被移植到 Linux，这与 EXT3 发布的年份相同。二十年来，EXT3/4 和 XFS 一直在 Linux 管理员（以及 Linux 发行版的开发者）中竞争，争夺默认文件系统的位置。
- en: XFS is also a *pure filesystem* and is very commonly used. XFS is famous for
    its extremely high performance and reliability. XFS is sometimes specifically
    recommended by high performance applications like databases to keep them running
    at their peak.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: XFS 也是一个*纯文件系统*，并且非常常用。XFS 以其极高的性能和可靠性而著称。有时，高性能应用程序（如数据库）会特别推荐使用 XFS，以保持它们在高峰期的运行。
- en: XFS is probably the most deployed filesystem when the system administrator is
    deliberately choosing the filesystem rather than simply taking the default, is
    probably the most recommended by application vendors, and is my own personal choice
    for most workloads where storage needs are non-trivial.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当系统管理员故意选择文件系统而不是简单地接受默认设置时，XFS 可能是部署最多的文件系统，也可能是应用程序供应商最推荐的文件系统，而且它是我自己在大多数存储需求非凡的工作负载中的个人选择。
- en: Over the years, EXT4 and XFS have gone back and forth in popularity. My own
    observations say that XFS has slowly been edging ahead over the years.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，EXT4 和 XFS 在流行度上一直在交替变化。我个人的观察是，XFS 在这些年中逐渐领先。
- en: The one commonly cited caveat of XFS compared to EXT4 is that EXT4 is able to
    either shrink *or* grow a volume once it has been deployed. XFS can grow, but
    cannot shrink, a volume that has been deployed. However, it is almost unheard
    of for a properly deployed production system to shrink a filesystem, so this is
    generally seen as trivia and not relevant to filesystem decision making (especially
    with the advent of thin provisioned block storage.)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 与 EXT4 相比，XFS 常被引用的一个警告是 EXT4 能够在部署后对卷进行缩小*或*扩展。XFS 可以扩展，但不能缩小已经部署的卷。然而，在一个正确部署的生产系统中，缩小文件系统几乎是闻所未闻的，因此这通常被视为琐事，与文件系统决策无关（尤其是在薄配置块存储出现之后）。
- en: ZFS
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ZFS
- en: Releasing to Solaris in 2006, **ZFS** is generally considered to be the foundation
    of truly modern filesystem design. By the time that work on ZFS had begun in 2001
    the industry was already beginning to take filesystem design very seriously and
    many new concepts were being introduced regularly, but ZFS really took these design
    paradigms to a new level and ZFS remains a significant leader in many areas still
    today.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS 于 2006 年发布给 Solaris，**ZFS** 通常被认为是现代文件系统设计的基础。当 ZFS 的开发工作在 2001 年开始时，业界已经开始非常认真地对待文件系统设计，并且许多新的概念定期被引入，但
    ZFS 确实将这些设计范式提升到一个新的高度，至今 ZFS 仍在许多领域中处于领先地位。
- en: 'ZFS really has three high level areas in which it attempted to totally disrupt
    the filesystem industry. First in size: ZFS was able to address multiple orders
    of magnitude more storage capacity than any filesystem before it. Second in reliability:
    ZFS introduced more robust data protection mechanisms than other filesystems had
    allowing it to protect against data loss in significant ways. And third, in integration:
    ZFS was the first real *non-pure* filesystem where ZFS represented a filesystem,
    a RAID system, and a logical volume manager all built into a single filesystem
    driver. We will go into depth about RAID and LVMs later in this chapter. This
    integration was significant as it allows the storage layers to communicate and
    coordinate like never before. Pure filesystems like EXT4 and XFS can and do use
    these technologies but do so through external components rather than integrated
    ones.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS 其实有三个高层次的领域，它试图彻底颠覆文件系统行业。首先是容量：ZFS 能够解决比任何之前的文件系统多几个数量级的存储容量。其次是可靠性：ZFS
    引入了比其他文件系统更强大的数据保护机制，使其能够以显著的方式防止数据丢失。第三是集成性：ZFS 是第一个真正的*非纯粹*文件系统，它代表了一个文件系统、一个
    RAID 系统和一个逻辑卷管理器，三者都集成在一个单一的文件系统驱动程序中。我们将在本章后面详细讨论 RAID 和 LVM。这个集成性非常重要，因为它使得存储层能够像从未有过的那样相互通信和协调。像
    EXT4 和 XFS 这样的纯文件系统可以使用这些技术，但通常是通过外部组件而非集成的方式。
- en: While ZFS is not new, having been in production systems for at least fifteen
    years, it is quite new for release on Linux. It took many years before a port
    of ZFS was made available for Linux, and then there were many years during which
    licensing concerns kept it from being released in a consumable format for Linux.
    Today the only major Linux distribution that officially supports and packages
    ZFS is Ubuntu, but Ubuntu's dominant market position makes ZFS automatically widely
    available. At this time, it is less than two years since ZFS was able to be used
    for the bootable root filesystem on Ubuntu. So ZFS is quite new in the production
    Linux space in any widely accessible way. Its use appears to be growing rapidly
    now that it is available.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 ZFS 并不新颖，它至少在生产系统中已经使用了十五年，但对于 Linux 的发布而言，它是相当新的。在为 Linux 提供 ZFS 移植版本之前，花费了多年时间，而且又有许多年因许可问题，ZFS
    没有以可消费的格式发布给 Linux。如今，唯一一个正式支持并打包 ZFS 的主要 Linux 发行版是 Ubuntu，但 Ubuntu 在市场上的主导地位使得
    ZFS 自动广泛可用。目前，距离 ZFS 可以用于 Ubuntu 上可引导的根文件系统还不到两年。因此，ZFS 在生产环境中的 Linux 系统中仍然是以一种广泛可访问的方式相当新的。现在，由于
    ZFS 已经可用，它的使用似乎正在迅速增长。
- en: ZFS represents probably the most advanced, reliable, and scalable filesystem
    available on Linux as of the time of this writing. It should be noted that from
    a purely filesystem-based performance perspective that ZFS is not known to shine.
    It is rare that storage performance tweaking at the filesystem level is considered
    valuable but when it is modern filesystems with all of their extra reliability
    typically cannot compete with older, more basic filesystems. This has to be noted
    as it is so often simply assumed that more modern systems are going to also be
    automatically faster, as well. This is not the case here.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS代表了截至目前Linux平台上最先进、最可靠、最具可扩展性的文件系统。需要注意的是，从纯粹的文件系统性能角度来看，ZFS并不是以性能突出而闻名。虽然存储性能调整在文件系统层面上很少被认为是有价值的，但当它成为必要时，现代文件系统由于其额外的可靠性，通常无法与较旧、更基础的文件系统竞争。必须指出这一点，因为人们常常默认现代系统也会自动更快，但实际上情况并非如此。
- en: BtrFS
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BtrFS
- en: Pronounced *Butter-F-S*, **BtrFS** is the current significant attempt to make
    a Linux native filesystem (there was a previous attempt called ReiserFS in the
    early 2000s that got some traction but ended badly for non-technical reasons.)
    BtrFS is intended to mimic the work of ZFS, but native to Linux and with a compatible
    license.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 读作*Butter-F-S*，**BtrFS**是当前Linux本地文件系统的重要尝试（在2000年代初曾有过一次名为ReiserFS的尝试，虽然取得了一些进展，但由于非技术原因最终失败）。BtrFS旨在模仿ZFS的工作方式，但它是本地化的Linux文件系统，并且具有兼容的许可证。
- en: BtrFS trails ZFS significantly with many features still not implemented, but
    with work ongoing. BtrFS is very much alive and increasingly more Linux distributions
    are supporting it and even choosing it as a default filesystem. BtrFS feels as
    if it might be the most likely long-term future for Linux.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: BtrFS在许多特性上落后于ZFS，许多功能仍未实现，但工作仍在继续。BtrFS依然充满活力，越来越多的Linux发行版开始支持它，甚至选择将其作为默认文件系统。BtrFS似乎是Linux长期未来中最有可能的选择。
- en: Like ZFS, BtrFS is a modern, heavily integrated filesystem that is beginning
    to include functionality from RAID and LVM layers of the storage stack. Performance
    is the weakest point for BtrFS today.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 像ZFS一样，BtrFS是一个现代化、深度集成的文件系统，开始包含存储堆栈中RAID和LVM层的功能。当前，BtrFS的性能是其最弱的环节。
- en: Stratis
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Stratis
- en: Given its industry support, we need to make mention of Stratis. Stratis is not
    a filesystem itself *per se* but act much like one. Stratis is an attempt to build
    the functionality of integrated (or *volume-managing file systems*) like ZFS and
    BtrFS using the existing components of XFS and the standard Linux LVM layer.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于其行业支持，我们需要提到Stratis。Stratis本身并不是一个文件系统，*本质上*它更像是一个文件系统。Stratis试图利用现有的XFS和标准Linux
    LVM层，构建集成（或*卷管理文件系统*）功能，如ZFS和BtrFS。
- en: In its early days on IRIX, XFS was designed to be used with IRIX's native LVM
    and the two integrated naturally providing something not entirely unlike ZFS or
    BtrFS today. When XFS was ported to Linux its associated LVM layer was not ported,
    but the native Linux LVM was made to work with it instead. XFS + LVM has long
    been an industry standard approach and Stratis mearly is attempting to provide
    a more accessible means of doing so while integrating best practices and simplified
    management.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期的IRIX上，XFS设计用于与IRIX的本地LVM一起使用，两者自然地集成，提供的功能与今天的ZFS或BtrFS非常相似。当XFS被移植到Linux时，关联的LVM层并没有被移植，而是使本地的Linux
    LVM与其兼容。XFS + LVM一直是业界的标准做法，而Stratis仅仅是试图提供一种更易于访问的方式，同时整合最佳实践和简化管理。
- en: This sums up the four current production filesystem options that you will likely
    encounter or be responsible for choosing between. Remember that you can mix and
    match filesystems on a single system. It is very common, in fact, to use EXT4
    as a boot filesystem for basic operating system functionality while then relying
    on XFS for a high-performance database storage filesystem or BtrFS for a large
    file server filesystem. Use what makes sense for the workload at an individual
    filesystem layer. Do not feel that you have to be stuck using only one filesystem
    across all systems, let alone within a single system!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这总结了你可能遇到或需要选择的四种当前生产环境中的文件系统选项。请记住，你可以在单一系统上混合使用不同的文件系统。事实上，使用EXT4作为基本操作系统功能的启动文件系统，同时依赖XFS作为高性能数据库存储文件系统，或者依赖BtrFS作为大文件服务器的文件系统，这种情况非常常见。根据每个文件系统层的工作负载选择最合适的文件系统。不要觉得你只能在所有系统上使用同一个文件系统，更不要觉得在一个系统内必须统一使用一个文件系统！
- en: Most of the real intense technical aspects of filesystems are in the algorithms
    that deal with searching for and storing the bits onto the block devices. The
    details of these algorithms are way beyond the scope of not only this book but
    systems administration in general. If you are interested in filesystems, learning
    how data is stored, protected, and retrieved from the disk can be truly fascinating.
    For systems administration tasks it is enough to understand filesystems at a high
    level.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统的许多技术细节都涉及到处理在块设备上查找和存储数据的算法。这些算法的细节远远超出了本书以及系统管理的范围。如果你对文件系统感兴趣，学习数据是如何存储、保护和从磁盘中检索的，可以说是非常迷人的。对于系统管理任务而言，理解文件系统的高层次内容就足够了。
- en: Sadly, there is no way to provide a real best practice around filesystem selection.
    It is unlikely that you are going to have reason to seriously consider the use
    of any rare filesystem not mentioned here in a production setting, but all four
    that are listed here have valuable use cases and all should be considered. Often
    the choice of filesystem is not made in isolation unless you are working with
    a very specific product that requires or recommends a specific one for the purpose
    of some feature. Instead, the choice of filesystem is normally going to be dependent
    on many other storage decisions including RAID, LVM, physical support needs, drive
    media, and so forth.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 可惜的是，没有办法提供一个关于文件系统选择的真正最佳实践。你不太可能在生产环境中需要认真考虑使用这里未提到的任何稀有文件系统，但这里列出的四个文件系统都有有价值的使用案例，所有这些都应该考虑。通常，文件系统的选择并不是孤立做出的，除非你在处理一个非常特定的产品，该产品需要或推荐使用某个特定的文件系统以支持某个功能。相反，文件系统的选择通常会依赖于许多其他存储决策，包括RAID、LVM、物理支持需求、驱动介质等。
- en: Clustered file systems
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群文件系统
- en: All of the filesystems that we have discussed thus far, regardless of how modern
    they are, are *standard* filesystems or *non-shared* filesystems. They are only
    viable when access is guaranteed to be from only a single operating system. In
    almost all cases, this is just fine.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止我们讨论的所有文件系统，无论它们多么现代，都是*标准*文件系统或*非共享*文件系统。它们仅在访问保证仅来自单一操作系统时才可行。在几乎所有情况下，这样是完全足够的。
- en: If you recall from our discussion on SANs, however, we mentioned that there
    are use cases where we may want multiple computer systems to be able to read and
    write for the same storage area at the same time. Clustered or *shared storage*
    filesystems are the mechanism that can allow that to work.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，回想一下我们在讨论SAN时提到的，我们提到过有些使用场景可能需要多个计算机系统能够同时读取和写入同一区域的存储。集群或*共享存储*文件系统就是能实现这一点的机制。
- en: Clustered filesystems work just like traditional filesystems do, but with the
    added features by which they will write locking and sharing information to the
    filesystem so that multiple computer systems are able to coordinate their use
    of the filesystem between attached nodes. In a standard filesystem there is only
    one computer accessing the filesystem at a time so knowing what file is open,
    when a file has been updated, when a write is cached and so forth are all handled
    in memory. If two or more computers try to share data from a traditional filesystem,
    they cannot share this data in memory and so will inevitably create data corruption
    as they overwrite each other's changes, fail to detect updated files, and do all
    sorts of nasty things from outdated write caches!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 集群文件系统的工作原理与传统文件系统相同，但它们具有额外的功能，能够将锁定和共享信息写入文件系统，使多个计算机系统能够协调它们之间连接的节点使用文件系统。在标准文件系统中，只有一台计算机在访问文件系统，因此知道哪个文件已打开、文件何时已更新、写入何时被缓存等都在内存中处理。如果两台或更多计算机尝试共享来自传统文件系统的数据，它们不能在内存中共享这些数据，因此不可避免地会因为相互覆盖对方的更改、未能检测到更新的文件，以及各种由于过时的写缓存导致的糟糕情况而导致数据损坏！
- en: Since the only shared component of these systems is the filesystem, all communications
    between nodes accessing a file system have to happen in the filesystem itself.
    There is literally no other possible way without going to a mechanism that is
    no longer shared storage but shared compute which is much more complicated and
    expensive.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些系统唯一共享的组件是文件系统，因此所有访问文件系统的节点之间的通信必须在文件系统内部进行。实际上，除非使用一种不再是共享存储而是共享计算的机制，否则没有其他可行的方法，而这种机制要复杂得多，成本也更高。
- en: To describe how clustered filesystems work in the simplest terms we can think
    of each computer knowing, from the filesystem, that a specific section of the
    block device (disks) is set aside in an extremely rigid format and size to be
    an area where the nodes read and write their current status of interaction with
    the filesystem. If node A needs to open File X, it will put in a note that it
    is holding that file open. If node B deletes a file, it will put in a note that
    it is going to delete and update it once the file is deleted. node C can tell
    what activity is going on just by reading this one small piece of the filesystem.
    All of the nodes connected know not to cache the data in this area, to state any
    action that they plan to take, and to log anything that they have done. If any
    node misbehaves, the whole system corrupts, and data is lost.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单描述集群文件系统的工作原理，我们可以将每台计算机视为通过文件系统得知，某个块设备（硬盘）上的特定区域被严格格式化并预留出一个固定大小的空间，用于记录节点与文件系统交互的当前状态。如果节点
    A 需要打开文件 X，它会在该区域记录下它正在保持该文件打开的状态。如果节点 B 删除一个文件，它会在该区域记录下它正在删除该文件，并会在文件删除后进行更新。节点
    C 通过读取文件系统中的这一小部分内容，能够知道正在进行什么操作。所有连接的节点都知道不缓存该区域的数据，记录它们计划执行的任何操作，并记录它们所做的任何事情。如果有任何节点行为不当，整个系统会崩溃，数据将丢失。
- en: Of course, as you can tell, this creates a lot of performance overhead at a
    minimum. And this system necessarily requires absolute trust between all connected
    nodes as the access and data integrity controls are left up to the individual
    nodes. There is not and there cannot be any mechanism to force the nodes to behave
    correctly. The nodes have to do so voluntarily. This means that any bug in the
    code, any failure of memory, any admin with root access, any malware that gains
    access to a single node, and others. can bypass *any* and *all* controls and read,
    modify, destroy, encrypt, and others, to any degree that it wishes and all of
    the kinds of security and controls that we normally assume protect us do not exist.
    Shared storage is extremely simple, but we are so used to storage abstractions
    that it becomes complex to try to think about how any storage system could be
    so simple.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，正如你所看到的，这至少会带来很大的性能开销。而且这个系统必然需要所有连接节点之间的绝对信任，因为访问控制和数据完整性控制完全依赖于各个节点。没有，也不可能有任何机制强制节点正常工作。节点必须自愿遵守规则。这意味着，代码中的任何
    bug、内存故障、任何拥有 root 权限的管理员、任何能够访问单个节点的恶意软件等等，都可以绕过*任何*和*所有*控制，读取、修改、销毁、加密等，任意程度地进行操作，并且我们通常认为能够保护我们的所有安全控制都不存在。共享存储非常简单，但由于我们习惯了存储抽象，反而变得难以想象任何存储系统会如此简单。
- en: Like with regular filesystems, Linux has multiple clustered filesystems that
    we will commonly see in use. The most common one is GFS2 followed by OCFS2\.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 和常规文件系统一样，Linux 有多个集群文件系统，这些文件系统我们经常会见到。最常见的是 GFS2，其次是 OCFS2。
- en: 'As with SANs in general, the same rule will apply to clustered file systems:
    you do not want to use them, until you have to.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 和 SAN 一样，集群文件系统也遵循相同的规则：你不会想要使用它们，除非你必须使用。
- en: Network filesystems
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络文件系统
- en: '**Network filesystems** are always a little bit hard to describe but benefit
    well from the *you will know it when you see it* phenomenon. Unlike regular filesystems
    that sit on top of a block device and provide a way to access storage in the form
    of files, network filesystems take a filesystem and extend it over a network.
    This might sound a lot like a SAN, but it is very different. A SAN shares a set
    of block devices over a network. Network filesystems share filesystems over a
    network.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络文件系统**总是有点难以描述，但从*你看到的时候就知道了*的现象来看，它们能很好地被理解。与常规文件系统不同，常规文件系统依赖于块设备并提供以文件形式访问存储的方式，而网络文件系统则将文件系统扩展到网络上。这听起来很像
    SAN，但它们实际上非常不同。SAN 是通过网络共享一组块设备，而网络文件系统则是通过网络共享文件系统。'
- en: Network filesystems are quite common, and you probably see them every day, but
    we often do not think about them being what they really are. We often refer to
    network filesystems as *shares* or *mapped drives* and the standard protocols
    used are NFS and SMB (sometimes called CIFS, which is not really accurate.) Servers
    that implement network filesystems are called file servers and if you make a file
    server into an appliance, it is called a NAS (for Network Attached Storage.) Network
    filesystems are also often thought of as *NAS protocols* for this reason, just
    as block over network protocols are thought of as *SAN protocols*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 网络文件系统非常常见，你可能每天都会看到它们，但我们常常没有意识到它们到底是什么。我们通常把网络文件系统称为*共享*或*映射驱动器*，而标准的协议有NFS和SMB（有时也叫CIFS，这其实并不完全准确）。实现网络文件系统的服务器被称为文件服务器，如果你将文件服务器做成设备，它叫做NAS（网络附加存储）。正因如此，网络文件系统也常常被认为是*NAS协议*，就像块级网络协议被认为是*SAN协议*一样。
- en: Unlike shared block protocols, network filesystems are *smart* with the machine
    that shares out the storage having a local filesystem that it understands and
    intelligence about the files involved so concepts like file locking, caching,
    file updates and so forth can be handled through a single gatekeeper that can
    enforce security and integrity and there is no need to trust the accessing nodes.
    The key difference is that a SAN is just storage blindly attached to a network,
    it can be as simple as a network adapter bolted onto a hard drive (and it often
    is, actually.) A device implementing a network filesystem, on the other hand,
    is a server and required a CPU, RAM, and an operating system to function. Shared
    block storage is almost exclusively used in very limited deployments with carefully
    controlled servers. Network filesystems can be used almost anywhere that a SAN
    can be used but are also commonly used to share storage directly to end user devices
    as their robust security, ease of use and lack of needed end point trust make
    them highly useful where SANs would be impossible to deploy.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 与共享块协议不同，网络文件系统是*智能的*，共享存储的机器拥有一个它理解的本地文件系统，并且对涉及的文件有一定的智能，这样像文件锁定、缓存、文件更新等概念就可以通过一个单一的守门员来处理，该守门员能够执行安全性和完整性验证，而且不需要信任访问节点。关键的区别在于，SAN只是盲目地连接到网络的存储，它可以简单到只是一个硬盘上连接的网络适配器（实际上它通常就是这样）。而实现网络文件系统的设备，则是一个服务器，要求有CPU、内存和操作系统才能工作。共享块存储几乎只在少数受控的服务器上使用。网络文件系统可以在几乎任何SAN可以使用的地方使用，但它们也常用于直接向最终用户设备共享存储，因为它们的强大安全性、易用性和对终端节点信任的缺乏，使它们在SAN无法部署的地方变得非常有用。
- en: Network filesystems run as an additional network-enabled layer on top of traditional
    filesystems and do not replace the *on disk* filesystems that we already have.
    In speaking of interfaces, we would describe network filesystems as *consuming
    a filesystem interface* and also *presenting a filesystem interface*. Basically,
    it is filesystem in, filesystem out.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 网络文件系统作为一个额外的网络启用层运行在传统文件系统之上，并不取代我们已经拥有的*磁盘上的*文件系统。在谈到接口时，我们会描述网络文件系统为*消耗文件系统接口*，同时也*呈现文件系统接口*。基本上，它就是文件系统进，文件系统出。
- en: Like with traditional filesystems, Linux actually offers a large range of network
    filesystem options, many of which are historical in nature or extremely niche.
    A common example is the **Apple Filing Protcol** or **AFP** (aka *AppleTalk*)
    which Linux offers, but is not used on any production operating system today.
    Today only NFS and SMB really see any real work usage in any way.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 就像传统文件系统一样，Linux实际上提供了许多网络文件系统选项，其中许多都是历史性的或极其小众的。一个常见的例子是**Apple文件协议**或**AFP**（也叫*AppleTalk*），Linux也提供这种协议，但今天已经没有任何生产操作系统在使用它。如今，只有NFS和SMB在任何实际工作中有应用。
- en: NFS
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NFS
- en: The original *network file system* in wide use and literally the source of the
    name, **NFS** dates back to 1984! NFS cannot be native to Linux as it predates
    Linux by seven years, but NFS has been the default network file system across
    all UNIX-based or inspired operating systems since its inception and represents
    a rather significant standard because of this. Because Linux is so prominent today,
    most people think of NFS as being *Linux' protocol*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的*网络文件系统*在广泛使用中，字面上也就是这个名称的来源，**NFS**可以追溯到1984年！NFS不能是Linux的原生协议，因为它早于Linux出现七年，但自诞生以来，NFS一直是所有基于UNIX或受其启发的操作系统中的默认网络文件系统，并因此代表了一个相当重要的标准。由于今天Linux如此突出，大多数人把NFS当作是*Linux的协议*。
- en: NFS is available on essentially any system. Any UNIX system, even macOS, offers
    NFS as does Windows Server! NFS is an open standard and all but universal. NFS
    maintains popularity by being simple to use, robust on the network and generally
    performs well. NFS remains heavily used on servers wherever direct filesharing
    between systems is required, especially on backup systems.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: NFS 几乎在所有系统上都有提供。任何 UNIX 系统，甚至 macOS，都提供 NFS，Windows Server 也支持 NFS！NFS 是一种开放标准，几乎是普遍存在的。NFS
    通过简单易用、网络稳健且通常表现良好的特点保持了其流行性。NFS 在服务器中仍然被广泛使用，尤其是在需要系统之间直接文件共享的备份系统中。
- en: SMB
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SMB
- en: The **Server Message Block** (**SMB**) protocol predates NFS and was originally
    available in 1983\. These are very old protocols indeed. SMB did not really find
    much widespread usage until Microsoft really began to promote it around 1990 and
    with the rise of the Windows NT platform throughout the 1990s SMB began to become
    quite popular along with it.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**Server Message Block**（**SMB**）协议早于 NFS，最初于 1983 年推出。这些确实是非常古老的协议。直到 1990
    年代微软开始积极推广 SMB，随着 Windows NT 平台在 1990 年代的崛起，SMB 才开始广泛使用并逐渐流行起来。'
- en: SMB really benefited from Microsoft's heavy use of mapped drives between their
    servers and workstations which made the SMB protocol very visible to many users
    both traditional and technical.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: SMB 受益于微软大量使用映射驱动器在其服务器与工作站之间进行连接，这使得 SMB 协议在传统用户和技术用户中都非常显眼。
- en: In Linux, support for the SMB protocol is provided by the Samba package (Samba
    is a joke on the SMB letters.) Linux has good support for SMB, but using it is
    more complex than working with NFS.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Linux 中，SMB 协议的支持由 Samba 包提供（Samba 这个名字其实是对 SMB 的一个玩笑）。Linux 对 SMB 的支持很好，但使用它比起
    NFS 更为复杂。
- en: Choosing between NFS and SMB for file sharing needs on Linux generally comes
    down to the use case. If working with predominantly UNIX systems, generally NFS
    makes the most sense. If working predominantly with Windows systems, then SMB
    generally makes the most sense. Both are powerful and robust and can service a
    wide variety of needs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Linux 上选择 NFS 或 SMB 进行文件共享，通常取决于具体的使用场景。如果主要使用 UNIX 系统，通常选择 NFS 更为合理。如果主要使用
    Windows 系统，则一般选择 SMB。两者都非常强大且稳健，可以满足各种需求。
- en: Where decision making can get extremely hard is in cases where we can provide
    for the same need it totally different ways. For example, if you need to provide
    shared storage backends for virtualization you might have the option of a network
    filesystem like NFS or a clustered filesystem on a SAN like GFS2\. These two approaches
    cannot be easily compared as every aspect of the two systems is likely to be different
    including the vendors and hardware and so comparisons typically must be done at
    a full stack level and not at the network technology level.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 做出决策时，难度通常非常大，尤其是在可以通过完全不同的方式提供相同需求的情况下。例如，如果需要为虚拟化提供共享存储后端，可能有网络文件系统如 NFS，或者在
    SAN 上使用集群文件系统如 GFS2。由于这两种方法涉及的每个方面可能都不同，包括供应商和硬件，因此通常需要在整个技术栈层面进行比较，而不仅仅是在网络技术层面进行比较。
- en: Now we have explored file systems technologies and seen a broad scope of real-world
    file system options for Linux systems and looked at how filesystems can be local
    or remote, single access, or clustered to allow for multiple access vectors. At
    the same time, we have a good idea of how to approach decisions involving filesystem
    selection and configuration. We know when choose different filesystem technologies
    as well as what to look for in new or alternative systems that we may not have
    looked at specifically here. Filesystems need not be scary or confusing but can
    be valuable tools in our bag of tricks that we can use to fine tune our systems
    for safety, scalability, access, or performance. Next, we will look at one of
    the least understood areas of storage, the *logical volume*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经探索了文件系统技术，了解了 Linux 系统中多种实际的文件系统选择，并且看到文件系统可以是本地的或远程的，可以是单一访问的，也可以是集群的以支持多重访问。在此过程中，我们对如何做出文件系统选择和配置决策有了更好的理解。我们知道何时选择不同的文件系统技术，并且知道在我们可能没有专门探讨的新的或替代的系统中应寻找什么。文件系统不必令人害怕或困惑，它们可以是我们工具箱中的宝贵工具，用来微调我们的系统，提升安全性、可扩展性、访问性或性能。接下来，我们将探讨存储领域中最难理解的部分之一——*逻辑卷*。
- en: Getting to know logical volume management (LVM)
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解逻辑卷管理（LVM）
- en: I hate to apply terms like *new* to technology that was in use by the late 1980s
    but compared to most concepts in computer storage **logical volume management**
    (**LVM**) is pretty new and is far less known than most other standard storage
    technologies to the majority of system administrators. LVMs were relegated to
    extremely high-end server systems prior to Linux introducing the first widely
    available product in 1998 and Microsoft following suit in 2000\. Today LVMs are
    ubiquitous and available, often natively and by default, on most operating systems.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我不喜欢用*新*这种词来形容那些在1980年代末就已经在使用的技术，但与大多数计算机存储概念相比，**逻辑卷管理**（**LVM**）确实相对较新，并且对大多数系统管理员来说，远不如其他标准存储技术那么知名。在Linux于1998年推出第一个广泛可用的产品之前，LVM曾经仅限于极高端的服务器系统，微软在2000年跟进。如今，LVM已经无处不在，并且通常在大多数操作系统中是原生且默认提供的。
- en: An LVM is the primary storage virtualization technology in use today. An LVM
    allows us to take an arbitrary number of block devices (meaning one or more, generally
    called *physical volumes*) and combine, split, or otherwise modify them and present
    them as an arbitrary number of block devices (generally called *logical volumes*)
    to the system. This might sound complex, but it really is not. A practical example
    can make it seem quite easy.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: LVM是目前使用的主要存储虚拟化技术。LVM允许我们将任意数量的块设备（通常称为*物理卷*）合并、分割或以其他方式修改它们，并将它们呈现为任意数量的块设备（通常称为*逻辑卷*）供系统使用。听起来可能很复杂，但实际上并非如此。通过一个实际的例子，我们会发现它实际上相当简单。
- en: An example is when we have a computer system that has three hard drives attached
    to it. They can be all the same, or they can be different. In fact, one could
    be a traditional spinning hard drive, one a modern SSD, and one an external USB
    drive (or a RAID array, SAN, you name it). We can add all three to our LVM as
    physical volumes. An LVM will allow us to treat this as a single storage pool
    and turn it into any configuration that we want. We might turn it all into a single
    logical volume so that we simply get the combined storage of the three. Or maybe
    we will create a dozen logical volumes and use each for a different purpose. We
    can have as many physical volumes as we want and create as many logical volumes
    as we want. Logical volumes can be any size that we want (in some cases even bigger
    than the total physical size!) We are not limited to traditional disk sizes. With
    logical volumes we often find it useful to make more, smaller volumes so that
    we can have more management and isolation.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是，当我们有一个计算机系统并连接了三块硬盘时。它们可以是完全相同的，也可以是不同的。事实上，其中一个可能是传统的旋转硬盘，另一个是现代的固态硬盘，还有一个是外部USB硬盘（或者是RAID阵列、SAN，随你取名）。我们可以将这三块硬盘作为物理卷添加到我们的LVM中。LVM允许我们将其视为一个单一的存储池，并将其转化为任何我们想要的配置。我们可能会将它们合并成一个逻辑卷，这样我们就能得到这三块硬盘的总存储空间。或者，也许我们会创建十几个逻辑卷，并将每个卷用于不同的目的。我们可以有任意数量的物理卷，也可以创建任意数量的逻辑卷。逻辑卷的大小可以是我们想要的任何大小（在某些情况下甚至可以超过总物理大小！）我们不再受到传统磁盘大小的限制。使用逻辑卷时，我们通常发现创建更多、更小的卷非常有用，这样可以更好地管理和隔离。
- en: With an LVM, we can think of the system as consuming a block device and presenting
    a block device. Because LVMs use and provide block devices (aka disk appearances)
    they are *stackable*, meaning if you wanted to, you could have a block device
    on which there is an LVM which makes a logical volume, that is used by another
    LVM, that makes another logical volume, that is used by another LVM, and so forth.
    This is in no way practical, but it helps to visualize how an LVM sits in a *storage
    stack*. It is always in the middle, somewhere, but other than being in the middle
    it is very flexible.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LVM时，我们可以将系统视为一个消费块设备并呈现一个块设备。由于LVM使用并提供块设备（也称为磁盘外观），它们是*可堆叠*的，意味着如果你愿意，你可以在一个块设备上使用LVM来创建一个逻辑卷，然后这个逻辑卷又被另一个LVM使用，再创建一个新的逻辑卷，以此类推。虽然这种方式并不实际，但它有助于我们理解LVM在*存储栈*中的位置。它总是处于中间，但除了在中间之外，它非常灵活。
- en: LVMs only need to provide this basic *block in, block out* functionality to
    be an LVM, but there are other features that are commonly added to LVMs to really
    make them incredibly useful. Some of the most standard features that we tend to
    expect to be found in an LVM include live resizing of logical volumes, *hot plugging*
    of physical devices, snapshot functionality, cache options, and thin provisioning.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: LVM 只需要提供基本的*块输入，块输出*功能，就能成为一个 LVM，但通常还会添加其他功能，使 LVM 更加实用。一些常见的标准功能包括逻辑卷的在线调整大小、*热插拔*物理设备、快照功能、缓存选项和精简配置。
- en: On Linux, as with most things, we have not one, but multiple logical volume
    managers! This is becoming more common as creating integrated filesystems with
    their own LVMs has become the trend in recent years. In production on Linux today
    we have LVM2, ZFS, and BtrFS. You will, of course, recognize the latter two as
    being filesystems that we mentioned earlier. When most people are talking about
    a logical volume manager on Linux, they mean LVM2, generally just called LVM.
    But ZFS and BtrFS' integrated logical volume managers are becoming increasingly
    popular approaches as well.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Linux 中，和大多数其他事情一样，我们不仅有一个，而是有多个逻辑卷管理器！随着近年来创建集成文件系统及其独立 LVM 的趋势，这种做法越来越普遍。目前在
    Linux 生产环境中，我们有 LVM2、ZFS 和 BtrFS。你当然会认出后两者是我们之前提到过的文件系统。当大多数人在谈论 Linux 上的逻辑卷管理器时，他们指的是
    LVM2，通常简称为 LVM。但 ZFS 和 BtrFS 的集成逻辑卷管理器也越来越受欢迎。
- en: Because of the *stackable* nature of an LVM, that is consuming and providing
    block devices, we are able to use LVM2 in conjunction with ZFS or BtrFS if we
    so choose and can either disable their integrated LVM layers as being unnecessary,
    or we can use them if they have features that we want to take advantage of! Talk
    about flexibility.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 LVM 的*可堆叠*特性，即它能够消费和提供块设备，我们可以选择将 LVM2 与 ZFS 或 BtrFS 一起使用，并且可以禁用它们的集成 LVM
    层（因为它们不必要），或者如果它们有我们希望利用的功能，我们也可以继续使用它们！这简直是灵活性极致体现。
- en: Whatever happen to partitions
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分区到底怎么了？
- en: If you recall working in IT in the 1990s, we used to talk about disk partitioning
    quite incessantly. It was a regular topic. How do you set the partitions, how
    many do you make, basic and extended partitions, what partitioning software to
    use, and so on and so forth. To be sure, partitions still exist, we just have
    not needed them for a very long time now (not since Windows 2000 or Linux 2.4,
    for example.)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得上世纪90年代在 IT 领域的工作，我们曾经经常讨论磁盘分区。那是一个常见的话题。如何设置分区，分多少个，基本分区和扩展分区，使用什么分区软件，等等。可以肯定的是，分区仍然存在，只是我们已经很久没有需要它们了（例如自
    Windows 2000 或 Linux 2.4 以来）。
- en: Partitions are a very rigid *on disk* system for slicing a physical disk into
    separate areas that can each be presented to the system as an individual block
    device (aka drive.) In this way, partitions are like a super basic LVM without
    the flexibility. Partitions are limited to existing as a part of a single block
    device and the mapping of which areas of the block device belong to which partition
    are kept in a simple partition table at the start of the device.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 分区是一种非常僵硬的*磁盘上*系统，用于将物理磁盘切割成多个区域，每个区域都可以作为单独的块设备（即驱动器）呈现给系统。这样，分区就像是没有灵活性的超级基础
    LVM。分区只能作为单个块设备的一部分存在，哪个区域属于哪个分区的映射信息保存在设备开头的一个简单分区表中。
- en: Partitions were the forerunner of logical volumes and some people still use
    them (but only because they are not familiar with logical volumes.) Partitions
    are not flexible and lack important options like thin provisioning and snapshots,
    that logical volumes can offer, and while resizing a partition is technically
    possible it is inflexible, hard, and extremely risky.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 分区是逻辑卷的前身，至今仍有人使用它们（但只是因为他们不熟悉逻辑卷）。分区缺乏灵活性，缺少诸如精简配置和快照等逻辑卷所能提供的重要选项，而且虽然技术上可以调整分区大小，但这既不灵活，又困难且极其危险。
- en: Everything that partitions offered LVMs offer too, plus lots more, without giving
    anything up. The need for partitioning (the act of creating multiple filesystems
    out of a block device) has decreased significantly over the years. In the late
    1990s it was standard and all but required for even the simplest server and often
    even a desktop to have good reason to be divided up into different filesystems.
    Today it is far more common to merge many block devices into a single filesystem.
    Mostly this is because filesystem performance and reliability have totally changed
    and the driving factors for partitioning have eroded. There are good reasons to
    still divide filesystems today. We simply need to do so far less of the time.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: LVM 提供了分区所提供的所有功能，并且拥有更多功能，而不需要放弃任何东西。多年来，分区（即将块设备分割成多个文件系统）的需求显著减少。在 1990 年代末期，即使是最简单的服务器，甚至是台式机，通常也需要有充分的理由将其划分为多个文件系统。如今，将多个块设备合并为一个文件系统已变得更为常见。这主要是因为文件系统的性能和可靠性发生了彻底的变化，而分区的驱动因素逐渐消失。今天仍然有必要进行文件系统划分的理由，但我们已经不需要那么频繁地这么做了。
- en: Many mechanisms today, such as backup utilities, leverage the power of the LVM
    layer to do tasks like freezing the state of the block device so that a complete
    backup can be taken. Because an LVM operates beneath the final filesystem layer
    it has certain capabilities lacking at other layers. LVM is the storage layer
    where we get the least critical features, but it tends to be where all of the
    magic happens. LVMs give us flexibility to modify storage layouts after initial
    deployment and to interact with those storage systems at a block level. An LVM
    is a core technology component in providing the feel of a modern twenty-first
    century operating system.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 目前许多机制，如备份工具，利用 LVM 层的功能执行任务，例如冻结块设备的状态，以便进行完整的备份。由于 LVM 在最终的文件系统层之下运行，因此它具备其他层所不具备的某些能力。LVM
    是存储层，它提供的功能相对较少关键，但正是这里发生了许多神奇的事情。LVM 为我们提供了灵活性，使得在初始部署后可以修改存储布局，并且能够在块级别与存储系统进行交互。LVM
    是现代二十一世纪操作系统提供使用体验的核心技术组件。
- en: Of course, any new technology layer will have caveats. An LVM adds another layer
    of complication and more pieces for you, as the system administrator to understand.
    Learning to manage the LVM is hardly a huge undertaking, but it is quite a bit
    more to have to learn than if you do not have one. LVMs also introduce a small
    amount of performance overhead as they translate between the physical devices
    and the logical ones. Typically, this is a truly tiny amount of overhead, but
    it is overhead nonetheless.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，任何新技术层都会有一些限制。LVM 增加了一个额外的复杂层，也给系统管理员带来了更多的理解内容。学习管理 LVM 并不是什么大工程，但比起没有 LVM
    时需要学习的内容，它还是要多得多。LVM 还会带来一定的性能开销，因为它需要在物理设备和逻辑设备之间进行转换。通常，这个开销非常小，但依然存在。
- en: In general the benefits of an LVM dramatically outweigh the cost and more and
    more systems are starting to simply deploy an LVM layer without asking the end
    user whether or not they want it because increasingly functionality that customers
    simply expect an operating system to have depend on the LVM layer and allowing
    systems to be deployed without one often leaves customers stranded unclear why
    their systems do not live up to expectations and often in a way that they do not
    realize for months or years after initial deployment.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，LVM 的好处远远超过了成本，越来越多的系统开始直接部署 LVM 层，而不询问最终用户是否需要它，因为越来越多的功能已经依赖于 LVM 层，而不允许系统在没有
    LVM 的情况下部署，往往会让客户陷入困境，无法理解他们的系统为何无法达到预期，通常这种情况可能在初次部署后几个月或几年才会被发现。
- en: Like other forms of virtualization, storage virtualization and LVMs are most
    important for *protecting against the unknown*. If we knew everything about how
    a system would be used for its entire lifespan, things like resizing, backups,
    consolidation and so forth would have little value, but this is not how the real-world
    works.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 像其他形式的虚拟化一样，存储虚拟化和 LVM 最重要的是 *防止未知的风险*。如果我们知道一个系统在整个生命周期内的所有使用情况，诸如调整大小、备份、整合等功能就几乎没有意义，但现实世界并非如此。
- en: 'Best practices when it comes to an LVM is generally accepted to be: Unless
    you can positively provide solid technical reasons why the overhead of the LVM
    is going to be impactful and that that impact outweighs the protections that an
    LVM provides, always deploy an LVM.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 关于LVM的最佳实践通常认为是：除非你能明确提供充分的技术理由，证明LVM的开销会产生显著影响，并且这种影响超出了LVM提供的保护，否则始终部署LVM。
- en: Logical volume managers provide a critical building block to robust storage
    solutions and are, in many ways, what separates modern storage from classical
    computing systems. Understanding how logical volumes abstract storage concepts
    and let us manipulate and build storage to act like we want gives us many options
    and lead us to additional concepts such as RAID and RAIN, which we will discuss
    next that use LVM to construct data protection, expansion, and performance capabilities.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑卷管理器为强大的存储解决方案提供了一个关键构建块，在许多方面，它们将现代存储与传统计算系统区分开来。了解逻辑卷如何抽象存储概念，并使我们能够以我们想要的方式操作和构建存储，为我们提供了许多选择，并引导我们进入RAID和RAIN等附加概念，接下来我们将讨论这些概念如何利用LVM构建数据保护、扩展和性能能力。
- en: Utilizing RAID and RAIN
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RAID和RAIN
- en: We have looked at so many ways of interfacing with our storage. But probably
    the most exciting is when we start to deal with **RAID** (**Redundant Array of
    Inexpensive Disks**) and by extension its descendant, **RAIN** (**Redundant Array
    of Independent Nodes**). Before we go too far, it must be noted that RAID is a
    huge topic that would require a book of its own to truly address in a meaningful
    way. Understanding how RAID works and all of the calculations necessary to understand
    the nuances of its performance and risk is a major subject all on its own. My
    goal here is to introduce the concept, explain how it fits into a design, expose
    the best practices around it, and prepare you for further research.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过了许多与存储接口的方式。但最令人兴奋的可能是当我们开始处理**RAID**（**廉价磁盘冗余阵列**）及其后代**RAIN**（**独立节点冗余阵列**）时。在深入讨论之前，必须指出，RAID是一个庞大的话题，要真正有意义地讨论它需要一本书。理解RAID的工作原理以及理解其性能和风险细节所需的所有计算是一个独立的重大课题。我的目标是介绍这个概念，解释它如何融入设计，展示关于它的最佳实践，并为进一步的研究做好准备。
- en: RAID and RAIN are mechanisms for taking many *storage devices* (block devices)
    and using the natural device multiplicity (often misstated as redundancy) to provide
    some combination of improved performance, reliability, or scalability over what
    possibility with only an individual drive is. Like an LVM, RAID and RAIN are *mid-stack*
    technologies that consumer block device interfaces and provide block device interfaces
    and therefore can be *stacked* on top of an LVM, below an LVM, on top of another
    RAID, on top of a mixture of hardware devices, and so forth. Very flexible.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: RAID和RAIN是将多个*存储设备*（块设备）结合起来，利用设备的自然多重性（通常误称为冗余）来提供在单一硬盘中无法实现的性能、可靠性或可扩展性的一些组合。与LVM类似，RAID和RAIN是*中间层*技术，消费块设备接口并提供块设备接口，因此可以*叠加*在LVM之上，LVM之下，另一个RAID之上，或者在各种硬件设备的组合上，等等。非常灵活。
- en: In fact, RAID and RAIN are actually each a specialized form of LVM! No one,
    ever, talks about these technologies in this way and you will get some strange
    looks at the company Christmas party if you start discussing RAID as a specialized
    LVM, but it actually is. RAID and RAIN are extreme subsets of LVM functionality
    with a very tight focus. It is not actually uncommon for general purpose LVMs
    to have RAID functionality built into them and the trend with the integrated filesystems
    is to have the LVM and RAID layers both integrated with the filesystem.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，RAID和RAIN实际上是LVM的特殊形式！没有人会这样讨论这些技术，如果你在公司圣诞派对上开始讨论RAID作为一种特殊的LVM，你可能会引来一些奇怪的眼光，但它实际上就是。RAID和RAIN是LVM功能的极端子集，关注点非常狭窄。实际上，通用LVM通常会内置RAID功能，而且集成文件系统的趋势是将LVM和RAID层都与文件系统集成。
- en: RAID
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAID
- en: RAID standard for *Redundant Array of Inexpensive Disks* and was originally
    introduced as a set of technologies that work at the block device level to turn
    multiple devices into one. RAID-like technologies go way back to even the 1960s
    and the term and modern definitions are from 1988, which means that RAID actually
    pre-dates more general purpose LVM.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: RAID是*廉价磁盘冗余阵列*（Redundant Array of Inexpensive Disks）的缩写，最初作为一套在块设备级别工作的技术被提出，用来将多个设备合并为一个。类似RAID的技术可以追溯到1960年代，RAID这个术语和现代定义来自1988年，这意味着RAID实际上比更通用的LVM要早出现。
- en: RAID essentially takes an array of block devices and puts them into lockstep
    with each other under one of many different data storage regimes, called *levels*.
    Each RAID level acts different and uses a different mechanism to merge the underlying
    physical disks into a single virtual disk. By making multiple drives act as if
    they were a single drive, we can extend different aspects of the storage as needed
    but gaining in one area normally comes at a cost in another so understanding the
    way that RAID works is important.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: RAID本质上是将多个块设备放在一起，并通过许多不同的数据存储机制（称为*级别*）使它们保持同步。每个RAID级别的工作方式不同，并采用不同的机制将底层的物理磁盘合并成一个虚拟磁盘。通过使多个硬盘像一个硬盘一样工作，我们可以根据需要扩展存储的不同方面，但在一个领域的收益通常会带来另一个领域的成本，因此理解RAID的工作方式非常重要。
- en: RAID is an area where there is a very high importance on the system administrator
    having a deep understanding of the inner workings of the storage subsystem. And
    surprisingly it is an area where very few system administrators truly know how
    their system works.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: RAID是一个非常依赖系统管理员深入了解存储子系统内部工作原理的领域。令人惊讶的是，这也是一个极少数系统管理员真正理解其系统工作原理的领域。
- en: While RAID comes defined as a series of *levels*, but do not be fooled. The
    levels are simply different types of storage that share the underlying RAID basics.
    RAID levels do not really build on one another and a higher number does not represent
    some intrinsically superior product.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RAID被定义为一系列*级别*，但不要被迷惑。级别只是不同类型的存储，它们共享RAID的基本原理。RAID级别并不是相互递进的，较高的级别并不代表某种内在上更优越的产品。
- en: Because RAID is really a form of an LVM, it can sit anywhere in the storage
    stack and in the wild can be found almost anywhere. Some very popular RAID levels
    are actually *stacked RAID* leveraging this innate artefact of its design, most
    notably RAID 10.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因为RAID实际上是一种LVM，它可以位于存储栈中的任何位置，并且在实际应用中几乎无处不在。一些非常流行的RAID级别实际上是*堆叠RAID*，利用了其设计的这一固有特性，最著名的就是RAID
    10。
- en: RAID also comes in both hardware and software variants. Hardware RAID is much
    like a graphics card that connects directly to a monitor and offloads work from
    the main computer systems and talks to the hardware directly. A hardware RAID
    card does exactly this reducing load on the main computer system, interfacing
    to hardware storage devices directly, and potentially offering special features
    (like a cache) in the hardware. Software RAID, instead, leverages the generally
    much more powerful system CPU and RAM and has more flexible configurations. Both
    approaches are completely viable.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: RAID也有硬件和软件两种变体。硬件RAID很像一个直接连接到显示器的显卡，它将工作从主计算机系统中卸载，并直接与硬件通信。硬件RAID卡正是执行这一任务，减轻主计算机系统的负载，直接与硬件存储设备接口，并可能提供一些特殊功能（如缓存）。而软件RAID则利用通常更强大的系统CPU和内存，并具有更灵活的配置。两种方法都是完全可行的。
- en: Each RAID level has a unique set of properties and makes sense to be used at
    a different time. RAID is a complex topic deserving of its own tome to tackle
    properly. RAID is not a topic that we can look at too quickly, which has been
    a danger with storage in the past. RAID risk factors are often distilled into
    meaningless statements such as stating numerically *how many drives can a RAID
    array of level X recover from?* This means nothing and is meant as a way of simplifying
    something very complex into something that can simply be memorized or thrown onto
    a chart. RAID does not work this way. Each RAID level has a complex story around
    performance, reliability, cost, scalability, real world implementations, and so
    forth.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 每种RAID级别都有一组独特的属性，并且在不同的时刻使用各自有意义。RAID是一个复杂的话题，值得单独成册深入探讨。RAID并不是我们可以草率了解的一个话题，这曾经是存储领域的一个隐患。RAID的风险因素常常被浓缩成一些毫无意义的陈述，比如*一个RAID
    X级别的阵列可以从多少个驱动器故障中恢复？* 这并没有任何意义，它的目的只是将一个非常复杂的东西简化为可以轻松记住或者直接放在图表上的内容。RAID并不是这样的。每种RAID级别在性能、可靠性、成本、可扩展性、实际应用等方面都有一个复杂的背景故事。
- en: RAIN
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAIN
- en: Over time as systems became larger and more complex, the limitations with the
    RAID approach began to become apparent. RAID is simple and easy to implement.
    But RAID is inflexible and there are some key features like simple resizing, automated
    rebalancing, and flexible node sizing that it handles poorly. A new family of
    technologies were needed.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 随着系统变得更大、更复杂，RAID方法的局限性开始显现。RAID简单且易于实现，但RAID缺乏灵活性，对于一些关键特性，如简单的大小调整、自动重平衡和灵活的节点大小，它处理得很差。因此，需要一种新的技术家族。
- en: 'RAIN eschews the *full block device* approach to arrays that RAID was based
    on and instead breaks up storage by smaller chunks, often by blocks, and handles
    replication at that level. In order to do this effectively, RAIN must not just
    understand the concept of these blocks, but also the block devices (or *disks*)
    that they are on, but also the nodes in which they exist. This nodal awareness
    lends RAIN its moniker: *Redundant Array of Independent Nodes*.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: RAIN摒弃了RAID基于的*全块设备*阵列方法，改为通过较小的块（通常是块）将存储分割，并在该级别处理复制。为了有效地做到这一点，RAIN不仅必须理解这些块的概念，还必须理解它们所在的块设备（或*磁盘*），以及它们存在的节点。这种节点感知赋予了RAIN其名字：*独立节点冗余阵列*。
- en: Oddly, in RAIN it is not really the nodes that are necessarily redundant but
    really the blocks and you can actually implement RAIN on a single physical device
    to directly compete with traditional RAID in its simplest form, but this is rarely
    done.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 奇怪的是，在RAIN中，实际上并不是节点必须冗余，而是块，实际上你可以在单一物理设备上实现RAIN，直接与传统RAID的最简单形式竞争，但这很少见。
- en: Because RAIN handles block level replication it can have many advantages over
    RAID. For example, it can use different sizes devices rather fluidly. Drives of
    varying sizes can be thrown *willy nilly* into servers and tied together with
    great efficiency.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RAIN处理的是块级复制，因此相对于RAID，它可以有许多优势。例如，它可以灵活地使用不同大小的设备。不同大小的驱动器可以非常高效地被放入服务器并结合使用。
- en: Under RAID if a drive fails, we need a replacement drive that is capable of
    taking over its place in the array. This is often problematic, especially with
    older arrays. RAIN can avoid this issue by allowing any combination of available
    capacity across the array absorb the lost capacity of a failed hard drive.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAID下，如果一个硬盘驱动器发生故障，我们需要一个能够接替它在阵列中位置的替代硬盘。这通常是个问题，尤其是在旧的阵列中。RAIN可以通过允许阵列中的任何可用容量来吸收故障硬盘的丢失容量，从而避免这个问题。
- en: RAIN comes in such a variety of implementations, each one essentially unique,
    that we really cannot talk about it in any standard way. Most solutions are proprietary
    today and while a few well known open products have been made and have made their
    way into being standard components of the Linux ecosystem generally they are external
    to the distributions they behave much like proprietary products in how we must
    approach them.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: RAIN有多种实现方式，每种实现方式本质上都是独特的，因此我们无法以任何标准化的方式讨论它。今天大多数解决方案都是专有的，虽然一些著名的开源产品已经问世，并成为Linux生态系统的标准组件，但它们通常是独立于发行版的，且在我们使用时的行为很像专有产品。
- en: In the future we may see significant consolidation or at least standardization
    within the RAIN market as these technologies become more available and well understood.
    Until then we need to approach RAIN with the understanding of how block replication
    *could work* and know that each implementation may make drastically different
    design choices. RAIN might be built into the kernel or might exist as an application
    running higher in the stack, in some cases it could even run in a virtual machine
    virtualized on top of a hypervisor! How RAIN will react to a lost drive, to load
    balancing, locational affinity, rebalancing during loss, rebuild after repair,
    and so forth are not defined by any standards. To use RAIN, you must research
    and learn in depth about any solution that you will be considering and think critically
    about how its artefacts will impact your workloads over time.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，随着这些技术变得更加普及和被理解，我们可能会看到RAIN市场内的显著整合，或者至少是标准化。在那之前，我们需要以理解块复制*如何工作*的方式来接触RAIN，并知道每种实现可能做出截然不同的设计选择。RAIN可能内建在内核中，也可能作为更高层次的应用程序运行在堆栈中，在某些情况下，它甚至可能运行在虚拟机中，通过虚拟化在虚拟机监控器上运行！RAIN如何应对丢失的硬盘、负载均衡、位置亲和性、故障丢失后的再平衡、修复后的重建等问题并没有任何标准来定义。使用RAIN时，必须深入研究和学习你将考虑的任何解决方案，并批判性地思考其工件如何随着时间的推移影响你的工作负载。
- en: RAIN is almost guaranteed to be the future of system storage. As we move more
    and more towards clusters, hyperconvergence, cloud and cloud-like designs RAIN
    feels more and more natural. And adoption of RAIN will only increase as understanding
    increases. This simply takes time even though the technology itself is not new.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: RAIN几乎可以确定将是系统存储的未来。随着我们越来越多地朝着集群、超融合、云和类云设计发展，RAIN显得越来越自然。随着理解的增加，RAIN的采用将只会增加。尽管这项技术本身并不新颖，但这显然需要时间。
- en: Nearly every production system that we will ever design or support, will use
    some form of RAID or RAIN whether locally or remotely. By now, we are prepared
    to think about how the decision of what RAID level or configuration or what RAIN
    implementation is chosen will impact our systems. Taking the time to deeply understand
    storage factors in these multi-device aggregation frameworks interact is one of
    the most valuable high level knowledge areas for system administrators across
    the board. In our next section, we will build on these technologies to see how
    local storage can be made redundant with external systems, or nodes.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有我们将设计或支持的生产系统，无论是本地还是远程，都将使用某种形式的RAID或RAIN。到现在为止，我们已经准备好思考选择何种RAID级别、配置或RAIN实现会如何影响我们的系统。花时间深入理解这些多设备聚合框架中存储因素如何相互作用，是系统管理员在各个领域最有价值的高级知识之一。在下一部分，我们将基于这些技术，看看如何通过外部系统或节点使本地存储实现冗余。
- en: Learning about replicated local storage
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习复制本地存储
- en: Possibly the most critical storage type, and the least understood, is **replicated
    local storage** or **RLS**. RLS is not a difficult concept, it is quite simple.
    But there are many myths surrounding other concepts, such as SAN, that they have
    clouded the functionality of RLS. For example, many people have started using
    the term *shared storage* as a proxy for *external storage* or possible for *SAN*.
    But external storage does not mean that it is or can be shared, and local storage
    does not mean that it is not or cannot be shared.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是最关键的存储类型，也是最难理解的存储类型之一，就是**复制本地存储**或**RLS**。RLS并不是一个难懂的概念，它其实很简单。但围绕着其他概念（如SAN）有许多误解，这些误解模糊了RLS的功能。例如，许多人开始将*共享存储*作为*外部存储*或*SAN*的代名词。但外部存储并不意味着它是或可以被共享的，本地存储也不意味着它不能或不会被共享。
- en: The term replicated local storage refers to two or more computer systems which
    have local storage that is replicated between them. From the perspective of each
    computer system, the storage is locally attached, just like normal. But there
    is a process that replicates the data from one system to another, so that changes
    made on one appear on the other.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: “复制本地存储”这一术语指的是两个或多个计算机系统，它们之间的本地存储是相互复制的。从每个计算机系统的角度来看，存储是本地附加的，就像正常情况一样。但是有一个过程会将数据从一个系统复制到另一个系统，这样在一个系统上进行的更改就会出现在另一个系统上。
- en: Replicated local storage can be achieved in multiple ways. The simplest, and
    earliest for was to use **Network RAID**, that is RAID technology simply used
    over a network. **Mirrored RAID** (aka **RAID 1**) is the simplest technology
    for this and makes for the best example.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 复制本地存储可以通过多种方式实现。最简单且最早的方式是使用**网络RAID**，即RAID技术通过网络来使用。**镜像RAID**（也称为**RAID
    1**）是实现这一目标的最简单技术，也是最好的示例。
- en: There are two ways to handle this scenario, one is a hot/cold pair where one
    node is *hot* and has access to write to the storage and the other node(s) can
    read from it and, presumably, take over as the hot writeable node should the original
    hot node fail or relinquish control. This model is easy and is similar to many
    traditional models for shared storage on a SAN as well. This approach allows the
    use of regular (non-clustered) filesystems such as XFS or ZFS.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方式来处理这种情况，一种是热/冷配对，其中一个节点是*热*的，可以访问并写入存储，而另一个节点（或多个节点）可以从中读取，并且在原本的热节点失败或放弃控制时，假设其他节点能够接管成为热写节点。这种模型很简单，类似于许多传统的SAN共享存储模型。这种方法允许使用常规（非集群）文件系统，如XFS或ZFS。
- en: The other approach is a live/live system where all nodes replicating the storage
    can read and write at any time. This requires the same clustered filesystem support
    that we would need with any shared block storage. Just like with a SAN being used
    at the same time by two nodes, the nodes in an RLS cluster will need to communicate
    by storing their activity data in a special area of the clustered file system.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是活/活系统，其中所有复制存储的节点都可以随时读取和写入。这要求与任何共享块存储一样的集群文件系统支持。就像两个节点同时使用SAN一样，RLS集群中的节点将需要通过在集群文件系统的特殊区域存储其活动数据来进行通信。
- en: Replicated local storage can give us many of the benefits typically associated
    with a SAN or other external storage, namely the ability for multiple nodes to
    access data at once. While also having the benefits of locality including improved
    performance and resilience because there are fewer dependencies. Of course, the
    replication traffic has its own overhead, and this has to be considered. There
    are many ways that replication can be configured, some with very little overhead,
    some with a great deal.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 复制本地存储可以为我们带来许多通常与 SAN 或其他外部存储相关的好处，即多个节点可以同时访问数据的能力。同时，它还具有本地性的好处，包括提高性能和弹性，因为依赖关系较少。当然，复制流量有其自身的开销，这需要考虑。有许多方式可以配置复制，有些开销非常小，有些则开销很大。
- en: It is common to feel that replicated local storage is new or novel or in some
    way unusual. Nothing could be further from reality. In fact, what is rarely understood,
    is that for high reliability storage systems RLS is always used. Whether it is
    used locally (that is, directly attached to the compute systems) or if it is used
    remotely (meaning that the remote storage uses RLS to make itself more reliable),
    RLS is at the core of nearly any true high availability storage layer.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的误解是认为复制本地存储是新的、创新的或在某种程度上不寻常的。事实恰恰相反。实际上，鲜为人知的是，对于高可靠性的存储系统，RLS 始终被使用。无论是本地使用（即直接连接到计算系统），还是远程使用（意味着远程存储使用
    RLS 以提高其可靠性），RLS 都是几乎所有真正高可用存储层的核心。
- en: RLS comes in multiple flavours, primarily Network RAID and RAIN. We could call
    it Network RAIN when used in this situation, but we do not. Unlike RAID, which
    is nearly always local only, RAIN is nearly always used in an RLS situation and
    so the network nature of it is nearly assumed, or at least the network option
    of it.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: RLS 有多种形式，主要有网络 RAID 和 RAIN。我们可以在这种情况下称之为网络 RAIN，但我们并不这样做。与几乎总是仅限于本地的 RAID 不同，RAIN
    几乎总是在 RLS 环境中使用，因此它的网络特性几乎是默认的，或者至少是网络选项。
- en: RLS in so many forms on Linux that we cannot really talk about all of the options.
    We will have to focus on a few more common ones. RLS is an area where there are
    many open sources as well as commercial and proprietary solutions with a wide
    variety of performance, reliability, and features; and implemented in often very
    different ways. RLS can add rather a new level of complexity to any storage situation
    because you have to consider the local storage communication, the replication
    communication, any potential network communication between nodes and remote storage
    (that is local to another node), and how the algorithms and protocols interact
    with each other.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 上有许多形式的 RLS，因此我们无法真正讨论所有选项。我们将专注于一些更常见的选项。RLS 是一个领域，既有开源解决方案，也有商业和专有解决方案，性能、可靠性和功能各异，且实施方式常常截然不同。RLS
    可能会为任何存储场景增加新的复杂性，因为你必须考虑本地存储通信、复制通信、节点间和远程存储之间的潜在网络通信（即本地存储位于其他节点上）以及算法和协议如何相互作用。
- en: DRBD
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DRBD
- en: The first and simplest RLS technology on Linux is **DRBD** or the **Distributed
    Replicated Storage System**, which is a Network RAID layer baked right into the
    Linux kernel. Wikipedia states that DRBD is not *Network RAID*, but then describes
    it as exactly Network RAID. Whether it is or not might be little more than semantics,
    in practice it is indistinguishable from Network RAID in use, in practice, by
    description, and even under the hood. Like all RAID, DRBD consumes block devices
    and appears as a block device allowing it to be stacked anywhere in the middle
    of the storage stack just like regular RAID and LVMs.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 上的第一个也是最简单的 RLS 技术是**DRBD**，即**分布式复制存储系统**，它是一个直接集成到 Linux 内核中的网络 RAID
    层。维基百科表示 DRBD 不是*网络 RAID*，但随后又将其描述为完全的网络 RAID。它是否真的是网络 RAID 可能仅仅是语义问题，实际上在使用中，它与网络
    RAID 在使用、描述，甚至底层实现上没有区别。像所有 RAID 一样，DRBD 消耗块设备并作为块设备出现，允许它像常规 RAID 和 LVM 一样，堆叠在存储栈的任何位置。
- en: DRBD is built on a RAID 1 (mirrored RAID) mechanism and so allows for two to
    many nodes with each node getting a copy of the data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: DRBD 基于 RAID 1（镜像 RAID）机制构建，因此支持两个或多个节点，每个节点获取一份数据副本。
- en: DRBD is very flexible and reliable, and because of its simplicity it is far
    easier for most system administrators to understand clearly as to how it works
    and fits into the storage infrastructure. But because DRBD is limited to full
    block device replication by way of mirroring, as is RAID 1, the ability to scale
    is quite limited. On its own, DRBD is very much focused on classic two node clusters
    or very niche use cases with a large number of compute nodes needing to share
    a small amount of identical data.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: DRBD 非常灵活且可靠，由于其简洁性，大多数系统管理员更容易清楚地理解它是如何工作的，以及如何融入存储基础设施。然而，由于 DRBD 限于通过镜像进行完整块设备复制，就像
    RAID 1 一样，因此其扩展能力相当有限。单独使用时，DRBD 非常专注于经典的双节点集群或有大量计算节点需要共享少量相同数据的特定用途案例。
- en: Making DRBD flexible
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使 DRBD 更加灵活
- en: Because DRBD is just a software RAID tool, in effect, and because you have complete
    management of it, and because RAID acts as an LVM with total flexibility to sit
    anywhere in a stack, you can take DRBD and turn it into something far more scalable
    than it might first appear. But currently this process is all manual, although
    in theory you could script it or create tools to otherwise automate these kinds
    of procedures.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 DRBD 本质上只是一个软件 RAID 工具，而且因为你对其有完全的管理权限，并且因为 RAID 作为一个 LVM 技术，具备在堆栈中任意位置灵活部署的能力，你可以将
    DRBD 转变为比初看上去更具可扩展性的工具。但目前这个过程完全是手动的，尽管理论上你可以通过脚本或工具自动化这些过程。
- en: One powerful technique that we can use is the concept of *staggering* our RAID
    1 with extra logical block devices to mimic RAID 1E which operates essentially
    like RAID 1 but is scalable. This technique works by taking the physical storage
    space on an individual node and logically breaking it into two (or theoretically
    more) sections with an LVM technology. In a standard Network RAID setup, the entire
    space of the storage on node 1 is mirrored to the entire storage space on node
    2\. But now that we have split storage on each node, we mirror the first portion
    of node 1 to a portion of node 2; and node 2 does the same but with node 3; and
    node 3 does the same but with node 4; and this same process carries on indefinitely
    until whatever the terminal node number is does this with node 1 completing the
    *circle* and every machine has RAID 1 for its data, split between two other nodes
    as its mirrored pair. In this way, we can make a Network RAID 1 ring indefinitely
    large.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用的一项强大技术是通过额外的逻辑块设备来*错开*我们的 RAID 1，从而模拟 RAID 1E，其基本上类似于 RAID 1，但具有可扩展性。该技术的原理是将单个节点上的物理存储空间通过
    LVM 技术逻辑性地划分为两部分（或理论上更多）。在标准的网络 RAID 设置中，节点 1 上的整个存储空间会镜像到节点 2 上的整个存储空间。现在由于我们在每个节点上划分了存储空间，我们将节点
    1 的第一部分镜像到节点 2 的一部分；节点 2 也会这样做，但是是与节点 3；节点 3 同样与节点 4 进行镜像；这个过程将无限延续，直到终端节点编号的机器与节点
    1 完成*循环*，每台机器的数据都采用 RAID 1，并且数据在两个其他节点之间分割作为镜像对。通过这种方式，我们可以使网络 RAID 1 环扩展到无限大。
- en: This technique is powerful, to be sure. But it is extremely cumbersome to document
    and maintain. If you have a static cluster that never changes, it can work very
    well. But if you regularly grow or modify the cluster, it can be quite problematic.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技术确实非常强大。但它在文档编写和维护方面非常繁琐。如果你有一个静态的集群且不会变化，它可以非常有效。但如果你经常扩展或修改集群，它可能会成为一个相当棘手的问题。
- en: DRBD, and most Network RAID technologies, are typically blessed with good overall
    performance and, perhaps more importantly, rather predictable performance. DRBD,
    by its nature of presenting a final block device, is inherently local. In order
    to access DRBD resources remotely it would be necessary to use DRBD as a building
    block to a SAN device which would then be shared remotely. This is, of course,
    semantics only. DRBD is always local because to DRBD the SAN is the local compute
    node, the SAN interface is another layer higher up the proverbial stack and so
    while the SAN would be remote, DRBD would be local!
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: DRBD 和大多数网络 RAID 技术通常具有良好的整体性能，更重要的是，其性能相当可预测。由于 DRBD 本质上提供最终的块设备，它本质上是本地的。为了远程访问
    DRBD 资源，必须将 DRBD 作为构建块应用于一个 SAN 设备，然后将其共享到远程。这个过程当然只是语义上的问题。DRBD 始终是本地的，因为对 DRBD
    来说，SAN 就是本地计算节点，SAN 接口是堆栈中的另一个更高层次，因此虽然 SAN 是远程的，但 DRBD 仍然是本地的！
- en: Gluster and CEPH
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gluster 和 CEPH
- en: While two different technologies entirely, **Gluster** and **CEPH** are both
    free, open source, modern RAIN solutions designed for Linux that allow for high
    levels of reliability and high degrees of scalability. Both of these solutions
    at least offer the option of having storage be local to the compute node in question.
    Both are very complex solutions with many deployment options. We cannot simply
    assume that the use of either technology tells us that storage is going to be
    local or remote. Local is the more common application, by far, but both have options
    to directly build a remotely accessible and separate storage tier that is accessed
    over a network if designed to do so.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**Gluster** 和 **CEPH** 完全是两种不同的技术，但它们都是免费的、开源的现代RAIN解决方案，专为Linux设计，提供高可靠性和高度可扩展性。这两种解决方案至少都提供了将存储设置为与计算节点本地化的选项。这两者都是非常复杂的解决方案，具有多种部署选项。我们不能简单地假设使用这两种技术中的任何一种就意味着存储是本地的还是远程的。就应用来说，本地化是更常见的情况，但两者都提供了构建一个可以远程访问的独立存储层的选项，如果设计成这样，它可以通过网络进行访问。'
- en: These technologies are far too complex and full of options for us to dig into
    here. We will necessarily have to treat them at a very high level only, although
    this should be more than adequate for our needs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术过于复杂，选项也太多，因此我们无法在此深入探讨。我们只能从非常高的层次上对其进行处理，尽管这应该足够满足我们的需求。
- en: RAIN storage of this nature is the most common approach to handling large pools
    of servers (compute nodes) that will share a pool of storage. This technique gives
    the storage the opportunity to be local, to rebalance itself automatically in
    the event of a failure, but rarely will guarantee data locality. The storage across
    the group is a pool, only. So there can be an affinity for locality with data,
    but there is not the strict enforcement of locality as there is with DRBD. This
    gives more control to DRBD, but far more flexibility and better utilization with
    Gluster or CEPH.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这种RAIN存储是处理大规模服务器池（计算节点）共享存储池的最常见方法。该技术使存储能够本地化，在发生故障时自动重新平衡，但很少能够保证数据本地性。整个组中的存储只是一个池。因此，数据可能有本地化的亲和性，但不像DRBD那样严格强制本地化。这给了DRBD更多控制，但Gluster或CEPH提供了更多的灵活性和更好的利用率。
- en: Proprietary and third-party open-source solutions
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专有和第三方开源解决方案
- en: In addition to what comes baked in or potentially included with a Linux distribution
    are many third party components that you can install. Nearly all of these products
    will fall into the RAIN category and very in price, support, and capabilities.
    A few worth knowing the names of include *LizardFS*, *MooseFS*, and *Lustre*.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 除了操作系统中自带或可能包含的Linux发行版组件外，还有许多第三方组件可以安装。几乎所有这些产品都属于RAIN类别，价格、支持和功能各异。一些值得了解的名字包括*LizardFS*、*MooseFS*和*Lustre*。
- en: It is impossible to cover the potential range of commercial products that may
    be or may become available. RAIN storage is an area of current development and
    still many vendors make products in this space but do not make them widely available.
    In some cases, you can find commercial RAID or RAIN systems that are only available
    when included with appliances of one type or another or when included in some
    other project. But all of these storage systems follow the same basic concepts
    and once you know what those are and how they can work, you can make good decisions
    about your storage systems even if you have not necessarily worked with a specific
    implementation previously.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 不可能涵盖所有可能的商业产品，这些产品可能已经存在或未来可能出现。RAIN存储是一个正在发展的领域，仍然有许多厂商在这个领域生产产品，但并未广泛提供。在某些情况下，你可能会找到商业RAID或RAIN系统，这些系统只有在与某种类型的设备一起提供或在某些其他项目中包含时才能使用。但所有这些存储系统遵循相同的基本概念，一旦你了解了这些概念以及它们的工作方式，即使你之前没有使用过某个具体的实现，也能做出关于存储系统的正确决策。
- en: Virtualization abstraction of storage
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储的虚拟化抽象
- en: It is all too easy to get lost when talking about storage and forget that most
    of the time, storage is not even something that we have to worry about as system
    administrators! At least not in the way that we have been approaching it.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在谈论存储时，往往很容易迷失方向，忘记了大多数情况下，存储根本不是我们作为系统管理员需要担心的问题！至少不像我们一直以来处理存储的方式。
- en: The storage administrator
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储管理员
- en: It is not uncommon in larger organizations to decide to separate storage and
    systems duties as there are so many complexities and nuances to storage that having
    a team that is dedicated to understanding them can make sense. If you have worked
    in a Fortune 500 environment you have probably witnessed this.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型组织中，将存储和系统职责分开并不罕见，因为存储本身有许多复杂性和细微之处，成立一个专门团队来理解它们是有意义的。如果你曾在财富500强企业工作过，你可能见过这种情况。
- en: Some of the biggest problems with this come from separating the people who deeply
    understand the workloads from some of the most important factors that determine
    the performance and reliability of those workloads. Separation often also requires
    that core architectural decisions be made politically rather than technically.
    If you use local storage, you cannot separate the storage and systems teams in
    any realistic way. Because of this, many organizations have used often terrible
    technical design decisions to create skill silos within their organizations without
    considering how this would impact workloads. The deployment of SAN technology
    is quite often done for this purpose.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这一问题的最大困境之一在于，将深刻理解工作负载的人与一些决定这些工作负载性能和可靠性的重要因素分开。分离往往还意味着核心架构决策需要通过政治手段而非技术手段来做出。如果使用本地存储，就无法在任何实际情况下将存储和系统团队分开。因此，许多组织使用了通常糟糕的技术设计决策，在组织内创建技能孤岛，而没有考虑这种做法会如何影响工作负载。SAN技术的部署通常就是为了这个目的。
- en: Irrespective of the efficacy of this approach, when in use this generally means
    that storage is taken out of the hands of systems administrators. This simplifies
    our role dramatically while simultaneously cutting us off at the knees when it
    comes to being able to provide ultimate value. We can request certain levels of
    performance or reliability and must trust that our needs will be met or that we,
    at the very least, will not be held accountable for their failures.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 无论这种方法的有效性如何，实际使用时通常意味着存储脱离了系统管理员的控制。这极大地简化了我们的工作，同时在提供最终价值的能力上却使我们受到了很大的制约。我们可以请求某些性能或可靠性水平，并且必须信任我们的需求会得到满足，或者至少不会因存储失败而被追究责任。
- en: Similarly, it is common to separate systems and platform teams. In this case
    we see the same effect. The platform team, which manages the hypervisor underneath
    the systems, will provide storage capacity to the systems team and systems must
    simple consume what is made available to them.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，系统和平台团队分离也很常见。在这种情况下，我们看到了同样的效果。管理底层虚拟化的虚拟化平台团队将提供存储容量给系统团队，而系统团队则必须简单地使用提供给他们的资源。
- en: In both of these cases storage is abstracted from the system and provided simple
    as a *blind block device(s)* to us on the systems team. When this happens, we
    still have to understand how underlying components might work, which questions
    to ask, and at the end of the day still have to manage file systems on top of
    the provided block devices. The block device interface remains the universal *hand
    off* interface from a storage or platform team to the systems team.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，存储都从系统中抽象出来，并简单地作为*盲目块设备*提供给我们系统团队。当这种情况发生时，我们仍然需要理解底层组件的工作原理，知道该问哪些问题，最终仍然需要在提供的块设备上管理文件系统。块设备接口依然是存储或平台团队与系统团队之间的通用*交接*接口。
- en: 'An additional aside: the same thing will often happen to the platform team.
    They might have to take blind storage from a storage team, apply it to the hypervisor
    layer, then carve up that block device into smaller block devices to give to the
    systems team!'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个附带说明：同样的情况通常也会发生在平台团队。它们可能需要从存储团队获取盲目存储，应用到虚拟化层，然后将该块设备划分成更小的块设备，提供给系统团队！
- en: In most cases today, our Linux systems will be virtualized in some manner. We
    have to understand storage all of the way down the stack because Linux itself
    may be the hypervisor (such as in the case of KVM) or be used to control the hypervisor
    (as is the case with Xen) or provide storage to a higher-level hypervisor (like
    VirtualBox) and in all these cases it is Linux managing every aspect of the storage
    experience potentially. Linux may also be being used to create a SAN device or
    storage layer in some other form. We have to understand storage inside and out,
    but the majority use cases will be that our Linux systems will be getting their
    storage from their hypervisor even if we are the managers of that hypervisor.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 今天在大多数情况下，我们的Linux系统会以某种方式被虚拟化。我们必须全面了解整个堆栈的存储，因为Linux本身可能是虚拟化管理程序（例如KVM的情况），或者被用来控制虚拟化管理程序（例如Xen的情况），或者为更高级别的虚拟化管理程序（如VirtualBox）提供存储，在所有这些情况下，Linux都有可能管理存储体验的每个方面。Linux还可能被用来创建SAN设备或其他形式的存储层。我们必须全面了解存储的内外，但在大多数情况下，我们的Linux系统将从其虚拟化管理程序获取存储，即使我们是该虚拟化管理程序的管理者。
- en: While they can choose to behave in many different ways, most people set up hypervisors
    to act like an LVM layer for storage. They are a bit of a special case because
    they convert from block to filesystem, then back to block for the handoff to the
    virtual machine, but the concept remains the same. Some hypervisor setups will
    simply pass through a direct block connection to underlying storage whether a
    local disk, a SAN, or a logical volume from an LVM. These are all valid approaches
    and leave more options for the virtual machine to dictate how it will interact
    with the storage. But by and large having the block layer of storage terminate
    with the hypervisor, be turned into a filesystem, creating *block device containers*
    on top of the filesystem and allowing the virtual machines to consume those devices
    as regular block devices is what is expected from virtualization that many people
    actually refer to this artefact of virtualization approaches as being intrinsic
    to virtualization itself, which it is not.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它们可以选择以多种不同的方式行事，但大多数人会设置虚拟化管理程序以充当存储的LVM层。它们有点特殊，因为它们将数据从块转换为文件系统，然后再转换为块以传递给虚拟机，但概念上是相同的。一些虚拟化管理程序设置将简单地通过直接块连接到底层存储，无论是本地磁盘、SAN还是来自LVM的逻辑卷。这些都是有效的方法，并且为虚拟机留下更多选择，以决定它将如何与存储交互。但总的来说，预期的虚拟化方式是将存储的块层终止于虚拟化管理程序，将其转换为文件系统，在文件系统之上创建*块设备容器*，并允许虚拟机像使用常规块设备一样消耗这些设备，许多人实际上将这种虚拟化方法的结果称为虚拟化本身的内在特性，但这并不正确。
- en: You can use this technique inside of a system as well. Examples of this include
    mounting file system file types like *qcow2*, *vhdx*, or *iso* files! Something
    that we do every day, but rarely think about or realize what we are actually doing.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在系统内部使用这种技术。例如，挂载文件系统文件类型如*qcow2*、*vhdx*或*iso*文件！这是我们每天都在做的事情，但很少有人会去思考或意识到我们实际上在做什么。
- en: Obviously when getting our storage from the hypervisor, concerns about standard
    (non-replicated) local storage, replicated local storage, standard (non-replicated)
    remote storage, or replicated remote storage are all made at a different layer
    than the system, but the decisions are still made, and those decisions completely
    impact how our systems will ultimately run.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，当从虚拟化管理程序获取存储时，对标准（非复制）本地存储、复制本地存储、标准（非复制）远程存储或复制远程存储的关注都是在系统之外的不同层次进行的，但决策仍然是必须的，并且这些决策完全影响我们的系统最终如何运行。
- en: We have learned about a lot of storage abstraction approaches and paradigms
    now with LVMs, RAID, and RAIN. Now we need to start to think about how we will
    put these technologies together to build our own storage solutions.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了很多存储抽象方法和范式，比如LVM、RAID和RAIN。现在我们需要开始考虑如何将这些技术结合起来构建我们自己的存储解决方案。
- en: Analyzing storage architectures and risk
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析存储架构和风险
- en: Nothing creates more risk for our systems than our storage. That should go without
    saying, but it has to be said. Storage is where we, as system administrators,
    have our greatest opportunity to make a difference, and it is the place where
    we are mostly likely to fail and fail spectacularly.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么比存储给我们的系统带来更大的风险了。这是不言而喻的，但必须要说。存储是我们作为系统管理员能够产生最大影响的地方，也是我们最有可能失败并且失败惨重的地方。
- en: In order to address risks and opportunities in regard to storage, we must understand
    our entire storage stack and how every layer and component interact with each
    other. Storage can feel overwhelming, there are so many moving pieces and optional
    components.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对存储中的风险和机会，我们必须理解整个存储堆栈，以及每一层和每个组件如何相互作用。存储可能让人感到压倒性，那里有太多的活动组件和可选组件。
- en: We can mitigate some of the overwhelming feelings by providing design patterns
    for success and understanding when different patterns should be considered.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过提供成功的设计模式，并理解在何种情况下应该考虑不同的模式，来缓解一些压倒性的感觉。
- en: General storage architectures
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一般存储架构
- en: 'There are two truly high-level axis in **storage architecture**: *local* versus
    *remote*, and *standard* versus *replicated*.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 存储架构有两个真正高层次的维度：**本地**与**远程**，以及**标准**与**复制**。
- en: Of course, the natural assumption for most people is to jump immediately to
    believing that replicated and remote are the obvious starting point. This is actually
    not true. This is probably the least sensible starting point for storage as it
    has the least likely to be useful combination of factors.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对于大多数人来说，自然的假设是立刻相信复制和远程是显而易见的起点。实际上，这并不正确。这可能是存储中最不合理的起点，因为它结合了最不可能有用的因素。
- en: 'Simple local storage: The brick'
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单的本地存储：砖块
- en: Believe it or not, the most commonly appropriate storage design for companies
    of all sizes is local, **non-replicated storage**! Bear in mind that *replicated*
    when we speak of storage architectures does *not* reference a lack of backups
    nor a local of local replication (such as RAID mirroring) but only refers to whether
    or not storage is replicated, in real or near-real time, to a second totally separate
    system.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 不管信不信，对于各类公司的最常见适用存储设计是本地的、**非复制存储**！请记住，当我们谈论存储架构时，*复制*并不指没有备份或没有本地复制（如RAID镜像），而仅仅指存储是否在实时或近实时的情况下复制到第二个完全独立的系统。
- en: We will cover this again, and slightly differently, when we look at total system
    design rather than looking at storage in isolation. Like most things in life,
    keeping things simple generally makes the most sense. Replication sounds amazing,
    a must have feature, but replication costs money and often a lot of it and to
    do replication well often impacts performance, potentially dramatically.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从整个系统设计角度来看，而非单独看存储时，我们会再次讨论这个问题，并且会有些不同。就像生活中的大多数事情一样，保持简单通常是最有意义的。复制听起来很惊人，是必备功能，但复制需要花费大量资金，且通常需要很多成本，而且要做好复制，通常会影响性能，可能影响得非常严重。
- en: Replicating disaster
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 复制灾难
- en: A common mistake made in storage design is getting an emotional feeling that
    the more replication that we have, the more that we are shielded from disaster.
    To some degree this is true, replicated some files locally with RAID 1 does a
    lot to protect against an individual hard drive failing and remote replication
    can protect against an entire node failing, but neither does anything to protect
    against much more common problems like accidental file deletion, malicious file
    destruction, file corruption, or ransomware.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 存储设计中的一个常见错误是产生一种情感上的感觉，认为复制越多，我们就越能免受灾难的影响。在某种程度上，这是真的，本地复制一些文件并使用RAID 1能有效保护我们免受硬盘故障的影响，远程复制则能保护我们免受整个节点失败的影响，但这两者都无法防止更常见的问题，如文件误删除、恶意文件破坏、文件损坏或勒索病毒。
- en: If we do something simple, like delete a file that we shouldn't, then instantly
    our high-power replication mechanism will ensure that our deletion ripples through
    the entire system in a millisecond or two. Instead of protecting us, it might
    be the mechanism that replicates our mistake faster than we can react. Overbuilding
    replication mechanisms typically protects only against hardware failure and can
    quickly turn into a situation of diminishing returns.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们做一些简单的事情，比如删除一个不该删除的文件，那么瞬间，我们强大的复制机制将确保我们的删除操作会在一两毫秒内传播到整个系统中。它可能不是保护我们，而是成为比我们反应更快地复制我们的错误的机制。过度构建复制机制通常只对抗硬件故障有保护作用，并且可能迅速变成回报递减的局面。
- en: That first level of RAID might be highly valuable because hard drive failure
    remains a very real risk and even the tiniest drive hiccup can cause significant
    data loss. But replicating between nodes will only protect against entire system
    loss which is quite a bit less common. RAID protection is relatively cheap, often
    costing only a few hundred dollars to implement. Nodal replication, however, will
    require dramatically more hardware to achieve replication generally costing thousands
    or tens of thousands of dollars for a fraction of the protection that RAID is
    already providing.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Mechanisms like RAID, especially RAID 1 (mirroring) are also extremely simple
    to implement and very straightforward. It is quite uncommon to encounter data
    loss caused by human error in mirrored RAID. The same cannot be said for replicated
    storage between nodes. There is far more to go wrong, and the chances of human
    error causes data loss is much higher. We do not simply mitigate risks by choosing
    to go this expensive route, we introduce other risks that we have to mitigate
    for as well.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Many system administrators feel that they cannot use simple, local, non-replicated
    storage, and problems with company politics cannot be overlooked. If your company
    is going to *play politics* and blame you, as the system administrator, even when
    the mistake is not yours and your decision was the best one for the business,
    then you are forced to make dangerous decisions that are not in the interest of
    the profitability of the business. That is not something that a system administrator
    can control.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: As a system administrator, we can manage this political problem *in some cases*
    by presenting (and documenting well) risk and financial decisions to demonstrate
    why a decision that may ultimately have led to data loss to have still been the
    correct decision. No decision can every eliminate all risks, as IT professionals
    and especially as system administrators we are always making the decision as to
    how much risk to attempt to mitigate and at what financial cost should we do so.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Risk assessments
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the hardest, yet most important, aspects of IT and especially systems
    administration is doing risk assessments to allow for proper planning. Risk is
    rarely taught either formally or organically. This is an area where nearly all
    businesses fail spectacularly and IT, where risk is absolutely key to everything
    that we do, is generally left with no training, no resources, and no support.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Teaching risk is a career in and of itself, but a few techniques that we should
    be using all of the time can be covered here. At its core, risk is about assigning
    a cost that we can apply against projected profits.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: We have two key aspects of risk. One is the chance that something bad will happen,
    the second is the impact of that event. The first we can express as a matter of
    *happens X times per year* if you find that handy. The second can be expressed
    in monetary terms such as *it will cost $5,000*. If something will happen once
    a decade then you could say it is .1x per year. So, something that impacts us
    for five grands would have an annual cost of $500\. This is ridiculously simplistic
    and not really how risk works. But it's an unbelievable useful tool in expressing
    risk decisions to management where they want millions of factors distilled to
    a single bottom line number.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Now we have to take our numbers that show the cost of a risk mitigation strategy.
    For example, if we are going to implement a replication technology that costs
    $300/year in licensing and requires ten hours of system administration time per
    year at $120/hour as can project a cost of mitigation to be $1500/year.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Next weeks need a mitigation effectiveness. Nothing is really one hundred percent.
    But a good replication strategy might be 95% or a typically one might be around
    65% effective. With these numbers we can do some hard math.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: We know that we are at risk of losing roughly $500 per year. If we spend $1500
    per year, we can 95% surely stop the $500 loss. $500 * .95 = $475\. So, take $1500-$475=$1025
    of loss per year caused by the risk mitigation strategy. These are numbers you
    can take to a CFO. Do this math, you should be able to show savings or protection,
    not a loss. If you show a loss then you really, really need to avoid that plan.
    It means that the risk protection mechanism is, for all intents and purposes,
    representative of a *disaster* simply by implementing it.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Math, it might sound trite to say, but the average system administration and
    even the average CFO will often run from using basic math to show if an idea is
    good or bad and go purely on emotion. Using math will protect you. It means you
    can go to the CFO and CEO and stand your ground. You cannot argue with math. Show
    them the math, if they decide that the math is wrong, have them work the numbers.
    If they decide to ignore the math, well, you know what kind of organization you
    work for and you should really think long and hard about what kind of future there
    is at a company that thinks that profits are not their driving factor. And if
    something goes wrong in the future and you get blamed, you can pull out the math
    and ask, *if this was not the right decision, why did we not see it in the math
    when we made the decision?*
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Nothing feels better that defending successfully a seemingly crazy decision
    that has been backed by math. Show that you are doing the best job possible. Do
    not just say it, do not make unsubstantiated claims. Use math and prove why you
    are making decisions. Elevate the state of decision making from guesswork to science.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Not all workloads can simple be treated this simply. But vastly more than are
    normally assumed can. This should be your standard assumption unless you have
    solid math to show otherwise. Or you are dealing with a situation that goes beyond
    math, such as life support systems where uptime and reliability are more important
    than money can every describe. Otherwise, use math.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: This simplest of all storage approaches is easily thought of as *the brick*.
    It is simple, it is stable, it is reliable, it is easy to backup and restore,
    it is easy to understand and support. This solution is so effective today, especially
    with modern storage technologies and hardware, that I will state that it is appropriate
    for 85-90% of all production workloads, regardless of company size, and at least
    99% of small business workloads.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'RLS: The ultra-high reliability solution'
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What workloads and situations do not make sense for the simple architecture
    above almost always fall into this category: **replicated local storage** (**RLS**).
    RLS allows for highly available storage with great performance at reasonable cost.
    Nothing will match the performance and cost of straight local storage that we
    mentioned first, but if you need higher availability and better reliability than
    that solution can provide, this is almost certainly the solution for you.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: RLS provides the highest level of reliability available because it has fewer
    moving parts. A remote high availability solution must necessarily deal with distance,
    cabling, and network protocols as additional components at a minimum, and typically
    with networking equipment (like switches) additionally, all above and beyond the
    risks of RLS. Remember that remote storage solutions wanting to accomplish true
    high availability are going to have to do so by using RLS locally in their own
    cluster, and then making that cluster of storage available remotely over a network
    so you have all of the complications and any potential problems with RLS, plus
    the overhead and risks of the remote access technology.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: If straight local storage with no remote replication takes ninety percent of
    all workloads, standard RLS much take ninety percent of what remains (the two
    should take about ninety nine percent together.) When doing proper planning, these
    two options are so simple, cost effective, and safe that they are just impossible
    to beat under normal circumstances. These are your break and butter options.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Alternative storage reliability
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While RLS might feel like the absolute end all, be all of storage protection,
    it really is not. It is great when you have to rely on the storage layer to handle
    reliability. But in that regards, it is a kludge or a band aid, not the ultimate
    solution.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: In a perfect world we have storage mechanisms that are a layer higher than our
    actual storage layer with things like databases. A database management system
    is able to do a much better job at maintaining availability and coordinating data
    replication between nodes than a blind block replication mechanism can ever do.
    Putting replication where the application intelligence is just makes sense.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, applications would use their databases as their layer for storage and
    state reliability and let intelligence systems that know how the data is used
    replicate what matters. Databases are one of the most ideal mechanisms for replication
    because they know the data that they have and are able to make good decisions
    about it.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, many enterprise applications do not use any form of storage
    or even systems replication whatsoever. Using *unreliable* systems and highly
    reliable *applications* is a solid strategy and offers benefits that you can get
    in no other way. Because of this, we can sometimes ignore high availability needs
    at the raw storage layer and just focus on performance.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'The lab environment: Remote shared standard storage'
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This architecture is very popular because it checks all of the boxes for salespeople:
    *expensive*, *risky*, and *confusing*. It is true, salespeople push this design
    more than any other because it generates so many ways to bill the customer for
    additional services while passing all responsibility on to the customer.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'All architectures have a legitimate place, more or less, in an ecosystem and
    this one is not an exception. But before we state a use case we should state where
    its benefits lie: non-replicated remote storage is *slower*, *riskier*, and more
    *expensive* (on the surface) than local storage mechanisms. Where do we find value
    for such a design?'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Primary the benefits are in lab, testing, and other environments where the value
    to data durability is negligible, but we can benefit from large environments where
    storage can be carved out judiciously to create maximum cost savings at a large
    scale.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Nearly every large environment has a need for this kind of storage at some point
    in their collection of workloads. The key is to identify where this mix of needs
    is sensible and not to attempt to apply it where it is inappropriate. Because
    this kind of storage is cheap at large scale (but outrageous expensive at small
    scale) and because managers so often mistake remote shared storage for the *magic
    box that cannot fail* it is often used where it is least applicable. There is
    no magic, it is a poor technology for most production workloads and should be
    chosen only with extreme caution.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The rule of thumb here is that you should only use this type of storage if you
    can decisively prove that it makes sense - that even standard reliability and
    performance are not valuable. If you have any doubts, or if the organization has
    any doubts, then choose a different storage architecture before making a mistake
    with this one means maximum risk. Performance is a *soft failure*, it is easy
    to correct after the fact and it normally has marginal impact if you get it wrong.
    Data loss is a *hard failure* where getting it wrong is not a graceful failure
    but a catastrophic failure and there is little ability to correct it later.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Fail gracefully
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: We cannot always avoid failure. In fact, much of the time skirting failure is
    a critical part of our job. In order to make this work we have to understand how
    to fail well and a key component of that is the idea of *failing gracefully*.
    And in storage, this is an area where this concept is more pronounced than in
    other areas.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: The idea with failing gracefully is that if we fail, we want it to be in a small,
    incremental way rather than in a tragic, total disaster kind of way. Many storage
    decisions that we make are designed around this. We know that we might get things
    wrong. So, we want to make sure that we are as close to correct as possible, but
    also with taking into consideration *what if* we are wrong.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: In this way we tend heavily towards RAID 10 in RAID and towards local storage
    in architecture. We want solutions that, if we are wrong, result in us being too
    safe rather than losing the data because we thought that it would not matter.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: While it takes a lot more than just bad storage decisions to determine an entire
    architecture, remote non-replicated storage is the foundation point of the popular
    design used by vendors and resellers which they typically call a **3-2-1 architecture**
    and what IT practitioners called the **Inverted Pyramid of Doom**.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: This architecture is traditionally deployed broadly but is by no small margin
    the least likely architecture to ever me appropriate in a production environment.
    It is slow, it is complex, it is costly, and it carries the highest risks. It
    makes sense primarily in lab environments were recreating the data being stored
    is, at most, time consuming. This is an architecture truly designed around the
    needs of typically non-production workloads.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'The giant scale: Remote replicated storage'
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our last main architecture to consider is the *biggest* of them all. Remote
    replicated storage. It might see like this would be the storage architecture that
    you would see in every enterprise, and while not exactly rare, it is seen far
    less commonly than you would guess.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Remote replicated storage is the costliest to implement at small scale but can
    become quite affordable at large scale. It suffers from extra complexity over
    RLS (which means less reliable) and lower performance than either RLS or straight
    local storage and so only makes sense when cost savings is at a premium, but a
    certain degree of reliability is still warranted. A bit of a niche.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Considering that the two local storage architectures were granted ninety nine
    percent of workload deployments (by me, of course) between them, this architecture
    gets most of the last percentage that is left, at least in production.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: The safest system is only so safe
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of my more dramatic stories from my decades as a system administrator comes
    from a time that I was working at a university library as a consultant. I was
    brought in to work on some large-scale databases in this two-admin shop. The senior
    admin was out on vacation and only the junior admin was still around. The environment
    used a highly redundant and high availability SAN system on which all systems
    depended. There was an extreme amount of protection of this system from UPS to
    generators to hardware redundancy all at great expense.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: While I was there, the junior admin decided to do some maintenance on the SAN
    and, for whatever reason, accidentally clicked to delete all of the volumes on
    the SAN. This was, of course, an accident but a very careless one by someone who
    was assuming that every possible failure scenario was carefully guarded against.
    But all of the high availability, all of the redundancy, all of the special technology
    did nothing to address simple human error.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: With one click of her mouse, the entire library's computer systems were gone.
    The applications, the databases, the users, the logs. All of it. Everything was
    dependent on a single storage system and that system could be turned off or, in
    this case, deleted with essentially no effort by someone with access. All of the
    eggs were in a single backet that, while made very sturdily, had a big opening
    and could easily be turned upside down.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: To make matters far, far worse the junior system administrator was not emotionally
    prepared for the event and was so terrified of losing her job that she had to
    be hospitalized and all of the potential IT staff that might have been able to
    have stepped in to assist were instead engaged in getting her an ambulance. Through
    poor technological planning, and through poor human planning, an easily avoidable
    disaster that should have only turned into a minor recovery disaster turned into
    a huge outage. Luckily there were good backups, and the system was able to be
    restored relatively quickly. But it highlighted well just how much we often invest
    in protecting against mechanical failure and how little we address human frailty
    and, in reality, it is human error that is far more likely to be the cause of
    a disaster than machines failing.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '*Storage architectures* and *risk* is the hardest part of storage design. Drilling
    down into filesystem details is generally fun and carries extremely little risk
    to us as system administrators. If we pick EXT4 when BtrFS would have been best,
    the penalty is probably nominal, so much so that we would never expect anyone
    to ever find out that we did not make the perfect decision. But choosing the wrong
    storage architecture could result in massive cost, large downtime, or big time
    data loss.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: We really have to take the time to understand the needs of our business and
    workloads. How much risk is okay, what performance do we need, how do we meet
    all needs at the optimum cost. If we do not know the answers then we need to find
    out.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Best practice is, as always, to take the time to learn all business needs, learn
    all of the available factors and apply. But that is very hard to do in practice.
    Some rules of thumb to assist us will come in very handy.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Storage best practices
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Attempting to distill storage into **best practices** is rather hard. At the
    highest level, the fundamental rule to storage is that there are no shortcuts,
    you need to understand all aspects of the storage infrastructure, understand the
    workloads, and apply that combined knowledge allow with an understanding of institutional
    risk aversion to determine workload ideals.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'Going further, best practices include:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'RAID: If data is worth storing, it is worth having on RAID (or RAIN.) If you
    are questioning the value of having RAID (at a minimum) on your servers, then
    reconsider storing the data at all.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RAID Physicality: Both hardware and software RAID implementations are equally
    viable. Determine what factors apply best to your needs.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LVM: Like general virtualization which we will touch on in future chapters,
    storage virtualization is not about providing a concrete feature set that we need
    on day one. It is about providing mechanisms to protect against the unknown and
    to be flexible for whatever happens in the future. Unless you can present an incredible
    strong argument for what LVM is not needed, use it.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Filesystem: Do not be caught up in hype or trends. Research the features that
    matter to you today and that are likely to protect you against the unknown in
    the future and use a filesystem that is reliable, robust, and tested where you
    can feel confident that your filesystem choice is not going to hamper you long
    term.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Storage Architecture: Unless you can prove that you require or financially
    benefit significantly from remote storage, keep your storage access local. And
    unless you can demonstrate clear benefit from nodal replication, do not replicate
    between nodes. Simple trumps complex.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a system administrator you might deal with storage design decisions infrequently.
    But no matter how infrequent, storage design decisions have some of the most dramatic
    impacts on our long-term system compared to any other decisions that we make.
    Take your time to really determine what is needed for every workload.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Storage example
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should step back and put together an example of how these pieces might fit
    together in a real life scenario. We cannot reasonably make examples for every
    common, let alone plausible, storage scenario but hopefully we can give a taste
    of what we are talking about in this chapter to make it all come together for
    you.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: To keep things reasonably simple, I am going to work as generically as possible
    with the absolutely most common setup found in small and medium businesses. Or
    at least what probably should be the most common setup for them.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Smaller businesses generally benefit from keeping their designs quite simple.
    Lacking large, experienced staff and often at high risk from turnover, small businesses
    need systems that require less maintenance and those that can easily be maintained
    by consultants or staff that may not possess tribal knowledge of the environment.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: For these kinds of environments, and also for many larger ones, hardware RAID
    with hot and blind swappable drives are important. They allow hardware maintenance
    to be done by bench professionals without a necessity to engage systems administration
    tasks. This becomes extremely critical when dealing with colocation or other distant
    facilities. In these cases, someone from the IT department may have no way to
    be physically involved at all.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: So, we start with some general case assumptions. At the physical layer we have
    eight hard drives. These can be spinning drives, SSD, NVMe, any block device.
    It does not really matter. What matters is that we have multiple, but we want
    them to all act as a single unit.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: So, we add to this a hardware RAID controller. This RAID controller we use to
    attach all of the drives and set it to put them into an appropriate RAID level.
    This could be any number of options, but for this example we will say that they
    are in RAID 10.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: From the very beginning of our example, without having even installed Linux
    or anything of the sort, we have used the hardware of our system to implement
    our first two layers of storage! The physical devices, and the first abstraction
    layer.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: We will not show actually inserting the drives here, that is purely a manual
    process and chances are your server vendor has done this already for you, anyway.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: As for setting up the RAID itself, every controller and vendor is slightly different,
    but the basics are the same and the task is always extremely simple. That is much
    of the point of the RAID controller - to reduce the amount of knowledge and planning
    necessary around RAID operations to the bare minimum both up front and during
    operational phases. For our example here, to make things easier to demonstrate,
    we are going to assume that we are dealing with a single array that is *not* the
    array from which our operating system is running so that we can show some of the
    steps more easily from the command line. But this is just an example.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Remember that hardware RAID is different for every vendor and potentially every
    product by a vendor. So you always need to be aware of exactly how your specific
    product works.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we should note that to the RAID controller, each drive attached to it
    is a block device. This is the unique case where the block device interface, pretending
    to be a physical hard drive, is actually and truly a hard drive! In every subsequent
    case we will be using software to implement the interface of a hard drive but
    the drive that we represent will be logical, not physical:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is a real syntax for a real-world RAID controller. Typically, you will
    do this task graphically from a GUI. But sometimes you will want to use a command
    line utility. When possible, I work from the command line, it is more repeatable
    and far easier to document.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Once we are past this phase of initial hardware configuration then we can proceed
    with the Linux specific portions of the example.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Hardware RAID controllers typically create their own naming conventions in the
    /dev filesystem. In our example case, the controller uses the `cciss` syntax and
    created device `c0d1` under that system. All of these will vary depending on the
    control and configuration that you use.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we are going to create a logical volume layer on top of the RAID layer
    to allow us more flexibility in our storage system. To do so we must start by
    adding the newly created device to Linux'' LVM system as *a physical device*.
    We do this with the `pcvreate` command and the path to our new device:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Very fast and easy. Now the LVM subsystem is aware of our new RAID array device.
    But of course, all that LVM knows and cares about is that it is a block device.
    That it is a RAID array specifically is actually not something that LVM can detect,
    nor does it matter. The point here is that it is abstracted and can be utilized
    the same no matter what it is. The speed, capacity, and safety characteristics
    are encapsulated in the RAID layer and now we can think of it purely as a hard
    drive as we move forward.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting point here is that when using a hardware RAID controller
    this abstract and virtualized hard drive representation is only a logical hard
    drive, but as a block device it is actually physical! Mind blowing, I know. It's
    really hardware, it just is not a hardware hard drive. Ponder on that for a moment.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'How that our RAID array is under LVM''s management we can add the drive to
    a volume group. In this example we will be adding it to a new volume group that
    we are creating just for the purposes of this example. We name this new group
    `vg1`. Here is an example command doing just this:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Okay, now we are getting somewhere. The capacity of the individual physical
    hard drives being combined by the RAID controller into a single logical drive
    under LVM control is now in a capacity pool or *volume group* where we can start
    to carve out actually useful subsets of that capacity to use on our server.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'With the volume group created, all that is left is to make the actual *logical
    volumes*. Remember that logical volumes have replaced *partitions* as the primary
    means of dividing a block device into consumable portions that we can use. For
    our example we are going to do the absolute simplest thing and tell the LVM system
    to make just one logical volume that is as large as possible; that is, using 100%
    of the available capacity of the volume group (which is currently at 0% utilization
    as this is the very first thing that we will have done with it.):'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This command tells LVM to create a new logical volume, using 100% of the free
    space that is available, in volume group `vg1`, and name the new logical volume
    `lv_data`. That is it. We now have a logical volume that we can use! It should
    be obvious that we could have made a smaller logical volume, say of 50% of the
    available space, and then made a second one, also of 100% of what was remaining
    after that to give us two equal sized logical volumes.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Remember that an LVM system like the one found here in Linux, gives us flexibility
    that we would often lack if we were to apply a filesystem directly to the physical
    drives or even to the virtual drive presented by the RAID controller hardware.
    The LVM system lets us add more physical devices to the volume group, for example,
    which gives us more capacity for making logical volumes. It will let us resize
    the individual logical volumes, both growing or shrinking them. LVM will also
    allow us to snapshot a logical volume which is very useful for building a backup
    mechanism or preparing to do a risky system modification so that we can revert
    quickly. LVM does very important things.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that `lv_data` has been created we will need, in most cases, to format
    it with a filesystem in order to make it truly useful. We will format with XFS.
    In the real world today, XFS would be the most likely to be recommended filesystem
    for general purpose needs:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Very simple. In a few seconds we should have a fully formatted logical volume.
    In applying the filesystem format we stop the chain of block device interfaces
    and now present a filesystem interface which is the change that allows applications
    to use the storage in a standard way instead of using block devices as are used
    by storage system components.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, one last step is necessary, we have to mount the new filesystem
    to a folder to make it usable:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That is it! We just implemented a multi-layer abstraction based storage system
    for one of the most common system scenarios. We have built an XFS file system
    on top of LVM logical volume management on top of a hardware RAID controller on
    top of multiple individual physical hard drives.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Because each layer uses the block device interface, we could have mixed and
    matched so many more additional features. Like using two RAID controllers and
    merging their capacity with the volume group. Or making multiple volume groups.
    We could have made many logical volumes. We could have used software RAID (called
    MD RAID in Linux) to create RAID using the output of the two RAID controllers!
    The sky is really the limit, but practicality keeps us grounded.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: At this point if you *cd /data* you can use the new filesystem just as if it
    has always been there. That it is a new filesystem, that it is built on all these
    abstraction layers, that there are multiple physical devices making all of this
    magic happen is completely hidden from you at this point. It just works.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Now in the past, if this was 2004, we would generally stop here and say that
    we have described what a real world server is likely going to look like if it
    is implemented well. But this is not 2004 by a long shot and we really need to
    talk more about how we are likely going to see our storage used in the most common
    scenarios. So today we need to think about how our virtualization layers are going
    to use this storage, because things get even more interesting here.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: We will assume that our /data filesystem that we just created will be used to
    store the drive images of a few virtual machines. Of course, these drive images
    are just individual files that we store in the filesystem. Very simple. No different
    than creating and storing a text file in /data (except VM drive images tend to
    be just a tad larger.)
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: What is neat about a drive imagine (this could be a QCOW, VHD, ISO, or other)
    is that they sit on top of a filesystem but, when opened by a special driver that
    is able to read them, they present a block interface again! That is right. We
    have gone from block to block to block to block to filesystem to block again!
    In some unique cases we might not even use a hypervisor but might use this new
    block device file somewhere in our regular server. Windows does this commonly
    with VHD files as a way to pass data around in certain circumstances. MacOS does
    this as their standard means of creating installer packages. On Linux it is far
    less common but just as available.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: But assuming that we are doing something normal, we will assume that we are
    running a hypervisor, KVM almost certainly, and that the virtual machines that
    are going to run on KVM are going to use disk image files storage on our newly
    minted file system. In this case, much of what we have done here is likely to
    happen yet again inside of that virtual machine.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Some portions would not be very sensible to recreate. The physical drives are
    already managed by a physical RAID controller. The speed, capacity, and reliability
    of the storage is already established by that system and does not need to be duplicated
    here. The standard approach is for a single drive image file to be presented to
    the operating system running in a virtual machine as a single block device. No
    different than how our operating system was presented with the block device from
    the hardware RAID controller.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Now inside of the virtual machine we will often run through the same exercise.
    We add the presented block device as an LVM physical volume. Then we add that
    physical volume to a volume group. Then we carve out one or more logical volumes
    from that volume group. We then format that logical volume with our filesystem
    of choice and mount it. Of course, typically much of that is not done by hand
    as we have done here but rather than the installation process automating much
    of it.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: We can add more steps such as using MD RAID. Or we can use fewer, such as by
    skipping LVM entirely. We could do all of the same steps with just a single hard
    drive and no RAID controller. This would be far less powerful physically, but
    all of the examples would work the same at a technical level. We could use VLM
    on the physical machine but not in the virtual machines, or vice versa! The flexibility
    is there to do what is needed. It is all about understanding how block devices
    can be layered, how a filesystem goes on a block device and how block files can
    turn a filesystem back into block devices!
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Abstraction and encapsulation are amazingly powerful tools in our IT arsenal
    and rarely are they so tangible.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have survived to the end of this chapter and are still hanging in with
    me, congrats, we made it! Storage is a big deal when it comes to systems administration
    and likely no other area that you manage will you be able to bring as much value
    to your organization.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: We have covered storage basics building on the concepts of block device interfaces,
    abstraction techniques, filesystems and their interfaces, and used these concepts
    to investigate multi-device redundancy and how it can be used to build complex
    and robust data storage, and how storage access across devices can be handled
    to meet any potential need. My goal here has been to give you the knowledge necessary
    to think carefully on your own about your storage needs for any given workload,
    and an understanding of availability technologies and how you can apply them to
    meet those goals most effectively.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Never again should you see storage as a magic black box or a daunting task that
    you dread to tackle. Instead, you can see storage as an opportunity to shine and
    to demonstrate how proper system administration best practices can be applied
    to maximize whatever storage factors matter most for your workload without simply
    throwing money at the challenge or worse, simply ignoring it and hoping that you
    can find another job before things fall apart.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we are going to look at system architecture at an even
    higher level. Many of the most interesting concepts from this chapter will be
    recurring there. System architecture relies on storage architecture very heavily
    and many redundancy and system protection paradigms are shared. It can be quite
    excited to see how good storage design elements can lead to a truly high performance,
    highly available, and cost-effective final solution.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
