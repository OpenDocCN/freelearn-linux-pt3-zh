- en: '*Chapter 10*: User and Access Management Strategies'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the **Linux** world it is all too easy to forget that we still have users,
    and they still need all of the oversight, security, and management that we would
    expect in the **Windows** or **macOS** worlds. Users are typically an afterthought
    on Linux-based operating systems as systems are often seen as just black box server
    workloads or bizarre appliances to which end users do not apply. This is not true,
    of course. Users matter on any Linux system just as they do on anything else.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to talk about user and user access management
    for both servers and for end user devices. We are going to look at approaches
    common in the Windows world, and approaches commonly known in the UNIX world,
    and we are going to talk about some alternative approaches that are starting to
    emerge in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: We will also look at remote access for Linux – that is, supporting or working
    from our systems remotely. Of course, all of this will be done in the context
    of security, as user management is fundamentally a security topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Local and remote users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User management mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remote access approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSH, key management and jump boxes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternative remote access approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terminal servers and VDI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local and remote users
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the highest level, there are two basic ways to think of user accounts on
    any system. The first is local accounts that exist on the local system where they
    are being used. The second is user accounts stored remotely on some sort of server
    that the local system references over the network. Of course, there are hybrid
    methods that combine these techniques in various ways as well.
  prefs: []
  type: TYPE_NORMAL
- en: We should start by talking about the obvious benefits to both approaches. With
    locally managed user accounts we have lightning-fast access to our account information
    and no dependence on the network. This provides obvious performance advantages,
    better security, and protection against services failing elsewhere impacting our
    local systems. Local users are robust and simple, fast and easy. Until the 1990s
    it was rare to even consider the possibility of anything else.
  prefs: []
  type: TYPE_NORMAL
- en: '*Remotely managed users* make up the overwhelming majority of cases today because
    this model allows for a *single source of truth* for users between different organizations,
    and in some cases, even inter-organizationally! This has many benefits, such as
    allowing users to make a single, complicated password that they actually have
    some chance of remembering and being able to use it in many places, change it
    once to change it everywhere, and making it simple for the support team (which
    might mean us as the system administrators) to reset, lock, or disable accounts.'
  prefs: []
  type: TYPE_NORMAL
- en: It has become common to assume that only remote user accounts are plausible
    in modern businesses, but local accounts remain fully functional and viable, and
    are a good choice for a great number of organizations of any size. Do not simply
    assume that because of your size or modernity that one style or the other is appropriate
    for your use case. Of course, nearly all existing organizations are heavily invested
    in whatever architecture that they are going to use and changing is a very large
    undertaking. It is rare that we get the joy of implementing something like user
    management on any scale in a green field scenario, but it does happen from time
    to time.
  prefs: []
  type: TYPE_NORMAL
- en: Local user management requires less network bandwidth, but that is essentially
    never a concern today. Far more important are the issues around security and availability.
    Most networks are extremely fast today and user management uses few resources;
    this has led to vendors providing user management as a service allowing centralized
    user accounts to be provided via hosted vendors over the Internet. If the Internet
    can adequately provide bandwidth for these services, LAN-based versions should
    have no issue at all.
  prefs: []
  type: TYPE_NORMAL
- en: Security becomes a problem because having a single account that is available
    in many locations means that there is a single account to attack aggressively
    and, if compromised, will essentially provide unlimited access to all places where
    that account is used. If we have local user accounts and reuse the same password
    over and over again this mimics this problem so in many cases this becomes a moot
    point unless we have a scenario where our users are truly using different credentials
    in different locations. For traditional end users, this is not very likely to
    be the case. For system administrators this might be relatively easy to do. High
    security users with training and an understanding of the importance of their credentials
    may easily take advantage of the flexibility of local accounts. You have to know
    your users and their willingness to participate in order to really evaluate the
    security posture potential in this case.
  prefs: []
  type: TYPE_NORMAL
- en: The bigger issue surrounding security is the specific mechanism that supports
    the remote accounts, which can be attacked separately from the accounts supported
    by it. Whatever mechanism you chose, there must be communications over the network
    in order for the accounts to be able to centralize. In almost all cases, there
    is also some form of local fallback – such as a credential cache – that can be
    attacked, so having accounts exist remotely almost never eliminates the need to
    also have them locally as well as some mechanism. If such a mechanism is lacking,
    then there are readily available methods to disrupt logins by attacking network
    access.
  prefs: []
  type: TYPE_NORMAL
- en: Remote user management benefits from having a central store that can receive
    more focused security attention and that can be kept far more easily from being
    physically exposed to attackers. End points typically have, at most, a small number
    of cached user accounts or emergency access accounts on them and not the entire
    user list minimizing the risks of a stolen laptop or compromised desktop from
    impacting beyond the built-in list of accounts cached on the device.
  prefs: []
  type: TYPE_NORMAL
- en: Neither method constitutes anything close to a best practice. Both are completely
    viable and should be evaluated both at an organizational, as well as a workload,
    level. It is not uncommon for businesses to elect to use different methods for
    different purposes. There is generally no need to institute a single mechanism
    as most organizations are large enough to have varying needs and to justify implementing
    more than one mechanism inside of the organization. Often this hybrid approach
    works best as centralized accounts tend towards being most beneficial for most
    end users and local accounts tend towards being best for IT and other technical
    departments.
  prefs: []
  type: TYPE_NORMAL
- en: With that foundation under our belts, we are prepared to talk about the mechanisms
    that can manage these users, wherever we choose to implement them.
  prefs: []
  type: TYPE_NORMAL
- en: User management mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the real world there are many user management mechanism implementations to
    consider. Some are native to UNIX or Linux, some are common in the Windows world,
    some are novel, and some are universal and agnostic.
  prefs: []
  type: TYPE_NORMAL
- en: It should go without saying that our first stop on any journey of investigating
    user management mechanisms is the Linux user system itself. Simple and universal,
    every Linux system of note ships with it. Of course, it can be replaced, but in
    practice it never is. This system carries the huge advantages of being always
    built in, very fast and secure, and well known by every UNIX admin anywhere. There
    is almost nothing to go wrong, nothing complex anywhere in the system. A few archaic
    components might linger on having been left over from the olden days that might
    be a little confusing if one finds it necessary to do manual configurations, but
    today almost no one manipulates these systems by hand anyway (although it is always
    good to know how to do so just in case.) Local users are always very easily automated
    by custom script, stock tools, or things like state machines.
  prefs: []
  type: TYPE_NORMAL
- en: Using automation to turn local uses into remote users
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here is one of those cases where we end up being unable to define the difference
    between a local user and a remote user. One important approach to local user management
    is to have a master user list stored elsewhere. This list might be a simple text
    file that we run a script against, or more likely a configuration file in a state
    management system used as part of an Infrastructure as Code implementation.
  prefs: []
  type: TYPE_NORMAL
- en: In this example our central management system can be used to push out user accounts,
    permissions, and details from the central configuration system onto each host
    in our network. The system might put all users on all machines, or just the users
    assigned to those machines. These kinds of decisions are all completely subject
    to your discretion during implementation.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, yes, we still use the local user mechanism on each computer
    and there is no need to reach out over the network in order to log in at any given
    time, however the accounts are still centrally controlled in a way that almost
    completely mimics a system like Active Directory. We can centrally enforce password
    rules, we can centrally create or revoke accounts, we can centrally control onto
    which machines logins can and cannot happen, and so forth. We essentially have
    a central user system, not a local one.
  prefs: []
  type: TYPE_NORMAL
- en: The key difference is that with a true central user directory such as Active
    Directory, user activity is directed to the central system and only optionally
    will hit a local cache when the system is offline or in some degraded state. When
    possible, network activity occurs to support the login process. With a mechanism
    such as the one we are describing, all logins happen purely using local – that
    is to say, *non-networked* – resources, and the central system is only used to
    update the local login list and details. A carefully worded semantic difference
    between local accounts and locally cached accounts to be sure, but the behaviors
    of the two truly are different. It is a very interesting thought experiment, though.
    It is assumed that locally maintained accounts will have to hold all users on
    each device, but in practice there are many ways to limit this to just a few and
    sometimes even just one user, while it is assumed that none or just a few users
    will be cached onto a local device from a remote user management system but in
    some cases the entire user list might be cached there. The difference here is
    how the user list is created locally and knowing how that mechanism works in your
    case combined with knowing how the system is used will tell you the exposure of
    both approaches for you.
  prefs: []
  type: TYPE_NORMAL
- en: However, automation of this nature provides a pretty significant opportunity
    to rethink our assumptions around local user limitations. In this kind of situation,
    we might be able to recreate many or all of the desired features that are generally
    assumed to be provided exclusively by heavier and more fragile remote user systems
    using only local users. In a modern environment with applications that typically
    do not authenticate to an extension of the operating system authentication system
    this can potentially work beautifully, especially on a Linux-based operating system
    where such integrations are not common. If your network architecture is one that
    uses shared authentication methods, like those provided by Active Directory, to
    allow access network resources then this methodology will fall short of the smooth,
    integrated experience that one traditionally has with those models. Network resource
    design models that rely on those shared authentication processes are starting
    to lose their ground as the dominant approach today as the user experience landscape
    starts to change in the wake of greater work from home, user mobility, and security
    concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Moving past local users for which there is effectively only the default mechanism
    we start to see a variety of legacy and more modern user management options, and
    more will certainly be coming in the future as this space heats up as new demands
    on infrastructure are shifting user management priorities for many businesses.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally in the UNIX landscape, which of course includes Linux, the standard
    user centralization service was the **Network Information Service** originally
    called YellowPages and eventually just known as **NIS**. NIS was introduced by
    **SUN Microsystems** in 1985 and it quickly caught on across the UNIX world and
    changed how people thought about users across systems. NIS was the vanguard of
    the movement that drove IT development in the 1990s as directory services became
    the hot strategic technology of the age. NIS may have not been the absolute first
    directory service (although maybe it was), but it was certainly the first to see
    widespread use or to fundamentally shift the industry's paradigm of user management.
  prefs: []
  type: TYPE_NORMAL
- en: NIS was extremely basic, but flexible and easy to manage, and lacked almost
    all security as it would be needed today - all things that made it effective in
    its time. NIS became all but universally available on all UNIX systems including
    commercial offerings like Solaris and AIX as well as on open source offerings
    like BSD and Linux-based operating systems. Learning NIS meant you could work
    across operating system divides easily and NIS provided interoperability between
    disparate UNIX flavors as well.
  prefs: []
  type: TYPE_NORMAL
- en: Given the immense age of NIS and the lack of security and scalability it might
    seem as though NIS would have faded into the past leaving us with nothing but
    stories for old timers to bring out over the campfire, but this is not the case.
    NIS remains in use today, especially in well entrenched, larger firms where NIS
    implementations might easily stretch back to the 1990s when it was the key technology
    still. New deployments might have all but disappeared over a decade ago, but old
    ones linger on. In fact, at the time of writing, every major Linux-based operating
    system still includes packages and support for NIS both to allow the system to
    act as a NIS client as well as creating new NIS servers! It has been proposed
    that RHEL will drop NIS in the next few years, but at this point it is still a
    proposal and still some ways off in the future. NIS has created quite the legacy
    for itself.
  prefs: []
  type: TYPE_NORMAL
- en: NIS lacked so much, especially in the areas of security and scalability, that
    its designers attempted to replace it after just seven years introducing NIS+
    in 1992 (I told you that the 1990s were big on directory services.) NIS+ was not
    a direct upgrade for NIS, however, and proved to be hard to manage and was not
    a smooth upgrade process from NIS. NIS+ never gained strong enough traction to
    make it a major player and NIS actually managed to outlive it in real world utilization
    and in software support. SUN, who made both NIS and NIS+, announced that NIS+
    was to be discontinued in 2002 and while it was still officially supported for
    several more years it was waning. NIS+ advanced the art of centralized user management,
    to be sure, but it never itself became a key technology. Its heyday was in the
    mid-1990s, but as so many technologies were being fielded during that era it was
    lost in a sea of competition from every angle - including from new players like
    Novell and Microsoft.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid going into a history of the convoluted, and mostly forgotten, world
    of directory services in the 1980s and 1990s, we need only focus on one dominant
    technology that was introduced in 1993 and that is LDAP (the Lightweight Directory
    Access Protocol.) LDAP was a game changer in many ways. For one, it was vendor
    neutral allowing any system to implement it freely. Second it had many advanced
    database, protocol, and security features that allowed it to be used flexibly
    and security while allowing it to scale. Other technologies existed, but by 1997
    when LDAPv3 was released, no other directory service was still making headlines.
    LDAP was seen as the clear winner and future of the LAN-based directory market.
  prefs: []
  type: TYPE_NORMAL
- en: LDAP began to replace NIS and NIS+ in the UNIX world, including on Linux-based
    operating systems, during the 1990s although the available implementations were
    complex and not well known yet. LDAP really lept forward when, in 2000, Microsoft
    announced that they were replacing their traditional Security Account Manager
    or SAM system (this is the one famous for having Primary and Backup Domain Controllers
    knows as PDCs and BDCs) with an LDAP implementation called Active Directory. Active
    Directory proved to be such a well-made and easy to manage LDAP implementation
    that it completely dominated the market and itself overshadowed LDAP to the point
    that few people are even aware that Active Directory is in essence just one of
    many LDAP implementations available on the market.
  prefs: []
  type: TYPE_NORMAL
- en: Linux-based operating systems can use nearly any LDAP server as a directory
    service source, or even Active Directory itself, which is an LDAP server but additionally
    has advanced requirements like Kerberos, in addition to the LDAP portion of the
    system, that given it expanded functionality and security over vanilla LDAP. Since
    the late 1990s, LDAP (in one form or another) has been the general standard for
    Linux systems that need to authenticate to a LAN-based directory service. Today,
    multiple LDAP server implementations are available that can run on Linux. There
    is even a SAM and Active Directory implementation for Linux to act as a server!
  prefs: []
  type: TYPE_NORMAL
- en: If using a local directory server is the chosen approach, then some implementation
    of LDAP is almost certainly going to be the only reasonable choice for a Linux-based
    operating system today. If the intent is to integrate with other systems, such
    as Windows or macOS, then an Active Directory flavor of LDAP with its additional
    *special sauce* is almost certainly the necessary option - either the version
    directly from Microsoft or the open-source Samba implementation available to run
    from almost any Linux. If implementing for an all UNIX set of devices then one
    of the more traditional LDAP server products is more likely to be appropriate
    such as OpenLDAP or 389 Directory Server.
  prefs: []
  type: TYPE_NORMAL
- en: LDAP is generally not considered to be viable for exposure to public networks
    (like the Internet) and is generally thought of as being LAN-centric, meaning
    that it relies at least partially on the network firewall to provide a safe operating
    space in which it can do its job. Exposing LDAP (even assuming SSL/TLS support
    via the upgrades LDAPS protocol) carries a bit of risk. A few companies still
    do this, and it can work, but it requires a lot of planning and an understanding
    as to the scope of exposure. Many companies accidentally expose LDAP components
    through a third-party proxy creating an exposure unintentionally (and highlighting
    the risks of the walled garden theory of LAN-centric security approaches.)
  prefs: []
  type: TYPE_NORMAL
- en: The famous RDP exposure risk
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Linux or UNIX circles, the RDP exposure risk example is not as widely known
    as in the Windows world, where remote access to systems by way of the Microsoft
    **Remote Desktop Protocol** (**RDP**) is a very common thing. However, the concept
    and problems often associated with RDP, such as vulnerability to brute force attacks
    and high visibility to potential attackers, are not actually related to Windows
    but to architectural design.
  prefs: []
  type: TYPE_NORMAL
- en: The issue is that exposing RDP on a publicly accessible IP address is considered
    to be very high risk. Yet the security on the RDP protocol is similar to that
    on SSH which is generally considered to be safe to exposure (within reason.) Why
    would two protocols of similar use and security result in two such wildly different
    security postures?
  prefs: []
  type: TYPE_NORMAL
- en: The secret lies either with the convention in the Windows world to use Active
    Directory (an LDAP implementation) ubiquitously or in the fact that Microsoft's
    standard multi-user RDP environment, **RDS** (for **Remote Desktop Services**),
    requires Active Directory. Active Directory essentially becomes a foregone conclusion
    when RDP is mentioned, but when using SSH it is assumed that Active Directory
    or some form of LDAP will not be used (at least as the external authentication
    method.)
  prefs: []
  type: TYPE_NORMAL
- en: Why does the underlying security method make a different when Active Directory
    and LDAP are highly secure on their own, and RDP is a very secure protocol? The
    answer is in that RDP forcibly exposes access to Active Directory in a manner
    very different to how it would be used on an internal LAN.
  prefs: []
  type: TYPE_NORMAL
- en: On the LAN we basically have an automatic whitelist that consists of the devices
    on our LAN. In many environments this will be additionally limited by VLANs that
    keep unnecessary devices (phones, IoT, and others) away from our end user devices.
    In larger environments network access controls may further limit potential exposure
    leaving us with a very protected environment in which our Active Directory can
    operate. Further, Active Directory itself generally protects itself by limiting
    attempts to log in to any given account locking the account for some period of
    time before it allows further attempts. Attacking Active Directory remotely on
    a LAN is generally quite difficult to do effectively.
  prefs: []
  type: TYPE_NORMAL
- en: When we add RDP and open it to the Internet at large, all of these controls
    drop away, completely. Of course, there are ways to limit this through IP whitelisting,
    VPN encapsulation, or other technique, but a stock deployment is going to expose
    our system broadly (if it does not do so, the purpose of exposing it is generally
    lost.) What is often missed is that the *lock after some number of failed attempts*
    mechanism that is so critical to securing Active Directory frequently offers a
    means of enacting denial-of-service (DoS) attacks on our users (that is, making
    it easy for an external user to block our users from logging in). In order to
    mitigate this attack risk, we have to disable this mechanism for RDP, in turn
    allowing essentially unlimited attacks on our accounts from the public space!
    Neither option is truly viable, and therefore, RDP becomes highly risky when used
    in a conventional way, even though both RDP and Active Directory are quite secure
    in their own right.
  prefs: []
  type: TYPE_NORMAL
- en: SSH and other mechanisms are most often used decoupled from central user account
    systems such as LDAP and Active Directory, allowing them to maintain a completely
    separate security mechanism and posture when compared to other user authentication
    methods. RDP can be used this way too, of course, but it is often assumed to have
    the functionality of central user account access because of Active Directory,
    making it difficult to treat it like a decoupled service because many in our organization
    many expect that because it is RDP that they will have immediate integration with
    Active Directory.
  prefs: []
  type: TYPE_NORMAL
- en: Today we are living in the post-LAN and generally post-LDAP world. Even LDAP's
    single biggest proponent, Microsoft, has moved away from it when possible and
    is investing probably more than any other company in LDAP alternatives, primarily,
    their own Azure AD service which, confusingly, keeps Active Directory in its name
    even though it is a completely unrelated mechanism to Active Directory (but can
    be tied to it to extend it.)
  prefs: []
  type: TYPE_NORMAL
- en: The biggest changes to industry accepted authentication systems is that most
    systems are hosted products rather than software that businesses are expected
    to deploy and maintain on their own; and most modern systems are Internet based
    allowing users to be located almost anywhere and, as long as they have an Internet
    connection, be able to connect to the authentication mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: These new mechanisms are coming in a continuously increasing variety of flavors
    from many different vendors and types of vendors. In the completely non-server,
    desktop only world of Chromebooks, Google themselves have an exclusive authentication
    mechanism and represents a very significant portion of the Linux-based end-user
    market today.
  prefs: []
  type: TYPE_NORMAL
- en: Are operating system logins relevant in the modern world?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This being a book on Linux administration, we are understandably talking about
    users at the level of the operating system, and we have to (as is best practice)
    question whether such a concept even matters today (or historically, for that
    matter) or if logging into operating systems may soon be a thing of the past entirely.
    Honestly, it is not an easy question to answer. The knee-jerk reaction is to jump
    to the conclusion that operating system users are incredibly relevant and underpin
    all security. But are they really?
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, I am going to lead with saying that: Yes, generally operating system
    logins and user management is still important today and always has been.'
  prefs: []
  type: TYPE_NORMAL
- en: How we think of user logins has changed immensely, though. The user landscape
    today is very different from what it was just one to two decades ago. Let's start
    with a little history.
  prefs: []
  type: TYPE_NORMAL
- en: During the pre-1990 era, the *archaic* computing world, very few systems used
    user identification mechanisms, at least at the operating system level. Systems
    that did, like UNIX and VMS, were special cases, big enterprise systems that were
    considered advanced and impressive. What most users would interact with, even
    using systems like Mac, Amiga, or MS-DOS, were single user, non-authenticated
    systems. As late as 2001, Microsoft's Windows **Millenium Edition** (**ME**) was
    released without true multi-user support (as it was still just a graphical shell
    layered on MS-DOS). In general, the idea that operating systems needed to manage
    multiple users was a foreign one.
  prefs: []
  type: TYPE_NORMAL
- en: In the 1990s, the shift to user security and access control as networking became
    generally accessible, the Internet began to come to fruition, and users needed
    more advanced functionality was significant. If anything, the 1990s were marked
    by being the era of user management (which we mentioned earlier.) Rather suddenly
    everyone was concerned with how we would handle many users sharing the same devices
    and how we would make the user experience portable across multiple devices. System
    administration was essentially flipped on its ear.
  prefs: []
  type: TYPE_NORMAL
- en: By the 2000s, the operating system level user experience expectations were well
    entrenched and user management moved from competitive advantage to commodity functionality.
    The last remaining operating systems of any note that did not support native multi-user
    functionality faded away and even casual end user products intended for entertainment
    user started to encourage user management and security.
  prefs: []
  type: TYPE_NORMAL
- en: When smart phones first entered the market, they were a throwback to the user-less
    systems of the 1980s. The holder of the device was assumed to be its universal
    and unnamed singular user. Even cell phones have moved to at least being a user
    and security centered device, even if they still focus on a single user, they
    do so with access controls and user identity in mind.
  prefs: []
  type: TYPE_NORMAL
- en: On the server side, user management has certainly lost its luster over the past
    decades. At its peak in the 1990s, Linux and its UNIX cousin systems were all
    the rage for direct end user logins and user management on the server was a significant
    portion of a system administrators' day. That trend fell away, and the idea that
    end users would need accounts or logins to a server at the operating system level
    now feels outdated, at best. Exceptions will always apply, but they are few and
    far between and becoming increasingly rare.
  prefs: []
  type: TYPE_NORMAL
- en: Even at its peak, the necessity that all administrators log in with individual
    user accounts was never universally accepted. It may never have been exactly normal
    for all administrators to share single accounts, but it was never exactly rare,
    either. Shared root (the default administration user account) account access has
    always been a common practice that few want to admit to having witnessed and exists
    commonly in Linux, Windows, and likely all other environments. The practice was
    (and surely still is) so widespread that techniques and tools for managing user
    access to the single user account through external mechanisms are even somewhat
    popular (such as logging into a third-party control console that grants access
    to root accounts on multiple servers as needed!)
  prefs: []
  type: TYPE_NORMAL
- en: As we are a book on best practices, I am going to take moment to point out that
    in our role as system administrators it is absolutely a best practice to maintain
    user identity and access control on our servers and that shared accounts are really
    never a good idea even when they are well managed and secured. The necessary effort
    to maintain more secure and auditable systems is simply not that great that we
    should be working to avoid it. That said, shared account access is not necessarily
    as risk as it sounds as there remain the potential for audit controls and access
    controls even when doing so and while I truly doubt that there is ever an actual
    justifiable use case for doing so, it is certainly possible to make shared access
    *secure enough* to function effectively.
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, for servers (and the administrative functions of end user
    workstations as well) modern design techniques using state machines, infrastructure
    as code, application encapsulation such as application containers, **MDM** (**mobile
    device management**) and even **RMM** (**remote monitoring and management**) tools
    may entirely eliminate the need for logins at all making the entire discussion
    of user management moot. If we never log in, if we never create a user, we need
    not consider the possibility of user account access security, whether that's remote
    or even local.
  prefs: []
  type: TYPE_NORMAL
- en: So, it is worth considering that from a server perspective, the need for users
    may have been nothing more than a passing fad or fallback crutch for shops that
    are not able to maintain the most modern of techniques. These are administrative
    users, of course, but that has long been the only users expected to exist on a
    server at all.
  prefs: []
  type: TYPE_NORMAL
- en: On end user devices, the concepts of users have begun to change dramatically
    as well, but for entirely different reasons. Traditionally work on end user devices
    was focused on locally installed applications that themselves have no user controls
    and simply run under the security controls of the environment provided by the
    operating system. This could be any kind of application from word process, image
    editor, or video game. The environment was primarily defined by the accessible
    local storage locations. For many users, especially home users, this has almost
    entirely changed.
  prefs: []
  type: TYPE_NORMAL
- en: Today most applications are web applications, applications that run in the browser
    rather than being installed at all. Users might log into the application, but
    this is almost always separate from any operating system login. Even locally installed
    applications are starting to connect over the Internet to authenticate to a service
    increasingly.
  prefs: []
  type: TYPE_NORMAL
- en: As this shift occurs, the concept of the operating system user being the primary
    component used to define the context of security access is fading rapidly, and
    the idea that users need to exist at the application level instead has already
    become the norm. As this happens, we have to start to consider the functionality
    of the operating system user accounts not to be a granular access control mechanism
    or the end all of user management but rather little more than a security step
    to attempt to guarantee a safe, protected environment from which we can launch
    our web or networked application and sign in at the application layer where our
    users and access controls are far more relevant. Moving user access controls to
    the application layer is critical as this is where the data intelligence resides,
    and control can be made at a granular level rather than granting access at the
    level of the entire application.
  prefs: []
  type: TYPE_NORMAL
- en: As the operating system starts to lose its traditional role in user management,
    the benefits of strong user management and control systems begin to erode. It
    is far more than just the shift of user access control up the stack from operating
    systems to applications as the industry and software matures that is causing this.
    Other factors are at play. It used to be that computers were expected to user
    printers heavily, something that has broadly stopped being true. Printers are
    now an afterthought, if they even exist, rather than being a primary functionality
    of computers. Likewise, modern applications do not use operating system managed
    storage in the same ways and the need to strictly administer local storage and
    mapping remote storage resources has fallen away.
  prefs: []
  type: TYPE_NORMAL
- en: Even just ten years ago user management systems, Active Directory as a prime
    example, were used first and foremost to coordinate printing and mapped drive
    resources for end users. Today, between application modernization and workforce
    mobility and computing ubiquity, both of these things have become legacy functions.
    A laptop user working from home might have absolutely zero need for either mapped
    drives or printers, and in order to allow mobile users to function in this way,
    in-office workers have had to adapt to this as well. The trend in access control
    mechanisms is moving away from the traditional uses of operating-system-level
    users.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, users still matter, but they do not maintain the relevance that
    they had in the past and the future looks like one where that value will continue
    to diminish.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing what remote access methods are going to be most appropriate for you
    requires more than just an understanding of how these systems work and how their
    implementations play into your environment is just the first piece of the puzzle
    in determining what mechanism should be chosen for your organization. Learning
    what products are currently on the market, their current features, limitations,
    pricing, and other business factors are necessary. User management is rapidly
    becoming a market where it is more about knowing the current offerings available
    and much less of something that we will implement ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to move past authenticating our users and
    now dig into how we can provide these users with remote access to the systems
    that they need.
  prefs: []
  type: TYPE_NORMAL
- en: Remote access approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assuming we are not using a *access-less* approach built off of state machine
    technology, we have a few different paths that we can popularly use to gain access
    to our Linux systems. In most cases with Linux based operating systems we are
    going to be discussing how system administrators, like you and me, are able to
    log in and use the operating system interactively, but any typical method that
    we are going to use to do this is going to be an option for end users as well.
    The needs of end users is generally very different from that of system administrators,
    but the tools that we can use are going to generally overlap.
  prefs: []
  type: TYPE_NORMAL
- en: For us, in the system administration role, access is most often defined by needing
    to be very quick to set up, quite temporary in its use, with the focus critically
    being on ensuring that the system is highly accessible and command line driven.
    For end users, we will expect the opposite. Administrators often have to log into
    many different operating systems, maybe one after another, maybe many all at the
    same time. A lengthy process to access them could significantly hinder our ability
    to be useful.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally end users will log into only a single system and remain attached
    to it for the duration of their work period, typically a workday or something
    similar. Taking more time to log in that one time, but having a more robust end
    user experience, is their priority. Because the needs are so different, remote
    access techniques may be separate. There is little need to feel compelled to merge
    them.
  prefs: []
  type: TYPE_NORMAL
- en: There are two main types of remote access. One is direct, meaning that we expose
    ports and have some protocol with as SSH, RDP, or RFB (VNC) that allows us to
    connect from some software client to our systems. This is the best-known type
    of technology and is the most straightforward to manage. It requires nothing complex
    and is well understood. In this approach we have a traditional service on the
    virtual machine in question (or potentially on the hypervisor, but the result
    is roughly the same) and an external client *reaches in* to access the system.
  prefs: []
  type: TYPE_NORMAL
- en: The other access methodology is indirect access where an access server is used
    to manage access for both the operating system to be access and for the client
    attempting to access it. This method requires a server that is hosted publicly
    (often provided as a service but can be self-hosted as well) and both the end
    points connect to it as clients so that nothing except for the access server need
    to be exposed outwardly.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of both solution types are pretty clear. Direct connections are simpler
    and have less to go wrong. Indirect connections require more infrastructure but
    reduce potential points of exposure, consolidate connections, and hide the presence
    of the networks making it harder to discover and attack a network based on its
    remote access publications.
  prefs: []
  type: TYPE_NORMAL
- en: For many reasons, there is a trend towards direct connection technologies, such
    as RDP and RFB (VNC), for regular end users and indirect connection technologies,
    such as MeshCentral, ConnectWise, and LogMeIn, for system administrators. For
    end users, the direct connection technologies tend to provide the most robust
    experience to replicate the feeling of working directly on the hardware as if
    it was sitting in front of you. For system administrators, the increased security
    and consolidation of access to systems potentially across many different physical
    locations is highly beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: Notice, however, in all cases we keep saying *tends*. There are no hard and
    fast rules here, only strong tendencies. Because we are dealing with Linux, we
    have other considerations, however. For example, RDP tends to be less easy to
    manage than on Windows, while indirect access methods might be easier but not
    as well understood. Also, our user base may have different inherent expectations
    – for example, where Windows users might expect Windows-native tooling, Linux-based
    operating system users might be more open to less familiar access options.
  prefs: []
  type: TYPE_NORMAL
- en: Like so many things in IT, understanding base level technologies is only the
    first step. In our next section we will discuss ways that we can make SSH, a direct
    connection technology, more flexible and robust and in some ways of looking at
    it we will be mimicking an indirect connection using direct ones. The differences
    between them are not so large.
  prefs: []
  type: TYPE_NORMAL
- en: Even a rule of thumb is difficult in this situation and likely much discussion
    of remote access will depend upon many factors such as how that access will be
    used, will the infrastructure be shared with end users, what security needs may
    exist, do other access considerations exist such as VPNs, and is there value to
    creating a unified connection process that is shared with other technologies or
    platforms?
  prefs: []
  type: TYPE_NORMAL
- en: And then we have to consider the possibility of having multiple access methods.
    It is not uncommon to use more than one to ensure availability even when one has
    failed. One may be the convenient, yet fragile, access method while the backup
    is heavily secured and cumbersome to use but serves as a critical backup should
    all else fail.
  prefs: []
  type: TYPE_NORMAL
- en: How do I approach remote access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a topic so devoid of strong guidance, I feel taking a moment to present my
    own typical access choices is valuable. I do this not to suggest that my approach
    is ideal, but to give some insight into a real-world decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time when deploying Linux servers where I am overseeing their system
    administration and we need to have remote access to a running system instead of
    being managed completely by way of state machines or infrastructure as code, we
    opt for a two-prong approach with one direct and one indirect access method.
  prefs: []
  type: TYPE_NORMAL
- en: For indirect access we use MeshCentral which, itself, is open-source software
    and runs on a Linux based operating system that we host ourselves. This allows
    us extensive flexibility and cost savings compared to most solutions and because
    we are able to run it on the same operating system that we deploy internally and
    for customers we are able to leverage processes, tools, and skills that we are
    already using other places in order to maximize our efficiency. With MeshCentral,
    as with many indirect remote access solutions, we have remote terminal access,
    remote graphical desktop access if a GUI is installed on the server, and many
    tools for monitoring, file moving, and remote command execution.
  prefs: []
  type: TYPE_NORMAL
- en: With this indirect access the system administrators have nearly instant access
    to all of the servers that we maintain, across highly disparate technology stacks
    and physical locations. Some servers are isolated on LANs without any port forwarding
    available, some have public IPs, some have graphical desktops, most are command
    line only. No matter what they location, use case, or disposition MeshCentral
    gives us access to do what is needed to manage the systems.
  prefs: []
  type: TYPE_NORMAL
- en: For situations mostly involving emergency access we also maintain direct SSH
    access to nearly all systems. This is important as the ability to reconfigure,
    patch, or restart a system where indirect access methods have failed is often
    critical. This access would almost always be limited to access only from a local
    LAN on which the workload directly sits and potentially even limited further within
    that network scope making SSH only available from select workstations or another
    server designated for the purpose of highly secure remote access. In some cases,
    the SSH service may not even be kept running by default and only turned on by
    way of a state machine setting being changed; or it may be changed manually via
    a form of out of band management. `SSH` may also be useful for some forms of automation.
  prefs: []
  type: TYPE_NORMAL
- en: Having two very different forms of access, with deep security controls in place,
    provides the right balance of accessibility, protection, and security for us.
    You have to be aware that each additional method of access means another avenue
    of attack by malicious actors, as well, so layering on access methods that are
    not necessary is generally not a good option. You want to ensure reliable access
    without putting systems at risk.
  prefs: []
  type: TYPE_NORMAL
- en: Something often overlooked in these types of discussions is that there are some
    direct remote access tools that work in ways we often do not anticipate such as
    web based direct access tools. These tools would include such products as **Cockpit**
    or **WebMin** which provide a web-based interface to our systems. These tools
    may give access to configure our systems via a web interface and may even allow
    for interactive console access through the web interface allowing us to publish
    and secure remote access in an entirely different way.
  prefs: []
  type: TYPE_NORMAL
- en: In the Linux world, by far the most common and assumed method of accessing a
    computer across the room or on the other side of the world is the ubiquitous SSH
    protocol. Next, we will look at ways to make SSH even more powerful than it is
    out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: SSH, key management, and jump boxes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using SSH for remote management of Linux based operating systems is so ubiquitous
    that it deserves special consideration. SSH on its own is efficient and very secure,
    but it is well known and generally exposes such extreme functionality in our systems
    that it is often the target a focused attacks. We cannot be complacent in the
    use of SSH, especially if exposed to the Internet, as the risks are simply too
    high.
  prefs: []
  type: TYPE_NORMAL
- en: When using SSH we have almost a laundry list of ways that it can be secured.
    We will touch on several of these and how they work together to make SSH extremely
    difficult to compromise. SSH on Linux is provided via OpenSSH which is mature
    and battle tested and receives more scrutiny than almost any software package
    made. SSH starts as an already very harded package from most perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: Our first tool for securing SSH is to consider completely removing password-based
    access to it in favor of using keys. Keys are fast and efficient allowing admins
    to access servers faster and more securely than with passwords. Keys also support
    passphrases which act as a form of two factor authentication. If this makes sense
    for your organization based on the security needs, then this requires someone
    to know an encrypted password and possess the private key simultaneous to attempt
    to breach a system through this channel. Keys have been around for remote access
    authentication for a very long time but have not enjoyed the popularity that they
    should. Too many companies take a *set up keys for yourself if you feel like it*
    approach, allowing far too many administrators to just not bother to take advantages
    of the efficiency and security that they offer.
  prefs: []
  type: TYPE_NORMAL
- en: Our second tool is account lockouts from failed attempts. The standard tool
    for this on Linux based operating systems is called Fail2Ban. Fail2Ban takes SSH
    and other services that have standard login modules, and works with them to detect
    suspected malicious attempts against our systems and automates our local (meaning
    the firewall on our Linux based operating system itself) firewall to halt traffic
    from the offending IP address(es) for some predetermined period of time, typically
    three to fifteen minutes. This approach is our most effective tool against brute
    force attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Do you still need both a network edge firewall and an operating system firewall?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might be shocked at just how often this question has come up over the years.
    What makes it such a surprising question is the context that it is always asked
    in.
  prefs: []
  type: TYPE_NORMAL
- en: First, some tech. In the real world, for many practical reasons, all routers
    are also firewalls. So functioning without a hardware firewall on the network
    edge is effectively impossible. In some cases, such as when installing VPS or
    cloud, we might be defaulted to a completely wide open firewall, but the firewall
    is at least still there. We have to start from an assumption that all operating
    system instances will always effectively be located behind a network firewall
    and that that firewall may be highly ineffective.
  prefs: []
  type: TYPE_NORMAL
- en: Second, some history. Operating system firewalls were extremely rare due to
    their performance impacts until the late 1990s. They were also not very important
    as the degree to which computers were networked was much lower until that era.
    Operating system firewalls were introduced even though network firewalls were
    already a universal assumption because they add granularity and automation such
    as we can get with Fail2Ban, and because they protect individual machines from
    attacks that have either breached the network firewall or, far more likely, originate
    from inside of the LAN.
  prefs: []
  type: TYPE_NORMAL
- en: Given the context, tech limitations, and history it seems ridiculous that someone
    would ever assume that both firewalls are not needed. If we had the ability to
    skip the network firewall, theoretically, we could do so, but there is no purpose
    to skipping the security at this layer; and while we can choose to not use the
    firewall in the operating system, this creates risks that has no other mitigation
    method. The operating system firewall is unique in its ability to defend against
    both local and remote threats, whereas the network firewall can protect against
    remote only. While nothing is going to be good as having both, it is the operating
    system firewall that we care about the most from a security perspective. Primarily
    we care about the network firewall only to lower the workload on the operating
    system firewall.
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, basic firewall functionality today does not require any measurable
    system overhead. This was the downside to using them (around a quarter of a century
    ago.) Today there is no effective caveats to the use of firewalls.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice here is extremely clear and quite important: both the network
    firewall and the operating system firewall are absolutely needed. There is no
    situation where one of these firewalls should be removed or disabled.'
  prefs: []
  type: TYPE_NORMAL
- en: Often operating system firewalls are disabled because system administrators
    do not want to be bothered to have to know their networking needs or to secure
    their workloads properly. This is never a valid excuse. We all feel lazy at times,
    and we would all like to avoid having to maintain yet another point of support
    on our systems, but this is fundamental security and being able to properly support
    a system in production would require that we have all of the knowledge necessary
    to enable and configure the firewall regardless. Any perceived value to disabling
    a firewall (outside of troubleshooting, of course) should be seen as a huge warning
    sign that something is wrong that we need to address.
  prefs: []
  type: TYPE_NORMAL
- en: Our third tool to secure `SSH`, after key-based authentication and account lockouts,
    is network limits, generally in the form of limiting the IP addresses from which
    requests to connect to SSH can originate. This can come in multiple forms. Often,
    we would do this with the firewall on the Linux system itself, but it could also
    be provided by the hardware firewall on the network edge or by a network firewall
    from a cloud provider and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: IP limits are often best set by whitelisting, when possible. Whitelisting allows
    us to restrict SSH traffic to just a single IP or small group of IP addresses
    that we believe to be known and safe such as from administrators' homes, a trusted
    data center, or the office IP address. This dramatically reduces the attack surface
    of our systems and makes them difficult to detect in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: For some, though, whitelisting is not possible or, at least, not plausible.
    In those cases, blacklisting of IP ranges that are certainly not going to need
    access will still increase security. In this case, for system administration,
    the use of national IP ranges to block traffic from certain countries and regions
    can be practical. I never recommend this practice for customer facing systems
    where you might accidentally block a customer or business partner without realizing
    it and risk backlash (remember, to someone trying to use a service from a blacklisted
    location does not see it as blocked, but as the service having failed and being
    offline), but for system administration access where the locations of your administrators
    should be roughly known and alternative back channels for communications should
    always exist and where security is far more critical it can make a lot of sense.
  prefs: []
  type: TYPE_NORMAL
- en: Fourth, the use of `sudo` to require an additional verification before executing
    escalated privilege commands is very useful. With `sudo` we can layer on more
    protection over what we already can have with keys or better, keys with passphrases.
    If we log in using a user account that possesses `sudo` permissions to the root
    superuser account, then we have already demonstrated one or two factors of authentication
    just to become a standard user. To use `sudo` we optionally require another, separate,
    password for that user to gain privilege escalation. That is a lot of potential
    protection. `Sudo` also helps us avoid dangerous errors that can happen, mostly
    from typos, when running as the root user directly. `Sudo` is more likely to protect
    us from ourselves than it is from external actors. It's a very useful tool.
  prefs: []
  type: TYPE_NORMAL
- en: Fifth, thanks to the power of `sudo` mentioned above, we can disable root user
    logins over `SSH` completely keeping the most well-known, and by far the riskiest
    account, out of the risk equation completely. There is no need to the root user
    to be directly exposed when we have `sudo` mechanisms to protect it and to log
    when it is accessed already.
  prefs: []
  type: TYPE_NORMAL
- en: Does changing the default port of SSH work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will find many people and articles that tell you that you should always
    change the default port of SSH, or any access protocol, to make it harder for
    attackers to detect. This will then be pointed out as being a form of security
    through obscurity which, is generally believed, to mean no security at all.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, both positions are a bit overstated. Changing the default port will
    truly do nothing for actual security as any real attack of any complexity or effort
    will find a non-standard port in a matter of seconds and likely the attacker will
    never even become aware that you attempted to thwart the attack by changing the
    address. Much like if you were to move the door to your house to somewhere along
    the side of the house, most thieves breaking in would not even take note of the
    fact that the door was in an odd location. The door is still the door and completely
    obvious.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking of using non-standard ports as a security measure is incorrect. At
    best it nominally improved the security posture, at worst it may advertise that
    you have attempted security through obscurity and may make a good target for a
    more focused attack. Where non-standard ports benefit us potentially is by reducing
    the amount of traffic that hits our ports making it easier to store and filter
    logs.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing log clutter can aid security, of course, simply by making a worrisome
    attack easier to spot or faster to diagnose, and reducing storage needs is always
    a nice benefit. So changing SSH ports may be beneficial, but the benefit should
    be kept in context.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond all of these best practice security methods for SSH there are many other
    standard hardening options that can be used, but I would argue that outside of
    these that we would struggle to define any more as best practices as opposed to
    highly recommended or well worth considering. With a little accommodation, SSH
    can be ridiculously secure for almost any organizational need.
  prefs: []
  type: TYPE_NORMAL
- en: To take security in SSH to another level you can also, and quite easily, apply
    one-time passwords for multi-factor authentication through tools like Google Authenticator.
    Many third-party security enhancements exist and SSH can effectively be as secure
    as you want it to be.
  prefs: []
  type: TYPE_NORMAL
- en: SSH key management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is generally assumed that if we are going to use SSH then we are going to
    use keys to secure the access to it. As mentioned before, they are fast and highly
    secure. There is little reason not to use them. It is not as simple as *just using
    them*, however. When we choose to use keys we then have to determine how we will
    manage those keys. How that is going to make sense will depend primarily on the
    size of our organization or, at least, on the size of our systems administration
    team and how many systems will be being accessed using said keys.
  prefs: []
  type: TYPE_NORMAL
- en: At its simplest SSH key management can be left up to the individual to manage
    on their own. If users have access to log in to an operating system already, then
    they are free to create and manage their own keys. For smaller organizations or
    those working with just a few system administrators this might make sense.
  prefs: []
  type: TYPE_NORMAL
- en: With SSH keys, of course, management includes two components, the public keys
    and the private keys. Typically users will manage their own private keys, keeping
    them safe, and public keys can be managed in nearly any fashion.
  prefs: []
  type: TYPE_NORMAL
- en: In a smaller organization looking to improve the management of keys with little
    other infrastructure, it can be as easy as storing the public keys for users (which
    includes system administrators) on a wiki or other simple documentation system
    where they can be easily obtained as needed. This one step alone can make a big
    difference in making keys very easy to use.
  prefs: []
  type: TYPE_NORMAL
- en: Keys can also be stored on the filesystem of something like a management server
    or workstation and pushed out through simple automation like a script that runs
    over SSH remotely and copies keys into place. Scripts could also pull public keys
    from a web page, a file share, or use something like GIT or Subversion to grab
    keys from a repository. Keys are simply text files and so managing them is flexible.
  prefs: []
  type: TYPE_NORMAL
- en: In a more advanced setting, state machines and infrastructure as code approaches
    can be used to automate key deployments through the same tooling as other automations.
    Keys can be just another set of files and do not need to be treated specially
    in any way. DevOps processes like state machines and infrastructure as code are
    great mechanisms to make SSH key management vastly simplified
  prefs: []
  type: TYPE_NORMAL
- en: All of that, however, is really barely worth considering. Once you are working
    at any scale with keys and are at a point where key management is going to become
    something on your system administration radar, then it is probably time to look
    at a **public key infrastructure** (**PKI**) system to manage certificates instead
    of keys. SSH uses TLS, the same mechanism as HTTPS and countless other secure
    protocols, and as such it can use the same PKI system that websites use.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in almost all cases using a publicly hosted PKI certificate system
    is going to be problematic for what are almost always private and internal hosts
    in our Linux infrastructure. So, we would be required to run our own certificate
    authority, known as a CA, but this is a standard practice, has extremely low cost
    and overhead, and while the skills to do so are not broadly available they are
    easily acquired. Using SSH certificates *instead* of SSH keys (instead is in quotes
    here because certificates contain keys, the keys always remain under the hood)
    gives us a mechanism to rapidly scale SSH key security for many administrators,
    potentially many end users.
  prefs: []
  type: TYPE_NORMAL
- en: I would not go so far as to call running your own certificate authority and
    building a PKI infrastructure is a best practice, it is a good rule of thumb for
    organizations with more than just a few users connecting to more than a few boxes
    over SSH. The network effect of many users to many different operating system
    instances can mean an explosion in SSH keys that could remain unmanaged if we
    do not take action. Ten administrators, twenty developers, ten testers, and one
    hundred virtual machines alone could create a need to monitor four thousand SSH
    key combinations!
  prefs: []
  type: TYPE_NORMAL
- en: With essentially all operating system supporting SSH today the benefits of a
    robust SSH security strategy are even larger. The easier SSH is to use securely,
    the more likely it is to be used over alternative technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Jump boxes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jump boxes are an important security and management tool that can simplify many
    aspects of system administration. As a concept they are very common, but as a
    term often even seasoned system administrators may not be familiar. A jump box
    is a system that is accessed remotely by system administrators (or regular users,
    but it is typically only used for technical support staff due to the cumbersome
    nature of the design) from which access is then granted on to the systems that
    will be managed.
  prefs: []
  type: TYPE_NORMAL
- en: It is called a jump box because you log into it before *jumping* to another
    system. It is a jumping off point for your tasks. It is common for jump boxes
    to be used for more than just access, but also as a central repository for tools,
    a temporary storage location, or a common location from which to run automation
    scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Jump boxes are often used to provide a central point of direct access to systems
    to get something akin to a hybrid between the features generally associated with
    indirect remote access technologies and traditional direct access. Technically
    a jump box is just a two stage direct access system, but one that can be highly
    useful and avoids the need to use a network router like a VPN or complex proxies
    to accomplish a consolidation of access.
  prefs: []
  type: TYPE_NORMAL
- en: Through access consolidation we can more practically secure our first line of
    access. It is common for jump boxes to receive the most complex IP filtering,
    strict Fail2Ban rules, detailed logging, two factor authentication, rapid patching,
    and so on to tightly secure the most vulnerable point of ingress. In this fashion,
    system administrators might start their day by logging into their jump box and
    then quickly and easily attach to the systems that they administer from that point.
  prefs: []
  type: TYPE_NORMAL
- en: Because of their design, it is often easy for jump boxes to exist inside of
    the LAN, be hosted in colocation, or even be cloud hosted. They can be put wherever
    is practical and given access to resources anywhere. They can be hardened, monitored,
    and then connected through direct access protocols or even a VPN or multiple VPNs
    to systems and sites as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Because a jump box is a single system it is easy to have the systems that we
    manage allow connections from its single IP address to allow good security even
    while using direct access technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Jump boxes for Linux based operating systems are typically used for SSH and
    so may be built as a lean server. A GUI-less Linux jump box can run on one of
    the smallest of virtual machines, these use almost no resources making them very
    easy to deploy wherever needed with little cost.
  prefs: []
  type: TYPE_NORMAL
- en: Jump boxes are also built to handle other protocols, often X or RDP, for example.
    This is uncommon for Linux system administration as rarely is a GUI anything but
    an encumbrance for us and when using a jump box the resource needs and complexity
    of providing a central GUI source will often make us reconsider providing a GUI
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: Jump boxes are not a best practice, but they are a common security and management
    tool and can be very helpful where direct access is needed to make it faster,
    easier, and more secure than it would likely be otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative remote access approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional remote access, at least as we tend to think of it, is all designed
    around the needs of end users needing to use remote sessions as a replacement
    for their local desktop. As system administrators, it is great for us to be able
    to use those tools when they make sense for us, and it is necessary that we understand
    those tools because they are generally components that fall to us to administer,
    but for our own usage they may not be the most practical.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we can include most indirect remote access technologies under the
    heading of alternative remote access approaches, but they are basically just traditional
    access that has been tweaked to be more practical for our use cases. As administrators
    we want to reduce our logins or interactive sessions with remote machines in the
    hopes of removing that access completely, at least in an ideal world.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, we have other methodologies today for running commands on our servers.
    There are not going to replace our existing methods in all cases, but they may
    be a steppingstone technology to help us move from where we are to where we want
    to be in the future.
  prefs: []
  type: TYPE_NORMAL
- en: What makes most of our system access methods *traditional* is really that they
    involve complete interactivity. That means that whether we are using RDP or Splashtop
    or an SSH session, we assume that we are establishing a full connection to the
    system to be managed, complete with user-level environmental settings, and working
    in a manner where we have continuous input and output from the system. This is
    so much assumed that many applications or tools actually assume that this method
    is used and may require session environmental variables that are not logical or
    appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Short of remove access altogether as we discussed earlier when talking about
    infrastructure as code, our interim access step is remote command execution or
    using non-interactive commands. This works effective just as our other access
    methods do, but without the ability to become interactive. Remote command execution
    lets us move from manual tasks to automation more easily and is great for auditing,
    security, and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: At its simplest, remote command execution can be handled through SSH using the
    same infrastructure that we would have for interactive sessions. SSH is designed
    to handle either method with aplomb and since it does so transparently it can
    be an easy tool for moving slowly from one method to the other. The methods can
    be mixed on a situational basis, or one administrator could use one method and
    a second could use the other.
  prefs: []
  type: TYPE_NORMAL
- en: With remote command execution we get the benefit that all commands are executed,
    and therefore can be recorded, on the originating system. Perfect for the use
    of something like a jump box or management server that could log all actions performed.
    Interactive sessions, however, even those initiated from a jump box, will log
    all of the important session information on the final operating system and the
    jump box will only know that a remote session was started - visibility into exactly
    what commands were run there and what their responses were will be lost to the
    central logging platform.
  prefs: []
  type: TYPE_NORMAL
- en: In many ways, remote command execution is a system administrator's analogue
    to functional programming in the software development world. Interactive sessions
    are more akin to procedural programming, where actions are seen as a sequence
    of events. Remote command execution is the use of singular remote functions to
    perform a task. A difficult analogy to apply if you are not familiar with these
    programming paradigms, but for those that have used them I think the example is
    valuable.
  prefs: []
  type: TYPE_NORMAL
- en: SSH may be ideal for sneaking in a casual introduction of remote command execution
    and is essentially always available for system administrators to use. Even in
    highly strict, structured, and formal process driven environments it would be
    highly unlikely for a policy decree to not allow administrators to use this approach
    whenever and wherever they want, assuming traditionally interactive SSH sessions
    are allowed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other tools, and with increasing popularity, also allow remote command execution
    today. This has become a standard option in most indirect remote access tools
    from large, commercial software as a service products to small open source products
    for self-hosting: almost always remote command execution is included in some fashion
    today. In some cases with extended features such as the ability to run the same
    command or set of commands against a set or list of systems simultaneously.'
  prefs: []
  type: TYPE_NORMAL
- en: RMM tools are often building in remote command execution systems in the same
    way. This is far easier than creating custom interactive session mechanisms and
    can be touted as a more advanced option, while being much simpler to implement.
  prefs: []
  type: TYPE_NORMAL
- en: The most interesting place to find remote command execution, in my opinion,
    is in state machine systems. Just as SSH can be viewed as having remote command
    execution as a an optional strategy, to move away from interactive sessions and
    ease into something a bit less familiar, state machines can and do implement remote
    command execution as a way to maintain one foot in a more traditional operational
    mode to allow system administrators a fallback method for when state definitions
    are too difficult or time consuming to implement.
  prefs: []
  type: TYPE_NORMAL
- en: Remote command execution from a state machine is also a method for testing state
    machine access or capabilities while developing state files. State is maintained,
    one way or another, by running commands on the system being managed. In some cases,
    commands are run by an agent at arm's length and while remote commands are executed,
    the exact commands are not sent by the state management central system. In other
    systems commands may be sent directly as typed on the management system and the
    state machine is just an execution assistant.
  prefs: []
  type: TYPE_NORMAL
- en: Remote access, as simple as it might seem at first, is not a one size fits all
    solution and we do not have to use only a single solution even inside of a single
    organization. Consider thinking outside of the box and trying new or different
    approaches to make your workflows more secure, stable, and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we are prepared to system administrator access and security across
    a broad variety of approaches, we should talk a little about technologies more
    applicable to end user access to systems: terminal servers and virtual desktop
    infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: Terminal servers and virtual desktop infrastructure (VDI)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike the Windows world, remote GUI access in the world of Linux based operating
    systems is relatively rare. This is just not part of the Linux wheelhouse in a
    self-fulfilling situation where customers do not demand it, so vendors do not
    specialize around it, leaving customers feeling that little is available for it
    and the cycle continues. But that is not to say that both terminal services and
    **VDI** (which stands for **Virtual Desktop Infrastructure** but is more meaningful
    and known simply by its acronym) options cannot or do not exist for Linux based
    systems, they most certainly do.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding terminal services and VDI conceptually
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is not uncommon for terminal servers and VDI architectures to become intertwined,
    this mostly has happened because of marketing departments trying to sell VDI where
    it does not apply and because overlapping technologies are often used. That VDI
    was presented as the hot, new technology as if it had not always existed added
    to this confusion. And Microsoft, the leader in this space, renaming their core
    product from **Terminal Server** to **Remote Desktop Server** (**RDS**) did not
    help any either. This led to the problem that many Windows administrators, let
    alone users, routinely confused RDP, the Remote Desktop Protocol, with RDS, Microsoft's
    terminal server product. One is a communications protocol used by and implemented
    by many different products; the other is a specific product, as well as licensing
    vehicle, that you have to purchase from Microsoft that may or may not utilize
    RDP.
  prefs: []
  type: TYPE_NORMAL
- en: So we have to start by defining these technologies. Both involve accessing a
    computer remotely. Both can and do use the same potential set of protocols to
    make this magic happen. Both serve the same basic purpose but implement it in
    two different ways.
  prefs: []
  type: TYPE_NORMAL
- en: 'A terminal server has always meant a *single server* (meaning: a *single operating
    system instance*) that can be accessed simultaneously by multiple remote users.
    In the earliest days, this was accomplished by using serial connections to dumb
    terminals to display text *remotely*. Later came technologies such as telnet and
    RSH and eventually SSH as we use today. All of these technologies advanced the
    state of security and accessibility, but fundamentally remote access remained
    a familiar command line activity resembling the original serial-based physical
    terminals. Decades ago, the remote use of graphical desktop environments became
    the norm for end users, and new protocols such as RDP, RFB, X, and NX became popular,
    but nothing really changed fundamentally. Many users would connect to a single
    operating system instance, and they would share its resources. There was only
    one operating system to patch, and everyone shared the same kernel and applications.
    A terminal server uses a *many-to-one* architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: VDI, which stands for virtual desktop infrastructure, refers to an alternative
    approach where users remotely access dedicated operating system instances that
    exist only for them, and which are virtualized. With VDI, each user's operating
    system instance could be completely different with different patch levels or even
    different operating systems entirely. One user might be on **Windows 11**, another
    on Windows XP, and another on Ubuntu. VDI means a *one-to-one* architecture.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, the concepts of and differences between these two approaches
    are created almost entirely out of the need to manage limitations created by Microsoft
    software licensing. In the Linux world, the difference between a terminal server
    or a VDI deployment is purely a matter of how any given system is used *in-the-moment*
    at the time that they are being accessed. Every Linux device is inherently multi-user
    already. Linux lacks the *one user at a time* framework that has always been a
    part of the Windows ecosystem. These concepts are often lost or confusing in a
    non-Windows context. For Linux administrators, any VDI system is just many terminal
    servers that tend to be lightly used. In the Windows world, licensing dictates
    every aspect of this equation in very complex ways.
  prefs: []
  type: TYPE_NORMAL
- en: In the Windows world, an end user workstation license as available for Windows
    7, 10, or 11 is always a single user license, no exceptions. Remote access is
    always allowed, but only for the singular user never for alternative or additional
    users. One user sitting at the console or that user remote, but never more than
    one at a time. Even Windows Server licenses only allow for one user at a time
    (with one additional license purely for the purposes of systems administration
    purposes) unless RDS licensing is purchased for additional users. RDS is only
    available as an add-on license to Windows Server. Because of these licensing rules,
    the ideas of what is a server and what is an end user workstation in the Windows
    world is exceptionally clear and obvious.
  prefs: []
  type: TYPE_NORMAL
- en: With Linux, of course, there is no such licensing limitations. Any workstation
    can do the job of a server, any server can be used as a desktop or for multiple
    users. Any machine can be used in any way, at any time, flexibly. Terms like terminal
    servers and VDI refer only to how we intend to use a system or how it is being
    used at any given moment, but to most non-Linux administrators it is a locked-in,
    set in stone, expensive, licensing-driven design and decision process and so we
    have to be adaptable to understanding how these terms and concepts play into the
    consciousness of those outside of our free and flexible universe.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there is an obvious, third option that has no name. Terminal services
    does not consider if a system is physical (that is to say, installed directly
    on bare metal hardware) or virtualized, in both cases it is a terminal server,
    and the virtualization is irrelevant. With VDI, virtualization is part of the
    name, so we only consider it to be VDI when it is also virtualized for no obvious
    reason. If we do the same logical architecture as VDI but do so on bare metal
    operating system installs, then the architecture has no name, at least no well-known
    name. One to one remote access without virtualization is actually the most common,
    base approach to remote access, so much so that most people do not even think
    about it as an architecture. It requires essentially no planning or coordination
    and is often used for a variety of purposes. This unnamed design is common for
    any operating system Linux, Windows, macOS, or otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linux based operating systems will support any remote access architecture that
    we desire and will even support protocols typically associated with other operating
    systems: namely RDP from the Windows ecosystem. In the Windows ecosystem, RDS
    is technologically bound to Active Directory causing user management to be closely
    coupled with remote access strategies. In Linux we have no such ties. Deploying
    an RDP-based terminal server can be done using any user management system that
    we desire.'
  prefs: []
  type: TYPE_NORMAL
- en: Tools like terminal servers and VDI are far more likely to be used by end users,
    from office workers to developers, than by system administrators, but this is
    not exclusive. In my own deployment today we maintain both for use exclusively
    by system administration. I will use this as an example scenario to show how these
    technologies may be used effectively on the administration side of the fence.
  prefs: []
  type: TYPE_NORMAL
- en: On occasion, system administrators may benefit from having a graphical session
    for support. This may be because graphical tools like a notepad, screenshots,
    web interface or such make the work more efficient, or in some cases the tooling
    requires a graphical session. It is sadly common that many systems, rarely Linux
    itself but often systems required for use by Linux system administrators, will
    only provide a web-based GUI interface or worse, something like a Java application
    interface or even a native application. Having a Linux terminal server built for
    multi-user RDP support in our case located inside our main, trusted datacenter
    with a static IP allows our administration team to have access for any and all
    team members at any time to open a graphical session in a trusted location to
    perform whatever work is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Having a graphical interface is also at times highly beneficial for making documentation
    while working, doing training via screensharing, or similar tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Terminal servers are perfect, especially with Linux based operating systems,
    for providing a clean, standard environment that everyone shares. VDI provides
    a competing approach and we, like many companies, use VDI to provide highly customized
    environments that individual system administrators may require such as alternative
    operating systems or desktop environments that would create conflicts with other
    users if these were implemented on a shared server environment. VDI is also better
    for situations where individual system administrators may require to be administrators
    of their own environments, often for testing, that may not be wise or plausible
    from a security perspective on a shared environment.
  prefs: []
  type: TYPE_NORMAL
- en: Both terminal servers and VDI, whether graphical or command line only, can be
    useful as platforms to be used as jump boxes, management stations, remote execution
    environments and so forth. We can also use them, of course, as ways to provide
    Linux based desktop environments to end users. There is quite simply, no reason
    to limit these types of technologies conceptually to Windows, Linux can shine
    here as well and, in many cases, excel.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Users and user access on Linux based operating systems is a complex topic, mostly
    because of the incredible flexibility that Linux affords us. We can approach where
    users exist, how we create them, how they are managed, where our source of truth
    resides, and how those users can access their systems in so many ways. We have
    ancient technologies, we have extremely modern technologies. We can use nearly
    any mechanism, from any era, from any ecosystem and we can have many that we build
    ourselves and our unique to us. We can stick to well-known traditional processes,
    or we can easily build our own and work in a unique way.
  prefs: []
  type: TYPE_NORMAL
- en: There is no simple best practice for user management on Linux. Instead, our
    best practice is, like it so often is, that we need to understand the range of
    technological possibilities, how different risks and benefits will apply to our
    unique organization and know the products that exist on the market from open source
    to commercial, from software to services and evaluate those needs across all axes
    to determine what is right for our organizations. There are no built-in assumptions.
    The use of local user accounts is not wrong, even when done at very large scale.
    Using remote users is not wrong, even at a very small scale. We do not have to
    maintain and run our own infrastructure for security, but we do not have to rely
    on third-party vendors to do it for us either. The sky is the limit – most approaches
    have benefits, but all of them have caveats, so they all warrant thorough investigation.
  prefs: []
  type: TYPE_NORMAL
- en: There is always a tendency to feel a need to make any system overly complex.
    We are taught that complex is good, as it is advanced, and it feels right. But
    in the end, simplicity usually provides the lowest total cost of ownership while
    carrying the least risk. With simple systems and simple designs there are fewer
    moving parts providing fewer opportunities for mistakes and this typically wins
    the day.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that with all of this information that you are armed with the knowledge
    and courage to approach user management in a different light. Too many user management
    solutions are chosen out of a misunderstanding that industry trends should drive
    decisions or that common solutions are far more secure than they actually are.
    What is best is always what is best for your situation and what is right for you
    is rarely what is right for someone else that you are comparing against and only
    with extreme rarity has anyone that you will compare against taken the time to
    have truly evaluated their own needs so allowing others' decisions to overly influence
    us tend to be very dangerous.
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we are going to be tackling the incredibly complex and
    often confusing topic of troubleshooting.
  prefs: []
  type: TYPE_NORMAL
