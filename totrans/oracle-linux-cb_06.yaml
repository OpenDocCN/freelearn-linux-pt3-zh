- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Eliminating All the SPOFs! An Exercise in Redundancy
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消除所有 SPOFs！冗余练习
- en: It’s crucial to have redundancy in your architecture to ensure smooth operations.
    Eliminating **Single Points of Failure** (**SPOFs**) is a common way to achieve
    this. Implementing **High-Availability** (**HA**) technology is one way to eliminate
    SPOFs. HA technology is crucial because it helps businesses maintain operational
    continuity, enhance performance, improve reliability, facilitate **Disaster Recovery**
    (**DR**), build customer trust, and comply with regulations. With HA technology,
    minimizing downtime and ensuring continuous service availability is possible,
    contributing to overall success and competitiveness in today’s technology-driven
    world.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在架构中确保冗余是至关重要的，以确保平稳的操作。消除**单点故障**（**SPOFs**）是实现这一目标的常见方法。实施**高可用性**（**HA**）技术是消除
    SPOFs 的一种方式。HA 技术至关重要，因为它帮助企业保持运营连续性，提升性能，提高可靠性，促进**灾难恢复**（**DR**），建立客户信任，并遵守法规。通过
    HA 技术，可以最小化停机时间并确保持续的服务可用性，从而为今天技术驱动的世界中的整体成功和竞争力做出贡献。
- en: Note
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 注释
- en: What about DR? We are not covering the failure of data centers in this chapter.
    That being said, the approach to implementing DR is very different than HA. When
    running in the cloud, look for services such as Oracle Full Stack Disaster Recovery
    Service that automate the DR process for the entire tech stack.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 那么灾难恢复呢？本章不涵盖数据中心故障的问题。话虽如此，灾难恢复的实施方法与 HA 非常不同。在云中运行时，可以寻找像 Oracle 全栈灾难恢复服务这样的服务，它们为整个技术堆栈自动化灾难恢复过程。
- en: 'The recipes in this chapter will help you eliminate SPOFs in your environment.
    We’ll start with a general overview of what HA is, as well as availability, and
    then get into several examples of how you can add redundancy to your application.
    The four most common technologies that are used to help eliminated SPOFs in applications
    are HAProxy, Corosync, Pacemaker, and GlusterFS. Each of these provides a specific
    set of features to help make an application highly available:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的操作指南将帮助你消除环境中的 SPOFs。我们将首先概述 HA 和可用性，然后介绍如何为你的应用程序增加冗余的几个示例。帮助消除应用程序中的 SPOFs
    的四种最常见技术是 HAProxy、Corosync、Pacemaker 和 GlusterFS。它们每个都提供了一套特定的功能来帮助使应用程序高度可用：
- en: '**HAProxy**: This is a load balancer, and allows you to balance web workloads
    between servers'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HAProxy**：这是一个负载均衡器，允许你在服务器之间平衡 Web 工作负载'
- en: '**Corosync**: This is a communication system that enables communication to
    help implement HA within applications'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Corosync**：这是一个通信系统，支持应用程序内部实现 HA 的通信。'
- en: '**Pacemaker**: Pacemaker is an open source resource manager that is used to
    build small and large clusters'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pacemaker**：Pacemaker 是一个开源资源管理器，用于构建小型和大型集群。'
- en: '**GlusterFS**: This is a scalable network filesystem that allows multiple nodes
    to read and write data to the same storage at the same time'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GlusterFS**：这是一种可扩展的网络文件系统，允许多个节点同时读取和写入同一存储中的数据。'
- en: 'The following recipes will help you implement these common HA technologies.
    This includes everything from load-balancing applications and clustering application
    systems to clustered storage and redundant networking:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的操作指南将帮助你实现这些常见的 HA 技术。这包括从负载均衡应用程序和集群应用系统到集群存储和冗余网络的一切内容：
- en: Getting 99.999% availability and beyond
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得 99.999% 的可用性及更高
- en: Load balancing a website
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网站的负载均衡
- en: Making HAProxy highly available with Keepalived
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keepalived 使 HAProxy 高可用
- en: HA clustering for all with Corosync and Pacemaker
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Corosync 和 Pacemaker 实现 HA 集群
- en: Sharing a filesystem across multiple machines – cluster or distribute?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个机器间共享文件系统——集群还是分布式？
- en: Generating, configuring, and monitoring Ethernet traffic over bond
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成、配置并监控通过 bond 的以太网流量
- en: Technical requirements
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For most of these recipes, you will need a pair of Oracle Linux 8 systems. As
    with most of these recipes, a VM on your desktop using a desktop virtualization
    product such as Oracle VirtualBox is recommended. A small VM is fine, with two
    cores, 2 GB RAM, and a few free gigabytes of disk space. You will also need some
    additional disks assigned to the VM, ideally at least five equally sized disks.
    Before you start, patch your system to the latest packages available. This only
    takes a few minutes and can save a ton of time when troubleshooting issues that
    are caused by a bug.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些大多数食谱，你需要一对Oracle Linux 8系统。与大多数这些食谱一样，建议在桌面上使用桌面虚拟化产品（如Oracle VirtualBox）创建虚拟机。小型虚拟机就足够了，配有两个核心、2GB内存和一些可用磁盘空间。你还需要为虚拟机分配一些额外的磁盘，最好至少有五个大小相等的磁盘。在开始之前，先将你的系统更新到最新的软件包。这样做只需要几分钟，但可以节省很多排除因错误导致问题的时间。
- en: Many of the recipes in this book have their related configuration files available
    on GitHub at [https://github.com/PacktPublishing/Oracle-Linux-Cookbook](https://github.com/PacktPublishing/Oracle-Linux-Cookbook).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的许多食谱相关配置文件可在GitHub上找到，链接：[https://github.com/PacktPublishing/Oracle-Linux-Cookbook](https://github.com/PacktPublishing/Oracle-Linux-Cookbook)。
- en: Getting 99.999% availability and beyond
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获得99.999%及以上的可用性
- en: 'This recipe will discuss the differences between DR and HA and how to architect
    HA solutions. Before we get into that, let’s refine the definition of a few key
    terms:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论灾难恢复（DR）和高可用性（HA）之间的区别，以及如何构建HA解决方案。在我们开始之前，先来澄清几个关键术语的定义：
- en: '**High availability**, or **HA**: This means protecting from the failure of
    a single component. Think of this as protecting against the failure of a system.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高可用性**，或**HA**：指的是防止单个组件故障的措施。可以将其理解为防止整个系统故障的保护措施。'
- en: '**Disaster recovery**, or **DR**: This is the failure of the data center or
    cloud region.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灾难恢复**，或**DR**：指的是数据中心或云区域的故障。'
- en: '**Availability nines**: When referring to *nines of availability*, it is a
    way to quantify the uptime or reliability of a system by specifying the number
    of nines in the uptime percentage. Each *nine* represents a decimal place in the
    uptime percentage.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性九个**：当提到*可用性的九个*时，它是通过指定正常运行时间百分比中的九个数字来量化系统的正常运行时间或可靠性。每个*九个*代表正常运行时间百分比中的一个小数位。'
- en: 'Here’s a breakdown of the most commonly used *nines* and their corresponding
    uptime percentages, assuming 24x7x365 operations:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是最常用的*nines*及其对应的正常运行时间百分比的分解，假设24x7x365的运营模式：
- en: '| **Nines** | **Downtime** **per Year** | **Downtime** **per Month** |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **九个** | **每年停机时间** | **每月停机时间** |'
- en: '| 99 | 3d 14h 56m 18s | 7h 14m 41s |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 99 | 3天 14小时 56分钟 18秒 | 7小时 14分钟 41秒 |'
- en: '| 99.9 | 8h 41m 38s | 43m 28s |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 99.9 | 8小时 41分钟 38秒 | 43分钟 28秒 |'
- en: '| 99.99 | 52m 10s | 4m 21s |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 99.99 | 52分钟 10秒 | 4分钟 21秒 |'
- en: '| 99.999 | 5m 13s | 26s |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 99.999 | 5分钟 13秒 | 26秒 |'
- en: '| 99.9999 | 31s | 2.6s |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 99.9999 | 31秒 | 2.6秒 |'
- en: Table 6.1 – Nines downtime
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.1 – 九个停机时间
- en: In the preceding table, each additional nine in the uptime percentage signifies
    a higher level of availability and a reduced tolerance for downtime. Achieving
    higher numbers of nines typically requires implementing redundant systems, failover
    mechanisms, and rigorous maintenance practices to minimize downtime and ensure
    continuous operation. In addition, when setting up a **Service-Level Agreement**
    (**SLA**), you can also define the uptime during business hours and exclude scheduled
    maintenance. As an example, using a working schedule of Monday through Friday
    with 12 working hours a day, and 10 holidays off per year, the matrix would look
    very different!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表格中，正常运行时间百分比中每增加一个九个，意味着更高的可用性和更低的停机容忍度。实现更多的九个通常需要实施冗余系统、故障切换机制和严格的维护实践，以最小化停机时间并确保持续运行。此外，在设置**服务级别协议**（**SLA**）时，还可以定义在工作时间内的正常运行时间，并排除计划的维护。例如，使用每周一到周五、每天12小时工作、每年10个假期的工作计划时，矩阵会看起来非常不同！
- en: '| **Nines** | **Downtime** **per Year** | **Downtime** **per Month** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **九个** | **每年停机时间** | **每月停机时间** |'
- en: '| 99 | 1d 7h 2m 58s | 2h 35m 14s |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 99 | 1天 7小时 2分钟 58秒 | 2小时 35分钟 14秒 |'
- en: '| 99.9 | 3h 6m 18s | 15m 31s |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 99.9 | 3小时 6分钟 18秒 | 15分钟 31秒 |'
- en: '| 99.99 | 18m 38s | 1m 33s |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 99.99 | 18分钟 38秒 | 1分钟 33秒 |'
- en: '| 99.999 | 1m 52s | 9s |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 99.999 | 1分钟 52秒 | 9秒 |'
- en: '| 99.9999 | 11s | 1s |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 99.9999 | 11秒 | 1秒 |'
- en: Table 6.2 – Business hours downtime
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.2 – 工作时间停机时间
- en: Note
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: When setting SLAs with the business, carefully understand the differences between
    including maintenance windows and operational hours within the SLA.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在与业务部门设置SLA时，务必清楚理解在SLA中包括维护窗口和运营时间的区别。
- en: Getting ready
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'When designing HA systems, there are several considerations that need to be
    taken into account to ensure the system is resilient and can handle failures.
    Here are some key considerations:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计HA系统时，需要考虑几个方面，以确保系统具有弹性并能够应对故障。以下是一些关键考虑因素：
- en: '**Redundancy**: Having redundancy in HA systems is essential. It requires replicating
    components or whole systems to eliminate potential SPOFs. Redundancy can be implemented
    at different levels, such as hardware, software, and network infrastructure. To
    minimize the impact of localized failures, it’s crucial to distribute redundant
    components across different physical locations.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冗余**：在HA系统中实现冗余是必不可少的。这需要复制组件或整个系统，以消除潜在的单点故障（SPOF）。冗余可以在不同的层面实现，如硬件、软件和网络基础设施。为了最小化局部故障的影响，必须将冗余组件分布在不同的物理位置。'
- en: '**Failover and load balancing**: It is important for HA systems to be equipped
    with failover mechanisms that enable automatic switching to a backup system whenever
    a failure occurs. One way to achieve this is through replicating data and services
    across multiple servers, coupled with the use of load-balancing techniques that
    ensure the even distribution of workload. With load balancing, traffic can be
    easily redirected to available servers in the event of a server failure.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障转移和负载均衡**：HA系统必须配备故障转移机制，使其在发生故障时能够自动切换到备份系统。实现这一点的一种方式是通过将数据和服务复制到多台服务器，并结合使用负载均衡技术，确保负载的均匀分配。通过负载均衡，流量可以在服务器故障时轻松地重定向到可用的服务器。'
- en: '**Scalability**: When designing HA systems, it is important to ensure that
    they can handle increased workloads and scale effortlessly. This can be achieved
    through horizontal scaling, which entails adding more servers to distribute the
    load, or vertical scaling, which involves adding resources to existing servers.
    Additionally, the system should be capable of dynamically adjusting resource allocation
    based on demand to prevent overloading.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：在设计高可用性（HA）系统时，确保系统能够处理增加的工作负载并能够轻松扩展非常重要。这可以通过水平扩展来实现，即添加更多的服务器以分配负载，或者通过垂直扩展来实现，即向现有服务器添加资源。此外，系统应能够根据需求动态调整资源分配，以防止过载。'
- en: '**Data replication and backup**: Maintaining data integrity and availability
    is crucial for HA systems. To ensure that data can still be accessed in case of
    a system failure, it is essential to replicate data across multiple storage systems
    or databases. Additionally, performing regular backups is vital to safeguard against
    potential data loss or corruption.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据复制和备份**：维护数据的完整性和可用性对HA系统至关重要。为了确保在系统故障时仍能访问数据，必须将数据复制到多个存储系统或数据库中。此外，定期备份对于防范潜在的数据丢失或损坏至关重要。'
- en: '**Fault tolerance**: Systems used for highly available architectures should
    have fault tolerance, which means they must be able to function even if specific
    components or subsystems malfunction. Achieving this requires creating a system
    that can manage errors with ease, recover automatically, and ensure continuity
    of service.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：用于高可用架构的系统应具备容错能力，这意味着即使特定组件或子系统发生故障，系统仍然能够正常运行。实现这一点需要构建一个能够轻松处理错误、自动恢复并确保服务连续性的系统。'
- en: '**Disaster recovery**: Having a DR plan is crucial for HA systems to effectively
    deal with catastrophic events such as natural disasters or widespread outages.
    This plan entails generating off-site backups, setting up secondary data centers,
    and relying on cloud-based services to guarantee business continuity, even amid
    extreme situations.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灾难恢复**：对于HA系统而言，拥有灾难恢复（DR）计划对于有效应对自然灾害或大规模停机等灾难性事件至关重要。该计划包括生成异地备份、设置备用数据中心以及依赖基于云的服务，确保即使在极端情况下也能保证业务连续性。'
- en: '**Documentation and testing**: It is crucial to document the system architecture,
    configurations, and procedures to effectively troubleshoot and maintain the HA
    system. Regular testing, such as failover tests, load testing, and DR drills,
    plays a significant role in identifying potential issues and ensuring the system
    operates as intended in various scenarios.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档和测试**：记录系统架构、配置和操作流程对于有效地排查故障和维护HA系统至关重要。定期测试，如故障转移测试、负载测试和灾难恢复演练，对于识别潜在问题并确保系统在各种场景中按预期运行起着重要作用。'
- en: '**Cost and complexity**: Designing, implementing, and maintaining HA systems
    can be both complex and costly. It is important to carefully consider the available
    budget, as well as the expertise and resources required to effectively manage
    and monitor the system.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本和复杂性**：设计、实施和维护高可用性系统可能既复杂又昂贵。重要的是仔细考虑可用预算，以及有效管理和监控系统所需的专业知识和资源。'
- en: By addressing these considerations, you can design a robust and resilient HA
    system that ensures HA, fault tolerance, and continuity of critical services.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解决这些考虑因素，您可以设计一个稳健且具有韧性的高可用性系统，确保高可用性、容错性和关键服务的连续性。
- en: How to do it…
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: As a rule, you should pick the right technology for the right subsystem and
    application.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，您应选择适合的技术来应对合适的子系统和应用程序。
- en: When aiming to achieve HA for a web application, the first step is to place
    a load balancer in front of the web servers. This enables scaling of the application
    while also offering some fault tolerance for these systems. However, attention
    should also be given to the data tier, which can be addressed by clustering the
    database or building a cluster capable of running the database, depending on the
    limitations of the database technology.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在旨在实现高可用性（HA）的Web应用程序中，第一步是将负载均衡器放置在Web服务器前面。这使得应用程序可以扩展，同时为这些系统提供一些容错能力。然而，还应关注数据层，可以通过对数据库进行集群化或根据数据库技术的限制构建一个能够运行数据库的集群来解决这一问题。
- en: If you are utilizing a technology such as Oracle Database, you have the option
    to establish a database-specific cluster known as **Oracle Real Application Clusters**
    (**Oracle RAC**). This cluster allows for both scalability and availability. With
    RAC, the database remains accessible for queries as long as one node is online.
    While other databases may utilize their own exclusive clustering technology (such
    as MySQL Cluster), you may opt to utilize generic cluster technologies such as
    Pacemaker for cluster management and Corosync for inter-cluster communications.
    This approach presents the advantage of enabling almost any technology to be made
    highly available in a Linux environment.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是如Oracle数据库这样的技术，您可以选择建立一个特定于数据库的集群，称为**Oracle Real Application Clusters**（**Oracle
    RAC**）。这个集群同时支持可扩展性和可用性。使用RAC时，只要一个节点在线，数据库就可以继续提供查询服务。虽然其他数据库可能会利用其独有的集群技术（例如MySQL
    Cluster），您也可以选择使用通用的集群技术，如Pacemaker进行集群管理，使用Corosync进行集群间的通信。这种方法的优点是可以在Linux环境下使几乎任何技术实现高可用性。
- en: You can achieve HA in storage by implementing filesystems across the entire
    cluster. Gluster allows you to mount a filesystem across multiple servers, while
    at the same time replicating the storage across servers. This provides both scalability
    and reliability at the filesystem level.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在整个集群上实现文件系统来实现存储的高可用性。Gluster允许您在多个服务器之间挂载文件系统，同时在服务器之间复制存储。这在文件系统级别提供了可扩展性和可靠性。
- en: Finally, the network is a common point of failure, and using network bonding
    technologies can enable both HA as well as some scaling abilities. This works
    by combining at least two network ports into a single virtual port.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，网络是常见的故障点，使用网络绑定技术可以实现高可用性以及一定的扩展能力。这通过将至少两个网络端口合并为一个虚拟端口来工作。
- en: Note
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The best HA architectures mix these approaches to cover the entire technology
    stack.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的高可用性架构将这些方法混合使用，覆盖整个技术栈。
- en: Load balancing a website
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网站负载均衡
- en: Nowadays, most applications are web-based, whether it’s a traditional web interface
    or a RESTful API. This first tier is typically set up for HA using a load balancer.
    A load balancer is a system that distributes incoming network traffic or workload
    across multiple servers or resources. Its main goal is to optimize resource utilization,
    improve performance, and ensure the reliability and availability of applications
    or services. When multiple servers are involved in serving a particular application
    or service, a load balancer acts as an intermediary between the client and the
    server pool. It receives incoming requests from clients and intelligently distributes
    them across the available servers based on various algorithms, such as round-robin,
    least connections, or weighted distribution.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，大多数应用程序都是基于 Web 的，无论是传统的 Web 界面还是 RESTful API。这一层通常会使用负载均衡器进行高可用性设置。负载均衡器是一个将传入的网络流量或工作负载分配到多个服务器或资源的系统。其主要目标是优化资源利用、提高性能，并确保应用程序或服务的可靠性和可用性。当多个服务器参与提供特定应用程序或服务时，负载均衡器充当客户端和服务器池之间的中介。它接收来自客户端的请求，并根据不同的算法（如轮询、最少连接数或加权分配等）智能地将请求分配到可用服务器上。
- en: The load balancer is responsible for ensuring the servers’ optimal health and
    performance by redirecting traffic from overloaded or problematic servers. This
    distribution of workloads helps to prevent any one server from getting overwhelmed,
    thus enhancing response time and overall system capacity and scalability.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器负责通过将流量从超负荷或有问题的服务器重定向到其他服务器，确保服务器的最佳健康和性能。这种工作负载的分配有助于防止任何一台服务器被过载，从而提高响应时间和整体系统的容量与可扩展性。
- en: Note
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: While a load balancer can help distribute the workloads, the actual server load
    is based on other factors, so do not expect all servers to have the same utilization
    of CPU, RAM, networking, and so on.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然负载均衡器可以帮助分配工作负载，但实际的服务器负载是由其他因素决定的，因此不要期望所有服务器的 CPU、内存、网络等资源的利用率相同。
- en: Load balancers not only distribute traffic but also offer advanced features,
    such as SSL termination, session persistence, caching, and content routing. They
    are extensively used in web applications, cloud-based services, and other environments
    that demand HA and scalability.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器不仅分配流量，还提供一些高级功能，例如 SSL 终止、会话持久性、缓存和内容路由等。它们广泛应用于 web 应用、云服务以及其他需要高可用性和可扩展性的环境中。
- en: One of the most popular load balancers is HAProxy.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的负载均衡器之一是 HAProxy。
- en: HAProxy is a great open source load-balancer option. Standing for **High Availability
    Proxy**, **HAProxy** is widely used due to its excellent performance and ability
    to improve the availability and scalability of applications. Operating at the
    application layer (Layer 7) of the OSI model, this software is able to make routing
    decisions based on specific application-level information, such as HTTP headers
    and cookies. Compared to traditional network-level (Layer 4) load balancers, HAProxy
    allows for more advanced load-balancing and traffic-routing capabilities.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy 是一个出色的开源负载均衡器选项。HAProxy 代表着 **高可用性代理**，因其卓越的性能以及提高应用程序可用性和可扩展性的能力而广泛应用。该软件工作在
    OSI 模型的应用层（第 7 层），能够根据特定的应用层信息（例如 HTTP 头部和 Cookie）做出路由决策。与传统的网络层（第 4 层）负载均衡器相比，HAProxy
    允许更先进的负载均衡和流量路由功能。
- en: 'Some key features and capabilities of HAProxy include the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy 的一些关键特性和功能包括以下内容：
- en: '**Load balancing**: With HAProxy, incoming traffic can be evenly distributed
    across multiple servers using various algorithms, such as round-robin, least connections,
    and source IP, among others.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载均衡**：使用 HAProxy，传入的流量可以通过多种算法均匀分配到多个服务器上，例如轮询、最少连接数和源 IP 等。'
- en: '**High availability**: One of the beneficial features of HAProxy is its ability
    to support active-passive failover setups. In the event the active server becomes
    unavailable, a standby server will take over. Additionally, it also has the capability
    to monitor the health of servers and make automatic adjustments to the load-balancing
    pool by adding or removing servers.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高可用性**：HAProxy 的一大优点是它支持主动-被动故障转移的设置。当活动服务器变得不可用时，备用服务器将接管。此外，它还能够监控服务器的健康状况，并通过添加或移除服务器自动调整负载均衡池。'
- en: '**Proxying**: One of HAProxy’s primary functions is to act as a reverse proxy,
    which involves receiving client requests and directing them to the correct backend
    servers. Additionally, it can function as a forward proxy by intercepting client
    requests and directing them to external servers.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**：HAProxy的主要功能之一是充当反向代理，它通过接收客户端请求并将其指向正确的后端服务器。此外，它还可以作为正向代理，拦截客户端请求并将其指向外部服务器。'
- en: '**SSL/TLS termination**: With HAProxy, SSL/TLS encryption and decryption can
    be efficiently managed, taking the load off of the backend servers.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SSL/TLS终止**：使用HAProxy，SSL/TLS加密和解密可以高效地管理，减轻后端服务器的负担。'
- en: '**Session persistence**: HAProxy is capable of preserving session affinity
    by routing follow-up requests from a client to the same backend server, thus guaranteeing
    the proper operation of session-based applications.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**会话保持**：HAProxy能够通过将来自客户端的后续请求路由到同一个后端服务器来保持会话亲和力，从而保证基于会话的应用程序的正常运行。'
- en: '**Health checks and monitoring**: To guarantee the availability and optimal
    performance of backend servers, HAProxy conducts routine health checks. It has
    the ability to identify failed servers and promptly exclude them from the load-balancing
    pool.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**健康检查和监控**：为了保证后端服务器的可用性和最佳性能，HAProxy会进行定期的健康检查。它能够识别出失败的服务器并及时将其排除在负载均衡池之外。'
- en: '**Logging and statistics**: With HAProxy, administrators can effectively monitor
    and analyze traffic patterns, performance metrics, and error conditions. Its detailed
    logging and statistics feature makes this possible.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志记录和统计**：通过HAProxy，管理员可以有效地监控和分析流量模式、性能指标和错误状况。其详细的日志记录和统计功能使这一切成为可能。'
- en: HAProxy can be deployed on various operating systems and is often used in high-traffic
    web environments, cloud infrastructure, and containerized deployments. Its versatility
    and extensive feature set make it a powerful tool for managing and optimizing
    application traffic and open source-based load balancers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy可以部署在多种操作系统上，且常用于高流量的Web环境、云基础设施和容器化部署。它的多功能性和丰富的功能集使其成为管理和优化应用流量以及开源负载均衡器的强大工具。
- en: 'For this recipe, we will put HAProxy on one system (as the load balancer) and
    then two identical web servers to balance traffic to:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将把HAProxy安装在一台系统上（作为负载均衡器），然后再部署两台相同的Web服务器来平衡流量到：
- en: '![Figure 6.1 – HAProxy example diagram](img/B18349_06_01.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – HAProxy示例图](img/B18349_06_01.jpg)'
- en: Figure 6.1 – HAProxy example diagram
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – HAProxy示例图
- en: Getting ready
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To get started, we first need three servers. For this exercise, we will call
    them `lb1`, `web1`, and `web2`. They are identical systems, each with 8 GB RAM,
    4 vCPUs, and 100 GB of drive space. The filesystems have 50 GB in `/`, 5 GB in
    `/home`, and 8 GB in swap. The remaining disk space is unallocated. You will also
    need the IP address for each host. In this example, the following IP addresses
    were used:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，首先我们需要三台服务器。在本练习中，我们将其命名为`lb1`、`web1`和`web2`。它们是相同的系统，每台有8GB内存、4个vCPU和100GB硬盘空间。文件系统中，`/`有50GB，`/home`有5GB，交换空间为8GB。剩余的磁盘空间为未分配。你还需要每台主机的IP地址。在本示例中，使用了以下IP地址：
- en: '| **Host** | **IP** |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| **Host** | **IP** |'
- en: '| **Web1** | `192.168.56.200` |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| **Web1** | `192.168.56.200` |'
- en: '| **Web2** | `192.168.56.201` |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| **Web2** | `192.168.56.201` |'
- en: '| **Lb1** | `192.168.56.202` |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **Lb1** | `192.168.56.202` |'
- en: Table 6.3 – HAProxy IP addresses
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3 – HAProxy IP地址
- en: 'Once the server is built, patch it to the current software with the following
    command:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦服务器搭建完成，使用以下命令将其更新到当前的软件版本：
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once the software is patched, reboot the systems.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦软件打上补丁，重启系统。
- en: Web servers
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Web服务器
- en: 'For both web servers, we will install Apache, using the following commands
    as the root user:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个Web服务器，我们将安装Apache，作为root用户，使用以下命令：
- en: '[PRE1]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We next need to enable port `80` to pass through the firewall, with the following
    command:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要启用端口`80`通过防火墙，使用以下命令：
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We then need to start the server and make it start on boot. This is done with
    the following command:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要启动服务器，并设置其开机启动。可以使用以下命令完成：
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We now need to put in a basic home page for this server.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要为该服务器设置一个基础的主页。
- en: Note
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Depending on your environment, you may need to edit the Apache config file
    in `/etc/httpd/conf/httpd.conf` to specify your `servername`. In the config file,
    it will be a single entry on a single line, like the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的环境，可能需要编辑Apache配置文件`/etc/httpd/conf/httpd.conf`来指定`servername`。在配置文件中，它将是单行条目，如下所示：
- en: '**ServerName server.m57.local:80**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**ServerName server.m57.local:80**'
- en: 'This needs to go into `/var/www/html/index.html`. The following is an example
    file. Adjust the text as needed so each server is unique. This way, you can see
    what server is being hit:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要放入 `/var/www/html/index.html`。以下是一个示例文件。根据需要调整文本，使每个服务器都是唯一的。这样，你可以看到是哪台服务器被访问：
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, point your browser to the system to test it:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用浏览器访问系统进行测试：
- en: '![Figure 6.2 – Simple website](img/B18349_06_02.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – 简单网站](img/B18349_06_02.jpg)'
- en: Figure 6.2 – Simple website
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 简单网站
- en: Next, repeat the process on the other web server. Once that system is up, we
    will set up the load balancer.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，重复在另一个 Web 服务器上的操作。一旦该系统启动，我们将设置负载均衡器。
- en: Note
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you are using `httpd` (TLS/SSL) on your server, don’t forget to enable `https`
    in your local firewalls as well.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在服务器上使用 `httpd`（TLS/SSL），别忘了在本地防火墙中启用 `https`。
- en: Load balancer
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载均衡器
- en: 'For the single load-balancer system, we will need to install HAProxy. This
    is done using the `dnf` command as root:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单一负载均衡系统，我们需要安装 HAProxy。这可以通过使用 `dnf` 命令以 root 用户身份完成：
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We next need to open up port `80` in the firewall with the following command:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要用以下命令在防火墙中打开 `80` 端口：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, we will need to edit the config file. The config file is located in `/etc/haproxy/haproxy.cfg`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要编辑配置文件。配置文件位于 `/etc/haproxy/haproxy.cfg`。
- en: 'The config file has two main sections, `global` and `defaults`. There can only
    be a single `global` section in the config file. This is where TLS/SSL config
    data, the logging configuration, and the user and group settings go for the user
    running `haproxy`. By default, `haproxy` runs as the user haproxy with the group
    `haproxy`. For most use cases, the `global` section should not need to be changed.
    An example is seen in the following figure:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件有两个主要部分，`global` 和 `defaults`。配置文件中只能有一个 `global` 部分。这部分用于配置 TLS/SSL 数据、日志配置以及运行
    `haproxy` 的用户和组设置。默认情况下，`haproxy` 作为用户 haproxy 和组 haproxy 运行。在大多数使用场景中，`global`
    部分不需要更改。以下图示为例：
- en: '![Figure 6.3 – HAProxy global settings](img/B18349_06_03.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – HAProxy 全局设置](img/B18349_06_03.jpg)'
- en: Figure 6.3 – HAProxy global settings
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – HAProxy 全局设置
- en: 'The next section, called the `defaults` section, is where you will make most
    of your edits. This is built using three subsections: `frontend`, `backend`, and
    `listen`.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分叫做 `defaults` 部分，是你将进行大部分编辑的地方。它由三个子部分组成：`frontend`、`backend` 和 `listen`。
- en: The `frontend` section listens on all IP addresses and ports that are defined.
    This is what users will connect to. A HAProxy server can have multiple `frontend`
    sections, though each one needs a unique name and IP/port combination.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`frontend` 部分监听所有定义的 IP 地址和端口。这是用户将连接的部分。一个 HAProxy 服务器可以有多个 `frontend` 部分，但每个部分都需要一个唯一的名称和
    IP/端口组合。'
- en: The `backend` section defines the servers being load balanced to, defining the
    load-balancing method as well as the servers and ports where traffic is being
    sent to. A HAProxy server can have multiple `backend` sections, though each one
    needs a unique name.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`backend` 部分定义了被负载均衡的服务器，定义了负载均衡方法以及流量发送到的服务器和端口。一个 HAProxy 服务器可以有多个 `backend`
    部分，但每个部分都需要一个唯一的名称。'
- en: The `listen` section is used to define how you can monitor the load balancer,
    with the port, URI, and authentication information needed to monitor HAProxy.
    Normally, you will only have one `listen` section.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`listen` 部分用于定义如何监控负载均衡器，提供端口、URI 和监控 HAProxy 所需的认证信息。通常你只有一个 `listen` 部分。'
- en: For the same frontend, we will be listening on port `80` of the `lb1` system,
    called `www_app`, and will define this IP/port combination to use the `www_servers`
    backend.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于同一个前端，我们将在 `lb1` 系统的 `80` 端口上监听，称为 `www_app`，并将定义这个 IP/端口组合使用 `www_servers`
    后端。
- en: frontend
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: frontend
- en: 'The `frontend` section of the HAProxy configuration file offers various options
    to manage incoming traffic behavior. These options can be used to control the
    traffic on a frontend. Here are some commonly used frontend options:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy 配置文件的 `frontend` 部分提供了多种选项来管理传入流量的行为。这些选项可以用来控制前端流量的行为。以下是一些常用的前端选项：
- en: '`bind`: Defines the IP address and port on which the frontend will listen for
    incoming traffic. For example, `bind *:80` listens on all IP addresses on port
    `80`.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bind`：定义前端监听传入流量的 IP 地址和端口。例如，`bind *:80` 表示在所有 IP 地址的 `80` 端口上监听。'
- en: '`mode`: Specifies the mode of the frontend, such as `http`, `tcp`, or `ssl`.
    For HTTP traffic, use `mode http`. If you want to load balance a generic TCP port,
    use `mode tcp`.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode`: 指定前端的模式，例如 `http`、`tcp` 或 `ssl`。对于 HTTP 流量，使用 `mode http`。如果要负载均衡一个通用的
    TCP 端口，请使用 `mode tcp`。'
- en: '`option`: Enables or disables specific options for the frontend. Some commonly
    used options are as follows:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option`: 启用或禁用前端的特定选项。一些常用选项如下：'
- en: '`option httplog`: Enables HTTP request/response logging'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option httplog`: 启用 HTTP 请求/响应日志记录'
- en: '`option dontlognull`: Prevents logging of requests with missing or empty user-agent
    strings'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option dontlognull`: 防止记录缺失或空白用户代理字符串的请求'
- en: '`option forwardfor`: Adds the client’s IP address to the HTTP request headers
    when using HTTP proxy mode'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option forwardfor`: 在使用 HTTP 代理模式时，将客户端的 IP 地址添加到 HTTP 请求头中'
- en: '`option http-server-close`: Forces the server connection to close after processing
    a request, rather than using keep-alive'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option http-server-close`: 强制服务器在处理请求后关闭连接，而不是使用持久连接（keep-alive）'
- en: '`timeout`: Configures various timeouts for the frontend:'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout`: 配置前端的各种超时时间：'
- en: '`timeout client`: Sets the maximum allowed time for the client to establish
    a connection and send data'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout client`: 设置客户端建立连接并发送数据的最大时间限制'
- en: '`timeout server`: Sets the maximum allowed time for the server to respond to
    a request'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout server`: 设置服务器响应请求的最大时间限制'
- en: '`timeout connect`: Sets the maximum time to wait for a connection to the backend
    server'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout connect`: 设置等待连接后端服务器的最大时间'
- en: '`acl`: Defines rules for matching specific conditions. ACLs are used in conjunction
    with backend configurations to control traffic routing based on various criteria.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acl`: 定义匹配特定条件的规则。ACL 与后端配置配合使用，根据不同标准控制流量路由。'
- en: '`use_backend`: Specifies which backend to use for handling traffic that matches
    specific ACL conditions. It allows you to direct traffic to different backend
    servers based on certain conditions.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_backend`: 指定用于处理符合特定 ACL 条件的流量的后端。它允许你根据特定条件将流量引导到不同的后端服务器。'
- en: '`default_backend`: Defines the default backend to use if no ACL conditions
    match the incoming traffic.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`default_backend`: 定义当没有 ACL 条件匹配传入流量时使用的默认后端。'
- en: '`redirect`: Performs a URL redirection for specific conditions. For example,
    you can use the [https://example.com](https://example.com) redirect location to
    redirect HTTP traffic to HTTPS.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`redirect`: 在特定条件下执行 URL 重定向。例如，可以使用[https://example.com](https://example.com)
    重定向地址将 HTTP 流量重定向到 HTTPS。'
- en: '`http-request` and `http-response`: These are used to add custom HTTP request/response
    headers or to perform specific actions based on HTTP request/response data.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`http-request` 和 `http-response`: 用于添加自定义 HTTP 请求/响应头，或根据 HTTP 请求/响应数据执行特定操作。'
- en: '`capture`: Captures parts of the request or response headers and saves them
    into variables.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`capture`: 捕获请求或响应头的部分内容并将其保存到变量中。'
- en: 'For the sample frontend, we will define the frontend as `www_app` binding to
    all IPs on the load-balancer system on port `80`. This looks like the following
    figure:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于示例前端，我们将前端定义为 `www_app`，绑定到负载均衡系统上所有 IP 的 `80` 端口。如下图所示：
- en: '![Figure 6.4 – Example frontend](img/B18349_06_04.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 示例前端](img/B18349_06_04.jpg)'
- en: Figure 6.4 – Example frontend
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 示例前端
- en: backend
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: backend
- en: 'When using HAProxy, the backend options play a crucial role in configuring
    the behavior of backend servers and the routing of traffic toward them. These
    options are specifically designated within the `backend` section of the HAProxy
    configuration file. Here are some frequently utilized backend options:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 HAProxy 时，后端选项在配置后端服务器行为和流量路由方面起着至关重要的作用。这些选项专门在 HAProxy 配置文件的 `backend`
    部分中指定。以下是一些常用的后端选项：
- en: '`mode`: Specifies the mode of the backend, such as `http`, `tcp`, or `ssl`.
    For HTTP traffic, use `http` mode.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode`: 指定后端的模式，例如 `http`、`tcp` 或 `ssl`。对于 HTTP 流量，请使用 `http` 模式。'
- en: '`balance`: Defines the load-balancing algorithm to distribute traffic across
    backend servers. Common options include the following:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`balance`: 定义负载均衡算法，用于将流量分配到后端服务器。常见的选项包括：'
- en: '`balance roundrobin`: Requests are distributed in a round-robin fashion to
    each server in sequence'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`balance roundrobin`: 按照轮询方式依次将请求分配给每个服务器'
- en: '`balance leastconn`: Traffic is sent to the server with the lowest number of
    active connections'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`balance leastconn`: 将流量发送到活动连接数最少的服务器'
- en: '`balance source`: Based on a hash of the client’s IP address, traffic is directed
    to a specific server consistently'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`balance source`: 基于客户端的 IP 地址哈希，将流量一致地导向特定服务器。'
- en: '`server`: Defines the backend servers and their addresses, ports, and optional
    parameters.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`server`: 定义后端服务器及其地址、端口和可选参数。'
- en: '`timeout`: Configures various timeouts for the backend:'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout`: 配置后端的各种超时：'
- en: '`timeout server`: Sets the maximum allowed time for the server to respond to
    a request'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout server`: 设置服务器响应请求的最大允许时间。'
- en: '`timeout tunnel`: Configures the maximum time allowed to establish a tunnel
    (used in TCP mode)'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout tunnel`: 配置建立隧道的最大时间（用于 TCP 模式）。'
- en: '`http-request` and `http-response`: Similar to frontend options, these are
    used to add custom HTTP request/response headers or perform specific actions based
    on HTTP request/response data.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`http-request` 和 `http-response`: 类似于前端选项，主要用于添加自定义 HTTP 请求/响应头，或根据 HTTP 请求/响应数据执行特定操作。'
- en: '`cookie`: Configures sticky session persistence using cookies. It allows the
    backend server to be selected based on a specific cookie value from the client.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cookie`: 使用 Cookie 配置会话粘性。它允许根据客户端的特定 Cookie 值选择后端服务器。'
- en: '`check`: Enables health checks for backend servers to determine their availability.
    If a server fails the health check, HAProxy will stop sending traffic to it until
    it recovers.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`check`: 启用后端服务器的健康检查，以确定其可用性。如果服务器未通过健康检查，HAProxy 将停止向其发送流量，直到其恢复。'
- en: '`option`: Enables or disables specific options for the backend. Some commonly
    used options include the following:'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option`: 启用或禁用后端的特定选项。一些常用的选项包括：'
- en: '`option httpchk`: Enables HTTP health checks instead of TCP health checks'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option httpchk`: 启用 HTTP 健康检查，替代 TCP 健康检查。'
- en: '`option redispatch`: Allows HAProxy to reselect a server if the connection
    to the selected server fails'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option redispatch`: 允许 HAProxy 在连接到选定服务器失败时重新选择服务器。'
- en: '`errorfile`: Specifies a file to use as a custom error page for backend server
    errors.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errorfile`: 指定用于作为自定义错误页面的文件，以应对后端服务器错误。'
- en: 'In the sample backend, it is defined as `www_servers` and will use `roundrobin`
    load balancing against the `web1` and `web2` servers:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例后端中，它定义为 `www_servers`，并使用 `roundrobin` 负载均衡 `web1` 和 `web2` 服务器：
- en: '![Figure 6.5 – HAProxy sample backend](img/B18349_06_05.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – HAProxy 示例后端](img/B18349_06_05.jpg)'
- en: Figure 6.5 – HAProxy sample backend
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – HAProxy 示例后端
- en: Note
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is highly recommended to always use the `check` option for your servers.
    If you do not run the checks, the system will still send traffic to the server!
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议始终为您的服务器使用 `check` 选项。如果您没有运行检查，系统仍然会向服务器发送流量！
- en: listen
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: listen
- en: 'In HAProxy, the `listen` section is used to define a frontend and backend configuration
    together in one block, making it a convenient way to combine both. The `listen`
    section allows you to define options specific to the listening socket and how
    the incoming traffic is handled. The following are some commonly used options
    in the `listen` section:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HAProxy 中，`listen` 部分用于将前端和后端配置结合在一个块中，是一个便捷的组合方式。`listen` 部分允许您定义特定于监听套接字的选项以及如何处理传入流量。以下是
    `listen` 部分常用的选项：
- en: '`bind`: Defines the IP address and port on which HAProxy will listen for incoming
    traffic. For example, `bind *:80` listens on all IP addresses on port `80`.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bind`: 定义 HAProxy 用于监听传入流量的 IP 地址和端口。例如，`bind *:80` 在端口 `80` 上监听所有 IP 地址。'
- en: '`stats`: Enables the HAProxy statistics page for monitoring and managing HAProxy.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stats`: 启用 HAProxy 统计页面，用于监控和管理 HAProxy。'
- en: '`stats enable`: Enables statistics monitoring for HAProxy.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stats enable`: 启用 HAProxy 的统计监控功能。'
- en: '`stats uri`: Specifies the URI path for accessing the statistics page. For
    example, `stats uri /haproxy_stats` sets the statistics page to be accessible
    at `http://your-haproxy-ip/haproxy_stats`.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stats uri`: 指定用于访问统计页面的 URI 路径。例如，`stats uri /haproxy_stats` 将统计页面设置为可通过 `http://your-haproxy-ip/haproxy_stats`
    访问。'
- en: '`stats realm`: Sets the realm (authentication realm) for HTTP basic authentication
    when accessing the statistics page. This adds a layer of security to prevent unauthorized
    access.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stats realm`: 设置访问统计页面时使用的 HTTP 基本身份验证的领域（认证领域）。此设置为防止未经授权的访问提供了一层安全保护。'
- en: '`stats auth`: Configures the username and password for HTTP basic authentication
    when accessing the statistics page. The format is `stats` `auth username:password`.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stats auth`: 配置访问统计页面时使用的 HTTP 基本身份验证的用户名和密码。格式为 `stats` `auth username:password`。'
- en: '`stats hide-version`: Hides the HAProxy version number from the statistics
    page to enhance security.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stats hide-version`：从统计页面隐藏 HAProxy 版本号，以增强安全性。'
- en: '`stats show-node`: Displays the server node names on the statistics page. This
    is useful when using dynamic server templates.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stats show-node`：在统计页面上显示服务器节点名称。当使用动态服务器模板时，这非常有用。'
- en: '`stats refresh`: Sets the interval (in milliseconds) for automatic refresh
    of the statistics page. For example, `stats refresh 10s` refreshes the page every
    10 seconds.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stats refresh`：设置统计页面自动刷新的间隔（以毫秒为单位）。例如，`stats refresh 10s` 每 10 秒刷新一次页面。'
- en: '`stats admin`: Specifies the IP address and port for allowing administrative
    access to HAProxy statistics. It allows remote management of HAProxy using the
    statistics page. For example, stats admin if `localhost` permits access only from
    the local machine.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stats admin`：指定允许访问 HAProxy 统计信息的 IP 地址和端口。它允许通过统计页面远程管理 HAProxy。例如，`localhost`
    只允许来自本地计算机的访问。'
- en: '`stats maxconn`: Limits the number of connections allowed to the statistics
    page. It helps to prevent overload and potential denial-of-service attacks.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stats maxconn`：限制允许连接到统计页面的连接数。它有助于防止过载和潜在的拒绝服务攻击。'
- en: '`errorfile`: Specifies a file to use as a custom error page for frontend errors.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errorfile`：指定用作前端错误的自定义错误页面的文件。'
- en: 'For the sample `listen` section, we will define it as metrics, allowing admin
    access from `192.168.56.1`. The user will use the username as `admin` and the
    password `passw0rd` to log in. This is seen in the following figure:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于示例中的 `listen` 部分，我们将其定义为指标，允许从 `192.168.56.1` 访问管理员。用户将使用用户名 `admin` 和密码 `passw0rd`
    进行登录。如下图所示：
- en: '![Figure 6.6 – HAProxy listen sample](img/B18349_06_06.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – HAProxy listen 示例](img/B18349_06_06.jpg)'
- en: Figure 6.6 – HAProxy listen sample
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – HAProxy listen 示例
- en: Since the status page is running on port `8080`, don’t forget to add the port
    to the firewall and reload the firewall. This can be done with the following command;
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 由于状态页面运行在端口 `8080`，请别忘了将端口添加到防火墙中并重新加载防火墙。这可以通过以下命令完成：
- en: '[PRE7]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How it works…
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Now that we have our two web servers, and the load balancer configured, we
    need to start the load balancer. This is done using `systemctl`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经配置了两个 Web 服务器和负载均衡器，我们需要启动负载均衡器。可以使用 `systemctl` 来完成：
- en: 'Use the following to start HAProxy:'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下命令启动 HAProxy：
- en: '[PRE8]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Use the following to check the status:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下命令检查状态：
- en: '[PRE9]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you edit the config file, do not forget to reload HAProxy with the following
    command:'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果编辑了配置文件，请不要忘记使用以下命令重新加载 HAProxy：
- en: '[PRE10]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, point your browser to the load balancer IP. You will get the web server
    page. This is seen in the following figure:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将浏览器指向负载均衡器 IP 地址。你将看到 Web 服务器页面。如下图所示：
- en: '![Figure 6.7 – Working HAProxy](img/B18349_06_07.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 工作中的 HAProxy](img/B18349_06_07.jpg)'
- en: Figure 6.7 – Working HAProxy
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 工作中的 HAProxy
- en: Since the rule is `roundrobin`, and we configured the timeout at one minute,
    wait a minute and then reload the page. You will see a new server.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于规则是`roundrobin`，且我们将超时设置为一分钟，等待一分钟后再重新加载页面。你将看到一个新的服务器。
- en: '![Figure 6.8 – Working load balancing](img/B18349_06_08.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – 工作中的负载均衡](img/B18349_06_08.jpg)'
- en: Figure 6.8 – Working load balancing
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – 工作中的负载均衡
- en: 'As an admin, you will also want to check on the health of your resources. Point
    your browser to the stats URL, and enter the username and password configured.
    This will show the stats page. In the case of this example, the URL is [http://lb1.m57.local:8080/stats](http://lb1.m57.local:8080/stats).
    You will see a sample in the following figure:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 作为管理员，你还需要检查资源的健康状态。将浏览器指向统计页面的 URL，并输入已配置的用户名和密码。这将显示统计页面。在此示例中，URL 为 [http://lb1.m57.local:8080/stats](http://lb1.m57.local:8080/stats)。你将在下图中看到一个示例：
- en: '![ Figure 6.9 – HAProxy status page](img/B18349_06_09.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – HAProxy 状态页面](img/B18349_06_09.jpg)'
- en: Figure 6.9 – HAProxy status page
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – HAProxy 状态页面
- en: On the sample page, you will see that `web1` is offline. You can also see how
    much traffic each frontend and backend rule has processed, and to what servers.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例页面中，你将看到 `web1` 离线。你还可以看到每个前端和后端规则处理了多少流量，以及流量分别发送到了哪些服务器。
- en: Making HAProxy highly available with Keepalived
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keepalived 使 HAProxy 高可用
- en: 'In the previous recipe, we used HAProxy to give our web servers some redundancy.
    The challenge with that solution is we now have a failure point in the load balancer
    itself. When architecting for HA, you need to cover all points of failure to make
    sure there is redundancy and that there is no SPOF. In this recipe, we will use
    Keepalived to add some HA to our configuration. Keepalived is a software application
    that is open source and designed for Linux-based systems. Its main function is
    to manage network load balancing and failover, ensuring the HA of web services.
    Keepalived is often used alongside HAProxy. The software primarily uses the **Virtual
    Router Redundancy Protocol** (**VRRP**) to achieve fault tolerance and evenly
    distribute the load. Keepalived uses the following features to provide its redundancy:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配置中，我们使用 HAProxy 为我们的 Web 服务器提供了一些冗余。但这个解决方案的挑战在于，现在负载均衡器本身成了故障点。在高可用性架构中，您需要覆盖所有故障点，确保有冗余并避免单点故障（SPOF）。在本配置中，我们将使用
    Keepalived 为我们的配置添加高可用性。Keepalived 是一款开源软件，专为基于 Linux 的系统设计。其主要功能是管理网络负载均衡和故障切换，确保
    Web 服务的高可用性。Keepalived 通常与 HAProxy 一起使用。该软件主要使用**虚拟路由冗余协议**（**VRRP**）实现容错和负载均衡。Keepalived
    使用以下功能来提供冗余：
- en: '**High availability**: With Keepalived, you can establish a cluster of backup
    servers that utilize a shared **Virtual IP** (**VIP**) address. This setup ensures
    that even if the primary server experiences a failure, a secondary server will
    automatically take over and handle incoming traffic, resulting in minimal downtime.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高可用性**：通过 Keepalived，您可以建立一个备份服务器集群，利用共享的**虚拟 IP**（**VIP**）地址。该配置确保即使主服务器发生故障，备用服务器也会自动接管并处理传入流量，从而实现最小的停机时间。'
- en: '**VRRP**: VRRP is a commonly used protocol that enables automatic router failover
    in IP networks. Keepalived utilizes VRRP to keep a VIP address operational, which
    can be assigned to any node within the cluster as needed.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VRRP**：VRRP 是一种常用协议，能够实现 IP 网络中的路由器自动故障切换。Keepalived 利用 VRRP 保持 VIP 地址的正常运行，可以根据需要将其分配给集群中的任何节点。'
- en: '**Health checking**: The monitoring system of Keepalived regularly checks the
    health of active servers. If a server becomes unresponsive, Keepalived will remove
    it from the pool and redirect traffic to the healthy servers.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**健康检查**：Keepalived 的监控系统定期检查活动服务器的健康状况。如果服务器变得无响应，Keepalived 会将其从池中移除，并将流量重定向到健康的服务器。'
- en: '**Notification mechanisms**: With Keepalived, it’s possible to set up notifications
    for failover events or when certain thresholds are exceeded. These notifications
    are useful for keeping an eye on the overall health of the cluster.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通知机制**：使用 Keepalived，可以为故障切换事件或超出某些阈值时设置通知。这些通知对于监控集群的整体健康状况非常有用。'
- en: In a cluster, Keepalived designates one node as the master and the others as
    backups. The master node handles incoming traffic and responds to ARP requests
    for the VIP address, while the backups act as standby routers and monitor the
    master’s status. The nodes communicate using the VRRP protocol, with the master
    periodically sending VRRP advertisements to show its functioning. Often, a VIP
    is used to allow a single IP address for end user access. This normal operation
    is seen in the following figure, where we have Keepalived managing two HAProxy
    systems.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中，Keepalived 将一个节点指定为主节点，其他节点作为备份节点。主节点处理传入流量并响应 VIP 地址的 ARP 请求，而备份节点充当待命路由器并监视主节点的状态。节点之间使用
    VRRP 协议进行通信，主节点定期发送 VRRP 广告以表明其正常运行。通常，VIP 被用来为终端用户访问提供单一的 IP 地址。下图展示了 Keepalived
    管理两个 HAProxy 系统的正常操作。
- en: '![Figure 6.10 – Keepalived normal operations](img/B18349_06_10.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – Keepalived 正常操作](img/B18349_06_10.jpg)'
- en: Figure 6.10 – Keepalived normal operations
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – Keepalived 正常操作
- en: If the backups stop receiving advertisements or detect any issues with the master,
    one backup node will take over as the new master. Keepalived checks the servers’
    health using mechanisms such as ICMP (ping) checks or Layer 4 checks (e.g., checking
    whether a specific port is open) and removes failed servers from the pool. The
    VRRP priority is adjusted to ensure a healthy backup server takes over. This is
    seen in the following figure, where the VIP that users connect to has migrated
    over to the second node, and that node is now using HAProxy to manage the workload.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果备用节点停止接收广告或检测到主节点的问题，某个备用节点将接管成为新的主节点。Keepalived 使用 ICMP（ping）检查或第4层检查（例如，检查特定端口是否开放）等机制来检查服务器的健康状况，并将失败的服务器从池中移除。调整
    VRRP 优先级以确保健康的备用服务器接管。这可以通过下图看到，用户连接的 VIP 已经迁移到第二个节点，而该节点现在使用 HAProxy 来管理工作负载。
- en: '![Figure 6.11 – Keepalived failed node](img/B18349_06_11.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – Keepalived 失败节点](img/B18349_06_11.jpg)'
- en: Figure 6.11 – Keepalived failed node
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – Keepalived 失败节点
- en: Getting ready
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'This recipe expands upon the previous one, adding a second load-balancer system
    and a VIP. You will need to build the second load balancer and acquire an additional
    IP address for the VIP. Your IPs should be similar to the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程扩展了前一个教程，添加了第二个负载均衡系统和 VIP。你需要搭建第二个负载均衡器并为 VIP 获取一个额外的 IP 地址。你的 IP 地址应该类似于以下：
- en: '| **Host** | **IP** |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| **Host** | **IP** |'
- en: '| **web1** | `192.168.56.200` |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| **web1** | `192.168.56.200` |'
- en: '| **web2** | `192.168.56.201` |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| **web2** | `192.168.56.201` |'
- en: '| **lb1** | `192.168.56.202` |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| **lb1** | `192.168.56.202` |'
- en: '| **lb2** | `192.168.56.203` |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| **lb2** | `192.168.56.203` |'
- en: '| **vip** | `192.168.56.204` |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| **vip** | `192.168.56.204` |'
- en: Table 6.4 – Keepalived IP addresses
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.4 – Keepalived IP 地址
- en: How to do it…
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: Before you start configuring Keepalived, you need to configure HAProxy on the
    second server. You can easily install HAProxy, open up the firewall ports, and
    copy over the config file from the existing system.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始配置 Keepalived 之前，你需要在第二台服务器上配置 HAProxy。你可以轻松安装 HAProxy，打开防火墙端口，并从现有系统中复制配置文件。
- en: You can test this by simply pointing your browser to the second load balancer
    and seeing the app server.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过简单地将浏览器指向第二个负载均衡器，查看应用服务器来进行测试。
- en: Next, we will start to configure Keepalived.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将开始配置 Keepalived。
- en: 'For each load balancer, you will need to install Keepalived as the root user:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个负载均衡器，你需要以 root 用户身份安装 Keepalived：
- en: '[PRE11]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Next, we will need to edit the Keepalived config file. This is found in `/etc/keepalived/keepalived.conf`.
    There are two major sections to edit, `global_defs` and `vrpp_instance`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要编辑 Keepalived 配置文件。该文件位于 `/etc/keepalived/keepalived.conf`。需要编辑的主要部分有
    `global_defs` 和 `vrrp_instance`。
- en: '`global_defs` is the global definition used by Keepalived. These settings are
    used by all `vrrp_instance` types configured in the system.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`global_defs` 是 Keepalived 使用的全局定义。这些设置会被系统中配置的所有 `vrrp_instance` 类型使用。'
- en: 'There are several parameters that you will need to update:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个参数需要更新：
- en: '`notification_email`: This is a list of email addresses that will be emailed
    when there is an event'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`notification_email`：这是一个电子邮件地址列表，当发生事件时会发送邮件。'
- en: '`notification_email_from_user`: This is the sending email address'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`notification_email_from_user`：这是发送电子邮件的地址'
- en: '`smtp_server`: This is the SMTP relay server'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`smtp_server`：这是 SMTP 中继服务器'
- en: '`router_id`: This is a unique name for this Keepalived cluster'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_id`：这是该 Keepalived 集群的唯一名称'
- en: 'For the sample, this section looks like the following:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 对于示例，此部分看起来如下：
- en: '![Figure 6.12 – Keepalived globals](img/B18349_06_12.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.12 – Keepalived 全局设置](img/B18349_06_12.jpg)'
- en: Figure 6.12 – Keepalived globals
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – Keepalived 全局设置
- en: The next section is `vrrp_instance`. You can have multiple `vrrp_instance` types
    in the cluster, each supporting different VIPs.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节是 `vrrp_instance`。你可以在集群中拥有多个 `vrrp_instance` 类型，每个都支持不同的 VIP。
- en: 'For `vrrp_instance`, you need to give each one a unique name. Additionally,
    there are several parameters that will need to be updated:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `vrrp_instance`，你需要为每个实例分配一个唯一的名称。此外，还有几个参数需要更新：
- en: '`state`: The state of the instance, usually master for the primary node and
    backup for the secondary node.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state`：实例的状态，通常主节点为 master，备用节点为 backup。'
- en: '`interface`: The Ethernet interface used for this host in the cluster.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`interface`：集群中用于该主机的以太网接口。'
- en: '`virtual_router_id`: A unique number for this instance. No other instances
    should use the same ID.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`virtual_router_id`：该实例的唯一编号。其他实例不能使用相同的 ID。'
- en: '`authentication`: This section defines how members are authenticated:'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`authentication`：此部分定义成员如何进行身份验证：'
- en: '`auth_type`: Normally sent to PASS, to allow nodes to authenticate as members
    of this instance. There is a second support type called `auth_pass`: The password
    for the instance.'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auth_type`：通常发送为 PASS，以允许节点作为此实例的成员进行身份验证。还有一种第二种支持类型，称为 `auth_pass`：该实例的密码。'
- en: '`virtual_ipaddress`: A list of VIPs managed by this instance.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`virtual_ipaddress`：由此实例管理的 VIP 列表。'
- en: In our example, the section will look as follows;
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，该部分将如下所示；
- en: '![Figure 6.13 – Keepalived vrrp_instance](img/B18349_06_13.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.13 – Keepalived vrrp_instance](img/B18349_06_13.jpg)'
- en: Figure 6.13 – Keepalived vrrp_instance
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 – Keepalived vrrp_instance
- en: Once the config file is built, copy it over to the second load balancer. Do
    not forget to change the state to `BACKUP` on the second system, and also update
    the interface if it is different on that system.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件创建完成后，将其复制到第二个负载均衡器。不要忘记将第二个系统的状态更改为 `BACKUP`，并且如果该系统的接口不同，也要进行更新。
- en: 'Next, start Keepalived on both nodes with the following command as the root
    user:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，以 root 用户身份在两个节点上启动 Keepalived，使用以下命令：
- en: '[PRE12]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can now point your browser to the VIP! This is seen in the following example.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将浏览器指向 VIP！这在以下示例中可以看到。
- en: '![Figure 6.14 – Keepalived VIP in use](img/B18349_06_14.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.14 – Keepalived VIP 使用中](img/B18349_06_14.jpg)'
- en: Figure 6.14 – Keepalived VIP in use
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14 – Keepalived VIP 使用中
- en: 'You can check the status by looking at the journal entries for the daemon,
    using the following command:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过查看守护进程的日志条目来检查状态，使用以下命令：
- en: '[PRE13]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This will show all the activity of the daemon. You should see `Sending gratuitous
    ARP` messages, as this is the system checking the health. You will also see messages
    about the state, such as `Entering MASTER STATE` or `Entering BACKUP STATE`, as
    the system switches between `MASTER` and `BACKUP`.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示守护进程的所有活动。你应该能看到 `Sending gratuitous ARP` 消息，这是系统在检查健康状况。你还会看到一些关于状态的消息，比如
    `Entering MASTER STATE` 或 `Entering BACKUP STATE`，当系统在 `MASTER` 和 `BACKUP` 之间切换时。
- en: HA clustering for all with Corosync and Pacemaker
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Corosync 和 Pacemaker 为所有提供 HA 集群
- en: In the previous recipes, we addressed HA by distributing traffic between two
    active application servers. However, this method is only effective for stateless
    applications where the server or browser doesn’t contain specific user or session
    data. For applications that are not stateless or run on a complex server, a different
    approach to HA is necessary. The solution is to start and stop the application
    components on different servers, using the combination of Pacemaker and Corosync.
    These two open source software projects work together to provide HA clustering
    for Linux-based systems. They coordinate and manage multiple nodes in a cluster,
    ensuring that critical services remain available even during hardware or software
    failures.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的配置中，我们通过在两个活跃的应用服务器之间分配流量来实现 HA。然而，这种方法仅对无状态应用有效，其中服务器或浏览器不包含特定的用户或会话数据。对于有状态应用或在复杂服务器上运行的应用，需要采用不同的
    HA 方法。解决方案是通过使用 Pacemaker 和 Corosync 的组合，在不同的服务器上启动和停止应用组件。这两个开源软件项目协同工作，为基于 Linux
    的系统提供 HA 集群。它们协调和管理集群中的多个节点，确保在硬件或软件故障期间，关键服务仍然可用。
- en: 'Corosync serves as the communication layer for the HA cluster stack, allowing
    for dependable communication between nodes. It utilizes a membership and quorum
    system to monitor the cluster’s active nodes and guarantee that only one node
    operates as the primary (or master) at a given time. The messaging layer is essential
    for sharing data regarding the cluster’s state, node status, and resource conditions.
    Corosync plays a vital role in the cluster’s functionality, providing key features
    such as the following:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Corosync 作为 HA 集群栈的通信层，允许节点之间进行可靠的通信。它利用成员资格和法定人数系统来监控集群的活动节点，并确保在任何时候只有一个节点作为主节点（或主控节点）运行。消息传递层对于共享有关集群状态、节点状态和资源条件的数据至关重要。Corosync
    在集群的功能中发挥着至关重要的作用，提供以下关键功能：
- en: '**Cluster communication**: Corosync enables nodes to exchange messages reliably
    and efficiently, allowing them to coordinate and synchronize their actions.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群通信**：Corosync 使得节点能够可靠高效地交换消息，允许它们协调和同步各自的操作。'
- en: '**Membership and quorum**: Corosync is a tool that keeps track of active nodes
    in a cluster and uses a quorum algorithm to ensure that there are enough nodes
    available to make decisions. This helps avoid split-brain scenarios and makes
    sure that only one node is active. It’s crucial to avoid split-brain clusters
    because they can cause data inconsistencies, corruption, and service disruptions.
    A split-brain scenario occurs when nodes in a cluster lose communication with
    each other. As a result, each node thinks it’s the only active one in the cluster.
    This can happen because of network issues, communication failures, or misconfigurations.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成员资格和法定人数**：Corosync 是一个跟踪集群中活跃节点的工具，使用法定人数算法来确保有足够的节点可用来做出决策。这有助于避免脑裂场景，并确保只有一个节点处于活动状态。避免脑裂集群至关重要，因为脑裂会导致数据不一致、损坏和服务中断。脑裂场景发生在集群中的节点失去相互通信时，导致每个节点认为自己是唯一的活跃节点。这可能是由于网络问题、通信故障或配置错误引起的。'
- en: Note
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When there is a split-brain scenario, several nodes within the cluster may begin
    running services or using shared resources on their own, thinking that they are
    the only active node. This can cause conflicts and data inconsistencies since
    each node operates independently without any coordination. When possible, use
    an odd number of nodes in a cluster, or enable some protection using quorum.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 当发生脑裂场景时，集群中的多个节点可能开始独立运行服务或使用共享资源，认为自己是唯一的活跃节点。这可能会导致冲突和数据不一致，因为每个节点独立操作而没有协调。如果可能，建议在集群中使用奇数个节点，或者启用法定人数保护。
- en: 'Pacemaker is a cluster resource manager that utilizes Corosync’s messaging
    and membership features to manage cluster resources and handle resource failover.
    It determines which node in the cluster should run specific services (resources)
    based on established policies and constraints. Pacemaker brings the following
    features to the cluster:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Pacemaker 是一个集群资源管理器，它利用 Corosync 的消息传递和成员管理功能来管理集群资源和处理资源故障转移。它根据既定的策略和约束，决定集群中哪个节点应该运行特定的服务（资源）。Pacemaker
    为集群带来了以下功能：
- en: '**Resource management**: With Pacemaker, administrators can set up resources
    that require strong availability, such as IP addresses, services, databases, and
    applications'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源管理**：使用 Pacemaker，管理员可以设置需要强大可用性的资源，如 IP 地址、服务、数据库和应用程序'
- en: '**Resource monitoring**: Pacemaker continuously monitors the status of resources
    and nodes to detect failures or changes in the cluster'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源监控**：Pacemaker 持续监控资源和节点的状态，以检测集群中的故障或变化'
- en: '**Resource failover**: If a node fails or there are resource problems, Pacemaker
    will begin a failover process, transferring resources to functioning nodes to
    guarantee uninterrupted availability'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源故障转移**：如果节点发生故障或出现资源问题，Pacemaker 将启动故障转移过程，将资源转移到正常运行的节点上，以确保不中断的可用性'
- en: '**Resource constraints**: Administrators can set constraints and rules for
    resource placement and failover, defining which nodes are preferred or prohibited
    for specific resources'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源约束**：管理员可以为资源的放置和故障转移设置约束和规则，定义哪些节点适合或禁止用于特定资源'
- en: '**Colocation and order constraints**: Pacemaker allows defining relationships
    between resources, specifying which resources must run together on the same node
    or in a specific order'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源共址和顺序约束**：Pacemaker 允许定义资源之间的关系，指定哪些资源必须在同一节点上一起运行，或按特定顺序运行'
- en: '**Cluster management**: Pacemaker provides various command-line utilities and
    graphical interfaces (such as Hawk) for managing and configuring the cluster'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群管理**：Pacemaker 提供多种命令行工具和图形界面（如 Hawk）来管理和配置集群'
- en: Getting ready
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, you will need two VMs, each with at least two vCPUs, 8 GB
    of RAM, and 50 GB of disk space. You should have Oracle Linux 8 installed, and
    also a third IP address for a floating VIP to be managed by the cluster. Both
    of the web servers will be patched to the latest software. For this example, the
    following IPs will be used:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此配置，您将需要两台虚拟机，每台虚拟机至少有两个虚拟 CPU、8 GB 内存和 50 GB 磁盘空间。您应安装 Oracle Linux 8，并为浮动
    VIP 配置一个第三个 IP 地址，以便由集群管理。两个 Web 服务器将更新到最新的软件版本。此示例将使用以下 IP 地址：
- en: '| **Host** | **IP** |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| **主机** | **IP 地址** |'
- en: '| **Web1** | `192.168.56.200` |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| **Web1** | `192.168.56.200` |'
- en: '| **Web2** | `192.168.56.201` |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| **Web2** | `192.168.56.201` |'
- en: '| **vip** | `192.168.56.204` |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| **vip** | `192.168.56.204` |'
- en: Table 6.5 – HA cluster IPs
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.5 – 高可用集群 IP 地址
- en: Before we start with the cluster, you will also need to set up an `httpd` (Apache
    2.4) server on each host. This is similar to other hosts set up in other recipes.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始配置集群之前，您还需要在每台主机上设置一个`httpd`（Apache 2.4）服务器。这与其他食谱中设置其他主机的方法类似。
- en: 'First, on both servers, as root, install the Apache web server:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在两个服务器上，以root身份安装Apache web服务器：
- en: '[PRE14]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We do need to enable the status page for Apache. This is one way the resource
    will be checked. To do this, copy the following lines into `/etc/httpd/conf.d/status.conf`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实需要启用Apache的状态页面。这是资源检查的一种方式。为此，将以下行复制到`/etc/httpd/conf.d/status.conf`中：
- en: '[PRE15]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We also need a simple web page. For testing purposes, put the following into
    `/var/www/html/index.html` on both servers.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个简单的网页。为了测试目的，请将以下内容放入两个服务器的`/var/www/html/index.html`中。
- en: Note
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When setting up an application such as a web server, putting your content directory
    (such as `/var/www/html`) on a Gluster filesystem makes it easier to manage updating
    your content. This also works for other data that the application uses, such as
    temporary state data.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置像Web服务器这样的应用程序时，将您的内容目录（例如`/var/www/html`）放在Gluster文件系统上，可以更轻松地管理更新内容。这对于应用程序使用的其他数据（如临时状态数据）也同样有效。
- en: 'Next, on both servers, add port `80` to the local firewall with the following
    command:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在两个服务器上，使用以下命令将端口`80`添加到本地防火墙中：
- en: '[PRE16]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, for testing purposes, manually start the server on both nodes. Do not
    enable the service to automatically start. This will be done later when Pacemaker
    is configured:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了测试目的，在两个节点上手动启动服务器。不要启用服务自动启动。稍后在配置Pacemaker时将会启用自动启动：
- en: '[PRE17]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You should now see a basic page on both servers, as shown in the following
    screenshot:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该能在两个服务器上看到一个基本页面，如下图所示：
- en: '![Figure 6.15 – httpd server test](img/B18349_06_15.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.15 – httpd 服务器测试](img/B18349_06_15.jpg)'
- en: Figure 6.15 – httpd server test
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – httpd 服务器测试
- en: 'You can also test the `server-status` page using the `wget` command:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`wget`命令测试`server-status`页面：
- en: '[PRE18]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Sample output of a success is seen in the following screenshot.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的示例输出如下截图所示。
- en: '![Figure 6.16 – Successful server-status](img/B18349_06_16.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.16 – 服务器状态成功](img/B18349_06_16.jpg)'
- en: Figure 6.16 – Successful server-status
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16 – 服务器状态成功
- en: You are now ready to install and configure Pacemaker and Corosync.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已准备好安装和配置Pacemaker和Corosync。
- en: How to do it…
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: Now that you have installed the Apache `httpd` server that we will cluster,
    let’s start by installing the software. First, we need to enable the `addons`
    repo. This is done with the following commands on both servers as root.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 既然您已经安装了我们将要集群的Apache `httpd`服务器，接下来让我们开始安装软件。首先，我们需要启用`addons` repo。可以在两个服务器上以root身份通过以下命令来完成：
- en: 'First, on both nodes, enable the repo with the following commands as root:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在两个节点上，以root身份使用以下命令启用repo：
- en: '[PRE19]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, you will install the software:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将安装软件：
- en: '[PRE20]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once the installation of these packages is complete, a new user named `hacluster`
    will be added to your system. Please note that remote login will be disabled for
    this user after installation. To carry out tasks such as synchronizing the configuration
    or starting services on other nodes, it is necessary to set the same password
    for the `hacluster` user on both nodes. We can use the `passwd` command to set
    the password:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 安装这些软件包完成后，系统会添加一个名为`hacluster`的新用户。请注意，安装完成后该用户将无法进行远程登录。为了在其他节点上执行任务（例如同步配置或启动服务），必须在两个节点上为`hacluster`用户设置相同的密码。我们可以使用`passwd`命令设置密码：
- en: '[PRE21]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we need to enable the `pcs` service and start it:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要启用`pcs`服务并启动它：
- en: '[PRE22]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we need to open up the firewall for the cluster port. This is done with
    the following command:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要为集群端口打开防火墙。可以通过以下命令来完成：
- en: '[PRE23]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now we’re done with both nodes for a bit. The next few commands can be done
    on either of the nodes, but note, you still should be root.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们暂时完成了两个节点的配置。接下来的几个命令可以在任一节点上执行，但请注意，仍然需要是root用户。
- en: 'Next, we need to add both nodes to the cluster:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将两个节点添加到集群中：
- en: '[PRE24]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If your nodes are not resolvable in DNS or the `/etc/hosts` file, you can optionally
    add `addr=$IPADDR` for each host after the hostname. But it’s highly recommended
    to make sure all hosts are resolvable. This option, if used, would look as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的节点在DNS或`/etc/hosts`文件中不可解析，您可以选择在主机名后添加`addr=$IPADDR`。但强烈建议确保所有主机都能被解析。如果使用此选项，它将如下所示：
- en: '**pcs host auth web1 addr=192.168.56.200 node2 addr=192.168.56.201 -****u hacluster**'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '**pcs host auth web1 addr=192.168.56.200 node2 addr=192.168.56.201 -****u hacluster**'
- en: 'Next, we will create the cluster:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建集群：
- en: '[PRE25]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, we can start the cluster:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以启动集群：
- en: '[PRE26]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To verify that the cluster is running, we can check with the `pcs` command:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证集群是否正常运行，我们可以使用`pcs`命令进行检查：
- en: '[PRE27]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'A healthy new cluster should return a similar output as the following example:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 一个健康的集群应该返回类似以下示例的输出：
- en: '![Figure 6.17 – Cluster status](img/B18349_06_17.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.17 – 集群状态](img/B18349_06_17.jpg)'
- en: Figure 6.17 – Cluster status
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17 – 集群状态
- en: 'In a cluster consisting of only two nodes, the quorum operates differently
    than in clusters with more nodes. In such a cluster, the quorum value is set to
    `1` to ensure that the primary node is always considered in quorum. If both nodes
    go offline due to a network outage, they compete to fence each other, and the
    first to succeed wins the quorum. To increase the chances of a preferred node
    winning the quorum, the fencing agent can be configured to give it priority. This
    is done with the following command:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在仅由两个节点组成的集群中，法定人数的运作方式与多节点集群不同。在这样的集群中，法定人数值设置为`1`，以确保主节点始终被认为是法定成员。如果由于网络故障两个节点都离线，它们会相互竞争以确定谁先成功“围栏”，先成功的节点赢得法定人数。为了增加优选节点赢得法定人数的机会，可以配置围栏代理赋予它优先权。这可以通过以下命令来完成：
- en: '[PRE28]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The last step is to disable **STONITH**, which stands for **Shoot The Other
    Node In The Head**. This is an advanced fencing tool that requires configuration
    specific to your environment. If you want to experiment with this technology then
    check out the official Oracle docs here – [https://docs.oracle.com/en/operating-systems/oracle-linux/8/availability/availability-ConfiguringFencingstonith.html#ol-pacemaker-stonith](https://docs.oracle.com/en/operating-systems/oracle-linux/8/availability/availability-ConfiguringFencingstonith.html#ol-pacemaker-stonith):'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是禁用**STONITH**，即**击杀另一节点**。这是一个高级围栏工具，要求根据您的环境进行特定配置。如果您想实验这个技术，请查看官方的 Oracle
    文档：[https://docs.oracle.com/en/operating-systems/oracle-linux/8/availability/availability-ConfiguringFencingstonith.html#ol-pacemaker-stonith](https://docs.oracle.com/en/operating-systems/oracle-linux/8/availability/availability-ConfiguringFencingstonith.html#ol-pacemaker-stonith)：
- en: '[PRE29]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: To set up a cluster, we’ll need to create resources. A resource agent name has
    two or three fields separated by a colon. The first field is the resource class
    that indicates the standard followed by the resource agent and helps Pacemaker
    locate the script. For example, the IPaddr2 resource agent follows the **Open
    Cluster Framework** (**OCF**) standard. The second field varies based on the standard
    used, and OCF resources use it for the OCF namespace. The third field denotes
    the name of the resource agent.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置集群，我们需要创建资源。资源代理名称由两个或三个字段组成，字段之间由冒号分隔。第一个字段是资源类，它表示资源代理遵循的标准，并帮助 Pacemaker
    定位脚本。例如，IPaddr2 资源代理遵循**开放集群框架**（**OCF**）标准。第二个字段根据使用的标准而有所不同，OCF 资源使用它来表示 OCF
    命名空间。第三个字段表示资源代理的名称。
- en: Meta-attributes and instance attributes are available for resources. Meta-attributes
    are not resource-type dependent, while instance attributes are specific to each
    resource agent.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 资源可以使用元属性和实例属性。元属性不依赖于资源类型，而实例属性则特定于每个资源代理。
- en: In a cluster, resource operations refer to the actions that can be taken on
    a specific resource, such as starting, stopping, or monitoring it. These operations
    are identified by the `op` keyword. To ensure the resource remains healthy, we
    will add a monitor operation with a 15-second interval. The criteria for determining
    whether the resource is healthy depends on the resource agent being used. This
    is also why we enabled the `server-status` page on the `httpd` server, as the
    `httpd` agent uses that page to help determine the health of the system.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中，资源操作是指可以对特定资源执行的操作，如启动、停止或监控它。这些操作通过`op`关键字来标识。为了确保资源保持健康，我们将添加一个间隔为 15
    秒的监控操作。判断资源是否健康的标准取决于所使用的资源代理。这也是为什么我们在`httpd`服务器上启用了`server-status`页面，因为`httpd`代理使用该页面来帮助确定系统的健康状态。
- en: 'So, let’s add the VIP address:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们来添加 VIP 地址：
- en: '[PRE30]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next up, we will add the `httpd` server:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加`httpd`服务器：
- en: '[PRE31]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now that we have two resources, we also will need to tie them together as a
    group. With most applications, multiple resources need to be on the same physical
    server at the same time. This could be IP addresses, Tomcat servers, `httpd` servers,
    and so on. We are going to call this group `WebApp` and add both the VIP and `https`
    servers to it. Each resource is added individually, so two commands will need
    to be run:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两个资源，我们还需要将它们绑定在一起，作为一个组。对于大多数应用，多个资源需要同时在同一台物理服务器上。这可以是IP地址、Tomcat服务器、`httpd`服务器等。我们将这个组命名为`WebApp`，并将VIP和`https`服务器添加到其中。每个资源需要单独添加，因此需要运行两条命令：
- en: '[PRE32]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we will use the `pcs status` command to check the configuration:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用`pcs status`命令检查配置：
- en: '[PRE33]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.18 – pcs status](img/B18349_06_18.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![图6.18 – pcs状态](img/B18349_06_18.jpg)'
- en: Figure 6.18 – pcs status
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 – pcs状态
- en: We can now see the cluster, with its resource group, `WebApp`, with both the
    server VIPs.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到集群及其资源组`WebApp`，以及两个服务器的VIP地址。
- en: How it works…
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Now that everything is configured, let’s start up the resources and manage
    them. We can first start the entire cluster. This will online all nodes in the
    cluster:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切配置完成，让我们启动资源并进行管理。我们可以首先启动整个集群。这将使集群中的所有节点上线：
- en: '[PRE34]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can also set up the cluster to start on boot with the following commands:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过以下命令设置集群在启动时自动启动：
- en: '[PRE35]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: You will need to run both of these commands as root on both nodes.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在两个节点上以root身份运行这两个命令。
- en: Next up, let’s look at a few useful commands.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来看几个有用的命令。
- en: 'Sometimes a resource gets broken; maybe it was a bad config file, or maybe
    you started a resource outside of the cluster control, confusing the cluster.
    Once you fix the issues, you will likely need to refresh the resource. This will
    tell the cluster to forget about the failure and restart the service clear of
    any errors:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，资源会出现故障；可能是配置文件有问题，或者你在集群控制之外启动了某个资源，导致集群混乱。一旦解决了问题，你可能需要刷新资源。这会告诉集群忘记故障并重新启动服务，清除所有错误：
- en: '[PRE36]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You also can check the details of a resource, using the `config` option. This
    is helpful if you forget how the resource was configured. An example is seen in
    the following figure:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用`config`选项检查资源的详细信息。如果你忘记了资源是如何配置的，这个功能非常有用。以下图所示为例：
- en: '![Figure 6.19 – Resource configuration](img/B18349_06_19.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![图6.19 – 资源配置](img/B18349_06_19.jpg)'
- en: Figure 6.19 – Resource configuration
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 – 资源配置
- en: 'Next, let’s move `WebApp` to server `web2`:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将`WebApp`迁移到服务器`web2`：
- en: '[PRE37]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'When you run `move`, you can also monitor the move by checking the constraints.
    This is a little cleaner than using the `pcs` `status` command:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行`move`时，你还可以通过检查约束来监控迁移情况。这比使用`pcs` `status`命令更清晰：
- en: '[PRE38]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.20 – Cluster constraints](img/B18349_06_20.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![图6.20 – 集群约束](img/B18349_06_20.jpg)'
- en: Figure 6.20 – Cluster constraints
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20 – 集群约束
- en: The power of the Pacemaker/Corosync technology is its flexibility. You can cluster
    just about anything with it, making it a powerful tool for the sysadmin.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Pacemaker/Corosync技术的强大之处在于它的灵活性。你几乎可以用它来集群任何东西，使它成为系统管理员的强大工具。
- en: Sharing a filesystem across multiple machines – cluster or distribute?
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在多台机器之间共享文件系统——集群还是分布式？
- en: When you start using technologies such as load balancers and clustering software,
    you often end up in a situation where you need the same files on multiple servers.
    While you could simply copy the files, what if you could mount the files on each
    of the servers, sharing the filesystem across the systems without the SPOF that
    an NFS server introduces? One of the easiest ways to do this is to use Gluster.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始使用负载均衡器和集群软件等技术时，你常常会遇到需要在多台服务器上拥有相同文件的情况。虽然你可以直接复制文件，但如果你能在每台服务器上挂载文件，并在系统间共享文件系统，而不引入NFS服务器所带来的单点故障（SPOF）呢？做到这一点最简单的方法之一是使用Gluster。
- en: '**Gluster**, also known as **GlusterFS**, is an open source distributed filesystem
    that provides scalable and flexible storage for large volumes of data. Initially
    developed by Gluster Inc., it is now maintained by the open source community.
    Gluster uses a distributed architecture to create a single and unified filesystem
    that can span across multiple servers and storage devices. This approach allows
    you to aggregate the storage capacity of multiple servers and present it as a
    single, well-structured filesystem to users and applications. It has a wide range
    of applications, such as data storage, backup, and content delivery.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '**Gluster**，也称为**GlusterFS**，是一个开源分布式文件系统，提供可扩展和灵活的大数据存储解决方案。最初由Gluster公司开发，现在由开源社区维护。Gluster采用分布式架构来创建一个统一的文件系统，可以跨多个服务器和存储设备扩展。这种方法允许你聚合多个服务器的存储容量，并将其作为一个单一、结构良好的文件系统提供给用户和应用程序。它具有广泛的应用场景，如数据存储、备份和内容分发。'
- en: 'Key features and concepts of Gluster include the following:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: Gluster的关键特性和概念包括以下内容：
- en: '**Scalability**: Adding more storage servers to the cluster allows Gluster
    to easily accommodate growing data storage needs while scaling horizontally.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：向集群中添加更多存储服务器可以使Gluster轻松适应日益增长的数据存储需求，同时实现横向扩展。'
- en: '**Redundancy**: Gluster ensures data availability by replicating data across
    multiple nodes for redundancy and fault tolerance.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冗余**：Gluster通过将数据复制到多个节点来确保数据的可用性，从而提供冗余和故障容忍。'
- en: '**Flexibility**: Gluster supports various storage options, including local
    disks, NAS, and cloud storage. It can be customized to fit specific use cases
    and technologies.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性**：Gluster支持多种存储选项，包括本地磁盘、NAS和云存储。它可以根据特定的使用场景和技术进行定制。'
- en: '**Filesystem abstraction**: It provides users and applications with a standard
    filesystem interface, making integration into existing systems relatively easy.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文件系统抽象**：它为用户和应用程序提供了标准的文件系统接口，使得集成到现有系统中相对容易。'
- en: '**Data distribution**: Data is distributed across the cluster in a way that
    improves both performance and reliability. Data can be distributed evenly or based
    on specific criteria.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分布**：数据以一种提高性能和可靠性的方式在集群中分布。数据可以均匀分布，也可以根据特定的标准进行分布。'
- en: '**Automatic healing**: Gluster has a self-healing feature that automatically
    detects and repairs data inconsistencies or corrupted files.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动修复**：Gluster具有自我修复功能，能够自动检测并修复数据不一致或损坏的文件。'
- en: Gluster is often used in environments where large-scale, distributed storage
    is required, such as web servers, cloud computing, big data analytics, and media
    streaming services. It provides a cost-effective and flexible solution for managing
    data across a network of servers and storage devices.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: Gluster通常用于需要大规模分布式存储的环境，如网页服务器、云计算、大数据分析和媒体流服务。它提供了一种具有成本效益和灵活性的解决方案，用于管理网络中多个服务器和存储设备上的数据。
- en: Getting ready
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: For this recipe, you will need two Oracle Linux 8 systems, each with access
    to YUM repos. For this exercise, we will call them `gluster1` and `gluster2`.
    They are identical systems, each with 8 GB RAM, 4 vCPUs, and 100 GB of drive space.
    The filesystems have 50 GB in `/`, 5 GB in `/home`, and 8 GB in swap. The remaining
    disk space is unallocated. Additionally, for this example, each node will have
    a 100 GB LUN used for storing Gluster data.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，你将需要两个Oracle Linux 8系统，每个系统都能够访问YUM仓库。在本次练习中，我们将其称为`gluster1`和`gluster2`。它们是相同的系统，每个系统有8
    GB的RAM，4个vCPU，和100 GB的磁盘空间。文件系统的配置为：`/`分区50 GB，`/home`分区5 GB，swap分区8 GB。其余的磁盘空间尚未分配。此外，在这个例子中，每个节点将有一个100
    GB的LUN用于存储Gluster数据。
- en: Warning
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Having at least three nodes in a cluster is highly recommended to avoid split-brain
    clusters. Although a two-node cluster is possible, it poses a risk of corrupt
    data if the system ever splits its brain. Split-brain clusters are undesirable
    in distributed computing environments because they can result in data inconsistency,
    corruption, and operational issues. Split brain occurs when the nodes in a cluster
    lose connectivity or communication with each other, leading to the cluster’s division
    into multiple isolated nodes. Each node thinks it is the active or primary cluster,
    resulting in the potential for conflicts and data discrepancies.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议在集群中至少使用三个节点，以避免发生脑裂（split-brain）现象。虽然可以使用两节点集群，但如果系统出现脑裂，将存在数据损坏的风险。脑裂集群在分布式计算环境中是不希望出现的，因为它们可能导致数据不一致、损坏和操作问题。脑裂发生在集群中的节点失去连接或相互通信时，导致集群分成多个孤立的节点。每个节点都认为自己是活动的或主要的集群，可能会导致冲突和数据不一致。
- en: 'On each server, you will need to perform the following prep work:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在每台服务器上，您需要执行以下准备工作：
- en: Create an XFS filesystem on the 100 GB LUN. This space will be used to store
    the Gluster data, known as **bricks**. In the context of Gluster, a *brick* refers
    to a basic storage unit within the storage cluster. A cluster is made up of multiple
    bricks, which are essentially directories on storage servers or devices where
    data is stored. Each brick represents a portion of the overall storage capacity
    of the cluster.
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在100 GB LUN上创建XFS文件系统。此空间将用于存储Gluster数据，称为**砖块**。在Gluster的上下文中，*砖块*指的是存储集群中的基本存储单元。一个集群由多个砖块组成，这些砖块本质上是存储服务器或设备上的目录，用于存储数据。每个砖块代表集群总存储容量的一部分。
- en: 'Since we will be using Gluster to manage the storage, we will not be using
    LVM on the filesystem. On these systems, `/dev/sdb` is the 100 GB LUN. The following
    commands are used to create and mount the filesystem:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们将使用Gluster来管理存储，因此我们不会在文件系统上使用LVM。在这些系统中，`/dev/sdb`是100 GB LUN。以下命令用于创建和挂载文件系统：
- en: '[PRE39]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![Figure 6.21 – Bricks mounted](img/B18349_06_21.jpg)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![图6.21 – 砖块已挂载](img/B18349_06_21.jpg)'
- en: Figure 6.21 – Bricks mounted
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21 – 砖块已挂载
- en: 'Next, we need to make sure that all the nodes are in the `/etc/hosts` file.
    In this example, `gluster1`, `gluster2`, and `gluster3` are in the file, using
    both the short name and the **Fully Qualified Doman Name** (**FQDN**). This is
    seen in the following code snippet:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要确保所有节点都在`/etc/hosts`文件中。在这个示例中，`gluster1`、`gluster2`和`gluster3`都在文件中，使用了短名称和**完全限定域名**（**FQDN**）。可以在以下代码片段中看到：
- en: '[PRE40]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: dnf -y install oracle-gluster-release-el8
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: dnf -y install oracle-gluster-release-el8
- en: dnf -y config-manager --enable ol8_gluster_appstream ol8_baseos_latest ol8_appstream
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: dnf -y config-manager --enable ol8_gluster_appstream ol8_baseos_latest ol8_appstream
- en: dnf -y module enable glusterfs
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: dnf -y module enable glusterfs
- en: dnf -y install @glusterfs/server
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: dnf -y install @glusterfs/server
- en: 'systemctl status glusterd command. Verify that the service is active and running,
    as seen in the following example:'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: systemctl status glusterd命令。验证该服务是否处于活动状态并正在运行，如下例所示：
- en: '[PRE41]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![Figure 6.22 – Gluster daemon is running](img/B18349_06_22.jpg)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![图6.22 – Gluster守护进程正在运行](img/B18349_06_22.jpg)'
- en: Figure 6.22 – Gluster daemon is running
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22 – Gluster守护进程正在运行
- en: 'Next, let’s configure the firewall to allow the `glusterfs` port with the following
    commands:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们配置防火墙以允许`glusterfs`端口，使用以下命令：
- en: '[PRE42]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Additionally, to improve security, let’s create a self-signed key, to encrypt
    the communication between the nodes:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，为了提高安全性，我们将创建一个自签名密钥，以加密节点之间的通信：
- en: '[PRE43]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this example, we are using a self-signed certificate. In a secure production
    environment, you will want to consider using a commercially signed certificate.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们使用的是自签名证书。在安全的生产环境中，您可能希望考虑使用商业签名的证书。
- en: We will use these files later to encrypt the communication.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后将使用这些文件来加密通信。
- en: How to do it…
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: Now that all the prep work has been completed on each of the nodes, we will
    create the trusted storage pools and encrypt the communications. This will have
    everything ready to create volumes, where data is stored and shared.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有节点的准备工作已完成，我们将创建受信的存储池并加密通信。这将为创建卷做好准备，数据将在这些卷中存储和共享。
- en: 'A trusted storage pool in Gluster pertains to a setup where a cluster of Gluster
    servers, referred to as storage nodes or peers, have established trust among themselves
    to work together within a storage cluster. This trust is established through a
    trusted storage pool configuration that typically involves the following steps:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: Gluster中的受信存储池是指一组Gluster服务器集群，这些服务器被称为存储节点或对等节点，它们之间建立了信任关系，以便在存储集群中协同工作。通过受信存储池配置建立这种信任，通常涉及以下步骤：
- en: '**Authentication**: Various methods, such as SSH keys, certificates, or shared
    secrets, can be used to authenticate nodes in the trusted storage pool. This ensures
    that only authorized servers are part of the storage cluster.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**身份验证**：可以使用多种方法，如SSH密钥、证书或共享密钥，对受信存储池中的节点进行身份验证。这确保只有授权的服务器才能成为存储集群的一部分。'
- en: '**Authorization**: After nodes are authenticated, they authorize each other
    to access and manipulate specific data within the Gluster storage cluster. The
    authorization settings determine which nodes have read and write access to particular
    volumes or bricks within the cluster.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**授权**：节点通过身份验证后，它们授权彼此访问和操作Gluster存储集群中的特定数据。授权设置决定了哪些节点可以读取和写入集群中特定的卷或砖块。'
- en: '**Communication**: Members of the trusted storage pool communicate over a secure
    network to replicate data, synchronize metadata, and perform other cluster-related
    operations, ensuring that the storage cluster functions cohesively.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通信**：受信存储池中的成员通过安全网络进行通信，以复制数据、同步元数据并执行其他与集群相关的操作，确保存储集群的协同工作。'
- en: '**Data integrity**: Trusted storage pools ensure data integrity and redundancy
    via distributed replication across multiple nodes.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据完整性**：受信存储池通过在多个节点之间进行分布式复制，确保数据的完整性和冗余。'
- en: '**Scalability**: It is possible to add more storage nodes to the trusted pool,
    which enhances storage capacity and performance. The trusted nature of the pool
    makes it easy for new nodes to join the cluster and contribute to its resources.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：可以向受信池中添加更多的存储节点，从而增强存储容量和性能。受信池的特性使得新节点可以轻松加入集群并贡献其资源。'
- en: In Gluster, a trusted storage pool is a crucial element as it lays the foundation
    for the fault-tolerant and distributed nature of the filesystem. It guarantees
    that all nodes within the cluster can work seamlessly and securely in collaboration
    with each other. The following steps will walk you through how to create a GlusterFS
    on two hosts.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在Gluster中，受信存储池是一个关键元素，它为文件系统的容错和分布式特性奠定了基础。它保证集群中的所有节点可以无缝、安全地协同工作。以下步骤将指导您如何在两台主机上创建GlusterFS。
- en: 'To create the pool, we need to probe the other nodes in the cluster. In this
    example, we will probe from `gluster1` to `gluster2` using the `gluster peer probe`
    `gluster2` command:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建存储池，我们需要探测集群中的其他节点。在此示例中，我们将从 `gluster1` 到 `gluster2` 使用 `gluster peer probe`
    `gluster2` 命令进行探测：
- en: '[PRE44]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[root@gluster2 etc]#  gluster pool list'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[root@gluster2 etc]#  gluster pool list'
- en: UUID                                 Hostname        State
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: UUID                                 主机名        状态
- en: b13801f3-dcbd-487b-b3f3-2e95afa8b632                         gluster1        Connected
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b13801f3-dcbd-487b-b3f3-2e95afa8b632                         gluster1        已连接
- en: 'gluster1 and localhost connected as this was run on gluster2. If you run the
    same command from gluster1, you will see gluster2 as the remote host:'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: gluster1 和 localhost 已连接，因为这是在 gluster2 上运行的。如果您从 gluster1 运行相同的命令，您将看到 gluster2
    作为远程主机：
- en: '[PRE45]'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Now that we have the cluster, let’s create a replicated volume. This volume
    will re-write the bricks across the cluster, enabling protection against failed
    storage or a failed node.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经有了集群，让我们创建一个复制卷。该卷将在集群中重写砖块，提供对存储故障或节点故障的保护。
- en: 'The following command will create the volume:'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下命令将创建该卷：
- en: '[PRE47]'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'df command:'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: df 命令：
- en: '[PRE48]'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![Figure 6.23 – data1 mounted on /mnt](img/B18349_06_23.jpg)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![图6.23 – data1 挂载在 /mnt](img/B18349_06_23.jpg)'
- en: Figure 6.23 – data1 mounted on /mnt
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.23 – data1 挂载在 /mnt
- en: 'If you want to mount this on other nodes, you will need to repeat the command
    on each node, updating the node name as needed. The following example shows mounting
    on `gluster1`:'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您想在其他节点上挂载此卷，您需要在每个节点上重复该命令，根据需要更新节点名称。以下示例显示了在 `gluster1` 上的挂载：
- en: '[PRE49]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, we need to enable the encryption. This is done by touching the secure-access
    file on each node using the following command:'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要启用加密。可以通过在每个节点上触摸 secure-access 文件来完成此操作，使用以下命令：
- en: '[PRE50]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: gluster volume set data1 client.ssl on
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: gluster volume set data1 client.ssl on
- en: 'glusterd:'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'glusterd:'
- en: '[PRE51]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Gluster communication is now encrypted for this volume.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Gluster通信已经为该卷加密。
- en: How it works…
  id: totrans-443
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'There is more you can do with Gluster. First, volumes have multiple options
    when you create them, each offering options for replication and distribution:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过Gluster做更多的事情。首先，在创建卷时，卷有多个选项，每个选项都提供复制和分布的选项：
- en: '**Distributed**: When using distributed volumes, files are randomly distributed
    across the bricks in the volume. This type of volume is useful when the need is
    to scale storage, and redundancy is not necessary or is already provided by other
    hardware or software layers. However, it is important to note that disk or server
    failures can result in significant data loss, as the data is spread randomly across
    the bricks in the volume. An example command to build a distributed volume is
    the following:'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式**：使用分布式卷时，文件在卷中的砖块上随机分布。此类型的卷在需要扩展存储时非常有用，并且冗余不必要，或已由其他硬件或软件层提供。然而，需要注意的是，磁盘或服务器故障可能会导致显著的数据丢失，因为数据在卷中的砖块上是随机分布的。构建分布式卷的示例命令如下：'
- en: '[PRE53]'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '**Replicated**: Files are copied across bricks for HA in replicated volumes.
    An example command to build a replicated volume is the following:'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复制**：文件在复制卷中跨砖块进行复制，以实现高可用性（HA）。构建复制卷的示例命令如下：'
- en: '[PRE54]'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '**Distributed replicated**: Distributed files across replicated bricks in the
    volume for improved read performance, HA, and reliability. When creating a distributed
    replicated volume, the number of nodes should be a multiple of the number of bricks.
    An example command to build a distributed replicated volume is the following:'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式复制**：将文件分布到卷中的复制砖块上，以提高读取性能、高可用性（HA）和可靠性。创建分布式复制卷时，节点的数量应为砖块数量的倍数。构建分布式复制卷的示例命令如下：'
- en: '[PRE55]'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '**Dispersed**: This volume type utilizes erasure codes to efficiently protect
    against disk or server failures. It works by striping the encoded data of files
    across multiple bricks in the volume while adding redundancy to ensure reliability.
    Dispersed volumes allow for customizable reliability levels with minimal space
    waste. A dispersed volume must have at least three bricks. An example command
    to build a dispersed volume is the following:'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分散**：此卷类型利用纠删码高效地保护磁盘或服务器故障。它通过将文件的编码数据条带化，分布到卷中的多个砖块上，并添加冗余以确保可靠性。分散卷允许通过最小的空间浪费来自定义可靠性级别。分散卷必须至少有三个砖块。构建分散卷的示例命令如下：'
- en: '[PRE56]'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '**Distributed dispersed**: Distributes data across dispersed bricks, providing
    the same benefits of distributed replicated volumes but using dispersed storage.
    A dispersed volume must have at least six bricks. An example command to build
    a distributed dispersed volume is the following:'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式分散**：将数据分布到分散的砖块上，提供与分布式复制卷相同的好处，但使用的是分散存储。分散卷必须至少有六个砖块。构建分布式分散卷的示例命令如下：'
- en: '[PRE57]'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'When adding bricks to any volume, you can put more than one brick on a Gluster
    node. Simply define the additional brick in the command. In this example, a distributed
    dispersed volume is created, by putting two bricks on each node:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 向任何卷中添加砖块时，您可以在同一Gluster节点上放置多个砖块。只需在命令中定义附加砖块即可。在此示例中，通过在每个节点上放置两个砖块来创建一个分布式分散卷：
- en: '[PRE58]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Volumes can be stopped with the `gluster stop volume volumename` command. An
    example to stop the `data1` volume is the following:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`gluster stop volume volumename`命令停止卷。停止`data1`卷的示例如下：
- en: '[PRE59]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'You can also add bricks to a volume to grow it. This can be done after a new
    node is added to the cluster. In the following example, `gluster3` was added to
    the cluster with the `gluster node probe gluster3` command first. Then, `data1`
    was grown with the following command:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以向卷中添加砖块来扩展它。这可以在将新节点添加到集群后完成。在以下示例中，首先使用`gluster node probe gluster3`命令将`gluster3`添加到集群中。然后，使用以下命令扩展`data1`卷：
- en: '[PRE60]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Note
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When adding bricks to a volume, make sure you add the required number of bricks.
    Volume types such as distributed replicated volumes will need more than a single
    brick added.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在向卷中添加砖块时，请确保添加所需数量的砖块。像分布式复制卷这样的卷类型需要添加多个砖块。
- en: 'You can also check the status of all volumes with the following command:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用以下命令检查所有卷的状态：
- en: '[PRE61]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'An example is seen in the following screenshot:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图中可以看到示例：
- en: '![Figure 6.24 – volume status](img/B18349_06_24.jpg)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![图6.24 – 卷状态](img/B18349_06_24.jpg)'
- en: Figure 6.24 – volume status
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.24 – 卷状态
- en: 'You can see the summary for a single volume by adding the volume name to the
    command. You can also see more details by adding the `detail` option. These can
    be combined, as seen in the following screenshot:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将卷名称添加到命令中来查看单个卷的摘要。还可以通过添加 `detail` 选项来查看更多详细信息。这些可以组合使用，如下图所示：
- en: '![Figure 6.25 – Volume details](img/B18349_06_25.jpg)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.25 – 卷详细信息](img/B18349_06_25.jpg)'
- en: Figure 6.25 – Volume details
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.25 – 卷详细信息
- en: 'If you want to see performance information about a volume, the `top` option
    can be used. This will show what bricks are being used for read/write activity
    as well as I/O throughput to each brick. The basic command is `gluster volume
    top volume_name option`, with `volume_name` being the name of the volume and the
    options being as follows:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看卷的性能信息，可以使用 `top` 选项。此命令将显示用于读/写操作的砖块，以及每个砖块的 I/O 吞吐量。基本命令是 `gluster volume
    top volume_name option`，其中 `volume_name` 是卷的名称，选项如下所示：
- en: '`read`: This shows the highest read calls for each brick, as well as the counts.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`read`：此命令显示每个砖块的最高读调用次数及其计数。'
- en: '`write`: This shows the highest write calls for each brick, as well as the
    counts.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`write`：此命令显示每个砖块的最高写调用次数及其计数。'
- en: '`open`: This shows what bricks have open file descriptors.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`open`：此命令显示哪些砖块有开放的文件描述符。'
- en: '`opendir`: This shows what bricks have open calls on each directory, as well
    as the counts.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`opendir`：此命令显示每个目录上有哪些砖块有开放调用，以及它们的调用次数。'
- en: '`read-perf`: This shows read-performance throughput by brick. Run using the
    options `bs` (for block size) `1024` and `count 1024`.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`read-perf`：此命令显示按砖块的读性能吞吐量。使用选项 `bs`（块大小）`1024` 和 `count 1024` 运行。'
- en: '`write-perf`: This shows read-performance throughput by brick. Run using the
    options `bs` (for block size) `1024` and `count 1024`.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`write-perf`：此命令显示按砖块的写性能吞吐量。使用选项 `bs`（块大小）`1024` 和 `count 1024` 运行。'
- en: 'Several examples are seen in the following figure:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中可以看到几个示例：
- en: '![Figure 6.26 – volume top examples](img/B18349_06_26.jpg)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.26 – 卷顶级示例](img/B18349_06_26.jpg)'
- en: Figure 6.26 – volume top examples
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.26 – 卷顶级示例
- en: Volumes can also be deleted. This is done with the `gluster volume delete volume_name`
    command, where `volume_name` is the volume being deleted. As a note, when deleting
    volumes, don’t forget to use the `rm` command to delete the bricks from storage.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 卷也可以被删除。使用命令 `gluster volume delete volume_name` 删除，其中 `volume_name` 是要删除的卷。需要注意的是，删除卷时，不要忘记使用
    `rm` 命令删除存储中的砖块。
- en: Generating, configuring, and monitoring Ethernet traffic over bond
  id: totrans-482
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成、配置和监控以太网流量通过 Bond
- en: 'When using bare-metal servers as dedicated hosts or Linux systems that host
    virtual machines using the KVM hypervisor, the network can be a weak point. Fortunately,
    this issue can be resolved by implementing Ethernet bonding, also known as network
    bonding, or **Network Interface Card** (**NIC**) bonding. It is a technology in
    Linux that allows you to combine multiple NICs into a single logical interface.
    This logical interface, known as a bond or bonded interface, provides increased
    network bandwidth, fault tolerance, and load balancing. These are summarized as
    follows:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用裸机服务器作为专用主机或使用 KVM 虚拟化技术的 Linux 系统托管虚拟机时，网络可能成为一个薄弱环节。幸运的是，通过实现以太网绑定，也称为网络绑定或
    **网卡**（**NIC**）绑定，可以解决此问题。它是 Linux 中的一项技术，允许将多个网卡组合成一个逻辑接口。这个逻辑接口被称为绑定接口（bonded
    interface），它提供了更高的网络带宽、容错性和负载均衡。具体如下：
- en: '**Load balancing**: Bonding distributes network traffic across multiple NICs,
    increasing bandwidth. Various algorithms, such as round-robin, active-backup,
    and XOR, can be used depending on specific requirements.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载均衡**：绑定将网络流量分配到多个网卡，增加带宽。根据具体要求，可以使用多种算法，如轮询、活动备份和异或（XOR）。'
- en: '**Fault tolerance**: In the event of an NIC or network link failure, Ethernet
    bonding can automatically switch traffic to another active NIC. This provides
    redundancy and fault tolerance, ensuring network connectivity remains available
    even if one NIC becomes unavailable.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：在网卡或网络链路故障的情况下，以太网绑定可以自动将流量切换到另一个活动的网卡。这样可以提供冗余和容错性，确保即使一个网卡不可用，网络连接仍然可用。'
- en: '**Link aggregation**: Bonding can be used to create **link aggregation groups**
    (**LAGs**) or NIC teams, which enhance bandwidth and redundancy in HA setups.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**链路聚合**：绑定可以用于创建 **链路聚合组**（**LAGs**）或网卡团队，这可以增强带宽和在高可用性设置中的冗余性。'
- en: In this recipe, we will configure bonding, and then show some common tools that
    will allow you to both monitor and generate Ethernet traffic over the bond.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将配置绑定，然后展示一些常用的工具，这些工具可以让你监控并生成通过绑定接口的以太网流量。
- en: Additionally, there are a few technologies you need to be familiar with.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些技术是你需要熟悉的。
- en: MAC
  id: totrans-489
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MAC
- en: A **Media Access Control** (**MAC**) address is a hardware identifier assigned
    to network interfaces such as Ethernet cards and Wi-Fi adapters for communication
    on a local network. It is hardcoded into the network hardware during manufacturing
    and is used at the data link layer (Layer 2) of the OSI model. One of the most
    important features of MAC addresses is that they must be unique. Each MAC address
    is meant to be globally unique, and manufacturers bear the responsibility of ensuring
    that no two network interfaces have the same MAC address, though this can be a
    challenging task to accomplish in practice, especially in virtualized environments.
    This can be an issue with networking, as duplicate MAC addresses on any network
    will cause issues. Additionally, many of the bonding modes rely on MAC addresses
    to load balance traffic.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '**媒体访问控制**（**MAC**）地址是分配给网络接口（如以太网卡和Wi-Fi适配器）的硬件标识符，用于在本地网络上进行通信。它在制造过程中硬编码到网络硬件中，并用于OSI模型的数据链路层（第二层）。MAC地址的一个重要特性是它们必须是唯一的。每个MAC地址应该是全局唯一的，制造商有责任确保没有两个网络接口具有相同的MAC地址，尽管在实际操作中，尤其是在虚拟化环境中，这可能是一项挑战。这可能会导致网络问题，因为任何网络上的MAC地址重复都会引发问题。此外，许多绑定模式依赖于MAC地址来进行流量负载均衡。'
- en: Bonding modes
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 绑定模式
- en: 'Bonding modes refer to the various strategies or algorithms used to determine
    how network traffic is distributed across the physical network interfaces that
    have been aggregated into a bonded interface using the Linux bonding driver. These
    modes control the load-balancing and failover behavior of the bonded interface.
    The choice of bonding mode depends on your specific network requirements and goals.
    Here are some common Linux bonding modes:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 绑定模式是指通过Linux绑定驱动程序将多个物理网络接口聚合成一个绑定接口时，决定如何分配网络流量的各种策略或算法。这些模式控制绑定接口的负载均衡和故障转移行为。选择绑定模式取决于你的特定网络需求和目标。以下是一些常见的Linux绑定模式：
- en: '`balance-rr`: In this mode, outgoing network traffic is distributed evenly
    across the available network interfaces in a round-robin fashion. It’s a simple
    load-balancing mode that provides improved outbound traffic performance but does
    not consider the state of the interfaces, which can lead to uneven inbound traffic
    distribution. Occasionally, this mode does not work well with some switching systems.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`balance-rr`：在此模式下，出站网络流量会均匀地分配到可用的网络接口上，采用轮询方式。这是一种简单的负载均衡模式，可以改善出站流量的性能，但不考虑接口的状态，这可能导致不均匀的入站流量分配。有时，这种模式在某些交换系统中可能无法正常工作。'
- en: '`active-backup`: A commonly used mode, which is often referred to as failover
    mode, this mode has a primary interface, while the others remain on standby. If
    the primary interface fails, the next available interface is automatically activated
    to ensure continuity. This mode provides redundancy and is one of the easiest
    modes to get working in any environment.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`active-backup`：一种常用模式，通常被称为故障转移模式，该模式有一个主接口，其他接口处于待机状态。如果主接口出现故障，下一个可用接口会自动激活以确保连续性。此模式提供冗余性，是在任何环境中最容易使用的模式之一。'
- en: '`balance-xor`: This mode utilizes a straightforward XOR operation to maintain
    a balance between the transmission and reception of data. The process involves
    distributing traffic based on the MAC addresses of the source and destination.
    This guarantees that packets between the same endpoints will always take the same
    path. The primary purpose of this mode is to ensure fault tolerance. Occasionally,
    this mode does not work well with some switching systems.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`balance-xor`：此模式利用简单的XOR运算来保持数据传输和接收之间的平衡。该过程根据源和目标的MAC地址分配流量，确保相同端点之间的包始终走相同的路径。此模式的主要目的是确保容错性。但有时，这种模式在某些交换系统中可能无法正常工作。'
- en: '`balance-tlb`: When operating in this mode, the outgoing traffic is distributed
    among all available interfaces based on their current load. However, incoming
    traffic is not actively balanced, and it is only received by the active interface.
    This mode is particularly useful when the switch does not support **Link Aggregation
    Control Protocol** (**LACP**). Occasionally, this mode does not work well with
    some switching systems.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`balance-tlb`：在此模式下，外发流量根据各接口的当前负载在所有可用接口之间分配。然而，入站流量不会主动平衡，仅由活动接口接收。当交换机不支持**链路聚合控制协议**（**LACP**）时，此模式特别有用。偶尔，这种模式与某些交换系统配合不良。'
- en: '`balance-alb`: This mode actively balances both incoming and outgoing traffic
    by considering the availability and load of each interface. Occasionally, this
    mode does not work well with some switching systems.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`balance-alb`：该模式通过考虑每个接口的可用性和负载，积极地平衡进出流量。偶尔，这种模式与某些交换系统配合不良。'
- en: LACP
  id: totrans-498
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LACP
- en: All of the preceding modes can operate without any changes to the switches that
    the server is connected to. However, there is another mode that is more commonly
    used, called LACP. LACP is the most complex mode used to aggregate multiple network
    connections, usually Ethernet, into a single high-bandwidth link. This process
    is commonly known as link aggregation, NIC teaming, or bonding. LACP is defined
    by the IEEE 802.3ad standard and is frequently used in enterprise and data center
    environments to enhance network performance, redundancy, and fault tolerance.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 所有前述模式均可在不更改服务器连接的交换机设置的情况下运行。然而，还有一种更常用的模式，称为LACP。LACP是用于将多个网络连接（通常是以太网）聚合成一个高带宽链路的最复杂模式。这个过程通常被称为链路聚合、网卡团队或绑定。LACP由IEEE
    802.3ad标准定义，常用于企业和数据中心环境中，以提高网络性能、冗余性和容错能力。
- en: 'However, to utilize LACP, switches must be configured to use it. As an administrator,
    it is essential to communicate your configuration requirements to ensure that
    the switch is configured in a compatible mode. The configuration must match on
    both ends for LACP to work correctly. Most enterprise-grade network switches and
    server NICs provide LACP support. Key characteristics and features of LACP include
    the following:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要使用LACP，交换机必须配置为支持LACP。作为管理员，必须明确传达配置要求，以确保交换机被配置为兼容模式。两端的配置必须匹配，LACP才能正常工作。大多数企业级网络交换机和服务器网卡都提供LACP支持。LACP的主要特点和功能包括以下几点：
- en: '**Aggregated links**: LACP enables the aggregation of multiple physical network
    links into a single logical link, which appears as a single interface to network
    devices.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合链路**：LACP允许将多个物理网络链路聚合成一个逻辑链路，该链路在网络设备上表现为一个单一接口。'
- en: '**Increased bandwidth**: Aggregating multiple links with LACP can boost network
    bandwidth for bandwidth-intensive applications and server-to-switch connections.
    However, each MAC-to-MAC connection is usually limited to the speed of a single
    member of the aggregated link. If you have a host with two 1 Gb/s ports in the
    link, you will likely be unable to get more than 1 Gb/s of communication between
    the host and a client.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带宽增加**：通过LACP聚合多个链路可以提升带宽，适用于带宽密集型应用和服务器到交换机的连接。然而，每个MAC到MAC的连接通常受到聚合链路单一成员速度的限制。如果你有一个主机，主机上有两个1
    Gb/s端口，可能无法在主机和客户端之间获得超过1 Gb/s的通信速度。'
- en: '**Load balancing**: LACP can distribute network traffic across aggregated links
    using various load-balancing algorithms, preventing network congestion on a single
    link while optimizing network utilization.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载均衡**：LACP可以通过各种负载均衡算法将网络流量分配到聚合链路上，防止单一链路上的网络拥塞，同时优化网络利用率。'
- en: '**Fault tolerance**: In addition to providing increased bandwidth, LACP also
    offers redundancy and fault-tolerance capabilities. If one physical link fails,
    LACP can automatically redirect traffic to the remaining active links, minimizing
    downtime and ensuring network availability.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错能力**：除了提供更高的带宽，LACP还提供冗余性和容错能力。如果一个物理链路发生故障，LACP可以自动将流量重定向到剩余的活动链路，从而最小化停机时间并确保网络可用性。'
- en: '**Dynamic protocol**: LACP is a dynamic protocol that dynamically negotiates
    and establishes link aggregations between network devices using LACP frames.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态协议**：LACP是一种动态协议，通过使用LACP帧动态协商并建立网络设备之间的链路聚合。'
- en: '**Modes**: LACP supports two modes of operation:'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式**：LACP支持两种操作模式：'
- en: '**Active mode**: In this mode, the device actively sends LACP frames to negotiate
    and establish link aggregations.'
  id: totrans-507
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主动模式**：在此模式下，设备主动发送LACP帧以协商并建立链路聚合。'
- en: '**Passive mode**: In passive mode, the device listens for LACP frames but does
    not actively send them. It relies on the other end configured in active mode to
    initiate the aggregation.'
  id: totrans-508
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**被动模式**：在被动模式下，设备监听LACP帧，但不会主动发送。它依赖于另一端配置为主动模式来启动聚合。'
- en: LACP is commonly used in scenarios where HA and network performance are critical,
    such as server-to-switch connections, inter-switch links, and connections to **Storage
    Area Networks** (**SANs**). It allows organizations to make efficient use of available
    network resources and improve network reliability.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: LACP通常用于对高可用性（HA）和网络性能要求较高的场景，例如服务器到交换机连接、交换机间连接和**存储区域网络**（**SANs**）连接。它允许组织高效利用可用的网络资源并提高网络可靠性。
- en: Getting ready
  id: totrans-510
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, you will need three Oracle Linux 8 systems, each with access
    to yum repos. For this exercise, we will call them `networking`, `client1`, and
    `client2`. They are mostly identical systems, each with 8 GB RAM, 4 vCPUs, and
    100 GB of drive space. The difference is that networking should have two network
    interfaces on the same network. The filesystems have 50 GB in `/`, 5 GB in `/home`,
    and 8 GB in swap. The remaining disk space is unallocated.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此操作，你需要三台Oracle Linux 8系统，每台系统可以访问yum仓库。我们将它们命名为`networking`、`client1`和`client2`。它们大致相同，每台系统有8GB内存、4个vCPU和100GB的磁盘空间。不同之处在于`networking`应具有两个在同一网络上的网络接口。文件系统中，`/`有50GB，`/home`有5GB，交换分区为8GB，其余磁盘空间未分配。
- en: How to do it…
  id: totrans-512
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: While this can be done from the GUI, this recipe will cover doing this from
    the command line. As a note, when working on the main network connection to the
    server, you will want to be on the system console. Doing this via a remote connection
    such as SSH can leave you in a situation where you lose access to the server.
    Following are the steps to configure a redundant connection.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然也可以通过图形界面完成这项操作，但此方案将介绍如何通过命令行完成。需要注意的是，在处理服务器的主要网络连接时，最好使用系统控制台。如果通过SSH等远程连接进行操作，可能会导致你无法访问服务器。以下是配置冗余连接的步骤。
- en: 'The first thing you will need to do is create the bond. In this example, we
    will create it using the `balance-alb` mode to best balance both incoming and
    outgoing traffic:'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要做的第一件事是创建绑定。在本例中，我们将使用`balance-alb`模式来最好地平衡进出流量：
- en: '[PRE62]'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Next, we will configure the bond to use DHCP. On production servers, you would
    normally use a manually configured IP address:'
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将配置绑定使用DHCP。在生产服务器上，通常会使用手动配置的IP地址：
- en: '[PRE63]'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Note
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you are adding a port into a bond that is already in use, you should delete
    that port now. In this case, `enp0s3` was in use, so it was deleted with the following
    command:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要将端口添加到已经在使用的绑定中，你应该现在删除该端口。在本例中，`enp0s3`正在使用，所以用以下命令删除了它：
- en: '**nmcli connection** **del enp0s3**'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '**nmcli connection** **del enp0s3**'
- en: '[PRE64]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now, we will start the connection and check the status:'
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将启动连接并检查状态：
- en: '[PRE65]'
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '![Figure 6.27 – nmcli device output with a working bind](img/B18349_06_27.jpg)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.27 – nmcli设备输出，带有有效绑定](img/B18349_06_27.jpg)'
- en: Figure 6.27 – nmcli device output with a working bind
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.27 – nmcli设备输出，带有有效绑定
- en: You can see `bond0` as the device, with members `enp0s3` and `enp0s8`.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到`bond0`是设备，成员为`enp0s3`和`enp0s8`。
- en: 'When you look at the IP address, you will see that that is now on the `bond0`
    device. This can be checked with the `ifconfig bond0` command, with the output
    as follows:'
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你查看IP地址时，你会发现它现在位于`bond0`设备上。可以通过`ifconfig bond0`命令来检查，输出如下：
- en: '![Figure 6.28 – Output from ifconfig bond0](img/B18349_06_28.jpg)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.28 – 来自ifconfig bond0的输出](img/B18349_06_28.jpg)'
- en: Figure 6.28 – Output from ifconfig bond0
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.28 – 来自ifconfig bond0的输出
- en: You can continue to use the system normally now, but with `bond0` being the
    network device.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以继续正常使用系统，但`bond0`是网络设备。
- en: How it works…
  id: totrans-531
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Now that we have a working bond, let’s look at the traffic going in and out.
    To do this, we need to install the `iptraf-ng` command:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个正常工作的绑定，让我们看看进出流量。为此，我们需要安装`iptraf-ng`命令：
- en: '[PRE66]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'This tool allows you to monitor Ethernet traffic on a server. For this example,
    we will run the `iptraf-ng` command. This will launch the program, and you will
    be on the main screen, as seen in the following screenshot:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工具允许你监控服务器上的以太网流量。在本例中，我们将运行`iptraf-ng`命令。这将启动程序，你将看到主屏幕，截图如下所示：
- en: '![Figure 6.29 – iptraf main menu](img/B18349_06_29.jpg)'
  id: totrans-535
  prefs: []
  type: TYPE_IMG
  zh: '![图6.29 – iptraf主菜单](img/B18349_06_29.jpg)'
- en: Figure 6.29 – iptraf main menu
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.29 – iptraf主菜单
- en: 'From here, we will look at the **General interface statistics** by hitting
    the *S* key. This will then show a real-time flow of traffic on each interface:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将按下*S*键查看**一般接口统计**，这将实时显示每个接口上的流量流动情况：
- en: '![Figure 6.30 – General interface statistics](img/B18349_06_30.jpg)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
  zh: '![图6.30 – 一般接口统计](img/B18349_06_30.jpg)'
- en: Figure 6.30 – General interface statistics
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.30 – 一般接口统计
- en: 'You can see that both in and out traffic is balanced between the two physical
    interfaces. This is because the bond was built using the `balance-alb` mode. To
    generate this traffic, a simple flood ping was used from the `client2` system.
    This was done with the following command:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，进出流量在两个物理接口之间是平衡的。这是因为使用了`balance-alb`模式建立了链路聚合。为了生成这些流量，使用了`client2`系统的简单洪泛ping命令。具体命令如下：
- en: '[PRE67]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Note
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Be careful using the `-f` option, as this will flood the network with traffic,
    and it is generally not acceptable to do so on production networks without coordinating
    with the network and security teams. It can cause performance issues for systems
    using the network.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`-f`选项时要小心，因为这会让网络充满流量，并且通常在生产网络上不接受这样做，除非事先与网络和安全团队协调。这可能会导致使用网络的系统出现性能问题。
- en: There is a better way to really stress the network, though. That is to use a
    tool that will generate the maximum levels of packets the interface will support.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，还有一种更好的方式来真正给网络施加压力。那就是使用一个能够生成接口最大支持的包数的工具。
- en: iperf – network stress tool
  id: totrans-545
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: iperf – 网络压力测试工具
- en: 'We will use a tool called `iperf`. It will generate the maximum amount of traffic
    that the interface can support. To install `iperf`, run the following command
    on all systems:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个名为`iperf`的工具，它将生成接口可以支持的最大流量。要安装`iperf`，在所有系统上运行以下命令：
- en: '[PRE68]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We also will need to open up a TCP port for the system to use and reload the
    firewall. Each instance of the server can only handle a single client connection.
    We will add multiple ports to enable running multiple servers, each with a different
    port. This is done with the following commands:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要为系统打开一个TCP端口并重新加载防火墙。每个服务器实例只能处理一个客户端连接。我们将添加多个端口以启用运行多个服务器，每个服务器使用不同的端口。可以使用以下命令完成此操作：
- en: '[PRE69]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '`iperf` works using a client-server model. In order to use it, we first need
    to start an `iperf` server on the networking system. With this example, we will
    set up the server to listen on port `8001` using TCP. This is started with the
    following command:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '`iperf`采用客户端-服务器模式工作。要使用它，我们首先需要在网络系统上启动一个`iperf`服务器。在这个例子中，我们将设置服务器监听`8001`端口并使用TCP协议。可以使用以下命令启动服务器：'
- en: '[PRE70]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We will repeat the same command in a different window, running on port `8002`
    instead:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在另一个窗口中重复相同的命令，改为使用`8002`端口：
- en: '[PRE71]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Now, with two servers running, we can run a test from `client1`.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，两个服务器都在运行，我们可以从`client1`进行测试。
- en: 'The test is run with the following command:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 测试使用以下命令运行：
- en: '[PRE72]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The test will kick off and for a few seconds run the maximum traffic possible,
    as seen in the following output:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 测试开始后，几秒钟内会产生最大的流量，如以下输出所示：
- en: '![Figure 6.31 – iperf client output](img/B18349_06_31.jpg)'
  id: totrans-558
  prefs: []
  type: TYPE_IMG
  zh: '![图6.31 – iperf客户端输出](img/B18349_06_31.jpg)'
- en: Figure 6.31 – iperf client output
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.31 – iperf客户端输出
- en: 'While the test runs, we can monitor the performance from the `iptraf` command,
    and we will see most of the traffic is hitting a single interface. This is seen
    in the following screenshot:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试运行时，我们可以通过`iptraf`命令监控性能，我们会看到大部分流量都集中在一个接口上。以下截图展示了这一点：
- en: '![Figure 6.32 – Single-client traffic](img/B18349_06_32.jpg)'
  id: totrans-561
  prefs: []
  type: TYPE_IMG
  zh: '![图6.32 – 单客户端流量](img/B18349_06_32.jpg)'
- en: Figure 6.32 – Single-client traffic
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.32 – 单客户端流量
- en: This traffic is all on the `enp0s8` interface. This is because the load-balancing
    algorithm uses the MAC address of the client. This limits the traffic to a single
    interface in the bond.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些流量都在`enp0s8`接口上。这是因为负载均衡算法使用了客户端的MAC地址，这限制了流量只通过链路聚合中的一个接口。
- en: 'Next up, can run a test from `client1` and `client2` simultaneously. The difference
    is that `client2` will use port `8002`. The results are seen in the following
    screenshot:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，可以同时从`client1`和`client2`运行测试。不同之处在于`client2`将使用`8002`端口。结果显示在以下截图中：
- en: '![Figure 6.33 – Test with two different clients](img/B18349_06_33.jpg)'
  id: totrans-565
  prefs: []
  type: TYPE_IMG
  zh: '![图6.33 – 使用两个不同客户端的测试](img/B18349_06_33.jpg)'
- en: Figure 6.33 – Test with two different clients
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.33 – 使用两个不同客户端的测试
- en: Here, we can see that both interfaces are being used for heavy network loads.
    This is happening because each client is using a different MAC address. If a third
    client were used, we would then see it competing for traffic for one of the interfaces,
    potentially causing issues.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到两个接口都在处理重负载的网络流量。这是因为每个客户端使用了不同的MAC地址。如果使用了第三个客户端，我们就会看到它与其中一个接口争夺流量，可能会导致问题。
- en: This is an important factor to consider when bonding in environments where heavy
    network performance is required from multiple clients. When using bonds, and you
    have random performance latency issues, monitor the ports. You might face some
    contention. One way to address this is to add additional ports to the bond. You
    can also upgrade the interfaces to units that can support more bandwidth.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个重要的因素，特别是在需要来自多个客户端的高网络性能的环境中进行绑定时需要考虑的因素。当使用绑定时，如果遇到随机性能延迟问题，需要监控端口。你可能会面临一些争用。解决这个问题的一种方法是向绑定中添加额外的端口。你还可以升级接口，选择能够支持更多带宽的设备。
