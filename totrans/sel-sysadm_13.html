<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div id="_idContainer031" class="calibre2">
			<h1 id="_idParaDest-289" class="calibre5"><em class="italic"><a id="_idTextAnchor293" class="pcalibre calibre6 pcalibre1"/>Chapter 11</em>: Enhancing the Security of Containerized Workloads</h1>
			<p class="calibre3">Container platforms and management frameworks provide application-level abstraction to administrators and developers. Lightweight container frameworks allow for rapid development and deployment of new applications, whereas heavier container platforms allow for optimal resource consumption and highly resilient hosting platforms.</p>
			<p class="calibre3">SELinux plays a vital role in many of these frameworks and platforms, ensuring that untrusted containers cannot escape or interact with resources they are not supported to interact with. In this chapter, we look at how SELinux is supported, ranging from <strong class="source-inline">systemd-nspawn</strong> to <strong class="source-inline">podman</strong> (and Docker), and finally in larger environments with Kubernetes. We also learn how to create custom SELinux domains for containers using the <strong class="source-inline">udica</strong> utility.</p>
			<p class="calibre3">In this chapter, we're going to cover the following main topics:</p>
			<ul class="calibre8">
				<li class="calibre9">Using SELinux with systemd's container support</li>
				<li class="calibre9">Configuring podman</li>
				<li class="calibre9">Leveraging Kubernetes' SELinux support</li>
			</ul>
			<h1 id="_idParaDest-290" class="calibre5"><a id="_idTextAnchor294" class="pcalibre calibre6 pcalibre1"/>Technical requirements</h1>
			<p class="calibre3">Check out the following video to see the Code in Action: <a href="https://bit.ly/34aHOfl" class="pcalibre calibre6 pcalibre1">https://bit.ly/34aHOfl</a></p>
			<h1 id="_idParaDest-291" class="calibre5"><a id="_idTextAnchor295" class="pcalibre calibre6 pcalibre1"/>Using SELinux with systemd's container support</h1>
			<p class="calibre3">In <a href="B16276_07_Final_VK.xhtml#_idTextAnchor216" class="pcalibre calibre6 pcalibre1"><em class="italic">Chapter 7</em></a>, <em class="italic">Configuring Application-Specific SELinux Controls</em>, we introduced systemd as an <a id="_idIndexMarker761" class="pcalibre calibre6 pcalibre1"/>SELinux-aware application suite, capable of launching different services with configurable SELinux contexts. Besides service support, systemd has quite a few other features up its sleeve. One of these features is <strong class="source-inline">systemd-nspawn</strong>.</p>
			<p class="calibre3">With <strong class="source-inline">systemd-nspawn</strong>, systemd provides container capabilities, allowing administrators to interact with systemd-managed containers in an integrated way, almost as if these containers were services themselves. It uses the same primitives as LXC from the Linux Containers project (which was the predecessor of the modern container frameworks) and Docker, based upon namespaces (hence the <strong class="source-inline">n</strong> in <strong class="source-inline">nspawn</strong>). </p>
			<p class="callout-heading">Informational note</p>
			<p class="callout"><strong class="bold">The Linux Containers project</strong> has a <a id="_idIndexMarker762" class="pcalibre calibre6 pcalibre1"/>product called <strong class="bold">LXC</strong> that <a id="_idIndexMarker763" class="pcalibre calibre6 pcalibre1"/>combines several isolation and resource management services within the Linux kernel, such as <strong class="bold">control groups</strong> (<strong class="bold">cgroups</strong>) and namespace isolation. cgroups allow for capping or throttling <a id="_idIndexMarker764" class="pcalibre calibre6 pcalibre1"/>resource consumption in the CPU, memory, and I/O, whereas namespaces allow for hiding information and limiting the view on system resources. Early versions of Docker were built upon LXC, although Docker has since embraced the Linux services itself directly without using LXC.</p>
			<p class="calibre3">SELinux-wise, the <a id="_idIndexMarker765" class="pcalibre calibre6 pcalibre1"/>software running inside the container might not have a correct view on the SELinux state (depending on the container configuration) as the container is isolated from the host itself. SELinux does not yet have namespace support to allow containers or other isolated processes to have their own SELinux view, so if a container has a view on the SELinux state, it should never be allowed to modify it.</p>
			<p class="calibre3">Now, unlike Docker, <strong class="source-inline">podman</strong>, and Kubernetes, which can use the sVirt approach we saw in <a href="B16276_09_Final_VK.xhtml#_idTextAnchor257" class="pcalibre calibre6 pcalibre1"><em class="italic">Chapter 9</em></a>, <em class="italic">Secure Virtualization</em>, the <strong class="source-inline">systemd-nspawn</strong> approach does not support this technology.</p>
			<p class="callout-heading">Informational note</p>
			<p class="callout">The <strong class="source-inline">systemd-nspawn</strong> command might not be installed by default. On CentOS, Debian, and related distributions, the package that provides this tool is called <strong class="source-inline">systemd-container</strong>. Other distributions such as Gentoo and Arch Linux have it installed as part of the default systemd installation.</p>
			<p class="calibre3">Let's see how <strong class="source-inline">systemd-nspawn</strong> works and what its SELinux support looks like.</p>
			<h2 id="_idParaDest-292" class="calibre10"><a id="_idTextAnchor296" class="pcalibre calibre6 pcalibre1"/>Initializing a systemd container</h2>
			<p class="calibre3">To <a id="_idIndexMarker766" class="pcalibre calibre6 pcalibre1"/>create a systemd container, we need to create a place on the filesystem where its files will be stored, and then call <strong class="source-inline">systemd-nspawn</strong> with the correct arguments. To prepare the filesystem, we can download prebuilt container images, or create one ourselves. Let's use the Jailkit software, as used in <a href="B16276_07_Final_VK.xhtml#_idTextAnchor216" class="pcalibre calibre6 pcalibre1"><em class="italic">Chapter 7</em></a>, <em class="italic">Configuring Application-Specific SELinux Controls</em>, and build a container from it:</p>
			<ol class="calibre18">
				<li class="calibre9">First, create the directory the container runtimes will be hosted in:<p class="source-code"><strong class="bold"># mkdir /srv/ctr</strong></p></li>
				<li class="calibre9">Edit the <strong class="source-inline">/etc/jailkit/jk_init.ini</strong> file and include the following section:<p class="source-code">[nginx]</p><p class="source-code">comment = nginx runtime</p><p class="source-code">paths = /usr/sbin/nginx, /etc/nginx, /var/log/nginx, /var/lib/nginx, /usr/share/nginx, /usr/lib64/nginx, /usr/lib64/perl5/vendor_perl</p><p class="source-code">users = root,nginx</p><p class="source-code">groups = root,nginx</p><p class="source-code">includesections = netbasics, uidbasics, perl</p><p class="calibre3">This section tells Jailkit what it should copy into the directory, and which users to support.</p></li>
				<li class="calibre9">Execute the <strong class="source-inline">jk_init</strong> command to populate the directory:<p class="source-code"><strong class="bold"># jk_init -v -j /srv/ctr/nginx nginx</strong></p></li>
				<li class="calibre9">Finally, start the container using <strong class="source-inline">systemd-nspawn</strong>:<p class="source-code"><strong class="bold"># systemd-nspawn -D /srv/ctr/nginx /usr/sbin/nginx \</strong></p><p class="source-code"><strong class="bold">  -g "daemon off;"</strong></p></li>
			</ol>
			<p class="calibre3">As Nginx will by default attempt to run as a daemon, the container would immediately stop as it no longer has an active process. By launching with the <strong class="source-inline">daemon off</strong> option, <strong class="source-inline">nginx</strong> will remain in the foreground, and the container can continue to work.</p>
			<h2 id="_idParaDest-293" class="calibre10"><a id="_idTextAnchor297" class="pcalibre calibre6 pcalibre1"/>Using a specific SELinux context</h2>
			<p class="calibre3">When we launch a<a id="_idIndexMarker767" class="pcalibre calibre6 pcalibre1"/> container directly, this container will run with the SELinux context of the user. We can, however, pass on the target context for the container using command-line arguments:</p>
			<ul class="calibre8">
				<li class="calibre9">The <strong class="source-inline">--selinux-context=</strong> option (<strong class="source-inline">-Z</strong> for short) allows the administrator to define the SELinux context for the runtime processes of the container.</li>
				<li class="calibre9">The <strong class="source-inline">--selinux-apifs-context=</strong> option (<strong class="source-inline">-L</strong> for short) allows the administrator to define the SELinux context for the files and filesystem of the container.</li>
			</ul>
			<p class="calibre3">The SELinux types that can be used here, however, need to be carefully selected. The processes running inside a container cannot perform any type of transitions, so regular SELinux domains are often not feasible to use. Taking our Nginx example again, the <strong class="source-inline">httpd_t</strong> domain cannot be used for this container.</p>
			<p class="calibre3">We can use the SELinux types that the distribution provides for container workloads. Recent CentOS versions will use a domain such as <strong class="source-inline">container_t</strong> (which was previously known as <strong class="source-inline">svirt_lxc_net_t</strong>) and a file-oriented SELinux type, <strong class="source-inline">container_file_t</strong>. While this domain does not hold all possible privileges needed for any container, it provides a good baseline for containers.</p>
			<p class="calibre3">Let's use this type for our container:</p>
			<ol class="calibre18">
				<li value="1" class="calibre9">First, we need to extend the <strong class="source-inline">container_t</strong> privileges with some additional rights for the <strong class="source-inline">nginx</strong> daemon. Create a CIL policy file with the following content:<p class="source-code">(typeattributeset cil_gen_require container_t)</p><p class="source-code">(typeattributeset cil_gen_require container_file_t)</p><p class="source-code">(typeattributeset cil_gen_require http_port_t)</p><p class="source-code">(typeattributeset cil_gen_require node_t)</p><p class="source-code">(allow container_t container_file_t (chr_file (read open getattr ioctl write)))</p><p class="source-code">(allow container_t self (tcp_socket (create setopt bind listen accept read write)))</p><p class="source-code">(allow container_t http_port_t (tcp_socket (name_bind)))</p><p class="source-code">(allow container_t node_t (tcp_socket (node_bind)))</p><p class="source-code">(allow container_t self (capability (net_bind_service setgid setuid)))</p></li>
				<li class="calibre9">Load this file <a id="_idIndexMarker768" class="pcalibre calibre6 pcalibre1"/>as a new SELinux module:<p class="source-code"><strong class="bold"># semodule -i custom_container.cil</strong></p></li>
				<li class="calibre9">Relabel the files of the container with the <strong class="source-inline">container_file_t</strong> SELinux type:<p class="source-code"><strong class="bold"># chcon -R -t container_file_t /srv/ctr/nginx</strong></p></li>
				<li class="calibre9">Launch the container with the appropriate labels:<p class="source-code"><strong class="bold"># systemd-nspawn -D /srv/ctr/nginx \</strong></p><p class="source-code"><strong class="bold">-Z system_u:system_r:container_t:s0 \</strong></p><p class="source-code"><strong class="bold">-L system_u:object_r:container_file_t:s0 \</strong></p><p class="source-code"><strong class="bold">/usr/sbin/nginx -g "daemon off;"</strong></p></li>
			</ol>
			<p class="calibre3">Whenever a container is launched, it remains attached to the current session. We can of course create service files that launch the containers in the background, or use session management services such as <strong class="source-inline">screen</strong> or <strong class="source-inline">tmux</strong>. A more user-friendly approach, however, is to use <strong class="source-inline">machinectl</strong>.</p>
			<h2 id="_idParaDest-294" class="calibre10"><a id="_idTextAnchor298" class="pcalibre calibre6 pcalibre1"/>Facilitating container management with machinectl</h2>
			<p class="calibre3">The <strong class="source-inline">machinectl</strong> command allows administrators to manage containers or even virtual<a id="_idIndexMarker769" class="pcalibre calibre6 pcalibre1"/> machines more easily through <a id="_idIndexMarker770" class="pcalibre calibre6 pcalibre1"/>systemd. For containers, <strong class="source-inline">machinectl</strong> will use <strong class="source-inline">systemd-nspawn</strong>.</p>
			<p class="calibre3">Let's use this <strong class="source-inline">machinectl</strong> command to download, start, and stop a container:</p>
			<ol class="calibre18">
				<li value="1" class="calibre9">First, download a ready-to-go container image with the <strong class="source-inline">pull-tar</strong> argument and prepare it on the system:<p class="source-code"><strong class="bold"># machinectl pull-tar https://nspawn.org/storage/archlinux/archlinux/tar/image.tar.xz archlinux</strong></p><p class="calibre3">We can also download the archive manually, and then import it using <strong class="source-inline">machinectl import-tar</strong>:</p><p class="source-code"><strong class="bold"># machinectl import-tar archlinux.tar.xz</strong></p></li>
				<li class="calibre9">List the available images with the <strong class="source-inline">list-images</strong> argument:<p class="source-code"><strong class="bold"># machinectl list-images</strong></p></li>
				<li class="calibre9">We can now clone this image and launch the container:<p class="source-code"><strong class="bold"># machinectl clone archlinux test</strong></p><p class="source-code"><strong class="bold"># machinectl start test</strong></p></li>
				<li class="calibre9">To access the container environment, use the <strong class="source-inline">shell</strong> argument:<p class="source-code"><strong class="bold"># machinectl shell test</strong></p></li>
				<li class="calibre9">We can shut down the container using the <strong class="source-inline">poweroff</strong> argument:<p class="source-code"><strong class="bold"># machinectl poweroff test</strong></p></li>
			</ol>
			<p class="calibre3">When <a id="_idIndexMarker771" class="pcalibre calibre6 pcalibre1"/>we use <strong class="source-inline">machinectl</strong>, the<a id="_idIndexMarker772" class="pcalibre calibre6 pcalibre1"/> containers will run in the <strong class="source-inline">unconfined_service_t</strong> SELinux domain. There is currently no way to override this. Luckily, we have other tools available to facilitate container management that do have more significant built-in SELinux support, such as Docker and <strong class="source-inline">podman</strong>.</p>
			<h1 id="_idParaDest-295" class="calibre5"><a id="_idTextAnchor299" class="pcalibre calibre6 pcalibre1"/>Configuring podman</h1>
			<p class="calibre3">The <strong class="source-inline">podman</strong> utility is <a id="_idIndexMarker773" class="pcalibre calibre6 pcalibre1"/>the default container management utility on CentOS 8 and other distributions derived from Red Hat Enterprise Linux. Other distributions such as Gentoo can also easily get access to <strong class="source-inline">podman</strong> by installing <strong class="source-inline">libpod</strong>.</p>
			<h2 id="_idParaDest-296" class="calibre10"><a id="_idTextAnchor300" class="pcalibre calibre6 pcalibre1"/>Selecting podman over Docker</h2>
			<p class="calibre3">When we <a id="_idIndexMarker774" class="pcalibre calibre6 pcalibre1"/>compare <strong class="source-inline">podman</strong> with Docker, we might not see a big difference when we are simply using it for basic container management operations. The commands are very similar, and <strong class="source-inline">podman</strong> even has a Docker compatibility layer that facilitates the usage of <strong class="source-inline">podman</strong> for administrators who are used to working with Docker.</p>
			<p class="calibre3">Under the hood though, there are quite a few differences. For one, <strong class="source-inline">podman</strong> is a daemon-less container management system, which allows end users to easily run containers within their confined space. The <strong class="source-inline">libpod</strong> project also uses different design principles and supports a different <a id="_idIndexMarker775" class="pcalibre calibre6 pcalibre1"/>container runtime, which<a id="_idIndexMarker776" class="pcalibre calibre6 pcalibre1"/> supports the <strong class="bold">Open Container Initiative</strong> (<strong class="bold">OCI</strong>)-based definitions, called the <strong class="bold">Container Runtime Interface for OCI</strong> (<strong class="bold">CRI-O</strong>).</p>
			<p class="calibre3">Let's use <strong class="source-inline">podman</strong> to deploy a PostgreSQL container on the system:</p>
			<ol class="calibre18">
				<li value="1" class="calibre9">First, we need to find the appropriate container. We can use the <strong class="source-inline">podman search</strong> command for this:<p class="source-code"><strong class="bold"># podman search postgresql</strong></p></li>
				<li class="calibre9">Of the various PostgreSQL containers listed, we pick the Bitnami one:<p class="source-code"><strong class="bold"># podman pull docker.io/bitnami/postgresql</strong></p><p class="calibre3">This command will download the container base image to the system, storing the files in <strong class="source-inline">/var/lib/containers/storage</strong>. </p></li>
				<li class="calibre9">We can now launch a container, assign a password to the PostgreSQL superuser (<strong class="source-inline">postgres</strong>), and make sure that the PostgreSQL port (<strong class="source-inline">5432</strong>) is made available to the system:<p class="source-code"><strong class="bold"># podman run -dit --name postgresql-test \</strong></p><p class="source-code"><strong class="bold">  -e POSTGRESQL_PASSWORD="pgsqlpass" \</strong></p><p class="source-code"><strong class="bold">  -p 5432:5432 postgresql</strong></p><p class="calibre3">This command will create a container definition, based on the container base we've just downloaded, and start it on the system.</p></li>
				<li class="calibre9">We can use <strong class="source-inline">psql</strong> to validate that the database runs:<p class="source-code"><strong class="bold"># psql -U postgres -h localhost</strong></p></li>
				<li class="calibre9">When we're done with the container, we can stop it (using <strong class="source-inline">podman stop</strong>), which keeps the current container information, allowing us to revive it again later (using <strong class="source-inline">podman start</strong>) or remove the container from the system completely:<p class="source-code"><strong class="bold"># podman rm postgresql-test</strong></p></li>
				<li class="calibre9">Removing the container removes the container runtime, but the base container image remains on the system:<p class="source-code"><strong class="bold"># podman images</strong></p><p class="calibre3">This allows<a id="_idIndexMarker777" class="pcalibre calibre6 pcalibre1"/> us to quickly start another container without having to download the files again.</p></li>
			</ol>
			<p class="calibre3">Using containers is a fast and effective way to quickly install and deploy software on the system. Additionally, SELinux provides some additional protections to make sure that these containers do not misbehave.</p>
			<h2 id="_idParaDest-297" class="calibre10"><a id="_idTextAnchor301" class="pcalibre calibre6 pcalibre1"/>Using containers with SELinux</h2>
			<p class="calibre3">When<a id="_idIndexMarker778" class="pcalibre calibre6 pcalibre1"/> we look at the active runtime, we will notice that <a id="_idIndexMarker779" class="pcalibre calibre6 pcalibre1"/>SELinux is already confining these containers in a way we understand:</p>
			<p class="source-code"># ps -efZ | grep postgres</p>
			<p class="source-code">system_u:system_r:container_t:s0:c182,c609 ... /opt/bitnami/postgres -D ...</p>
			<p class="calibre3">The running processes have two categories assigned and are executing in the <strong class="source-inline">container_t</strong> SELinux domain. This is the sVirt approach we saw in <a href="B16276_09_Final_VK.xhtml#_idTextAnchor257" class="pcalibre calibre6 pcalibre1"><em class="italic">Chapter 9</em></a>, <em class="italic">Secure Virtualization</em>. Unlike virtual machines though, containers are often used in a more transient way: when a new version of the container base is released, the containers are scrapped, and new ones are started. Virtual machines often undergo in-system upgrades, and thus have a longer lifespan.</p>
			<p class="calibre3">The transient approach with containers also means that we need to provide data persistence in a different way. The approach that most containers use is to allow mapping locations from the host into the container environment. </p>
			<p class="calibre3">Let's use <strong class="source-inline">podman</strong> to map a location<a id="_idIndexMarker780" class="pcalibre calibre6 pcalibre1"/> outside the container to the <strong class="source-inline">/bitnami/postgresql</strong> location inside the container, as needed by the PostgreSQL container:</p>
			<ol class="calibre18">
				<li value="1" class="calibre9">First, create the location where we want to store the PostgreSQL data(base) on the host:<p class="source-code"><strong class="bold"># mkdir -p /srv/db/postgresql-test</strong></p></li>
				<li class="calibre9">Next, change the ownership of this location to the user with user ID <strong class="source-inline">1001</strong> (the user ID that the container uses internally):<p class="source-code"><strong class="bold"># chown -R 1001:1001 /srv/db/postgresql-test</strong></p></li>
				<li class="calibre9">Now start the<a id="_idIndexMarker781" class="pcalibre calibre6 pcalibre1"/> container, creating a mapping from this location to the container:<p class="source-code"><strong class="bold"># podman run -dit --name postgresql-test \</strong></p><p class="source-code"><strong class="bold">  -e POSTGRESQL_PASSWORD="pgsqlpass" \</strong></p><p class="source-code"><strong class="bold">  -v /srv/db/postgresql-test:/bitnami/postgresql:Z \</strong></p><p class="source-code"><strong class="bold">  -p 5432:5432 postgresql</strong></p><p class="calibre3">This will have the PostgreSQL data stored in <strong class="source-inline">/srv/db/postgresql-test</strong>. If we later delete the container and create a new one (for instance, because an update for the container base has been made available), the database itself is not affected.</p></li>
			</ol>
			<p class="calibre3">The mapping itself contains an SELinux-specific variable, namely the trailing <strong class="source-inline">:Z</strong>. If we were to omit this from the mapping, then the location would still be made accessible inside the container. However, the PostgreSQL runtime would not be able to use it.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The use of <strong class="source-inline">:Z</strong> in directory mappings (or <strong class="bold">volume mounts</strong> as they are also often called) is the most frequently forgotten option that system administrators are confronted with. Whenever SELinux is active on the system and the container runtime uses sVirt, you are more likely to need <strong class="source-inline">:Z</strong> (or <strong class="source-inline">:z</strong>, as we will see shortly) than not!</p>
			<p class="calibre3">Containers are still part of the host operating system. When we create the <strong class="source-inline">/srv/db/postgresql-test</strong> location, it will receive the <strong class="source-inline">var_t</strong> SELinux type by default. Containers that want to use this location would require write privileges to <strong class="source-inline">var_t</strong>. However, this privilege is not one we want to provide. After all, the containers should be isolated as much as <a id="_idIndexMarker782" class="pcalibre calibre6 pcalibre1"/>possible from the host—this isolation is what the<a id="_idIndexMarker783" class="pcalibre calibre6 pcalibre1"/> sVirt technology is about after all.</p>
			<p class="calibre3">Hence, we need to relabel this location accordingly. The SELinux type to use for generic containers is <strong class="source-inline">container_file_t</strong>. Moreover, we want to make sure that only the right container can access this location. Restricting and isolating access is what the <strong class="source-inline">:Z</strong> (with a capitalized <strong class="source-inline">Z</strong>) does in the command: labeling the directory with the <strong class="source-inline">container_file_t</strong> type and associating the right categories with it.</p>
			<p class="calibre3">If we want to have a location accessible by <em class="italic">multiple containers</em>, we can tell <strong class="source-inline">podman</strong> to <em class="italic">share</em> the location, yet still be labeled with the <strong class="source-inline">container_file_t</strong> SELinux type. To accomplish that, we would use the <strong class="source-inline">:z</strong> argument (with a lowercase <strong class="source-inline">z</strong>), like so:</p>
			<p class="source-code"># podman run -dit --name postgresql-test \</p>
			<p class="source-code"> -e POSTGRESQL_PASSWORD="pgsqlpass" \</p>
			<p class="source-code"> -v /srv/db/postgresql-test:/bitnami/postgresql:z \</p>
			<p class="source-code"> -p 5432:5432 postgresql</p>
			<p class="calibre3">Creating appropriate mappings is not the only approach where SELinux configuration comes into play. If we want, we can also tell <strong class="source-inline">podman</strong> to use different SELinux domains for the container as well.</p>
			<h2 id="_idParaDest-298" class="calibre10"><a id="_idTextAnchor302" class="pcalibre calibre6 pcalibre1"/>Changing a container's SELinux domain</h2>
			<p class="calibre3">To control the SELinux <a id="_idIndexMarker784" class="pcalibre calibre6 pcalibre1"/>context under which a container is launched, we<a id="_idIndexMarker785" class="pcalibre calibre6 pcalibre1"/> use the <strong class="source-inline">--security-opt</strong> argument to the <strong class="source-inline">podman</strong> command. For instance, to run an Nginx container with the <strong class="source-inline">container_logreader_t</strong> SELinux domain, we use the following:</p>
			<p class="source-code"># podman run -dit --name nginx-test -p 80:80 \</p>
			<p class="source-code">  --security-opt label=type:container_logreader_t nginx</p>
			<p class="calibre3">This domain is slightly more privileged than the default <strong class="source-inline">container_t</strong> domain, as it also has read privileges on log files. We could use this to have a web server expose the log files, for instance.</p>
			<p class="calibre3">Other labeling options that we can pass on are as follows:</p>
			<ul class="calibre8">
				<li class="calibre9">The SELinux user, with the <strong class="source-inline">label=user:&lt;SELinux user&gt;</strong> argument.</li>
				<li class="calibre9">The SELinux role, with the <strong class="source-inline">label=role:&lt;SELinux role&gt;</strong> argument.</li>
				<li class="calibre9">The SELinux sensitivity level, with the <strong class="source-inline">label=level:&lt;SELinux level&gt;</strong> argument.</li>
				<li class="calibre9">The SELinux type for the files, with the <strong class="source-inline">label=filetype:&lt;SELinux type&gt;</strong> argument. This sets the SELinux context for the location mappings that have the <strong class="source-inline">:Z</strong> and <strong class="source-inline">:z</strong> suffixes set. The selected type must be an entry point for the container's SELinux domain.</li>
			</ul>
			<p class="calibre3">There is also another <a id="_idIndexMarker786" class="pcalibre calibre6 pcalibre1"/>option that we can use, namely <strong class="source-inline">label=disable</strong>. With<a id="_idIndexMarker787" class="pcalibre calibre6 pcalibre1"/> this argument set, a container will run without any SELinux isolation. Now, it does not disable SELinux for the container, but associates an unconfined domain called <strong class="source-inline">spc_t</strong> with the container:</p>
			<p class="source-code"># podman run -dit --name nginx-test -p 80:80 \</p>
			<p class="source-code">  --security-opt label=disable \</p>
			<p class="source-code">  -v /srv/web/localhost:/usr/share/nginx/html nginx</p>
			<p class="source-code"># ps -efZ | grep nginx</p>
			<p class="source-code">unconfined_u:system_r:spc_t:s0 ... nginx: worker process</p>
			<p class="calibre3">While for most use cases, the default <strong class="source-inline">container_t</strong> domain is sufficiently privileged, it might be too privileged for some. Luckily, we can easily create new SELinux domains specific to our use case.</p>
			<h2 id="_idParaDest-299" class="calibre10"><a id="_idTextAnchor303" class="pcalibre calibre6 pcalibre1"/>Creating custom domains with udica</h2>
			<p class="calibre3">The <strong class="source-inline">container_t</strong> domain is configured to be widely reusable, which implies that it has many privileges <a id="_idIndexMarker788" class="pcalibre calibre6 pcalibre1"/>for common use cases, which you might not <a id="_idIndexMarker789" class="pcalibre calibre6 pcalibre1"/>want to give to each container. Furthermore, if we would launch a container but need to associate more privileges with it, then we would have to extend <strong class="source-inline">container_t</strong> with more privileges, resulting in all containers receiving this privilege extension.</p>
			<p class="calibre3">To quickly build up new policies, a tool called <strong class="source-inline">udica</strong> can be used. The <strong class="source-inline">udica</strong> tool reads the container definition and creates a custom SELinux policy from it. We can then use this custom policy for this particular container, allowing other containers to remain untouched.</p>
			<p class="calibre3">Let's use this for a Jupyter <a id="_idIndexMarker790" class="pcalibre calibre6 pcalibre1"/>Notebook, which we want to grant read/write <a id="_idIndexMarker791" class="pcalibre calibre6 pcalibre1"/>privileges to a (shared) user home directory location:</p>
			<ol class="calibre18">
				<li value="1" class="calibre9">First, we create the definition of the container:<p class="source-code"><strong class="bold"># podman run -dit --name notebook-test -p 8888:8888 \</strong></p><p class="source-code"><strong class="bold">  -v /home/lisa/work:/home/jovyan/shared scipy-notebook</strong></p></li>
				<li class="calibre9">Next, inspect this container using <strong class="source-inline">podman inspect</strong> and store the results in a file:<p class="source-code"><strong class="bold"># podman inspect notebook-test &gt; notebook-test.json</strong></p></li>
				<li class="calibre9">Use <strong class="source-inline">udica</strong> to generate an SELinux policy for it:<p class="source-code"><strong class="bold"># udica -j notebook-test.json custom-notebook-test</strong></p><p class="source-code"><strong class="bold">Policy custom-notebook-test created!</strong></p><p class="source-code"><strong class="bold">Please load these modules using:</strong></p><p class="source-code"><strong class="bold"># semodule -i custom-notebook-test.cil /usr/share/udica/</strong><strong class="bold">templates/{base_container.cil,net_container.cil}</strong></p><p class="source-code"><strong class="bold">Restart the container with: "--security-opt </strong><strong class="bold">label=type:custom-notebook-test.process" parameter</strong></p></li>
				<li class="calibre9">Load the custom policy as mentioned by the <strong class="source-inline">udica</strong> output:<p class="source-code"><strong class="bold"># semodule -i custom-notebook-test.cil \</strong></p><p class="source-code"><strong class="bold">    /usr/share/udica/templates/base_container.cil \</strong></p><p class="source-code"><strong class="bold">    /usr/share/udica/templates/net_container.cil</strong></p></li>
				<li class="calibre9">Stop and remove the container, and then recreate it with the parameter as mentioned in the <strong class="source-inline">udica</strong> output:<p class="source-code"><strong class="bold"># podman stop notebook-test</strong></p><p class="source-code"><strong class="bold"># podman rm notebook-test</strong></p><p class="source-code"><strong class="bold"># podman run -dit --name notebook-test -p 8888:8888 \</strong></p><p class="source-code"><strong class="bold">  -v /home/lisa/work:/home/jovyan/shared \</strong></p><p class="source-code"><strong class="bold">  --security-opt \</strong></p><p class="source-code"><strong class="bold">  label=type:custom-notebook-test.process scipy-notebook</strong></p></li>
			</ol>
			<p class="calibre3">The custom SELinux <a id="_idIndexMarker792" class="pcalibre calibre6 pcalibre1"/>policy has the privileges to write to the<a id="_idIndexMarker793" class="pcalibre calibre6 pcalibre1"/> home directory, as the container had a mapping from <strong class="source-inline">/home/lisa/work</strong>, and <strong class="source-inline">udica</strong> automatically created the permissions for it. If we wanted the container to only have read-only privileges, we could use a mapping with a trailing <strong class="source-inline">:ro</strong> (rather than <strong class="source-inline">:Z</strong> or <strong class="source-inline">:z</strong> for SELinux-specific changes). This would map the location inside the container with read-only access, and <strong class="source-inline">udica</strong> would only create read privileges for the associated SELinux type.</p>
			<p class="calibre3">If creating custom policies is a bit too specific, we can also fine-tune the privileges of the <strong class="source-inline">container_t</strong> domain with the appropriate SELinux booleans.</p>
			<h2 id="_idParaDest-300" class="calibre10"><a id="_idTextAnchor304" class="pcalibre calibre6 pcalibre1"/>Toggling container_t privileges with SELinux booleans</h2>
			<p class="calibre3">The <strong class="source-inline">container_t</strong> SELinux domain is <a id="_idIndexMarker794" class="pcalibre calibre6 pcalibre1"/>associated with <a id="_idIndexMarker795" class="pcalibre calibre6 pcalibre1"/>the <strong class="source-inline">svirt_sandbox_domain</strong> attribute, and through that association, will automatically be managed by several of the <strong class="source-inline">virt_*</strong> SELinux booleans that we saw in <a href="B16276_09_Final_VK.xhtml#_idTextAnchor257" class="pcalibre calibre6 pcalibre1"><em class="italic">Chapter 9</em></a>, <em class="italic">Secure Virtualization</em>.</p>
			<p class="calibre3">There are a few container-specific SELinux booleans as well:</p>
			<ul class="calibre8">
				<li class="calibre9">With <strong class="source-inline">container_use_cephfs</strong>, containers can use CephFS-based storage. This is predominantly used when the containers are managed by larger container-cluster software such as Kubernetes.</li>
				<li class="calibre9">With <strong class="source-inline">container_manage_cgroup</strong>, containers can manage cgroups. This is needed when the container hosts systemd inside, which is often the case for full-blown container runtimes (rather than process-specific containers). Such containers host almost complete Linux systems.</li>
				<li class="calibre9">With <strong class="source-inline">container_connect_any</strong>, the <strong class="source-inline">container_t</strong> SELinux domain can connect to any TCP port.</li>
			</ul>
			<p class="calibre3">Keep in mind though <a id="_idIndexMarker796" class="pcalibre calibre6 pcalibre1"/>that these booleans influence the <a id="_idIndexMarker797" class="pcalibre calibre6 pcalibre1"/>privileges of the <strong class="source-inline">container_t</strong> domain, and thus are in effect for all containers.</p>
			<h2 id="_idParaDest-301" class="calibre10"><a id="_idTextAnchor305" class="pcalibre calibre6 pcalibre1"/>Tuning the container hosting environment</h2>
			<p class="calibre3">The <strong class="source-inline">podman</strong> utility will by <a id="_idIndexMarker798" class="pcalibre calibre6 pcalibre1"/>default store its container volumes and base images in <strong class="source-inline">/var/lib/containers</strong>. Administrators can add more locations through the <strong class="source-inline">storage.conf</strong> configuration file available in <strong class="source-inline">/etc/containers</strong>. However, you need to adjust the SELinux configuration accordingly as well.</p>
			<p class="calibre3">Suppose that the <strong class="source-inline">/srv/containers</strong> location will be used, then we need to create an equivalence rule to make sure that this location is labeled appropriately:</p>
			<p class="source-code"># semanage fcontext -a -e /var/lib/containers \</p>
			<p class="source-code">  /srv/containers</p>
			<p class="source-code"># restorecon -R -v /srv/containers</p>
			<p class="calibre3">If the location is a network mount, you might need to change the appropriate SELinux booleans as well.</p>
			<h1 id="_idParaDest-302" class="calibre5"><a id="_idTextAnchor306" class="pcalibre calibre6 pcalibre1"/>Leveraging Kubernetes' SELinux support</h1>
			<p class="calibre3">When containers are used in a<a id="_idIndexMarker799" class="pcalibre calibre6 pcalibre1"/> larger environment, they are often managed through container orchestration frameworks that allow scaling container deployment and management across multiple systems. Kubernetes is a popular container orchestration framework with a good community, as well as commercial support.</p>
			<p class="calibre3">Kubernetes uses the container software found on the machines under the hood. When, for instance, we install Kubernetes on Fedora's CoreOS, it will detect that Docker is available and use the Docker engine for managing the containers.</p>
			<h2 id="_idParaDest-303" class="calibre10"><a id="_idTextAnchor307" class="pcalibre calibre6 pcalibre1"/>Configuring Kubernetes with SELinux support</h2>
			<p class="calibre3">Installing <a id="_idIndexMarker800" class="pcalibre calibre6 pcalibre1"/>Kubernetes can be a daunting task, and several <a id="_idIndexMarker801" class="pcalibre calibre6 pcalibre1"/>methods exist, ranging from single-node playground deployments up to commercially supported installations. One of the well-documented installation methods on the Kubernetes website is to use <strong class="source-inline">kubeadm</strong> for bootstrapping Kubernetes clusters.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The installation of Kubernetes is documented on the Kubernetes website at <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm" class="pcalibre calibre6 pcalibre1">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm</a>. In this section, we will not go through the individual steps to set up a working Kubernetes instance, but give pointers as to which changes are needed for having proper SELinux support.</p>
			<p class="calibre3">The <strong class="source-inline">kubeadm</strong> command, when initializing the Kubernetes cluster, will download and run the various Kubernetes services as containers. Unfortunately, Kubernetes' services use several mappings from the host system into the container to facilitate their operations. These mappings are not done using the <strong class="source-inline">:Z</strong> or <strong class="source-inline">:z</strong> options—it would even be wrong to do so, as the locations are system-wide locations that should retain their current SELinux labels.</p>
			<p class="calibre3">As a result, Kubernetes' services will be running with the default <strong class="source-inline">container_t</strong> SELinux domain (as Docker will happily apply the sVirt protections), which does not have access to these locations. The most obvious change we can apply is to have the services run with the highly privileged <strong class="source-inline">spc_t</strong> domain for now. Applying this change however during the installation is hard, as we would need to change the domain sufficiently quickly before the installation fails.</p>
			<p class="calibre3">While we can create deployment configuration information for Kubernetes that immediately configures the services with <strong class="source-inline">spc_t</strong>, another method can be pursued:</p>
			<ol class="calibre18">
				<li value="1" class="calibre9">Mark the <strong class="source-inline">container_t</strong> type as a permissive domain before the installation starts. While this will prevent any SELinux controls on the container, we can argue that the installation of Kubernetes is done in a contained and supervised manner:<p class="source-code"><strong class="bold"># semanage permissive -a container_t</strong></p></li>
				<li class="calibre9">Run <strong class="source-inline">kubeadm init</strong>, which will install the services on the system:<p class="source-code"><strong class="bold"># kubeadm init</strong></p></li>
				<li class="calibre9">When the services are installed, go to <strong class="source-inline">/etc/kubernetes/manifests</strong>. Inside this directory, you will find four manifests, each one representing a Kubernetes service:<p class="source-code"><strong class="bold"># cd /etc/kubernetes/manifests</strong></p></li>
				<li class="calibre9">Edit each<a id="_idIndexMarker802" class="pcalibre calibre6 pcalibre1"/> manifest file (<strong class="source-inline">etcd.yaml</strong>, <strong class="source-inline">kube-apiserver.yml</strong>, <strong class="source-inline">kube-controller-manager.yml</strong>, and <strong class="source-inline">kube-scheduler.yml</strong>) and add a security context definition that<a id="_idIndexMarker803" class="pcalibre calibre6 pcalibre1"/> configures the service to run with the <strong class="source-inline">spc_t</strong> domain. This is done as a configuration directive under the <strong class="source-inline">containers</strong> section:<p class="source-code">apiVersion: v1</p><p class="source-code">kind: Pod</p><p class="source-code">metadata:</p><p class="source-code">  name: etcd</p><p class="source-code">spec:</p><p class="source-code">  containers:</p><p class="source-code">  - command: …</p><p class="source-code"><strong class="bold">  securityContext:</strong></p><p class="source-code"><strong class="bold">    seLinuxOptions:</strong></p><p class="source-code"><strong class="bold">      type: spc_t</strong></p><p class="source-code">    image: k8s.gcr.io/etcd:3.4.3-0</p><p class="source-code">    …</p></li>
				<li class="calibre9">During the Kubernetes installation, the <strong class="source-inline">kubelet</strong> service will be installed, which will detect that these files have been changed, and will automatically restart the containers. If not, you can shut down and remove the container definitions within Docker, and <strong class="source-inline">kubelet</strong> will automatically recreate them:<p class="source-code"><strong class="bold"># docker ps</strong></p><p class="source-code"><strong class="bold">CONTAINER ID      ...     NAMES</strong></p><p class="source-code"><strong class="bold">548f0c3ed18e          k8s_POD_etcd-ppubssa3ed_kube…</strong></p><p class="source-code"><strong class="bold">b7b1df2d0027          k8s_POD_kube-apiserver-…</strong></p><p class="source-code"><strong class="bold">eecd4d4ad108          k8s_POD_kube-scheduler-…</strong></p><p class="source-code"><strong class="bold">76da4910b927          k8s_POD_kube-controller-…</strong></p><p class="source-code"><strong class="bold"># for n in 548f0c3ed18e b7b1df2d0027 eecd4d4ad108 76da4910b927; do docker stop $n; docker rm $n; done</strong></p></li>
				<li class="calibre9">Verify that the services are now running with the privileged <strong class="source-inline">spc_t</strong> domain:<p class="source-code"><strong class="bold"># ps -ef | grep spc_t</strong></p></li>
				<li class="calibre9">Remove the<a id="_idIndexMarker804" class="pcalibre calibre6 pcalibre1"/> permissive state of <strong class="source-inline">container_t</strong> so that it<a id="_idIndexMarker805" class="pcalibre calibre6 pcalibre1"/> is back to enforcing mode:<p class="source-code"><strong class="bold"># semanage permissive -d container_t</strong></p></li>
			</ol>
			<p class="calibre3">With these slight adjustments during the installation, Kubernetes is now running fine with SELinux support enabled.</p>
			<h2 id="_idParaDest-304" class="calibre10"><a id="_idTextAnchor308" class="pcalibre calibre6 pcalibre1"/>Setting SELinux contexts for pods</h2>
			<p class="calibre3">Within<a id="_idIndexMarker806" class="pcalibre calibre6 pcalibre1"/> Kubernetes, containers are part of pods. A <strong class="bold">pod</strong> is a <a id="_idIndexMarker807" class="pcalibre calibre6 pcalibre1"/>group of containers that all see the same resources and can interact with each other seamlessly. Previously, in the <em class="italic">Configuring podman</em> section, we worked on the container level. The <strong class="source-inline">podman</strong> utility is also able to use the pods concept (hence the name). For instance, we could put the Nginx container in a pod called <strong class="source-inline">webserver</strong> like so:</p>
			<p class="source-code"># podman pod create -p 80:80 --name webserver</p>
			<p class="source-code"># podman pull docker.io/library/nginx</p>
			<p class="source-code"># podman run -dit --pod webserver --name nginx-test nginx</p>
			<p class="calibre3">Unlike <strong class="source-inline">podman</strong>, Kubernetes does not rely on command-line interaction to create and manage resources such as pods. Instead, it uses manifest files (as we've briefly touched upon in the <em class="italic">Configuring Kubernetes with SELinux support</em> section). Kubernetes administrators or DevOps teams will create manifest files and apply those to the environment.</p>
			<p class="calibre3">For instance, to have the Nginx containers run on Kubernetes, the following manifest could be used:</p>
			<p class="source-code">apiVersion: apps/v1</p>
			<p class="source-code">kind: Deployment</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: nginx-test-deployment</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  selector:</p>
			<p class="source-code">    matchLabels:</p>
			<p class="source-code">      app: nginx-test</p>
			<p class="source-code">  replicas: 2</p>
			<p class="source-code">  template:</p>
			<p class="source-code">    metadata:</p>
			<p class="source-code">      labels:</p>
			<p class="source-code">        app: nginx-test</p>
			<p class="source-code">    spec:</p>
			<p class="source-code">      containers:</p>
			<p class="source-code">      - name: nginx-test</p>
			<p class="source-code">        image: nginx:latest</p>
			<p class="source-code">        ports:</p>
			<p class="source-code">        - containerPort: 80</p>
			<p class="calibre3">This manifest is a <a id="_idIndexMarker808" class="pcalibre calibre6 pcalibre1"/>Kubernetes deployment, and tells Kubernetes that we want to run<a id="_idIndexMarker809" class="pcalibre calibre6 pcalibre1"/> two Nginx containers. To apply this to the environment, use <strong class="source-inline">kubectl apply</strong>:</p>
			<p class="source-code">$ kubectl apply -f simple-nginx.yml</p>
			<p class="calibre3">As with the manifests for the Kubernetes services, we can tell Kubernetes to use a specific SELinux type:</p>
			<p class="source-code">…</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  containers: ...</p>
			<p class="source-code">  securityContext:</p>
			<p class="source-code">    seLinuxOptions:</p>
			<p class="source-code">      type: "container_logreader_t"</p>
			<p class="calibre3">The <strong class="source-inline">seLinuxOptions</strong> block can contain <strong class="source-inline">user</strong>, <strong class="source-inline">role</strong>, <strong class="source-inline">type</strong>, and <strong class="source-inline">level</strong> to define the SELinux user, SELinux role, SELinux type and SELinux sensitivity level.</p>
			<p class="calibre3">Unlike the regular container management services (such as Docker or CRI-O), Kubernetes does not allow changing SELinux labels on mapped volumes (except on single-node deployments): when we map volumes into containers, they retain their current SELinux label on the system. Hence, if you want to make sure that the resources are accessible from a regular <strong class="source-inline">container_t</strong> domain, you need to make sure these locations are labeled with <strong class="source-inline">container_file_t</strong>.</p>
			<p class="calibre3">Kubernetes does<a id="_idIndexMarker810" class="pcalibre calibre6 pcalibre1"/> offer advanced access controls itself. Enabling volumes within the containers is also handled by a plugin architecture, with several<a id="_idIndexMarker811" class="pcalibre calibre6 pcalibre1"/> plugins already available. When the plugin enables SELinux labeling, then Kubernetes will attempt to relabel the resource and assign the categories (as with sVirt). However, this support is currently only made available on single-node deployments (using the local host storage plugin)—and for such deployments, using <strong class="source-inline">podman</strong> is much simpler.</p>
			<h1 id="_idParaDest-305" class="calibre5"><a id="_idTextAnchor309" class="pcalibre calibre6 pcalibre1"/>Summary</h1>
			<p class="calibre3">Containerized workloads allow administrators to add capabilities quickly and easily to a system, while retaining possible dependencies within a container. Each container hosts its own dependencies, allowing containers to be removed and added from the system without affecting others. With SELinux, this workload is further isolated from the host and, in case of sVirt protections, also from each other.</p>
			<p class="calibre3">We've seen how systemd has container support but lacks sVirt-based protections, and how <strong class="source-inline">podman</strong> can apply sVirt protections on its own container environments. We learned that Docker and <strong class="source-inline">podman</strong> are very similar in usage, yet different under the hood. Both frameworks allow us to apply different SELinux types to the containers and resources, and with <strong class="source-inline">udica</strong> we've learned how to create custom policies without much development effort. Finally, we've seen how Kubernetes can be configured to use SELinux labeling as well.</p>
			<p class="calibre3">With all these SELinux-capable technologies behind us, we are ready to tackle the SELinux policy development itself. In the next chapter, we'll learn to work with SELinux policies in depth and tune the policies to our needs.</p>
			<h1 id="_idParaDest-306" class="calibre5"><a id="_idTextAnchor310" class="pcalibre calibre6 pcalibre1"/>Questions</h1>
			<ol class="calibre18">
				<li value="1" class="calibre9">Why is Docker or <strong class="source-inline">podman</strong> preferred over <strong class="source-inline">machinectl</strong> for SELinux?</li>
				<li class="calibre9">How do we ensure host data is properly mapped within a container?</li>
				<li class="calibre9">How can we create a custom policy from a container definition?</li>
				<li class="calibre9">Where in Kubernetes' manifests can we place SELinux settings?</li>
			</ol>
		</div>
	</div></body></html>