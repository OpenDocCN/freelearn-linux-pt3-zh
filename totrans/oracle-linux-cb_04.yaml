- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating and Managing Single-Instance Filesystems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Without data, there is no reason for a system to exist, and with that thought,
    the data has to live somewhere. In this chapter, we will cover the two most popular
    filesystems used to manage data that is local to the server: **B-Tree File System**
    (**Btrfs**, pronounced *Butter F S*) and **eXtended File System** (**XFS**, pronounced
    *X* *F S*).'
  prefs: []
  type: TYPE_NORMAL
- en: These are single-instance filesystems, which are basically filesystems that
    are only mounted on a single server at any one time. There are also multi-instance
    filesystems that are mounted on multiple systems at the same time. Common examples
    are **Oracle Clustered File System version 2** (**OCFS2**) and **Global File System
    2** (**GFS2**). All of these examples use shared block storage for the underlying
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there is **Ceph**, which is not an acronym, but instead a reference
    to **cephalopod**. This is because Ceph is a distributed architecture that stores
    data on all nodes of a Ceph cluster. This allows Ceph to offer scalable storage
    with some additional complexity. **Gluster** is another example of a distributed
    filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Why not **ZFS**? Because Btrfs and XFS are built into the Oracle Linux **Unbreakable
    Enterprise Kernel** (**UEK**), and ZFS is not available outside of third-party
    repositories.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following recipes that will help you understand and manage
    local filesystems:'
  prefs: []
  type: TYPE_NORMAL
- en: What you need to know about local filesystems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Btrfs – creating, resizing, and monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Btrfs – subvolumes, snapshots, quotas, and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protecting data with mdadm – a software RAID solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing with logical volume management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XFS – creating, modifying, and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this recipe, you will need an **Oracle Linux 8** system. As with most of
    these recipes, a VM on your desktop using a desktop virtualization product such
    as **Oracle VirtualBox** is recommended. A small VM with two cores, 2 GB RAM,
    and a few free gigabytes of disk space is fine. You will also need some additional
    disks assigned to the VM, ideally at least five equally sized disks. Ideally,
    before you start, patch your system to the latest packages available. This only
    takes a few minutes and can save a ton of time when troubleshooting issues caused
    by a bug.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the recipes in this book have their related configuration files available
    on GitHub, located at [https://github.com/PacktPublishing/Oracle-Linux-Cookbook](https://github.com/PacktPublishing/Oracle-Linux-Cookbook).
  prefs: []
  type: TYPE_NORMAL
- en: What you need to know about local filesystems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will discuss the differences between local and remote filesystems,
    as well as the core differences between Btrfs and ZFS.
  prefs: []
  type: TYPE_NORMAL
- en: The backbone of an **operating system** (**OS**) is the local filesystem. It
    enables efficient storage and management of files and directories on a computer
    or server using a hierarchical structure. This structure allows users and programs
    to easily create, modify, and access files on local storage devices such as hard
    disks, solid-state drives, and storage **logical unit numbers** (**LUNs**) from
    a local **storage area network** (**SAN**) or cloud provider. These filesystems
    are designed specifically for file and folder management efficiency, protecting
    files from accidental deletion or corruption. They come equipped with features
    such as file permissions, ownership, and access control, which provide users with
    utmost security and privacy. In comparison to remote filesystems, local filesystems
    offer superior performance, though files are not available on other systems unless
    paired with a remote filesystem technology. Notable examples of local Linux filesystems
    include Btrfs, XFS, ext4, fat32, and even ZFS.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: While ZFS is a local filesystem, it is not included in the kernel and needs
    to be added using software from [https://zfsonlinux.org](https://zfsonlinux.org).
  prefs: []
  type: TYPE_NORMAL
- en: A remote filesystem allows you to access files and directories on a remote server
    through a network. This system provides the convenience of accessing and manipulating
    files on a remote machine as if they were stored locally, eliminating the need
    to transfer them physically. Remote filesystems are widely used in distributed
    computing environments where multiple computers or servers need to share data
    and resources. They are also valuable in cloud computing and web hosting environments
    where data is stored remotely and accessed over the internet. However, it’s important
    to note that using remote filesystems can impact performance when sharing files
    between multiple servers over a network.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of remote filesystems include **Network File System** (**NFS**), **Server
    Message Block** (**SMB**), and **Common Internet File System** (**CIFS**), which
    are widely used in Unix, Linux, and Windows environments, respectively. Other
    popular remote filesystems include **s3fs**, which allows users to access files
    securely over cloud-based object storage.
  prefs: []
  type: TYPE_NORMAL
- en: For optimal performance when managing MySQL, Postgres, and Oracle databases,
    it’s highly recommended to utilize local filesystems instead of network filesystems.
    This strategy can also be effectively applied to the OS.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to understand the core differences between the two filesystems. The
    XFS filesystem is surprisingly much older than many admins realize. It started
    back in 1993, as the filesystem for the **Silicon Graphics IRIX** OS, and was
    ported over to Linux in 2001\. Btrfs is much newer, being developed in 2007 by
    Oracle (as an open source project) for Linux. Btrfs is also more than a filesystem,
    as it includes the volume manager, data redundancy, and filesystem functionality
    in one technology.
  prefs: []
  type: TYPE_NORMAL
- en: With XFS, you need to combine it with a logical volume manager for dynamic volumes
    and also `mdadm` command) to provide for fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Btrfs, you have the choice of five types of RAID volumes. What you pick
    is based on your use case as it’s a balance between performance, disk space required,
    and the usable capacity of the volume. The details are in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Description** | **Performance** | **Redundance** | **Capacity**
    |'
  prefs: []
  type: TYPE_TB
- en: '| RAID 0 | Striping across disks | Best | None | 100% |'
  prefs: []
  type: TYPE_TB
- en: '| RAID 1 | Mirror two disks | Good | 1 drive failure | 50% |'
  prefs: []
  type: TYPE_TB
- en: '| RAID 10 | Mirrored then striped, min of 4 disks | Almost the best | 1 drive
    failure | 50% |'
  prefs: []
  type: TYPE_TB
- en: '| RAID1C3 | 3 copies of the metadata, min of 3 disks | Average | 2 drive failures
    | 66% |'
  prefs: []
  type: TYPE_TB
- en: '| RAID1C4 | 4 copies of the metadata, min of 4 disks | Lowest | 3 drive failures
    | 75% |'
  prefs: []
  type: TYPE_TB
- en: Table 4.1 – Btrfs RAID options
  prefs: []
  type: TYPE_NORMAL
- en: Both systems have the same limitation for the maximum filesystem size of 8 exabytes!
    But Btrfs also adds features such as snapshots, transparent compression, integrated
    checksum-based data integrity, and rollback capabilities. XFS is not left out
    though, with higher performance through I/O threads and more bandwidth, though
    these advantages may not be realized once you integrate XFS with an LVM and RAID
    technology. One other major difference is that Btrfs requires that you use the
    UEK, though XFS works well with both UEKs and **Red Hat Compatible** **Kernels**
    (**RHCKs**).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Oracle Linux by default uses the XFS filesystem, but when doing the installation,
    you can use Btrfs as the root filesystem. If you want to use XFS as the boot filesystem,
    install it as you normally would. If you want to use Btrfs, then you should continue.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Your boot filesystem can be different from the data filesystems on the server.
    You can easily have the root use XFS and the data filesystem use Btrfs, or vice
    versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to run Btrfs is to pick it when doing an installation using
    the UEK boot disk. This will let you choose Btrfs as the destination filesystem.
    When running the install, select **Install Destination**, and then select a custom
    storage configuration. This will then give you the manual partitioning option
    where you can use the dropdown and select **Btrfs**, as seen in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Btrfs selection](img/B18349_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Btrfs selection
  prefs: []
  type: TYPE_NORMAL
- en: 'When setting up the custom configuration, you can add additional directories
    for `/var` or `/var/tmp` (a **secure technical implementation guide** (**STIG**)
    requirement). These are actually not just directories, but subvolumes of the main
    volume. This is why they show the same available space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Btrfs mountpoints](img/B18349_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Btrfs mountpoints
  prefs: []
  type: TYPE_NORMAL
- en: This is because with Btrfs, they all use the same volume and have the same usable
    disk space. We can limit this later using quotas.
  prefs: []
  type: TYPE_NORMAL
- en: Finish the install using your normal settings for network and software source,
    though as a note, you will likely need to set **Installation Source** as a URL
    or local network share. You will also need to add the UEK repository if running
    8.7 or earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Once the system is booted, you will see that it is now using Btrfs. This can
    be checked once the system is up.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can check the filesystem in several ways, the easiest being by checking
    `/etc/fstab` to see how the filesystem was mounted. This is seen in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Btrfs fstab example](img/B18349_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Btrfs fstab example
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use the `df` command with the `T` option to show the filesystem
    type. Running the following command will show you this info:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will show the filesystem type for each mounted file, as seen in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – df -T with Btrfs filesystems](img/B18349_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – df -T with Btrfs filesystems
  prefs: []
  type: TYPE_NORMAL
- en: Btrfs – creating, resizing, and monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will create a new RAIDed Btrfs volume and filesystem, using
    multiple disks for fault-tolerant storage. We will then add a new LUN, growing
    the filesystem. We will wrap up by modifying the filesystem to compress the data!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started, I added five 10 GB drives to the OS. These will be used to
    build a new RAID1C4 volume. I can see these new devices by using the `fdisk -l`
    command, grepping for `GiB` using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – fdisk output](img/B18349_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – fdisk output
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the 10 GB devices are `sdb`, `sdc`, `sbd`, `sbe`, and
    `sbf`. We will need this info to make the Btrfs volume.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we know the devices, let’s manually create a RAID1C3 volume. We will
    use all five devices in a RAID1C3 configuration and name the volume `data`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then use the following command to make the volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Please refer to the following figure to view the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – mkfs.btrfs output](img/B18349_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – mkfs.btrfs output
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When mounting a Btrfs volume, you normally use the first device in the volume
    or the UUID. The UUID is reported by `mkfs.btrfs` when the volume is created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s mount this in `/data`. Make the `/data` directory, then mount it
    with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Optional, though highly recommended, is to add this to the `fstab` file. With
    this example, we are using the UUID of the volume, and since data has no subvolume,
    the `subvol` parameter is defined but left blank:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Sample fstab using UUID](img/B18349_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Sample fstab using UUID
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have a mounted volume, let’s do a few things with it! First, we
    can use the `btrfs` command to check several things. The first is to check the
    device’s health, which is useful to see whether the RAID has any failing devices.
    The `btrfs stats /$DEVICE` command is used to show the status. Don’t forget to
    replace `$DEVICE` with the actual Btrfs device you are checking:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Healthy devices](img/B18349_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Healthy devices
  prefs: []
  type: TYPE_NORMAL
- en: When a device starts to fail, you should start to see errors in this report.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up, we will add a few more devices to the volume. Four more 10 GB disks
    were added: `sdg`, `sdh`, `sdi`, and `sdj`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we add the device, we can see that `/data` has 50 GB of usable raw space.
    This is seen using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the command is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Btrfs filesystem usage](img/B18349_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Btrfs filesystem usage
  prefs: []
  type: TYPE_NORMAL
- en: Here we can see the stats, mainly the 50 GB of free raw space, as well as the
    other metrics, including the space allocated to each device in the volume and
    which devices have the metadata.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s add the four new devices. This is done with the `btrfs device`
    `add` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Do this for each device being added to the volume, or bulk add them with `/dev/sd[a-z]`,
    replacing `a` and `z` with the appropriate range. When done, you can check using
    the usage option, as seen in the following sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Btrfs devices added](img/B18349_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Btrfs devices added
  prefs: []
  type: TYPE_NORMAL
- en: 'You will now see the device at 90 GB. Now, as space in `/data` gets consumed,
    you should start to see the available space go down, as well as the distribution
    of data against the individual disks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Btrfs space used](img/B18349_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Btrfs space used
  prefs: []
  type: TYPE_NORMAL
- en: 'For the last example, we will be removing a device to free up space, and then
    rebalancing the data. To delete a physical device, use the `btrfs device delete`
    command, passing the device mountpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This will remove the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Device removal](img/B18349_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Device removal
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the device is removed, rerun the usage report. What you will now see is
    the remaining devices, and which data is on which device:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Unbalanced usage](img/B18349_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Unbalanced usage
  prefs: []
  type: TYPE_NORMAL
- en: 'While it may appear to be a minor issue, it has the potential to cause complications
    down the line. Fortunately, the solution is simple – a system rebalance. This
    is done using the `balance` option in the `btrfs` command. The following command
    will be used to balance the `/``data` filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will then rebalance the data chunks, and when done, the usage
    will show the data balanced across the disks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Balanced usage](img/B18349_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Balanced usage
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To ensure a well-balanced distribution of data, it is recommended to always
    balance your system when adding or removing devices, even though there may be
    slight variations. This practice is essential for maintaining an optimally performing
    filesystem. The `btrfsmaintenance` package in the `ol8_developer` repo is a great
    tool for automating all the required Btrfs maintenance tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Btrfs – subvolumes, snapshots, quotas, and more
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Btrfs can do so much more than the older XFS technology. This includes subvolumes,
    snapshots, and quotas. Btrfs subvolumes are an exceptional tool that allows users
    to create multiple snapshots or subfilesystems within a single Btrfs filesystem.
    These subvolumes are displayed as distinct directories in the filesystem hierarchy,
    but they utilize the same storage space and can be managed independently.
  prefs: []
  type: TYPE_NORMAL
- en: The flexibility and versatility of subvolumes make them ideal for various purposes,
    such as creating backups or isolating different parts of the filesystem for easier
    management. Snapshots are particularly useful since they offer read-only copies
    of the filesystem at a specific point in time. With snapshots, users can restore
    files or entire subvolumes to a previous state or easily create replicable backups
    that can be moved to another system.
  prefs: []
  type: TYPE_NORMAL
- en: Subvolumes also enable users to manage disk space more efficiently. For example,
    users can create a subvolume for a specific application or project and restrict
    its disk usage to a certain amount to prevent it from using up too much space
    on the filesystem. Additionally, subvolumes can be used to implement access controls
    by assigning different permissions to different subvolumes or creating separate
    subvolumes for different users or groups. This recipe will go over how to do all
    of this.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will require a Btrfs filesystem, and will use the data filesystem
    created in the previous recipe for the examples.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a subvolume in `/data` and mount it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a quota on the subvolume.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a snapshot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable compression.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Creating a subvolume is straightforward and is done by using the `btrfs` command
    and specifying the full path to the subvolume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Once created, simply add it to `fstab`, this time declaring the subvolume name
    in the fourth column. This is seen in the following screenshot, where `/data/vol1`
    is mounted using subvolume `vol1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Subvolume in fstab](img/B18349_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – Subvolume in fstab
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the subvolume mounted, let’s add a quota to limit it to 5
    GB. To do this, we first need to enable quotas for the volume. This is done with
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to assign a quota-group limit to the subvolume. This will restrict
    the subvolume to the size defined. This is done using the `limit` option, as seen
    in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see what quotas are defined in a volume using the `btrfs qgroup show`
    command with the `-``reF` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The command and its output are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Set quotas](img/B18349_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Set quotas
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a quota set, let’s create a snapshot for backups. We do need
    a place for the backups, so let’s create a subvolume for the backups with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Btrfs snapshots are highly useful copies of a Btrfs filesystem, capturing a
    specific point in time. Through a seamless copy-on-write process, these snapshots
    separate any changes made to the filesystem from the snapshot itself. This makes
    them ideal for different purposes, such as creating backups, testing software
    configurations, and providing an effortless way to undo system updates. Additionally,
    backups can be created rapidly and with minimal storage space, as the snapshots
    only store the differences between the current state of the filesystem and the
    state at the moment the snapshot was taken. To create a Btrfs snapshot, you can
    use the `btrfs subvolume snapshot` command, specifying the subvolume you want
    to snapshot and the name and location of the snapshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This command creates a read-only snapshot of the `/data/vol1` subvolume and
    saves it as a separate subvolume in the `/``data/backup` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry if you’ve already taken a snapshot – you can easily revert the filesystem
    to its exact state at that time using the powerful Btrfs rollback feature. With
    Btrfs snapshot rollback, you can restore your Btrfs filesystem to a previous state
    by simply selecting a snapshot. Rolling back to a snapshot discards all changes
    made to the filesystem since the snapshot was taken and restores the filesystem
    to the exact state it was in when the snapshot was created.
  prefs: []
  type: TYPE_NORMAL
- en: 'To roll back a Btrfs snapshot, you can use the `btrfs subvolume snapshot` command
    with the `-r` option, which specifies that the snapshot should be used for a rollback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This command rolls back `/data/vol1` to the state it was in when the `/data/backup/vol1_backlup1`
    snapshot was created.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that rolling back a snapshot will discard any changes
    made to the filesystem since the snapshot was taken. However, this feature is
    extremely useful when you want to fully revert the filesystem to a previous state.
    Btrfs snapshots provide a simple and effective solution for data management and
    protection, allowing for effortless backup creation and easy restoration of prior
    filesystem versions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Automatic snapshots can be enabled so a `dnf` transaction will create a snapshot.
    This is done by the `dnf-plugin-snapper` tool. More information can be found here:
    [https://docs.oracle.com/en/operating-systems/oracle-linux/8/fsadmin/fsadmin-ManagingtheBtrfsFileSystem.html#snapper-btrfs](https://docs.oracle.com/en/operating-systems/oracle-linux/8/fsadmin/fsadmin-ManagingtheBtrfsFileSystem.html#snapper-btrfs).'
  prefs: []
  type: TYPE_NORMAL
- en: This section will cover how to enable compression using the Btrfs filesystem.
    Btrfs compression is a powerful feature that allows you to compress data in real
    time while writing it to the filesystem. This feature can significantly reduce
    storage space, making it ideal for filesystems that store vast amounts of data,
    such as media archives and backup systems. Additionally, it’s beneficial for systems
    with limited storage space, such as mobile devices and embedded systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Btrfs compression uses various compression algorithms to compress data while
    writing it to the filesystem. The compressed data is then stored on the disk and
    automatically decompressed on the fly when accessed. This process is seamless
    to the applications accessing the data, so there’s no need for them to be aware
    of the compression process as Btrfs handles it. Btrfs supports three compression
    algorithms, `zlib`, `lzo`, and `zstd`, each with its own strengths and weaknesses:'
  prefs: []
  type: TYPE_NORMAL
- en: '`zlib`: This is a widely used general-purpose compression algorithm that provides
    good compression ratios but can be relatively slow, especially at higher compression
    levels. It is suitable for compressing general-purpose data and text files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lzo`: This is a lightweight compression algorithm that provides good compression
    ratios and is relatively fast. It is suitable for compressing data that is already
    compressed, such as media files and archives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zstd`: This is a newer compression algorithm that provides a good balance
    between compression ratio and speed. It is suitable for compressing a wide range
    of data types, including text, media files, and archives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of compression algorithm depends on the specific use case and performance
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable compression on a filesystem, you can use the `btrfs property` command
    to set the compression for the filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When compression is enabled, only new data being written to the filesystem
    is compressed. However, you can also use the `defragment` command to compress
    the data that was on the filesystem before compression was enabled. To do this
    on a subvolume, you will also need to recursively run the command using the `-r`
    option along with the `-c` option to compress data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Compression efficiency will vary, depending on the data and algorithm used.
    Let’s first check how much space we have, using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s create a 2 GB file with random data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can use the `df` and `du` commands to compare the space consumed and
    the space used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The sample outputs for these commands are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see 2 GB is used (via the `du` command), and also 2 GB (via the
    `df` command) is actually consumed. So, for this random data file, we can see
    no benefit from the compression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s delete the `test1` file now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To speed things up, let’s defrag and rebalance the volume. This is needed to
    actually free up the space from the deleted file now, instead of waiting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, with a file with lots of repeating data, we will see more compression.
    To create a 2 GB file with repeating data, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we repeat the same `du` and `df` commands as before, but we will see very
    different results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, we see 4 GB is allocated, but only 62 MB is actually used! That’s a huge
    benefit from the compression.
  prefs: []
  type: TYPE_NORMAL
- en: Protecting data with mdadm – a software RAID solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern computing systems rely on RAID technology to ensure data integrity, availability,
    and performance. By distributing data across multiple disks in various configurations,
    RAID provides fault tolerance, allowing systems to continue functioning even if
    one or more disks fail. This redundancy is critical to prevent data loss and minimize
    downtime.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, RAID configurations such as striping and mirroring can significantly
    improve read and write performance by allowing data to be accessed in parallel
    from multiple disks. As data volume and importance continue to increase in today’s
    digital world, RAID plays a vital role in protecting, optimizing, and maintaining
    storage system reliability. The easiest way to do this when not using Btrfs with
    Oracle Linux 8 is to use a tool called **mdadm**.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'mdadm is a software utility for managing and configuring software RAID arrays
    in Linux systems. It stands for **multiple device administration** and is commonly
    used to create, manage, monitor, and maintain RAID arrays. These arrays leverage
    a kernel driver called **multiple device** (**MD**). mdadm allows users to create
    various RAID levels, including RAID, RAID 0, RAID 1, RAID 5, RAID 6, RAID 0+1,
    and RAID 10, using a combination of multiple physical disks:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Description** | **Notes** |'
  prefs: []
  type: TYPE_TB
- en: '| RAID | Spanning | No redundancy or performance advantages. |'
  prefs: []
  type: TYPE_TB
- en: '| RAID-0 | Striping | No redundancy, but better performance versus RAID. |'
  prefs: []
  type: TYPE_TB
- en: '| RAID-1 | Mirroring | Mirrored redundancy, but no performance advantage in
    write workloads. Read workloads may improve. |'
  prefs: []
  type: TYPE_TB
- en: '| RAID-5 | Striping with double parity | Redundancy and good performance. Can
    lose one disk without data loss. Recommended to use at least four disks. |'
  prefs: []
  type: TYPE_TB
- en: '| RAID-6 | Striping with tripple parity | Redundancy and good performance.
    Can lose two disks without data loss. Recommended to use at least five disks.
    |'
  prefs: []
  type: TYPE_TB
- en: '| RAID 0+1 | Mirroring of striped disks | Great performance, but recovering
    a lost disk takes a long time. This requires at least four disks. |'
  prefs: []
  type: TYPE_TB
- en: '| RAID 10 | Striping of mirrored disks | Most expensive option, but generally
    considered the best for redundancy, performance, and rebuild time. This requires
    at least four disks. |'
  prefs: []
  type: TYPE_TB
- en: Table 4.2 – MD RAID options
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It’s not uncommon to confuse a **JBOD** (**just a box of disks**) enclosure
    with a RAID enclosure. JBOD is a simple disk enclosure with no hardware RAID.
    A RAID disk enclosure has a hardware controller that offloads all the RAID logic
    for the disks within the enclosure. For this example, hardware RAID is not being
    used, only JBOD. All the RAID logic is being performed by Oracle Linux.
  prefs: []
  type: TYPE_NORMAL
- en: mdadm is an essential tool for administrators to create, modify, and monitor
    RAID arrays. It allows users to add or remove disks from an existing array, perform
    data recovery operations, and configure various parameters such as RAID level
    and spare disks. With its command-line interface, mdadm provides flexibility in
    managing and maintaining disk redundancy and performance. It is a reliable and
    robust solution that optimizes storage performance, ensures data redundancy, and
    maintains high availability of data. mdadm is normally installed when Oracle Linux
    is installed.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating an MD device is fairly simple. But before you create the device, you
    need to plan a few things in advance:'
  prefs: []
  type: TYPE_NORMAL
- en: What type of RAID will you be using? As mentioned previously, the MD kernel
    driver supports many types of RAID algorithms. You need to pick one before running
    the command. This maps to the `–``level` option.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many drives will you be using in the device? Most RAID types use even numbers
    of disks, but you still need to know how many disks will be used for data. This
    maps to the `–``raid-devices` option.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will you configure a hot spare device? Hot spares are a great option when systems
    need to automatically rebuild the data if a device fails. It’s not uncommon in
    remote locations to have many hot spares. When picking hot spares, balance the
    required space versus how long it will take for a replacement to be installed.
    This maps to the `–``spare-devices` option.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the number of the MD device? Traditionally, you start with 0 and work
    your way up, but track the numbers so you do not accidentally use the wrong device.
    This maps to `/dev/md#`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the paths to the drives that you are going to use? They are normally
    `/dev/sd#`, `/dev/nvme#`, or even `/dev/vd#`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following example, we will create a RAID-5 array with one hot spare.
    The device being used is `/dev/sd[bcdef]` and this will be the `/``dev/md0` device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'You can quickly check the status by cating the `/``proc/mdstat` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17 – /proc/mdstat](img/B18349_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – /proc/mdstat
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step, while optional, is highly recommended. This will save the current
    config to the mdadm configuration file. This helps the kernel assemble the array
    at boot. This is done with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s look at a few things about the MD device. First, if we use the `lsblk`
    command, we will see that the disks used by `md0` are now identified as having
    `md0` as children. This is because `md0` is a child of the actual disk:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18 – lsblk command output](img/B18349_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – lsblk command output
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check the device with the `-Q` option to the `mdamd` command. Simply
    pass the device as a parameter, and the command will give you a short summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – mdadm -Q](img/B18349_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 – mdadm -Q
  prefs: []
  type: TYPE_NORMAL
- en: 'Optionally, you can pass the `–-detail` option to get significantly more information
    about the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20 – mdadm --detail](img/B18349_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – mdadm --detail
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see not only the health of the array but also its creation date
    and lower-level metrics such as block size and layout.
  prefs: []
  type: TYPE_NORMAL
- en: Once the volume is created, you can now use it for filesystems or as storage
    for a logical volume manager.
  prefs: []
  type: TYPE_NORMAL
- en: Playing with logical volume management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to filesystems, one of the biggest issues is their inflexibility
    when it comes to storage. Creating a volume on a disk means the space is locked
    in for the volume, which also locks in the size of the filesystem. However, **Logical
    Volume Manager** (**LVM**) provides a solution to this problem. LVM is a widely
    used tool in the field of computer storage management that acts as a layer of
    abstraction between physical storage devices, such as hard drives or SSDs, and
    the OS. This enables the flexible and efficient management of storage resources.
    LVM is especially valuable for Linux systems, as it offers a flexible and scalable
    storage management solution.
  prefs: []
  type: TYPE_NORMAL
- en: With LVM, administrators can dynamically allocate and resize storage volumes
    without the need to repartition disks or disrupt the system. This flexibility
    is particularly useful in environments where storage requirements change frequently
    or where efficient resource allocation is needed. Additionally, LVM introduces
    the concept of volume groups, which act as logical containers for physical storage
    devices. By creating logical volumes within volume groups, administrators can
    easily allocate and manage storage space, simplifying the management of storage
    resources in Linux systems and making it easier to organize and utilize storage
    resources effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'With LVM, there are three core components of the storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Physical volumes** (**PVs**): These are the block-level disk devices that
    are used for storage. They can be physical disks, virtual disks (such as the MD
    devices created in the mdadm recipe), or other block-level devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volume groups** (**VGs**): These are groups of PVs that are combined into
    a single logical device. They can be a single device to start with, with new devices
    added later to add capacity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logical volumes** (**LVs**): These are logical disks built into the VG. They
    are used to create filesystems and can be dynamically resized as needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LVM, a VG, is a central component that acts as a logical container for one or
    more PVs. A VG is created by combining physical storage devices, such as hard
    drives or SSDs, into a single storage pool.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will show you how to initialize PVs, create a VG, and then
    add LVs for future use by a filesystem. We will also show some of the basic management
    commands.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The examples in this recipe will use the `/dev/md0` virtual disk created in
    the mdadm recipe, in addition to a few extra LUNs. The LVM RPMs are normally installed
    by default with a normal installation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step is to identify what disks we can use. This is done with the
    `lvmdiskscan` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.21 – lvmdiskscan](img/B18349_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – lvmdiskscan
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see three devices, `md0`, `sda1`, and `sda2`. We can also see
    that `sda2` is already initialized as a PV. We can use the `pvs` command to display
    the PVs on the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.22 – pvs](img/B18349_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 – pvs
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that the `/dev/sda2` device is used by the `ol` VG. Let’s
    go ahead and use `pvcreat` to initialize `/dev/md0`. This is done with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the device is initialized, we can create a VG. This is done with the
    `vgcreate` command. The command uses the first parameter as the name of the VG
    and then a list of devices. In this case, we will only use `/dev/md0` to create
    the `DATA` VG:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can see the list of VGs on a system using the `vgs` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.23 – vgs](img/B18349_04_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.23 – vgs
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see two VGs: the freshly created `DATA` VG and the existing `ol`
    VG where the OS is installed. We will then use the VG to create an LV. This is
    done with the `lvcreate` command. The `lvcreate` command takes a few parameters:
    `-L` to set the size of the volume, `-n` for the name, and then at the end, the
    VG where the volume will exist. Let’s create a 3 GB volume named `xfs1` in the
    `DATA` VG:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also tell `lvcreate` to use all available space. This is done using
    a lowercase `l` and the special `100%FREE` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `lvs` command will show all the LVs on a server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.24 – lvs](img/B18349_04_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.24 – lvs
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see the `xfs1` and `xfs2` volumes and their size. Additionally,
    we can see all the volumes in `ol`, where Linux was installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go ahead and free up some space by deleting the `xfs2` LV. This is done
    with the `lvremove` command. When using `lvremove`, use the VG/LV to identify
    the LV being deleted. The following command will remove the `xfs2` LV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'You will also need to acknowledge the removal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.25 – lvremove](img/B18349_04_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.25 – lvremove
  prefs: []
  type: TYPE_NORMAL
- en: 'The last trick is to see the details of a specific LV. This is done with the
    `lvdisplay` command. You can run the command by itself, and this will show the
    details for all LVs on the server. Optionally, you can use the command to report
    on a single LV, in this case the `xfs1` LV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.26 – lvdisplay](img/B18349_04_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.26 – lvdisplay
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can see the volume creation date, the UUID, and the size of the volume.
  prefs: []
  type: TYPE_NORMAL
- en: Once created, there are various ways to access LVs, but the most common method
    is by using their mapper addresses. In Linux, the `/dev/mapper` directory is utilized
    to access device mapper devices, which is a kernel-level framework that allows
    for the creation and management of virtual block devices. With a device mapper,
    advanced storage functionalities such as software RAID, encryption, and LVM can
    be achieved. By using LVM, a device mapper creates virtual devices that are represented
    as device mapper devices under the `/dev/mapper` directory, which act as abstractions
    and provide an interface for accessing and managing the underlying storage features.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, logical volumes set up using LVM are mapped to device mapper
    devices located in the `/dev/mapper` directory. Each logical volume has a corresponding
    device mapper device entry that can be used to interact with the logical volume
    as if it were a regular block device. The path for accessing it is `/dev/mapper`
    followed by the `VG-LV` name. Thus, `DATA/xfs1` would be `/dev/mapper/DATA-xfs1`.
    You can use the `dmsetup ls` command to show all of the mapped devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.27 – dmsetup ls](img/B18349_04_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.27 – dmsetup ls
  prefs: []
  type: TYPE_NORMAL
- en: XFS – creating, modifying, and more
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XFS is a highly advanced and established filesystem that boasts an array of
    benefits and features, making it an ideal option for a wide range of use cases.
    XFS is designed to handle vast storage capacities, making it suitable for environments
    with heavy data demands. It can support filesystems and files up to 8 exabytes
    in size, allowing for the management of vast amounts of data. This scalability
    makes XFS highly suitable for big data applications, enterprise storage systems,
    and large-scale storage deployments. XFS has exceptional performance capabilities.
    It employs advanced techniques such as allocation-group-based block mapping, delayed
    allocation, and asynchronous I/O, which optimize disk I/O operations and improve
    overall throughput. XFS is particularly effective at handling large files and
    performing tasks that involve intensive read and write operations.
  prefs: []
  type: TYPE_NORMAL
- en: XFS incorporates data protection features to safeguard against data corruption.
    It employs checksumming for both metadata and data, allowing the filesystem to
    detect and address potential data integrity issues. Additionally, XFS supports
    **copy-on-write** (**COW**) snapshots, enabling efficient point-in-time backups
    and data recovery options. XFS also uses a journaling mechanism that provides
    fast recovery in case of system crashes or power failures. The journaling feature
    records modifications to the filesystem metadata, ensuring the consistency and
    integrity of data. This results in faster boot times and improved reliability,
    as well as the reduction of time required for filesystem checks during system
    startup.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Oracle actively contributes to many Linux technologies. The COW XFS feature
    was one example of an Oracle contribution to the community, keeping Linux free
    and open source. For more info, refer to [https://blogs.oracle.com/linux/post/xfs-data-block-sharing-reflink](https://blogs.oracle.com/linux/post/xfs-data-block-sharing-reflink).
  prefs: []
  type: TYPE_NORMAL
- en: With XFS, administrators can perform a broad range of filesystem operations
    while the filesystem is mounted and in use. This includes online resizing, enabling
    seamless expansion or contraction of filesystems without requiring unmounting
    or disruption of services. These online administration capabilities make XFS highly
    suitable for environments that demand continuous availability and minimal downtime.
    XFS also offers extensive support for extended attributes, **access-control lists**
    (**ACLs**), and timestamps with nanosecond precision. These features provide flexibility
    in managing file metadata and enable the implementation of complex permission
    structures and custom metadata schemes.
  prefs: []
  type: TYPE_NORMAL
- en: XFS is natively supported by the Linux kernel, making it a well-integrated and
    widely adopted choice for Linux distributions. It is the default filesystem in
    Oracle Linux and continues to benefit from ongoing development and improvement
    by Oracle and the Linux community. Additionally, XFS comes with a comprehensive
    set of tools for filesystem management and administration, making it an all-around
    top-notch option.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The examples in this recipe will use the previously created `xfs1` LV. XFS filesystems
    can be created on any block device.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most common way to create an XFS filesystem is to use the `mkfs.xfs` command
    with a single block device. This will place both the data and the journal on the
    same device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, you can get more control using additional parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Option** | **Description** | **Samples** |'
  prefs: []
  type: TYPE_TB
- en: '| `-L` | This adds a label to the filesystem. | `-``L Test` |'
  prefs: []
  type: TYPE_TB
- en: '| `-b` | This sets the block size. | `-``b 8192` |'
  prefs: []
  type: TYPE_TB
- en: '| `-f` | This is the force option. | `-f` |'
  prefs: []
  type: TYPE_TB
- en: '| `-l` | This sets the location and size for the journal. This is commonly
    used to tune the performance by enabling the journal to be on a fast device while
    the data is on a slower device.`Size=20m /dev/$DEVICE` | `-l` `20m /dev/mapper/journal1`
    |'
  prefs: []
  type: TYPE_TB
- en: Table 4.3 – XFS options
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, add in the `/etc/fstab` entry info. Make sure to verify whether the mountpoint
    exists, and that you’re using the correct `/dev/mapper` path. The updated files
    are seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.28 – /etc/fstab](img/B18349_04_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.28 – /etc/fstab
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now mount the filesystem with the following `mount` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can clearly see the filesystem is mounted now, as seen in the output from
    the `df` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.29 – df -h](img/B18349_04_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.29 – df -h
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that the filesystem is mounted, we can do a few things to it. The first
    task is to grow the filesystem as 2 GB was a tad small for the application. This
    is done in a few steps. First, we grow the LV that holds the filesystem, and then
    we can grow the filesystem. Let’s grow this to 10 GB.
  prefs: []
  type: TYPE_NORMAL
- en: 'To grow the volume, we will use the `lvextend` command, passing to it the `+`
    option to add an additional 8 GB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.30 – lvextend](img/B18349_04_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.30 – lvextend
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the LV is grown, we need to extend the actual filesystem. This is done
    with the `xfs_growfs` command, passing the mapper path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on how much activity is on the filesystem, the growth can take a
    few minutes. The output from the `xfs_growfs` command will show the details of
    the filesystem when it completes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.31 – xfs_growfs](img/B18349_04_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.31 – xfs_growfs
  prefs: []
  type: TYPE_NORMAL
- en: You can also see these details by using the `xfs_info` command, passing the
    mountpoint of the filesystem or the mapper path.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you encounter problems mounting an `xfs` filesystem, you can use the `xfs_repair`
    command-line utility to repair and recover it. However, keep in mind that the
    command must be run on an unmounted filesystem. The main purpose of `xfs_repair`
    is to fix inconsistencies and repair filesystem corruption in XFS partitions caused
    by power failures, system crashes, or hardware issues. You can also use the `-n`
    option to check a filesystem without repairing it. For example, you can check
    the `xfs1` filesystem without repairing it by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
